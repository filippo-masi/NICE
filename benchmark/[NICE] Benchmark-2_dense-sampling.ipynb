{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e5ff77c",
   "metadata": {},
   "source": [
    "# Neural integration for constitutive equations \n",
    "\n",
    "### Benchmark #2: porous, elasto-plastic material\n",
    "\n",
    "Authors: Filippo Masi, Itai Einav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb870a3b",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a8d1509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 200x160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.optimize import root\n",
    "from torchdiffeq import odeint\n",
    "from nice_module import NICE, EarlyStopping, shuffle, slice_data, GetParams\n",
    "\n",
    "np.random.seed(6)\n",
    "torch.manual_seed(6)\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.style.use('classic')\n",
    "plt.rcParams.update({\"axes.grid\" : False, \"grid.color\": 'black', \"grid.alpha\":0.4})\n",
    "font = {'size'   : 10}\n",
    "matplotlib.rc('font', **font)\n",
    "plt.rcParams['axes.facecolor']='none'\n",
    "plt.rcParams['savefig.facecolor']='none'\n",
    "plt.rcParams['figure.facecolor']='none'\n",
    "plt.rcParams[\"figure.figsize\"] = (2.5,2)\n",
    "plt.tight_layout(pad=2.5, w_pad=3.5, h_pad=3.5)\n",
    "colorb = (0.2,0.4,0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ac1de",
   "metadata": {},
   "source": [
    "#### 1.1 Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a4cd933",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_frequency = 10 # frequency in epochs for printing loss at training\n",
    "step_size = 1 # to reproduce training process set = 20, else 1 (for fast training)\n",
    "device = torch.device('cuda:' + str(gpu) if torch.cuda.is_available() else 'cpu')\n",
    "corrupted_training_data = False # boolean for adding normally distributed noise to training data set\n",
    "delta = 0. # if corrupted_training_data = True, set noise amplitude (percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caccd145",
   "metadata": {},
   "source": [
    "### 2. Import and prepare data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca061767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    }
   ],
   "source": [
    "file = './dataset/benchmark2_data_training'\n",
    "with open(file, 'rb') as f_obj:\n",
    "    data = pickle.load(f_obj)\n",
    "\n",
    "strain_t,strain_tdt,r_t,r_tdt,z_t,z_tdt,stress_t,dt,n_reset = data\n",
    "batch_time = n_reset\n",
    "data_size = n_reset\n",
    "dim = 2\n",
    "prm_dt = 1/data_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd063371",
   "metadata": {},
   "source": [
    "#### 3.1 Reshape \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20cad77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dstrain = strain_tdt - strain_t\n",
    "dr = r_tdt - r_t\n",
    "dz = z_tdt - z_t\n",
    "dstrain/=prm_dt\n",
    "dr/=prm_dt\n",
    "dz/=prm_dt\n",
    "\n",
    "strain_t = np.reshape(strain_t,(batch_time,-1,dim),order='F')\n",
    "strain_tdt = np.reshape(strain_tdt,(batch_time,-1,dim),order='F')\n",
    "dstrain = np.reshape(dstrain,(batch_time,-1,dim),order='F')\n",
    "r_t = np.reshape(r_t,(batch_time,-1,1),order='F')\n",
    "dr = np.reshape(dr,(batch_time,-1,1),order='F')\n",
    "z_t = np.reshape(z_t,(batch_time,-1,1),order='F')\n",
    "dz = np.reshape(dz,(batch_time,-1,1),order='F')\n",
    "stress_t = np.reshape(stress_t,(batch_time,-1,dim),order='F')\n",
    "\n",
    "data_size = strain_t.shape[0]\n",
    "number_IC = strain_t.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8076cbd9",
   "metadata": {},
   "source": [
    "#### 3.2 Split data in training, validation, and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a418e711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  52\n",
      "Training samples :  34\n",
      "Validation samples :  9\n",
      "Test samples :  9\n",
      "Total :  52\n"
     ]
    }
   ],
   "source": [
    "train_percentage = .65\n",
    "\n",
    "train = int(round(number_IC * train_percentage))\n",
    "val = int(round(number_IC * 0.5 *(1.- train_percentage)))\n",
    "test = val\n",
    "\n",
    "print(\"Number of samples: \", number_IC)\n",
    "print(\"Training samples : \", train)\n",
    "print(\"Validation samples : \", val)\n",
    "print(\"Test samples : \", test)\n",
    "print(\"Total : \", test + val + train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79ab40b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.arange(0,number_IC,1)\n",
    "rnd = np.arange(len(n))\n",
    "np.random.shuffle(rnd[:-4])\n",
    "n = n[rnd]\n",
    "\n",
    "ntrain = n[:train-4]\n",
    "ntrain = np.hstack((ntrain,n[-4:]))\n",
    "cut = len(ntrain)\n",
    "nval = n[train:train+val]\n",
    "ntrainval = np.hstack((ntrain,nval))\n",
    "# ntest = np.arange(0,number_IC)\n",
    "ntest = n[train+val:]\n",
    "\n",
    "\n",
    "rnd = np.arange(len(ntrainval))\n",
    "ntrain = rnd[:train]\n",
    "nval = rnd[val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aae50299",
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_t_tv,strain_t_test = slice_data(strain_t,ntrainval,ntest)\n",
    "dstrain_tv,dstrain_test = slice_data(dstrain,ntrainval,ntest)\n",
    "stress_t_tv,stress_t_test = slice_data(stress_t,ntrainval,ntest)\n",
    "r_t_tv,r_t_test = slice_data(r_t,ntrainval,ntest)\n",
    "dr_tv,dr_test = slice_data(dr,ntrainval,ntest)\n",
    "z_t_tv,z_t_test = slice_data(z_t,ntrainval,ntest)\n",
    "dz_tv,dz_test = slice_data(dz,ntrainval,ntest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21918990",
   "metadata": {},
   "outputs": [],
   "source": [
    "prm_e = GetParams(strain_t_tv).to(device)\n",
    "prm_de = GetParams(dstrain_tv).to(device)\n",
    "prm_s = GetParams(stress_t_tv).to(device)\n",
    "prm_r = GetParams(r_t_tv).to(device)\n",
    "prm_dr = GetParams(dr_tv).to(device)\n",
    "prm_z = GetParams(z_t_tv).to(device)\n",
    "prm_dz = GetParams(dz_tv).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b59fcc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if corrupted_training_data:\n",
    "    # Stress noise vector\n",
    "    noise = delta/100 * np.random.normal(0,1,((data_size+1,number_IC,dim)))\n",
    "    # Corrupt stress with noise\n",
    "    noise_stress_t = stress_t_tv.copy()\n",
    "    noise_stress_t[:,:,0] = np.mean(stress_t_tv[:,:,0])*noise[:-1,:,0]\n",
    "    noise_stress_t[:,:,1] = np.mean(stress_t_tv[:,:,1])*noise[:-1,:,1]\n",
    "    noise_stress_t[0] *=0.0\n",
    "    stress_t_tv += noise_stress_t\n",
    "    # Dissipative state variable noise vector\n",
    "    noise = delta/100 * np.random.normal(0,1,((data_size+1,number_IC,1)))\n",
    "    # Corrupt state variable with noise\n",
    "    noise_svars_z_t = z_t_tv.copy()\n",
    "    noise_svars_z_tdt = z_tdt_tv.copy()\n",
    "    noise_svars_z_t = np.mean(z_t_tv)*noise[:-1,:]\n",
    "    noise_svars_z_tdt = np.mean(z_tdt_tv)*noise[1:,:]\n",
    "    noise_svars_z_t[0] *=0.0\n",
    "    z_t_tv[:,:] += noise_svars_z_t\n",
    "    z_tdt_tv[:,:] += noise_svars_z_tdt\n",
    "    # Evaluate corrupted rate\n",
    "    dz_tv = (z_tdt_tv - z_t_tv)/prm_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ecebc",
   "metadata": {},
   "source": [
    "### 4. Neural integration for constitutive equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4906d0f4",
   "metadata": {},
   "source": [
    "#### 4.1 Constructu neural net and set integration scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fb5fa08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NICE(\n",
       "  (NeuralNetEvolution): Sequential(\n",
       "    (0): Linear(in_features=6, out_features=42, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=42, out_features=42, bias=True)\n",
       "    (3): GELU(approximate='none')\n",
       "    (4): Linear(in_features=42, out_features=42, bias=True)\n",
       "    (5): GELU(approximate='none')\n",
       "    (6): Linear(in_features=42, out_features=3, bias=True)\n",
       "  )\n",
       "  (NeuralNetEnergy): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Softplus(beta=1, threshold=20)\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype=torch.float64\n",
    "hidden_num = 0\n",
    "NNf_params = [2*dim+2,dim+1,[6*(2*dim+3),6*(2*dim+3),6*(2*dim+3),],'gelu']\n",
    "NNu_params = [dim+2,1,[2**6,2**6],'softplus']\n",
    "norm_params = [prm_e,prm_de,prm_r,prm_z,prm_dz,prm_s,prm_dt]\n",
    "nsvars = 3\n",
    "NICE_network = NICE(NNf_params,NNu_params,hidden_num,len(ntrainval),norm_params,dim,nsvars,dtype).to(device)\n",
    "NICE_network.to(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6a2216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prm_dt = 1/data_size\n",
    "t = torch.arange(0,1.0,prm_dt)\n",
    "stress_tv = torch.from_numpy(np.float64(stress_t_tv)).to(device)\n",
    "dstrain_tv = torch.from_numpy(np.float64(dstrain_tv)).to(device)\n",
    "stress_test = torch.from_numpy(np.float64(stress_t_test)).to(device)\n",
    "dstrain_test = torch.from_numpy(np.float64(dstrain_test)).to(device)\n",
    "\n",
    "svars_tv = torch.cat((torch.from_numpy(np.float64(r_t_tv)),torch.from_numpy(np.float64(z_t_tv)),),-1).to(device)\n",
    "svars_test = torch.cat((torch.from_numpy(np.float64(r_t_test)),torch.from_numpy(np.float64(z_t_test)),),-1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed30eca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NICE_network.solver = \"midpoint\"\n",
    "NICE_network.scheme = \"forward\"\n",
    "NICE_network.step_size = prm_dt/step_size\n",
    "NICE_network.init_interp(dstrain_tv,t)\n",
    "NICE_network.inference = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb25f454",
   "metadata": {},
   "source": [
    "#### 4.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ab2b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer, loss function, and hyperparameters\n",
    "learningRate = 1e-2\n",
    "optimizer =  torch.optim.Adam(NICE_network.parameters(),lr=learningRate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma=0.999)\n",
    "w_reg = 1.e-5 # l2 penalty weight \n",
    "Nepochs = 1000000 # number of epochs\n",
    "MSE = torch.nn.MSELoss()\n",
    "\n",
    "# Early stopping criterion\n",
    "checkpoint_path = 'checkpoint_benchmark2_dense.pt'\n",
    "early_stopping = EarlyStopping(patience=10000, delta=1.e-9, verbose=False, path=checkpoint_path)\n",
    "\n",
    "# List for storing training and validation loss\n",
    "training_loss_hist = []\n",
    "validation_loss_value_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafac170",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | training loss: 1.9945e+00 | validation loss: 1.6620e+00\n",
      "Epoch: 20 | training loss: 9.4394e-01 | validation loss: 8.1216e-01\n",
      "Epoch: 30 | training loss: 5.5585e-01 | validation loss: 3.7120e-01\n",
      "Epoch: 40 | training loss: 3.7634e-01 | validation loss: 2.9455e-01\n",
      "Epoch: 50 | training loss: 2.2203e-01 | validation loss: 1.7295e-01\n",
      "Epoch: 60 | training loss: 1.0998e-01 | validation loss: 8.0650e-02\n",
      "Epoch: 70 | training loss: 5.6262e-02 | validation loss: 3.9742e-02\n",
      "Epoch: 80 | training loss: 2.9969e-02 | validation loss: 2.1494e-02\n",
      "Epoch: 90 | training loss: 1.6989e-02 | validation loss: 1.3086e-02\n",
      "Epoch: 100 | training loss: 1.1549e-02 | validation loss: 9.4721e-03\n",
      "Epoch: 110 | training loss: 9.2760e-03 | validation loss: 7.7135e-03\n",
      "Epoch: 120 | training loss: 7.8282e-03 | validation loss: 6.6760e-03\n",
      "Epoch: 130 | training loss: 6.8902e-03 | validation loss: 5.7392e-03\n",
      "Epoch: 140 | training loss: 6.2779e-03 | validation loss: 5.0719e-03\n",
      "Epoch: 150 | training loss: 5.8175e-03 | validation loss: 4.6210e-03\n",
      "Epoch: 160 | training loss: 5.4653e-03 | validation loss: 4.2532e-03\n",
      "Epoch: 170 | training loss: 5.1840e-03 | validation loss: 3.9546e-03\n",
      "Epoch: 180 | training loss: 4.9512e-03 | validation loss: 3.7160e-03\n",
      "Epoch: 190 | training loss: 4.7549e-03 | validation loss: 3.5090e-03\n",
      "Epoch: 200 | training loss: 4.5864e-03 | validation loss: 3.3326e-03\n",
      "Epoch: 210 | training loss: 4.4391e-03 | validation loss: 3.1792e-03\n",
      "Epoch: 220 | training loss: 4.3085e-03 | validation loss: 3.0413e-03\n",
      "Epoch: 230 | training loss: 4.1914e-03 | validation loss: 2.9169e-03\n",
      "Epoch: 240 | training loss: 4.0856e-03 | validation loss: 2.8039e-03\n",
      "Epoch: 250 | training loss: 3.9891e-03 | validation loss: 2.7007e-03\n",
      "Epoch: 260 | training loss: 3.9006e-03 | validation loss: 2.6054e-03\n",
      "Epoch: 270 | training loss: 3.8189e-03 | validation loss: 2.5168e-03\n",
      "Epoch: 280 | training loss: 3.7430e-03 | validation loss: 2.4341e-03\n",
      "Epoch: 290 | training loss: 3.6723e-03 | validation loss: 2.3566e-03\n",
      "Epoch: 300 | training loss: 3.6061e-03 | validation loss: 2.2836e-03\n",
      "Epoch: 310 | training loss: 3.5440e-03 | validation loss: 2.2148e-03\n",
      "Epoch: 320 | training loss: 3.4856e-03 | validation loss: 2.1497e-03\n",
      "Epoch: 330 | training loss: 3.4307e-03 | validation loss: 2.0882e-03\n",
      "Epoch: 340 | training loss: 3.3790e-03 | validation loss: 2.0299e-03\n",
      "Epoch: 350 | training loss: 3.3302e-03 | validation loss: 1.9749e-03\n",
      "Epoch: 360 | training loss: 3.2840e-03 | validation loss: 1.9227e-03\n",
      "Epoch: 370 | training loss: 3.2404e-03 | validation loss: 1.8735e-03\n",
      "Epoch: 380 | training loss: 3.2868e-03 | validation loss: 1.9525e-03\n",
      "Epoch: 390 | training loss: 3.1944e-03 | validation loss: 1.8455e-03\n",
      "Epoch: 400 | training loss: 3.1729e-03 | validation loss: 1.7569e-03\n",
      "Epoch: 410 | training loss: 3.1478e-03 | validation loss: 1.8054e-03\n",
      "Epoch: 420 | training loss: 3.0850e-03 | validation loss: 1.6685e-03\n",
      "Epoch: 430 | training loss: 3.0374e-03 | validation loss: 1.6566e-03\n",
      "Epoch: 440 | training loss: 3.0033e-03 | validation loss: 1.5971e-03\n",
      "Epoch: 450 | training loss: 2.9729e-03 | validation loss: 1.5761e-03\n",
      "Epoch: 460 | training loss: 2.9441e-03 | validation loss: 1.5356e-03\n",
      "Epoch: 470 | training loss: 2.9165e-03 | validation loss: 1.5097e-03\n",
      "Epoch: 480 | training loss: 2.8901e-03 | validation loss: 1.4800e-03\n",
      "Epoch: 490 | training loss: 2.8648e-03 | validation loss: 1.4505e-03\n",
      "Epoch: 500 | training loss: 2.8403e-03 | validation loss: 1.4238e-03\n",
      "Epoch: 510 | training loss: 2.8167e-03 | validation loss: 1.3980e-03\n",
      "Epoch: 520 | training loss: 2.7939e-03 | validation loss: 1.3728e-03\n",
      "Epoch: 530 | training loss: 2.7718e-03 | validation loss: 1.3484e-03\n",
      "Epoch: 540 | training loss: 2.7504e-03 | validation loss: 1.3249e-03\n",
      "Epoch: 550 | training loss: 2.7297e-03 | validation loss: 1.3022e-03\n",
      "Epoch: 560 | training loss: 2.7096e-03 | validation loss: 1.2802e-03\n",
      "Epoch: 570 | training loss: 2.6902e-03 | validation loss: 1.2589e-03\n",
      "Epoch: 580 | training loss: 2.6713e-03 | validation loss: 1.2383e-03\n",
      "Epoch: 590 | training loss: 2.6530e-03 | validation loss: 1.2183e-03\n",
      "Epoch: 600 | training loss: 2.6352e-03 | validation loss: 1.1990e-03\n",
      "Epoch: 610 | training loss: 2.6179e-03 | validation loss: 1.1803e-03\n",
      "Epoch: 620 | training loss: 2.6012e-03 | validation loss: 1.1621e-03\n",
      "Epoch: 630 | training loss: 2.5849e-03 | validation loss: 1.1445e-03\n",
      "Epoch: 640 | training loss: 2.5690e-03 | validation loss: 1.1274e-03\n",
      "Epoch: 650 | training loss: 2.5536e-03 | validation loss: 1.1108e-03\n",
      "Epoch: 660 | training loss: 2.5386e-03 | validation loss: 1.0947e-03\n",
      "Epoch: 670 | training loss: 2.5240e-03 | validation loss: 1.0791e-03\n",
      "Epoch: 680 | training loss: 2.5098e-03 | validation loss: 1.0638e-03\n",
      "Epoch: 690 | training loss: 2.4960e-03 | validation loss: 1.0489e-03\n",
      "Epoch: 700 | training loss: 2.4828e-03 | validation loss: 1.0323e-03\n",
      "Epoch: 710 | training loss: 2.6950e-03 | validation loss: 1.1561e-03\n",
      "Epoch: 720 | training loss: 2.6328e-03 | validation loss: 1.2418e-03\n",
      "Epoch: 730 | training loss: 2.5702e-03 | validation loss: 1.0589e-03\n",
      "Epoch: 740 | training loss: 2.4586e-03 | validation loss: 1.0370e-03\n",
      "Epoch: 750 | training loss: 2.4274e-03 | validation loss: 9.7391e-04\n",
      "Epoch: 760 | training loss: 2.4152e-03 | validation loss: 9.6200e-04\n",
      "Epoch: 770 | training loss: 2.4049e-03 | validation loss: 9.6018e-04\n",
      "Epoch: 780 | training loss: 2.3931e-03 | validation loss: 9.3737e-04\n",
      "Epoch: 790 | training loss: 2.3818e-03 | validation loss: 9.3024e-04\n",
      "Epoch: 800 | training loss: 2.3715e-03 | validation loss: 9.2049e-04\n",
      "Epoch: 810 | training loss: 2.3613e-03 | validation loss: 9.0771e-04\n",
      "Epoch: 820 | training loss: 2.3513e-03 | validation loss: 8.9752e-04\n",
      "Epoch: 830 | training loss: 2.3416e-03 | validation loss: 8.8803e-04\n",
      "Epoch: 840 | training loss: 2.3322e-03 | validation loss: 8.7814e-04\n",
      "Epoch: 850 | training loss: 2.3229e-03 | validation loss: 8.6833e-04\n",
      "Epoch: 860 | training loss: 2.3138e-03 | validation loss: 8.5890e-04\n",
      "Epoch: 870 | training loss: 2.3049e-03 | validation loss: 8.4979e-04\n",
      "Epoch: 880 | training loss: 2.2963e-03 | validation loss: 8.4091e-04\n",
      "Epoch: 890 | training loss: 2.2877e-03 | validation loss: 8.3222e-04\n",
      "Epoch: 900 | training loss: 2.2794e-03 | validation loss: 8.2372e-04\n",
      "Epoch: 910 | training loss: 2.2712e-03 | validation loss: 8.1542e-04\n",
      "Epoch: 920 | training loss: 2.2632e-03 | validation loss: 8.0731e-04\n",
      "Epoch: 930 | training loss: 2.2554e-03 | validation loss: 7.9939e-04\n",
      "Epoch: 940 | training loss: 2.2477e-03 | validation loss: 7.9164e-04\n",
      "Epoch: 950 | training loss: 2.2402e-03 | validation loss: 7.8408e-04\n",
      "Epoch: 960 | training loss: 2.2328e-03 | validation loss: 7.7668e-04\n",
      "Epoch: 970 | training loss: 2.2255e-03 | validation loss: 7.6944e-04\n",
      "Epoch: 980 | training loss: 2.2184e-03 | validation loss: 7.6236e-04\n",
      "Epoch: 990 | training loss: 2.2115e-03 | validation loss: 7.5544e-04\n",
      "Epoch: 1000 | training loss: 2.2046e-03 | validation loss: 7.4866e-04\n",
      "Epoch: 1010 | training loss: 2.1979e-03 | validation loss: 7.4203e-04\n",
      "Epoch: 1020 | training loss: 2.1913e-03 | validation loss: 7.3555e-04\n",
      "Epoch: 1030 | training loss: 2.1849e-03 | validation loss: 7.2920e-04\n",
      "Epoch: 1040 | training loss: 2.1785e-03 | validation loss: 7.2298e-04\n",
      "Epoch: 1050 | training loss: 2.1723e-03 | validation loss: 7.1689e-04\n",
      "Epoch: 1060 | training loss: 2.1662e-03 | validation loss: 7.1093e-04\n",
      "Epoch: 1070 | training loss: 2.1601e-03 | validation loss: 7.0509e-04\n",
      "Epoch: 1080 | training loss: 2.1542e-03 | validation loss: 6.9937e-04\n",
      "Epoch: 1090 | training loss: 2.1484e-03 | validation loss: 6.9377e-04\n",
      "Epoch: 1100 | training loss: 2.1427e-03 | validation loss: 6.8828e-04\n",
      "Epoch: 1110 | training loss: 2.1371e-03 | validation loss: 6.8290e-04\n",
      "Epoch: 1120 | training loss: 2.1316e-03 | validation loss: 6.7763e-04\n",
      "Epoch: 1130 | training loss: 2.1262e-03 | validation loss: 6.7246e-04\n",
      "Epoch: 1140 | training loss: 2.1209e-03 | validation loss: 6.6740e-04\n",
      "Epoch: 1150 | training loss: 2.1157e-03 | validation loss: 6.6243e-04\n",
      "Epoch: 1160 | training loss: 2.1105e-03 | validation loss: 6.5756e-04\n",
      "Epoch: 1170 | training loss: 2.1054e-03 | validation loss: 6.5279e-04\n",
      "Epoch: 1180 | training loss: 2.1005e-03 | validation loss: 6.4810e-04\n",
      "Epoch: 1190 | training loss: 2.0956e-03 | validation loss: 6.4351e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1200 | training loss: 2.0907e-03 | validation loss: 6.3900e-04\n",
      "Epoch: 1210 | training loss: 2.0860e-03 | validation loss: 6.3458e-04\n",
      "Epoch: 1220 | training loss: 2.0813e-03 | validation loss: 6.3024e-04\n",
      "Epoch: 1230 | training loss: 2.0767e-03 | validation loss: 6.2598e-04\n",
      "Epoch: 1240 | training loss: 2.0722e-03 | validation loss: 6.2180e-04\n",
      "Epoch: 1250 | training loss: 2.0677e-03 | validation loss: 6.1769e-04\n",
      "Epoch: 1260 | training loss: 2.0633e-03 | validation loss: 6.1367e-04\n",
      "Epoch: 1270 | training loss: 2.0590e-03 | validation loss: 6.0971e-04\n",
      "Epoch: 1280 | training loss: 2.0548e-03 | validation loss: 6.0583e-04\n",
      "Epoch: 1290 | training loss: 2.0506e-03 | validation loss: 6.0201e-04\n",
      "Epoch: 1300 | training loss: 2.0464e-03 | validation loss: 5.9826e-04\n",
      "Epoch: 1310 | training loss: 2.0424e-03 | validation loss: 5.9456e-04\n",
      "Epoch: 1320 | training loss: 2.0383e-03 | validation loss: 5.9079e-04\n",
      "Epoch: 1330 | training loss: 2.0345e-03 | validation loss: 5.8587e-04\n",
      "Epoch: 1340 | training loss: 2.0434e-03 | validation loss: 5.7904e-04\n",
      "Epoch: 1350 | training loss: 2.1380e-03 | validation loss: 6.3549e-04\n",
      "Epoch: 1360 | training loss: 2.0395e-03 | validation loss: 5.7526e-04\n",
      "Epoch: 1370 | training loss: 2.0203e-03 | validation loss: 5.7901e-04\n",
      "Epoch: 1380 | training loss: 2.0188e-03 | validation loss: 5.8226e-04\n",
      "Epoch: 1390 | training loss: 2.0144e-03 | validation loss: 5.7683e-04\n",
      "Epoch: 1400 | training loss: 2.0096e-03 | validation loss: 5.6951e-04\n",
      "Epoch: 1410 | training loss: 2.0057e-03 | validation loss: 5.6359e-04\n",
      "Epoch: 1420 | training loss: 2.0022e-03 | validation loss: 5.5918e-04\n",
      "Epoch: 1430 | training loss: 1.9988e-03 | validation loss: 5.5590e-04\n",
      "Epoch: 1440 | training loss: 1.9954e-03 | validation loss: 5.5315e-04\n",
      "Epoch: 1450 | training loss: 1.9921e-03 | validation loss: 5.5056e-04\n",
      "Epoch: 1460 | training loss: 1.9888e-03 | validation loss: 5.4790e-04\n",
      "Epoch: 1470 | training loss: 1.9856e-03 | validation loss: 5.4514e-04\n",
      "Epoch: 1480 | training loss: 1.9824e-03 | validation loss: 5.4234e-04\n",
      "Epoch: 1490 | training loss: 1.9792e-03 | validation loss: 5.3960e-04\n",
      "Epoch: 1500 | training loss: 1.9761e-03 | validation loss: 5.3692e-04\n",
      "Epoch: 1510 | training loss: 1.9730e-03 | validation loss: 5.3431e-04\n",
      "Epoch: 1520 | training loss: 1.9700e-03 | validation loss: 5.3175e-04\n",
      "Epoch: 1530 | training loss: 1.9670e-03 | validation loss: 5.2922e-04\n",
      "Epoch: 1540 | training loss: 1.9640e-03 | validation loss: 5.2673e-04\n",
      "Epoch: 1550 | training loss: 1.9611e-03 | validation loss: 5.2428e-04\n",
      "Epoch: 1560 | training loss: 1.9582e-03 | validation loss: 5.2187e-04\n",
      "Epoch: 1570 | training loss: 1.9553e-03 | validation loss: 5.1949e-04\n",
      "Epoch: 1580 | training loss: 1.9525e-03 | validation loss: 5.1715e-04\n",
      "Epoch: 1590 | training loss: 1.9497e-03 | validation loss: 5.1485e-04\n",
      "Epoch: 1600 | training loss: 1.9469e-03 | validation loss: 5.1257e-04\n",
      "Epoch: 1610 | training loss: 1.9442e-03 | validation loss: 5.1034e-04\n",
      "Epoch: 1620 | training loss: 1.9415e-03 | validation loss: 5.0813e-04\n",
      "Epoch: 1630 | training loss: 1.9388e-03 | validation loss: 5.0596e-04\n",
      "Epoch: 1640 | training loss: 1.9362e-03 | validation loss: 5.0382e-04\n",
      "Epoch: 1650 | training loss: 1.9336e-03 | validation loss: 5.0171e-04\n",
      "Epoch: 1660 | training loss: 1.9310e-03 | validation loss: 4.9962e-04\n",
      "Epoch: 1670 | training loss: 1.9284e-03 | validation loss: 4.9757e-04\n",
      "Epoch: 1680 | training loss: 1.9259e-03 | validation loss: 4.9555e-04\n",
      "Epoch: 1690 | training loss: 1.9234e-03 | validation loss: 4.9356e-04\n",
      "Epoch: 1700 | training loss: 1.9209e-03 | validation loss: 4.9159e-04\n",
      "Epoch: 1710 | training loss: 1.9185e-03 | validation loss: 4.8965e-04\n",
      "Epoch: 1720 | training loss: 1.9161e-03 | validation loss: 4.8774e-04\n",
      "Epoch: 1730 | training loss: 1.9137e-03 | validation loss: 4.8585e-04\n",
      "Epoch: 1740 | training loss: 1.9113e-03 | validation loss: 4.8399e-04\n",
      "Epoch: 1750 | training loss: 1.9090e-03 | validation loss: 4.8215e-04\n",
      "Epoch: 1760 | training loss: 1.9067e-03 | validation loss: 4.8034e-04\n",
      "Epoch: 1770 | training loss: 1.9044e-03 | validation loss: 4.7855e-04\n",
      "Epoch: 1780 | training loss: 1.9021e-03 | validation loss: 4.7679e-04\n",
      "Epoch: 1790 | training loss: 1.8998e-03 | validation loss: 4.7504e-04\n",
      "Epoch: 1800 | training loss: 1.8976e-03 | validation loss: 4.7333e-04\n",
      "Epoch: 1810 | training loss: 1.8954e-03 | validation loss: 4.7163e-04\n",
      "Epoch: 1820 | training loss: 1.8932e-03 | validation loss: 4.6996e-04\n",
      "Epoch: 1830 | training loss: 1.8911e-03 | validation loss: 4.6830e-04\n",
      "Epoch: 1840 | training loss: 1.8890e-03 | validation loss: 4.6667e-04\n",
      "Epoch: 1850 | training loss: 1.8868e-03 | validation loss: 4.6506e-04\n",
      "Epoch: 1860 | training loss: 1.8848e-03 | validation loss: 4.6347e-04\n",
      "Epoch: 1870 | training loss: 1.8827e-03 | validation loss: 4.6190e-04\n",
      "Epoch: 1880 | training loss: 1.8806e-03 | validation loss: 4.6035e-04\n",
      "Epoch: 1890 | training loss: 1.8786e-03 | validation loss: 4.5882e-04\n",
      "Epoch: 1900 | training loss: 1.8766e-03 | validation loss: 4.5731e-04\n",
      "Epoch: 1910 | training loss: 1.8746e-03 | validation loss: 4.5581e-04\n",
      "Epoch: 1920 | training loss: 1.8726e-03 | validation loss: 4.5434e-04\n",
      "Epoch: 1930 | training loss: 1.8707e-03 | validation loss: 4.5288e-04\n",
      "Epoch: 1940 | training loss: 1.8687e-03 | validation loss: 4.5144e-04\n",
      "Epoch: 1950 | training loss: 1.8668e-03 | validation loss: 4.5002e-04\n",
      "Epoch: 1960 | training loss: 1.8649e-03 | validation loss: 4.4861e-04\n",
      "Epoch: 1970 | training loss: 1.8630e-03 | validation loss: 4.4723e-04\n",
      "Epoch: 1980 | training loss: 1.8612e-03 | validation loss: 4.4585e-04\n",
      "Epoch: 1990 | training loss: 1.8593e-03 | validation loss: 4.4450e-04\n",
      "Epoch: 2000 | training loss: 1.8575e-03 | validation loss: 4.4316e-04\n",
      "Epoch: 2010 | training loss: 1.8557e-03 | validation loss: 4.4184e-04\n",
      "Epoch: 2020 | training loss: 1.8539e-03 | validation loss: 4.4053e-04\n",
      "Epoch: 2030 | training loss: 1.8521e-03 | validation loss: 4.3923e-04\n",
      "Epoch: 2040 | training loss: 1.8504e-03 | validation loss: 4.3796e-04\n",
      "Epoch: 2050 | training loss: 1.8486e-03 | validation loss: 4.3669e-04\n",
      "Epoch: 2060 | training loss: 1.8469e-03 | validation loss: 4.3544e-04\n",
      "Epoch: 2070 | training loss: 1.8452e-03 | validation loss: 4.3421e-04\n",
      "Epoch: 2080 | training loss: 1.8435e-03 | validation loss: 4.3299e-04\n",
      "Epoch: 2090 | training loss: 1.8418e-03 | validation loss: 4.3178e-04\n",
      "Epoch: 2100 | training loss: 1.8401e-03 | validation loss: 4.3059e-04\n",
      "Epoch: 2110 | training loss: 1.8385e-03 | validation loss: 4.2941e-04\n",
      "Epoch: 2120 | training loss: 1.8368e-03 | validation loss: 4.2824e-04\n",
      "Epoch: 2130 | training loss: 1.8352e-03 | validation loss: 4.2709e-04\n",
      "Epoch: 2140 | training loss: 1.8336e-03 | validation loss: 4.2595e-04\n",
      "Epoch: 2150 | training loss: 1.8320e-03 | validation loss: 4.2482e-04\n",
      "Epoch: 2160 | training loss: 1.8304e-03 | validation loss: 4.2370e-04\n",
      "Epoch: 2170 | training loss: 1.8288e-03 | validation loss: 4.2260e-04\n",
      "Epoch: 2180 | training loss: 1.8273e-03 | validation loss: 4.2151e-04\n",
      "Epoch: 2190 | training loss: 1.8257e-03 | validation loss: 4.2043e-04\n",
      "Epoch: 2200 | training loss: 1.8242e-03 | validation loss: 4.1936e-04\n",
      "Epoch: 2210 | training loss: 1.8227e-03 | validation loss: 4.1830e-04\n",
      "Epoch: 2220 | training loss: 1.8212e-03 | validation loss: 4.1726e-04\n",
      "Epoch: 2230 | training loss: 1.8197e-03 | validation loss: 4.1622e-04\n",
      "Epoch: 2240 | training loss: 1.8182e-03 | validation loss: 4.1520e-04\n",
      "Epoch: 2250 | training loss: 1.8167e-03 | validation loss: 4.1419e-04\n",
      "Epoch: 2260 | training loss: 1.8153e-03 | validation loss: 4.1318e-04\n",
      "Epoch: 2270 | training loss: 1.8139e-03 | validation loss: 4.1219e-04\n",
      "Epoch: 2280 | training loss: 1.8124e-03 | validation loss: 4.1121e-04\n",
      "Epoch: 2290 | training loss: 1.8110e-03 | validation loss: 4.1024e-04\n",
      "Epoch: 2300 | training loss: 1.8096e-03 | validation loss: 4.0928e-04\n",
      "Epoch: 2310 | training loss: 1.8082e-03 | validation loss: 4.0833e-04\n",
      "Epoch: 2320 | training loss: 1.8068e-03 | validation loss: 4.0739e-04\n",
      "Epoch: 2330 | training loss: 1.8055e-03 | validation loss: 4.0646e-04\n",
      "Epoch: 2340 | training loss: 1.8041e-03 | validation loss: 4.0553e-04\n",
      "Epoch: 2350 | training loss: 1.8027e-03 | validation loss: 4.0462e-04\n",
      "Epoch: 2360 | training loss: 1.8014e-03 | validation loss: 4.0372e-04\n",
      "Epoch: 2370 | training loss: 1.8001e-03 | validation loss: 4.0282e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2380 | training loss: 1.7988e-03 | validation loss: 4.0194e-04\n",
      "Epoch: 2390 | training loss: 1.7975e-03 | validation loss: 4.0106e-04\n",
      "Epoch: 2400 | training loss: 1.7962e-03 | validation loss: 4.0019e-04\n",
      "Epoch: 2410 | training loss: 1.7949e-03 | validation loss: 3.9933e-04\n",
      "Epoch: 2420 | training loss: 1.7936e-03 | validation loss: 3.9848e-04\n",
      "Epoch: 2430 | training loss: 1.7923e-03 | validation loss: 3.9764e-04\n",
      "Epoch: 2440 | training loss: 1.7911e-03 | validation loss: 3.9681e-04\n",
      "Epoch: 2450 | training loss: 1.7898e-03 | validation loss: 3.9598e-04\n",
      "Epoch: 2460 | training loss: 1.7886e-03 | validation loss: 3.9517e-04\n",
      "Epoch: 2470 | training loss: 1.7874e-03 | validation loss: 3.9436e-04\n",
      "Epoch: 2480 | training loss: 1.7862e-03 | validation loss: 3.9355e-04\n",
      "Epoch: 2490 | training loss: 1.7850e-03 | validation loss: 3.9276e-04\n",
      "Epoch: 2500 | training loss: 1.7838e-03 | validation loss: 3.9197e-04\n",
      "Epoch: 2510 | training loss: 1.7826e-03 | validation loss: 3.9120e-04\n",
      "Epoch: 2520 | training loss: 1.7814e-03 | validation loss: 3.9042e-04\n",
      "Epoch: 2530 | training loss: 1.7802e-03 | validation loss: 3.8966e-04\n",
      "Epoch: 2540 | training loss: 1.7791e-03 | validation loss: 3.8890e-04\n",
      "Epoch: 2550 | training loss: 1.7779e-03 | validation loss: 3.8815e-04\n",
      "Epoch: 2560 | training loss: 1.7768e-03 | validation loss: 3.8741e-04\n",
      "Epoch: 2570 | training loss: 1.7756e-03 | validation loss: 3.8668e-04\n",
      "Epoch: 2580 | training loss: 1.7745e-03 | validation loss: 3.8595e-04\n",
      "Epoch: 2590 | training loss: 1.7734e-03 | validation loss: 3.8523e-04\n",
      "Epoch: 2600 | training loss: 1.7723e-03 | validation loss: 3.8451e-04\n",
      "Epoch: 2610 | training loss: 1.7712e-03 | validation loss: 3.8380e-04\n",
      "Epoch: 2620 | training loss: 1.7701e-03 | validation loss: 3.8310e-04\n",
      "Epoch: 2630 | training loss: 1.7690e-03 | validation loss: 3.8241e-04\n",
      "Epoch: 2640 | training loss: 1.7680e-03 | validation loss: 3.8172e-04\n",
      "Epoch: 2650 | training loss: 1.7669e-03 | validation loss: 3.8104e-04\n",
      "Epoch: 2660 | training loss: 1.7658e-03 | validation loss: 3.8036e-04\n",
      "Epoch: 2670 | training loss: 1.7648e-03 | validation loss: 3.7969e-04\n",
      "Epoch: 2680 | training loss: 1.7637e-03 | validation loss: 3.7903e-04\n",
      "Epoch: 2690 | training loss: 1.7627e-03 | validation loss: 3.7837e-04\n",
      "Epoch: 2700 | training loss: 1.7617e-03 | validation loss: 3.7772e-04\n",
      "Epoch: 2710 | training loss: 1.7607e-03 | validation loss: 3.7707e-04\n",
      "Epoch: 2720 | training loss: 1.7597e-03 | validation loss: 3.7643e-04\n",
      "Epoch: 2730 | training loss: 1.7586e-03 | validation loss: 3.7580e-04\n",
      "Epoch: 2740 | training loss: 1.7577e-03 | validation loss: 3.7517e-04\n",
      "Epoch: 2750 | training loss: 1.7567e-03 | validation loss: 3.7454e-04\n",
      "Epoch: 2760 | training loss: 1.7557e-03 | validation loss: 3.7393e-04\n",
      "Epoch: 2770 | training loss: 1.7547e-03 | validation loss: 3.7331e-04\n",
      "Epoch: 2780 | training loss: 1.7537e-03 | validation loss: 3.7271e-04\n",
      "Epoch: 2790 | training loss: 1.7528e-03 | validation loss: 3.7211e-04\n",
      "Epoch: 2800 | training loss: 1.7518e-03 | validation loss: 3.7151e-04\n",
      "Epoch: 2810 | training loss: 1.7509e-03 | validation loss: 3.7092e-04\n",
      "Epoch: 2820 | training loss: 1.7499e-03 | validation loss: 3.7033e-04\n",
      "Epoch: 2830 | training loss: 1.7490e-03 | validation loss: 3.6975e-04\n",
      "Epoch: 2840 | training loss: 1.7481e-03 | validation loss: 3.6918e-04\n",
      "Epoch: 2850 | training loss: 1.7472e-03 | validation loss: 3.6861e-04\n",
      "Epoch: 2860 | training loss: 1.7462e-03 | validation loss: 3.6804e-04\n",
      "Epoch: 2870 | training loss: 1.7453e-03 | validation loss: 3.6748e-04\n",
      "Epoch: 2880 | training loss: 1.7444e-03 | validation loss: 3.6692e-04\n",
      "Epoch: 2890 | training loss: 1.7436e-03 | validation loss: 3.6637e-04\n",
      "Epoch: 2900 | training loss: 1.7427e-03 | validation loss: 3.6583e-04\n",
      "Epoch: 2910 | training loss: 1.7418e-03 | validation loss: 3.6528e-04\n",
      "Epoch: 2920 | training loss: 1.7409e-03 | validation loss: 3.6475e-04\n",
      "Epoch: 2930 | training loss: 1.7400e-03 | validation loss: 3.6421e-04\n",
      "Epoch: 2940 | training loss: 1.7392e-03 | validation loss: 3.6368e-04\n",
      "Epoch: 2950 | training loss: 1.7383e-03 | validation loss: 3.6316e-04\n",
      "Epoch: 2960 | training loss: 1.7375e-03 | validation loss: 3.6264e-04\n",
      "Epoch: 2970 | training loss: 1.7366e-03 | validation loss: 3.6213e-04\n",
      "Epoch: 2980 | training loss: 1.7358e-03 | validation loss: 3.6162e-04\n",
      "Epoch: 2990 | training loss: 1.7350e-03 | validation loss: 3.6111e-04\n",
      "Epoch: 3000 | training loss: 1.7341e-03 | validation loss: 3.6061e-04\n",
      "Epoch: 3010 | training loss: 1.7333e-03 | validation loss: 3.6011e-04\n",
      "Epoch: 3020 | training loss: 1.7325e-03 | validation loss: 3.5961e-04\n",
      "Epoch: 3030 | training loss: 1.7317e-03 | validation loss: 3.5912e-04\n",
      "Epoch: 3040 | training loss: 1.7309e-03 | validation loss: 3.5864e-04\n",
      "Epoch: 3050 | training loss: 1.7301e-03 | validation loss: 3.5816e-04\n",
      "Epoch: 3060 | training loss: 1.7293e-03 | validation loss: 3.5768e-04\n",
      "Epoch: 3070 | training loss: 1.7285e-03 | validation loss: 3.5720e-04\n",
      "Epoch: 3080 | training loss: 1.7277e-03 | validation loss: 3.5673e-04\n",
      "Epoch: 3090 | training loss: 1.7270e-03 | validation loss: 3.5627e-04\n",
      "Epoch: 3100 | training loss: 1.7262e-03 | validation loss: 3.5580e-04\n",
      "Epoch: 3110 | training loss: 1.7254e-03 | validation loss: 3.5534e-04\n",
      "Epoch: 3120 | training loss: 1.7247e-03 | validation loss: 3.5489e-04\n",
      "Epoch: 3130 | training loss: 1.7239e-03 | validation loss: 3.5444e-04\n",
      "Epoch: 3140 | training loss: 1.7232e-03 | validation loss: 3.5399e-04\n",
      "Epoch: 3150 | training loss: 1.7224e-03 | validation loss: 3.5355e-04\n",
      "Epoch: 3160 | training loss: 1.7217e-03 | validation loss: 3.5310e-04\n",
      "Epoch: 3170 | training loss: 1.7209e-03 | validation loss: 3.5267e-04\n",
      "Epoch: 3180 | training loss: 1.7202e-03 | validation loss: 3.5223e-04\n",
      "Epoch: 3190 | training loss: 1.7195e-03 | validation loss: 3.5180e-04\n",
      "Epoch: 3200 | training loss: 1.7188e-03 | validation loss: 3.5138e-04\n",
      "Epoch: 3210 | training loss: 1.7180e-03 | validation loss: 3.5095e-04\n",
      "Epoch: 3220 | training loss: 1.7173e-03 | validation loss: 3.5053e-04\n",
      "Epoch: 3230 | training loss: 1.7166e-03 | validation loss: 3.5011e-04\n",
      "Epoch: 3240 | training loss: 1.7159e-03 | validation loss: 3.4970e-04\n",
      "Epoch: 3250 | training loss: 1.7152e-03 | validation loss: 3.4929e-04\n",
      "Epoch: 3260 | training loss: 1.7145e-03 | validation loss: 3.4888e-04\n",
      "Epoch: 3270 | training loss: 1.7139e-03 | validation loss: 3.4848e-04\n",
      "Epoch: 3280 | training loss: 1.7132e-03 | validation loss: 3.4808e-04\n",
      "Epoch: 3290 | training loss: 1.7125e-03 | validation loss: 3.4768e-04\n",
      "Epoch: 3300 | training loss: 1.7118e-03 | validation loss: 3.4728e-04\n",
      "Epoch: 3310 | training loss: 1.7112e-03 | validation loss: 3.4689e-04\n",
      "Epoch: 3320 | training loss: 1.7105e-03 | validation loss: 3.4650e-04\n",
      "Epoch: 3330 | training loss: 1.7098e-03 | validation loss: 3.4612e-04\n",
      "Epoch: 3340 | training loss: 1.7092e-03 | validation loss: 3.4573e-04\n",
      "Epoch: 3350 | training loss: 1.7085e-03 | validation loss: 3.4535e-04\n",
      "Epoch: 3360 | training loss: 1.7079e-03 | validation loss: 3.4498e-04\n",
      "Epoch: 3370 | training loss: 1.7072e-03 | validation loss: 3.4460e-04\n",
      "Epoch: 3380 | training loss: 1.7066e-03 | validation loss: 3.4423e-04\n",
      "Epoch: 3390 | training loss: 1.7060e-03 | validation loss: 3.4386e-04\n",
      "Epoch: 3400 | training loss: 1.7053e-03 | validation loss: 3.4350e-04\n",
      "Epoch: 3410 | training loss: 1.7047e-03 | validation loss: 3.4313e-04\n",
      "Epoch: 3420 | training loss: 1.7041e-03 | validation loss: 3.4277e-04\n",
      "Epoch: 3430 | training loss: 1.7035e-03 | validation loss: 3.4241e-04\n",
      "Epoch: 3440 | training loss: 1.7029e-03 | validation loss: 3.4206e-04\n",
      "Epoch: 3450 | training loss: 1.7023e-03 | validation loss: 3.4171e-04\n",
      "Epoch: 3460 | training loss: 1.7017e-03 | validation loss: 3.4136e-04\n",
      "Epoch: 3470 | training loss: 1.7010e-03 | validation loss: 3.4101e-04\n",
      "Epoch: 3480 | training loss: 1.7005e-03 | validation loss: 3.4067e-04\n",
      "Epoch: 3490 | training loss: 1.6999e-03 | validation loss: 3.4032e-04\n",
      "Epoch: 3500 | training loss: 1.6993e-03 | validation loss: 3.3998e-04\n",
      "Epoch: 3510 | training loss: 1.6987e-03 | validation loss: 3.3965e-04\n",
      "Epoch: 3520 | training loss: 1.6981e-03 | validation loss: 3.3931e-04\n",
      "Epoch: 3530 | training loss: 1.6975e-03 | validation loss: 3.3898e-04\n",
      "Epoch: 3540 | training loss: 1.6969e-03 | validation loss: 3.3865e-04\n",
      "Epoch: 3550 | training loss: 1.6964e-03 | validation loss: 3.3832e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3560 | training loss: 1.6958e-03 | validation loss: 3.3800e-04\n",
      "Epoch: 3570 | training loss: 1.6952e-03 | validation loss: 3.3768e-04\n",
      "Epoch: 3580 | training loss: 1.6947e-03 | validation loss: 3.3736e-04\n",
      "Epoch: 3590 | training loss: 1.6941e-03 | validation loss: 3.3704e-04\n",
      "Epoch: 3600 | training loss: 1.6936e-03 | validation loss: 3.3672e-04\n",
      "Epoch: 3610 | training loss: 1.6930e-03 | validation loss: 3.3641e-04\n",
      "Epoch: 3620 | training loss: 1.6925e-03 | validation loss: 3.3610e-04\n",
      "Epoch: 3630 | training loss: 1.6919e-03 | validation loss: 3.3579e-04\n",
      "Epoch: 3640 | training loss: 1.6914e-03 | validation loss: 3.3548e-04\n",
      "Epoch: 3650 | training loss: 1.6909e-03 | validation loss: 3.3518e-04\n",
      "Epoch: 3660 | training loss: 1.6903e-03 | validation loss: 3.3488e-04\n",
      "Epoch: 3670 | training loss: 1.6898e-03 | validation loss: 3.3458e-04\n",
      "Epoch: 3680 | training loss: 1.6893e-03 | validation loss: 3.3428e-04\n",
      "Epoch: 3690 | training loss: 1.6888e-03 | validation loss: 3.3398e-04\n",
      "Epoch: 3700 | training loss: 1.6882e-03 | validation loss: 3.3369e-04\n",
      "Epoch: 3710 | training loss: 1.6877e-03 | validation loss: 3.3340e-04\n",
      "Epoch: 3720 | training loss: 1.6872e-03 | validation loss: 3.3311e-04\n",
      "Epoch: 3730 | training loss: 1.6867e-03 | validation loss: 3.3282e-04\n",
      "Epoch: 3740 | training loss: 1.6862e-03 | validation loss: 3.3253e-04\n",
      "Epoch: 3750 | training loss: 1.6857e-03 | validation loss: 3.3225e-04\n",
      "Epoch: 3760 | training loss: 1.6852e-03 | validation loss: 3.3197e-04\n",
      "Epoch: 3770 | training loss: 1.6847e-03 | validation loss: 3.3169e-04\n",
      "Epoch: 3780 | training loss: 1.6842e-03 | validation loss: 3.3141e-04\n",
      "Epoch: 3790 | training loss: 1.6837e-03 | validation loss: 3.3114e-04\n",
      "Epoch: 3800 | training loss: 1.6832e-03 | validation loss: 3.3086e-04\n",
      "Epoch: 3810 | training loss: 1.6827e-03 | validation loss: 3.3059e-04\n",
      "Epoch: 3820 | training loss: 1.6823e-03 | validation loss: 3.3032e-04\n",
      "Epoch: 3830 | training loss: 1.6818e-03 | validation loss: 3.3005e-04\n",
      "Epoch: 3840 | training loss: 1.6813e-03 | validation loss: 3.2979e-04\n",
      "Epoch: 3850 | training loss: 1.6808e-03 | validation loss: 3.2952e-04\n",
      "Epoch: 3860 | training loss: 1.6804e-03 | validation loss: 3.2926e-04\n",
      "Epoch: 3870 | training loss: 1.6799e-03 | validation loss: 3.2900e-04\n",
      "Epoch: 3880 | training loss: 1.6794e-03 | validation loss: 3.2874e-04\n",
      "Epoch: 3890 | training loss: 1.6790e-03 | validation loss: 3.2848e-04\n",
      "Epoch: 3900 | training loss: 1.6785e-03 | validation loss: 3.2823e-04\n",
      "Epoch: 3910 | training loss: 1.6781e-03 | validation loss: 3.2797e-04\n",
      "Epoch: 3920 | training loss: 1.6776e-03 | validation loss: 3.2772e-04\n",
      "Epoch: 3930 | training loss: 1.6772e-03 | validation loss: 3.2747e-04\n",
      "Epoch: 3940 | training loss: 1.6767e-03 | validation loss: 3.2722e-04\n",
      "Epoch: 3950 | training loss: 1.6763e-03 | validation loss: 3.2698e-04\n",
      "Epoch: 3960 | training loss: 1.6758e-03 | validation loss: 3.2673e-04\n",
      "Epoch: 3970 | training loss: 1.6754e-03 | validation loss: 3.2649e-04\n",
      "Epoch: 3980 | training loss: 1.6749e-03 | validation loss: 3.2625e-04\n",
      "Epoch: 3990 | training loss: 1.6745e-03 | validation loss: 3.2601e-04\n",
      "Epoch: 4000 | training loss: 1.6741e-03 | validation loss: 3.2577e-04\n",
      "Epoch: 4010 | training loss: 1.6737e-03 | validation loss: 3.2553e-04\n",
      "Epoch: 4020 | training loss: 1.6732e-03 | validation loss: 3.2530e-04\n",
      "Epoch: 4030 | training loss: 1.6728e-03 | validation loss: 3.2506e-04\n",
      "Epoch: 4040 | training loss: 1.6724e-03 | validation loss: 3.2483e-04\n",
      "Epoch: 4050 | training loss: 1.6720e-03 | validation loss: 3.2460e-04\n",
      "Epoch: 4060 | training loss: 1.6716e-03 | validation loss: 3.2437e-04\n",
      "Epoch: 4070 | training loss: 1.6711e-03 | validation loss: 3.2414e-04\n",
      "Epoch: 4080 | training loss: 1.6707e-03 | validation loss: 3.2392e-04\n",
      "Epoch: 4090 | training loss: 1.6703e-03 | validation loss: 3.2369e-04\n",
      "Epoch: 4100 | training loss: 1.6699e-03 | validation loss: 3.2347e-04\n",
      "Epoch: 4110 | training loss: 1.6695e-03 | validation loss: 3.2325e-04\n",
      "Epoch: 4120 | training loss: 1.6691e-03 | validation loss: 3.2303e-04\n",
      "Epoch: 4130 | training loss: 1.6687e-03 | validation loss: 3.2281e-04\n",
      "Epoch: 4140 | training loss: 1.6683e-03 | validation loss: 3.2259e-04\n",
      "Epoch: 4150 | training loss: 1.6679e-03 | validation loss: 3.2238e-04\n",
      "Epoch: 4160 | training loss: 1.6675e-03 | validation loss: 3.2216e-04\n",
      "Epoch: 4170 | training loss: 1.6672e-03 | validation loss: 3.2195e-04\n",
      "Epoch: 4180 | training loss: 1.6668e-03 | validation loss: 3.2174e-04\n",
      "Epoch: 4190 | training loss: 1.6664e-03 | validation loss: 3.2153e-04\n",
      "Epoch: 4200 | training loss: 1.6660e-03 | validation loss: 3.2132e-04\n",
      "Epoch: 4210 | training loss: 1.6656e-03 | validation loss: 3.2111e-04\n",
      "Epoch: 4220 | training loss: 1.6652e-03 | validation loss: 3.2091e-04\n",
      "Epoch: 4230 | training loss: 1.6649e-03 | validation loss: 3.2070e-04\n",
      "Epoch: 4240 | training loss: 1.6645e-03 | validation loss: 3.2050e-04\n",
      "Epoch: 4250 | training loss: 1.6641e-03 | validation loss: 3.2030e-04\n",
      "Epoch: 4260 | training loss: 1.6638e-03 | validation loss: 3.2010e-04\n",
      "Epoch: 4270 | training loss: 1.6634e-03 | validation loss: 3.1990e-04\n",
      "Epoch: 4280 | training loss: 1.6630e-03 | validation loss: 3.1970e-04\n",
      "Epoch: 4290 | training loss: 1.6627e-03 | validation loss: 3.1950e-04\n",
      "Epoch: 4300 | training loss: 1.6623e-03 | validation loss: 3.1931e-04\n",
      "Epoch: 4310 | training loss: 1.6619e-03 | validation loss: 3.1911e-04\n",
      "Epoch: 4320 | training loss: 1.6616e-03 | validation loss: 3.1892e-04\n",
      "Epoch: 4330 | training loss: 1.6612e-03 | validation loss: 3.1873e-04\n",
      "Epoch: 4340 | training loss: 1.6609e-03 | validation loss: 3.1854e-04\n",
      "Epoch: 4350 | training loss: 1.6605e-03 | validation loss: 3.1835e-04\n",
      "Epoch: 4360 | training loss: 1.6602e-03 | validation loss: 3.1816e-04\n",
      "Epoch: 4370 | training loss: 1.6598e-03 | validation loss: 3.1797e-04\n",
      "Epoch: 4380 | training loss: 1.6595e-03 | validation loss: 3.1779e-04\n",
      "Epoch: 4390 | training loss: 1.6592e-03 | validation loss: 3.1760e-04\n",
      "Epoch: 4400 | training loss: 1.6588e-03 | validation loss: 3.1742e-04\n",
      "Epoch: 4410 | training loss: 1.6585e-03 | validation loss: 3.1724e-04\n",
      "Epoch: 4420 | training loss: 1.6582e-03 | validation loss: 3.1706e-04\n",
      "Epoch: 4430 | training loss: 1.6578e-03 | validation loss: 3.1688e-04\n",
      "Epoch: 4440 | training loss: 1.6575e-03 | validation loss: 3.1670e-04\n",
      "Epoch: 4450 | training loss: 1.6572e-03 | validation loss: 3.1652e-04\n",
      "Epoch: 4460 | training loss: 1.6568e-03 | validation loss: 3.1634e-04\n",
      "Epoch: 4470 | training loss: 1.6565e-03 | validation loss: 3.1617e-04\n",
      "Epoch: 4480 | training loss: 1.6562e-03 | validation loss: 3.1599e-04\n",
      "Epoch: 4490 | training loss: 1.6559e-03 | validation loss: 3.1582e-04\n",
      "Epoch: 4500 | training loss: 1.6555e-03 | validation loss: 3.1565e-04\n",
      "Epoch: 4510 | training loss: 1.6552e-03 | validation loss: 3.1548e-04\n",
      "Epoch: 4520 | training loss: 1.6549e-03 | validation loss: 3.1531e-04\n",
      "Epoch: 4530 | training loss: 1.6546e-03 | validation loss: 3.1514e-04\n",
      "Epoch: 4540 | training loss: 1.6543e-03 | validation loss: 3.1497e-04\n",
      "Epoch: 4550 | training loss: 1.6540e-03 | validation loss: 3.1481e-04\n",
      "Epoch: 4560 | training loss: 1.6537e-03 | validation loss: 3.1464e-04\n",
      "Epoch: 4570 | training loss: 1.6534e-03 | validation loss: 3.1448e-04\n",
      "Epoch: 4580 | training loss: 1.6531e-03 | validation loss: 3.1431e-04\n",
      "Epoch: 4590 | training loss: 1.6528e-03 | validation loss: 3.1415e-04\n",
      "Epoch: 4600 | training loss: 1.6525e-03 | validation loss: 3.1399e-04\n",
      "Epoch: 4610 | training loss: 1.6522e-03 | validation loss: 3.1383e-04\n",
      "Epoch: 4620 | training loss: 1.6519e-03 | validation loss: 3.1367e-04\n",
      "Epoch: 4630 | training loss: 1.6515e-03 | validation loss: 3.1351e-04\n",
      "Epoch: 4640 | training loss: 1.6512e-03 | validation loss: 3.1334e-04\n",
      "Epoch: 4650 | training loss: 1.6509e-03 | validation loss: 3.1318e-04\n",
      "Epoch: 4660 | training loss: 1.6506e-03 | validation loss: 3.1302e-04\n",
      "Epoch: 4670 | training loss: 1.6503e-03 | validation loss: 3.1285e-04\n",
      "Epoch: 4680 | training loss: 1.6500e-03 | validation loss: 3.1269e-04\n",
      "Epoch: 4690 | training loss: 1.6497e-03 | validation loss: 3.1252e-04\n",
      "Epoch: 4700 | training loss: 1.6494e-03 | validation loss: 3.1236e-04\n",
      "Epoch: 4710 | training loss: 1.6491e-03 | validation loss: 3.1219e-04\n",
      "Epoch: 4720 | training loss: 1.6488e-03 | validation loss: 3.1202e-04\n",
      "Epoch: 4730 | training loss: 1.6485e-03 | validation loss: 3.1185e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4740 | training loss: 1.6481e-03 | validation loss: 3.1169e-04\n",
      "Epoch: 4750 | training loss: 1.6478e-03 | validation loss: 3.1152e-04\n",
      "Epoch: 4760 | training loss: 1.6475e-03 | validation loss: 3.1135e-04\n",
      "Epoch: 4770 | training loss: 1.6472e-03 | validation loss: 3.1118e-04\n",
      "Epoch: 4780 | training loss: 1.6469e-03 | validation loss: 3.1101e-04\n",
      "Epoch: 4790 | training loss: 1.6465e-03 | validation loss: 3.1083e-04\n",
      "Epoch: 4800 | training loss: 1.6462e-03 | validation loss: 3.1066e-04\n",
      "Epoch: 4810 | training loss: 1.6459e-03 | validation loss: 3.1049e-04\n",
      "Epoch: 4820 | training loss: 1.6456e-03 | validation loss: 3.1032e-04\n",
      "Epoch: 4830 | training loss: 1.6452e-03 | validation loss: 3.1014e-04\n",
      "Epoch: 4840 | training loss: 1.6449e-03 | validation loss: 3.0997e-04\n",
      "Epoch: 4850 | training loss: 1.6446e-03 | validation loss: 3.0979e-04\n",
      "Epoch: 4860 | training loss: 1.6442e-03 | validation loss: 3.0962e-04\n",
      "Epoch: 4870 | training loss: 1.6439e-03 | validation loss: 3.0944e-04\n",
      "Epoch: 4880 | training loss: 1.6436e-03 | validation loss: 3.0926e-04\n",
      "Epoch: 4890 | training loss: 1.6432e-03 | validation loss: 3.0908e-04\n",
      "Epoch: 4900 | training loss: 1.6429e-03 | validation loss: 3.0891e-04\n",
      "Epoch: 4910 | training loss: 1.6426e-03 | validation loss: 3.0873e-04\n",
      "Epoch: 4920 | training loss: 1.6422e-03 | validation loss: 3.0855e-04\n",
      "Epoch: 4930 | training loss: 1.6419e-03 | validation loss: 3.0837e-04\n",
      "Epoch: 4940 | training loss: 1.6415e-03 | validation loss: 3.0819e-04\n",
      "Epoch: 4950 | training loss: 1.6412e-03 | validation loss: 3.0800e-04\n",
      "Epoch: 4960 | training loss: 1.6408e-03 | validation loss: 3.0782e-04\n",
      "Epoch: 4970 | training loss: 1.6405e-03 | validation loss: 3.0764e-04\n",
      "Epoch: 4980 | training loss: 1.6401e-03 | validation loss: 3.0745e-04\n",
      "Epoch: 4990 | training loss: 1.6398e-03 | validation loss: 3.0727e-04\n",
      "Epoch: 5000 | training loss: 1.6394e-03 | validation loss: 3.0709e-04\n",
      "Epoch: 5010 | training loss: 1.6391e-03 | validation loss: 3.0690e-04\n",
      "Epoch: 5020 | training loss: 1.6387e-03 | validation loss: 3.0671e-04\n",
      "Epoch: 5030 | training loss: 1.6384e-03 | validation loss: 3.0653e-04\n",
      "Epoch: 5040 | training loss: 1.6380e-03 | validation loss: 3.0634e-04\n",
      "Epoch: 5050 | training loss: 1.6377e-03 | validation loss: 3.0615e-04\n",
      "Epoch: 5060 | training loss: 1.6373e-03 | validation loss: 3.0596e-04\n",
      "Epoch: 5070 | training loss: 1.6369e-03 | validation loss: 3.0577e-04\n",
      "Epoch: 5080 | training loss: 1.6366e-03 | validation loss: 3.0558e-04\n",
      "Epoch: 5090 | training loss: 1.6362e-03 | validation loss: 3.0539e-04\n",
      "Epoch: 5100 | training loss: 1.6358e-03 | validation loss: 3.0520e-04\n",
      "Epoch: 5110 | training loss: 1.6355e-03 | validation loss: 3.0501e-04\n",
      "Epoch: 5120 | training loss: 1.6351e-03 | validation loss: 3.0482e-04\n",
      "Epoch: 5130 | training loss: 1.6347e-03 | validation loss: 3.0462e-04\n",
      "Epoch: 5140 | training loss: 1.6344e-03 | validation loss: 3.0443e-04\n",
      "Epoch: 5150 | training loss: 1.6340e-03 | validation loss: 3.0423e-04\n",
      "Epoch: 5160 | training loss: 1.6336e-03 | validation loss: 3.0404e-04\n",
      "Epoch: 5170 | training loss: 1.6332e-03 | validation loss: 3.0384e-04\n",
      "Epoch: 5180 | training loss: 1.6329e-03 | validation loss: 3.0365e-04\n",
      "Epoch: 5190 | training loss: 1.6325e-03 | validation loss: 3.0345e-04\n",
      "Epoch: 5200 | training loss: 1.6321e-03 | validation loss: 3.0325e-04\n",
      "Epoch: 5210 | training loss: 1.6317e-03 | validation loss: 3.0305e-04\n",
      "Epoch: 5220 | training loss: 1.6313e-03 | validation loss: 3.0285e-04\n",
      "Epoch: 5230 | training loss: 1.6309e-03 | validation loss: 3.0265e-04\n",
      "Epoch: 5240 | training loss: 1.6305e-03 | validation loss: 3.0245e-04\n",
      "Epoch: 5250 | training loss: 1.6302e-03 | validation loss: 3.0225e-04\n",
      "Epoch: 5260 | training loss: 1.6298e-03 | validation loss: 3.0205e-04\n",
      "Epoch: 5270 | training loss: 1.6294e-03 | validation loss: 3.0185e-04\n",
      "Epoch: 5280 | training loss: 1.6290e-03 | validation loss: 3.0164e-04\n",
      "Epoch: 5290 | training loss: 1.6286e-03 | validation loss: 3.0144e-04\n",
      "Epoch: 5300 | training loss: 1.6282e-03 | validation loss: 3.0123e-04\n",
      "Epoch: 5310 | training loss: 1.6278e-03 | validation loss: 3.0103e-04\n",
      "Epoch: 5320 | training loss: 1.6274e-03 | validation loss: 3.0082e-04\n",
      "Epoch: 5330 | training loss: 1.6270e-03 | validation loss: 3.0061e-04\n",
      "Epoch: 5340 | training loss: 1.6266e-03 | validation loss: 3.0041e-04\n",
      "Epoch: 5350 | training loss: 1.6262e-03 | validation loss: 3.0020e-04\n",
      "Epoch: 5360 | training loss: 1.6258e-03 | validation loss: 2.9999e-04\n",
      "Epoch: 5370 | training loss: 1.6254e-03 | validation loss: 2.9978e-04\n",
      "Epoch: 5380 | training loss: 1.6250e-03 | validation loss: 2.9957e-04\n",
      "Epoch: 5390 | training loss: 1.6246e-03 | validation loss: 2.9936e-04\n",
      "Epoch: 5400 | training loss: 1.6241e-03 | validation loss: 2.9915e-04\n",
      "Epoch: 5410 | training loss: 1.6237e-03 | validation loss: 2.9894e-04\n",
      "Epoch: 5420 | training loss: 1.6233e-03 | validation loss: 2.9872e-04\n",
      "Epoch: 5430 | training loss: 1.6229e-03 | validation loss: 2.9851e-04\n",
      "Epoch: 5440 | training loss: 1.6225e-03 | validation loss: 2.9830e-04\n",
      "Epoch: 5450 | training loss: 1.6221e-03 | validation loss: 2.9808e-04\n",
      "Epoch: 5460 | training loss: 1.6216e-03 | validation loss: 2.9787e-04\n",
      "Epoch: 5470 | training loss: 1.6212e-03 | validation loss: 2.9765e-04\n",
      "Epoch: 5480 | training loss: 1.6208e-03 | validation loss: 2.9743e-04\n",
      "Epoch: 5490 | training loss: 1.6204e-03 | validation loss: 2.9721e-04\n",
      "Epoch: 5500 | training loss: 1.6199e-03 | validation loss: 2.9700e-04\n",
      "Epoch: 5510 | training loss: 1.6195e-03 | validation loss: 2.9678e-04\n",
      "Epoch: 5520 | training loss: 1.6191e-03 | validation loss: 2.9656e-04\n",
      "Epoch: 5530 | training loss: 1.6186e-03 | validation loss: 2.9634e-04\n",
      "Epoch: 5540 | training loss: 1.6182e-03 | validation loss: 2.9612e-04\n",
      "Epoch: 5550 | training loss: 1.6178e-03 | validation loss: 2.9589e-04\n",
      "Epoch: 5560 | training loss: 1.6173e-03 | validation loss: 2.9567e-04\n",
      "Epoch: 5570 | training loss: 1.6169e-03 | validation loss: 2.9545e-04\n",
      "Epoch: 5580 | training loss: 1.6164e-03 | validation loss: 2.9522e-04\n",
      "Epoch: 5590 | training loss: 1.6160e-03 | validation loss: 2.9500e-04\n",
      "Epoch: 5600 | training loss: 1.6156e-03 | validation loss: 2.9478e-04\n",
      "Epoch: 5610 | training loss: 1.6151e-03 | validation loss: 2.9455e-04\n",
      "Epoch: 5620 | training loss: 1.6147e-03 | validation loss: 2.9432e-04\n",
      "Epoch: 5630 | training loss: 1.6142e-03 | validation loss: 2.9410e-04\n",
      "Epoch: 5640 | training loss: 1.6138e-03 | validation loss: 2.9387e-04\n",
      "Epoch: 5650 | training loss: 1.6133e-03 | validation loss: 2.9364e-04\n",
      "Epoch: 5660 | training loss: 1.6129e-03 | validation loss: 2.9341e-04\n",
      "Epoch: 5670 | training loss: 1.6124e-03 | validation loss: 2.9318e-04\n",
      "Epoch: 5680 | training loss: 1.6119e-03 | validation loss: 2.9295e-04\n",
      "Epoch: 5690 | training loss: 1.6115e-03 | validation loss: 2.9272e-04\n",
      "Epoch: 5700 | training loss: 1.6110e-03 | validation loss: 2.9249e-04\n",
      "Epoch: 5710 | training loss: 1.6106e-03 | validation loss: 2.9225e-04\n",
      "Epoch: 5720 | training loss: 1.6101e-03 | validation loss: 2.9202e-04\n",
      "Epoch: 5730 | training loss: 1.6096e-03 | validation loss: 2.9179e-04\n",
      "Epoch: 5740 | training loss: 1.6092e-03 | validation loss: 2.9155e-04\n",
      "Epoch: 5750 | training loss: 1.6087e-03 | validation loss: 2.9131e-04\n",
      "Epoch: 5760 | training loss: 1.6082e-03 | validation loss: 2.9108e-04\n",
      "Epoch: 5770 | training loss: 1.6077e-03 | validation loss: 2.9084e-04\n",
      "Epoch: 5780 | training loss: 1.6073e-03 | validation loss: 2.9060e-04\n",
      "Epoch: 5790 | training loss: 1.6068e-03 | validation loss: 2.9037e-04\n",
      "Epoch: 5800 | training loss: 1.6063e-03 | validation loss: 2.9013e-04\n",
      "Epoch: 5810 | training loss: 1.6058e-03 | validation loss: 2.8989e-04\n",
      "Epoch: 5820 | training loss: 1.6053e-03 | validation loss: 2.8965e-04\n",
      "Epoch: 5830 | training loss: 1.6049e-03 | validation loss: 2.8941e-04\n",
      "Epoch: 5840 | training loss: 1.6044e-03 | validation loss: 2.8916e-04\n",
      "Epoch: 5850 | training loss: 1.6039e-03 | validation loss: 2.8892e-04\n",
      "Epoch: 5860 | training loss: 1.6034e-03 | validation loss: 2.8868e-04\n",
      "Epoch: 5870 | training loss: 1.6029e-03 | validation loss: 2.8843e-04\n",
      "Epoch: 5880 | training loss: 1.6024e-03 | validation loss: 2.8819e-04\n",
      "Epoch: 5890 | training loss: 1.6019e-03 | validation loss: 2.8794e-04\n",
      "Epoch: 5900 | training loss: 1.6014e-03 | validation loss: 2.8770e-04\n",
      "Epoch: 5910 | training loss: 1.6009e-03 | validation loss: 2.8745e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5920 | training loss: 1.6004e-03 | validation loss: 2.8720e-04\n",
      "Epoch: 5930 | training loss: 1.5999e-03 | validation loss: 2.8696e-04\n",
      "Epoch: 5940 | training loss: 1.5994e-03 | validation loss: 2.8671e-04\n",
      "Epoch: 5950 | training loss: 1.5989e-03 | validation loss: 2.8646e-04\n",
      "Epoch: 5960 | training loss: 1.5984e-03 | validation loss: 2.8621e-04\n",
      "Epoch: 5970 | training loss: 1.5979e-03 | validation loss: 2.8596e-04\n",
      "Epoch: 5980 | training loss: 1.5974e-03 | validation loss: 2.8571e-04\n",
      "Epoch: 5990 | training loss: 1.5969e-03 | validation loss: 2.8545e-04\n",
      "Epoch: 6000 | training loss: 1.5964e-03 | validation loss: 2.8520e-04\n",
      "Epoch: 6010 | training loss: 1.5958e-03 | validation loss: 2.8495e-04\n",
      "Epoch: 6020 | training loss: 1.5953e-03 | validation loss: 2.8469e-04\n",
      "Epoch: 6030 | training loss: 1.5948e-03 | validation loss: 2.8444e-04\n",
      "Epoch: 6040 | training loss: 1.5943e-03 | validation loss: 2.8418e-04\n",
      "Epoch: 6050 | training loss: 1.5938e-03 | validation loss: 2.8393e-04\n",
      "Epoch: 6060 | training loss: 1.5932e-03 | validation loss: 2.8367e-04\n",
      "Epoch: 6070 | training loss: 1.5927e-03 | validation loss: 2.8341e-04\n",
      "Epoch: 6080 | training loss: 1.5922e-03 | validation loss: 2.8315e-04\n",
      "Epoch: 6090 | training loss: 1.5916e-03 | validation loss: 2.8289e-04\n",
      "Epoch: 6100 | training loss: 1.5911e-03 | validation loss: 2.8264e-04\n",
      "Epoch: 6110 | training loss: 1.5906e-03 | validation loss: 2.8237e-04\n",
      "Epoch: 6120 | training loss: 1.5900e-03 | validation loss: 2.8211e-04\n",
      "Epoch: 6130 | training loss: 1.5895e-03 | validation loss: 2.8185e-04\n",
      "Epoch: 6140 | training loss: 1.5890e-03 | validation loss: 2.8159e-04\n",
      "Epoch: 6150 | training loss: 1.5884e-03 | validation loss: 2.8133e-04\n",
      "Epoch: 6160 | training loss: 1.5879e-03 | validation loss: 2.8106e-04\n",
      "Epoch: 6170 | training loss: 1.5873e-03 | validation loss: 2.8080e-04\n",
      "Epoch: 6180 | training loss: 1.5868e-03 | validation loss: 2.8053e-04\n",
      "Epoch: 6190 | training loss: 1.5862e-03 | validation loss: 2.8027e-04\n",
      "Epoch: 6200 | training loss: 1.5857e-03 | validation loss: 2.8000e-04\n",
      "Epoch: 6210 | training loss: 1.5851e-03 | validation loss: 2.7973e-04\n",
      "Epoch: 6220 | training loss: 1.5846e-03 | validation loss: 2.7946e-04\n",
      "Epoch: 6230 | training loss: 1.5840e-03 | validation loss: 2.7920e-04\n",
      "Epoch: 6240 | training loss: 1.5835e-03 | validation loss: 2.7893e-04\n",
      "Epoch: 6250 | training loss: 1.5829e-03 | validation loss: 2.7866e-04\n",
      "Epoch: 6260 | training loss: 1.5823e-03 | validation loss: 2.7839e-04\n",
      "Epoch: 6270 | training loss: 1.5818e-03 | validation loss: 2.7811e-04\n",
      "Epoch: 6280 | training loss: 1.5812e-03 | validation loss: 2.7784e-04\n",
      "Epoch: 6290 | training loss: 1.5806e-03 | validation loss: 2.7757e-04\n",
      "Epoch: 6300 | training loss: 1.5801e-03 | validation loss: 2.7730e-04\n",
      "Epoch: 6310 | training loss: 1.5795e-03 | validation loss: 2.7702e-04\n",
      "Epoch: 6320 | training loss: 1.5789e-03 | validation loss: 2.7675e-04\n",
      "Epoch: 6330 | training loss: 1.5783e-03 | validation loss: 2.7647e-04\n",
      "Epoch: 6340 | training loss: 1.5778e-03 | validation loss: 2.7620e-04\n",
      "Epoch: 6350 | training loss: 1.5772e-03 | validation loss: 2.7592e-04\n",
      "Epoch: 6360 | training loss: 1.5766e-03 | validation loss: 2.7564e-04\n",
      "Epoch: 6370 | training loss: 1.5760e-03 | validation loss: 2.7537e-04\n",
      "Epoch: 6380 | training loss: 1.5754e-03 | validation loss: 2.7509e-04\n",
      "Epoch: 6390 | training loss: 1.5749e-03 | validation loss: 2.7481e-04\n",
      "Epoch: 6400 | training loss: 1.5743e-03 | validation loss: 2.7453e-04\n",
      "Epoch: 6410 | training loss: 1.5737e-03 | validation loss: 2.7425e-04\n",
      "Epoch: 6420 | training loss: 1.5731e-03 | validation loss: 2.7397e-04\n",
      "Epoch: 6430 | training loss: 1.5725e-03 | validation loss: 2.7368e-04\n",
      "Epoch: 6440 | training loss: 1.5719e-03 | validation loss: 2.7340e-04\n",
      "Epoch: 6450 | training loss: 1.5713e-03 | validation loss: 2.7312e-04\n",
      "Epoch: 6460 | training loss: 1.5707e-03 | validation loss: 2.7283e-04\n",
      "Epoch: 6470 | training loss: 1.5701e-03 | validation loss: 2.7255e-04\n",
      "Epoch: 6480 | training loss: 1.5695e-03 | validation loss: 2.7227e-04\n",
      "Epoch: 6490 | training loss: 1.5689e-03 | validation loss: 2.7198e-04\n",
      "Epoch: 6500 | training loss: 1.5683e-03 | validation loss: 2.7169e-04\n",
      "Epoch: 6510 | training loss: 1.5676e-03 | validation loss: 2.7141e-04\n",
      "Epoch: 6520 | training loss: 1.5670e-03 | validation loss: 2.7112e-04\n",
      "Epoch: 6530 | training loss: 1.5664e-03 | validation loss: 2.7084e-04\n",
      "Epoch: 6540 | training loss: 1.5658e-03 | validation loss: 2.7115e-04\n",
      "Epoch: 6550 | training loss: 1.5652e-03 | validation loss: 2.7076e-04\n",
      "Epoch: 6560 | training loss: 1.5649e-03 | validation loss: 2.6822e-04\n",
      "Epoch: 6570 | training loss: 1.5641e-03 | validation loss: 2.6875e-04\n",
      "Epoch: 6580 | training loss: 1.5634e-03 | validation loss: 2.7029e-04\n",
      "Epoch: 6590 | training loss: 1.5628e-03 | validation loss: 2.6929e-04\n",
      "Epoch: 6600 | training loss: 1.5622e-03 | validation loss: 2.6847e-04\n",
      "Epoch: 6610 | training loss: 1.5615e-03 | validation loss: 2.6841e-04\n",
      "Epoch: 6620 | training loss: 1.5609e-03 | validation loss: 2.6830e-04\n",
      "Epoch: 6630 | training loss: 1.5603e-03 | validation loss: 2.6798e-04\n",
      "Epoch: 6640 | training loss: 1.5597e-03 | validation loss: 2.6763e-04\n",
      "Epoch: 6650 | training loss: 1.5591e-03 | validation loss: 2.6731e-04\n",
      "Epoch: 6660 | training loss: 1.5584e-03 | validation loss: 2.6702e-04\n",
      "Epoch: 6670 | training loss: 1.5578e-03 | validation loss: 2.6674e-04\n",
      "Epoch: 6680 | training loss: 1.5572e-03 | validation loss: 2.6645e-04\n",
      "Epoch: 6690 | training loss: 1.5566e-03 | validation loss: 2.6616e-04\n",
      "Epoch: 6700 | training loss: 1.5559e-03 | validation loss: 2.6586e-04\n",
      "Epoch: 6710 | training loss: 1.5553e-03 | validation loss: 2.6557e-04\n",
      "Epoch: 6720 | training loss: 1.5547e-03 | validation loss: 2.6528e-04\n",
      "Epoch: 6730 | training loss: 1.5540e-03 | validation loss: 2.6498e-04\n",
      "Epoch: 6740 | training loss: 1.5534e-03 | validation loss: 2.6468e-04\n",
      "Epoch: 6750 | training loss: 1.5528e-03 | validation loss: 2.6439e-04\n",
      "Epoch: 6760 | training loss: 1.5521e-03 | validation loss: 2.6409e-04\n",
      "Epoch: 6770 | training loss: 1.5515e-03 | validation loss: 2.6379e-04\n",
      "Epoch: 6780 | training loss: 1.5508e-03 | validation loss: 2.6350e-04\n",
      "Epoch: 6790 | training loss: 1.5502e-03 | validation loss: 2.6320e-04\n",
      "Epoch: 6800 | training loss: 1.5495e-03 | validation loss: 2.6290e-04\n",
      "Epoch: 6810 | training loss: 1.5489e-03 | validation loss: 2.6260e-04\n",
      "Epoch: 6820 | training loss: 1.5482e-03 | validation loss: 2.6230e-04\n",
      "Epoch: 6830 | training loss: 1.5476e-03 | validation loss: 2.6200e-04\n",
      "Epoch: 6840 | training loss: 1.5469e-03 | validation loss: 2.6170e-04\n",
      "Epoch: 6850 | training loss: 1.5462e-03 | validation loss: 2.6139e-04\n",
      "Epoch: 6860 | training loss: 1.5456e-03 | validation loss: 2.6109e-04\n",
      "Epoch: 6870 | training loss: 1.5449e-03 | validation loss: 2.6079e-04\n",
      "Epoch: 6880 | training loss: 1.5442e-03 | validation loss: 2.6048e-04\n",
      "Epoch: 6890 | training loss: 1.5436e-03 | validation loss: 2.6018e-04\n",
      "Epoch: 6900 | training loss: 1.5429e-03 | validation loss: 2.5988e-04\n",
      "Epoch: 6910 | training loss: 1.5422e-03 | validation loss: 2.5957e-04\n",
      "Epoch: 6920 | training loss: 1.5416e-03 | validation loss: 2.5927e-04\n",
      "Epoch: 6930 | training loss: 1.5409e-03 | validation loss: 2.5904e-04\n",
      "Epoch: 6940 | training loss: 1.5407e-03 | validation loss: 2.6150e-04\n",
      "Epoch: 6950 | training loss: 1.5412e-03 | validation loss: 2.5525e-04\n",
      "Epoch: 6960 | training loss: 1.5390e-03 | validation loss: 2.5674e-04\n",
      "Epoch: 6970 | training loss: 1.5383e-03 | validation loss: 2.5885e-04\n",
      "Epoch: 6980 | training loss: 1.5376e-03 | validation loss: 2.5832e-04\n",
      "Epoch: 6990 | training loss: 1.5369e-03 | validation loss: 2.5706e-04\n",
      "Epoch: 7000 | training loss: 1.5362e-03 | validation loss: 2.5651e-04\n",
      "Epoch: 7010 | training loss: 1.5355e-03 | validation loss: 2.5636e-04\n",
      "Epoch: 7020 | training loss: 1.5349e-03 | validation loss: 2.5620e-04\n",
      "Epoch: 7030 | training loss: 1.5342e-03 | validation loss: 2.5594e-04\n",
      "Epoch: 7040 | training loss: 1.5335e-03 | validation loss: 2.5564e-04\n",
      "Epoch: 7050 | training loss: 1.5328e-03 | validation loss: 2.5532e-04\n",
      "Epoch: 7060 | training loss: 1.5322e-03 | validation loss: 2.5501e-04\n",
      "Epoch: 7070 | training loss: 1.5315e-03 | validation loss: 2.5469e-04\n",
      "Epoch: 7080 | training loss: 1.5308e-03 | validation loss: 2.5438e-04\n",
      "Epoch: 7090 | training loss: 1.5301e-03 | validation loss: 2.5407e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7100 | training loss: 1.5294e-03 | validation loss: 2.5376e-04\n",
      "Epoch: 7110 | training loss: 1.5287e-03 | validation loss: 2.5346e-04\n",
      "Epoch: 7120 | training loss: 1.5280e-03 | validation loss: 2.5315e-04\n",
      "Epoch: 7130 | training loss: 1.5274e-03 | validation loss: 2.5284e-04\n",
      "Epoch: 7140 | training loss: 1.5267e-03 | validation loss: 2.5253e-04\n",
      "Epoch: 7150 | training loss: 1.5260e-03 | validation loss: 2.5222e-04\n",
      "Epoch: 7160 | training loss: 1.5253e-03 | validation loss: 2.5191e-04\n",
      "Epoch: 7170 | training loss: 1.5246e-03 | validation loss: 2.5160e-04\n",
      "Epoch: 7180 | training loss: 1.5239e-03 | validation loss: 2.5129e-04\n",
      "Epoch: 7190 | training loss: 1.5231e-03 | validation loss: 2.5098e-04\n",
      "Epoch: 7200 | training loss: 1.5224e-03 | validation loss: 2.5067e-04\n",
      "Epoch: 7210 | training loss: 1.5217e-03 | validation loss: 2.5036e-04\n",
      "Epoch: 7220 | training loss: 1.5210e-03 | validation loss: 2.5004e-04\n",
      "Epoch: 7230 | training loss: 1.5203e-03 | validation loss: 2.4973e-04\n",
      "Epoch: 7240 | training loss: 1.5196e-03 | validation loss: 2.4942e-04\n",
      "Epoch: 7250 | training loss: 1.5189e-03 | validation loss: 2.4910e-04\n",
      "Epoch: 7260 | training loss: 1.5181e-03 | validation loss: 2.4879e-04\n",
      "Epoch: 7270 | training loss: 1.5174e-03 | validation loss: 2.4847e-04\n",
      "Epoch: 7280 | training loss: 1.5167e-03 | validation loss: 2.4816e-04\n",
      "Epoch: 7290 | training loss: 1.5160e-03 | validation loss: 2.4789e-04\n",
      "Epoch: 7300 | training loss: 1.5154e-03 | validation loss: 2.4892e-04\n",
      "Epoch: 7310 | training loss: 1.5145e-03 | validation loss: 2.4677e-04\n",
      "Epoch: 7320 | training loss: 1.5140e-03 | validation loss: 2.4552e-04\n",
      "Epoch: 7330 | training loss: 1.5134e-03 | validation loss: 2.4492e-04\n",
      "Epoch: 7340 | training loss: 1.5124e-03 | validation loss: 2.4569e-04\n",
      "Epoch: 7350 | training loss: 1.5117e-03 | validation loss: 2.4627e-04\n",
      "Epoch: 7360 | training loss: 1.5110e-03 | validation loss: 2.4606e-04\n",
      "Epoch: 7370 | training loss: 1.5102e-03 | validation loss: 2.4554e-04\n",
      "Epoch: 7380 | training loss: 1.5095e-03 | validation loss: 2.4508e-04\n",
      "Epoch: 7390 | training loss: 1.5088e-03 | validation loss: 2.4470e-04\n",
      "Epoch: 7400 | training loss: 1.5081e-03 | validation loss: 2.4438e-04\n",
      "Epoch: 7410 | training loss: 1.5073e-03 | validation loss: 2.4407e-04\n",
      "Epoch: 7420 | training loss: 1.5066e-03 | validation loss: 2.4376e-04\n",
      "Epoch: 7430 | training loss: 1.5059e-03 | validation loss: 2.4345e-04\n",
      "Epoch: 7440 | training loss: 1.5052e-03 | validation loss: 2.4314e-04\n",
      "Epoch: 7450 | training loss: 1.5044e-03 | validation loss: 2.4282e-04\n",
      "Epoch: 7460 | training loss: 1.5037e-03 | validation loss: 2.4251e-04\n",
      "Epoch: 7470 | training loss: 1.5030e-03 | validation loss: 2.4219e-04\n",
      "Epoch: 7480 | training loss: 1.5022e-03 | validation loss: 2.4187e-04\n",
      "Epoch: 7490 | training loss: 1.5015e-03 | validation loss: 2.4155e-04\n",
      "Epoch: 7500 | training loss: 1.5007e-03 | validation loss: 2.4124e-04\n",
      "Epoch: 7510 | training loss: 1.5000e-03 | validation loss: 2.4092e-04\n",
      "Epoch: 7520 | training loss: 1.4992e-03 | validation loss: 2.4060e-04\n",
      "Epoch: 7530 | training loss: 1.4985e-03 | validation loss: 2.4029e-04\n",
      "Epoch: 7540 | training loss: 1.4977e-03 | validation loss: 2.3997e-04\n",
      "Epoch: 7550 | training loss: 1.4970e-03 | validation loss: 2.3965e-04\n",
      "Epoch: 7560 | training loss: 1.4962e-03 | validation loss: 2.3933e-04\n",
      "Epoch: 7570 | training loss: 1.4955e-03 | validation loss: 2.3901e-04\n",
      "Epoch: 7580 | training loss: 1.4947e-03 | validation loss: 2.3869e-04\n",
      "Epoch: 7590 | training loss: 1.4939e-03 | validation loss: 2.3837e-04\n",
      "Epoch: 7600 | training loss: 1.4932e-03 | validation loss: 2.3805e-04\n",
      "Epoch: 7610 | training loss: 1.4924e-03 | validation loss: 2.3773e-04\n",
      "Epoch: 7620 | training loss: 1.4917e-03 | validation loss: 2.3741e-04\n",
      "Epoch: 7630 | training loss: 1.4909e-03 | validation loss: 2.3710e-04\n",
      "Epoch: 7640 | training loss: 1.4901e-03 | validation loss: 2.3711e-04\n",
      "Epoch: 7650 | training loss: 1.4929e-03 | validation loss: 2.4653e-04\n",
      "Epoch: 7660 | training loss: 1.4888e-03 | validation loss: 2.3805e-04\n",
      "Epoch: 7670 | training loss: 1.4882e-03 | validation loss: 2.3859e-04\n",
      "Epoch: 7680 | training loss: 1.4872e-03 | validation loss: 2.3693e-04\n",
      "Epoch: 7690 | training loss: 1.4863e-03 | validation loss: 2.3521e-04\n",
      "Epoch: 7700 | training loss: 1.4856e-03 | validation loss: 2.3446e-04\n",
      "Epoch: 7710 | training loss: 1.4848e-03 | validation loss: 2.3421e-04\n",
      "Epoch: 7720 | training loss: 1.4841e-03 | validation loss: 2.3405e-04\n",
      "Epoch: 7730 | training loss: 1.4833e-03 | validation loss: 2.3382e-04\n",
      "Epoch: 7740 | training loss: 1.4825e-03 | validation loss: 2.3355e-04\n",
      "Epoch: 7750 | training loss: 1.4818e-03 | validation loss: 2.3324e-04\n",
      "Epoch: 7760 | training loss: 1.4810e-03 | validation loss: 2.3293e-04\n",
      "Epoch: 7770 | training loss: 1.4802e-03 | validation loss: 2.3261e-04\n",
      "Epoch: 7780 | training loss: 1.4795e-03 | validation loss: 2.3229e-04\n",
      "Epoch: 7790 | training loss: 1.4787e-03 | validation loss: 2.3198e-04\n",
      "Epoch: 7800 | training loss: 1.4779e-03 | validation loss: 2.3166e-04\n",
      "Epoch: 7810 | training loss: 1.4771e-03 | validation loss: 2.3135e-04\n",
      "Epoch: 7820 | training loss: 1.4764e-03 | validation loss: 2.3103e-04\n",
      "Epoch: 7830 | training loss: 1.4756e-03 | validation loss: 2.3070e-04\n",
      "Epoch: 7840 | training loss: 1.4748e-03 | validation loss: 2.3038e-04\n",
      "Epoch: 7850 | training loss: 1.4740e-03 | validation loss: 2.3006e-04\n",
      "Epoch: 7860 | training loss: 1.4732e-03 | validation loss: 2.2974e-04\n",
      "Epoch: 7870 | training loss: 1.4724e-03 | validation loss: 2.2942e-04\n",
      "Epoch: 7880 | training loss: 1.4717e-03 | validation loss: 2.2910e-04\n",
      "Epoch: 7890 | training loss: 1.4709e-03 | validation loss: 2.2878e-04\n",
      "Epoch: 7900 | training loss: 1.4701e-03 | validation loss: 2.2846e-04\n",
      "Epoch: 7910 | training loss: 1.4693e-03 | validation loss: 2.2813e-04\n",
      "Epoch: 7920 | training loss: 1.4685e-03 | validation loss: 2.2781e-04\n",
      "Epoch: 7930 | training loss: 1.4677e-03 | validation loss: 2.2749e-04\n",
      "Epoch: 7940 | training loss: 1.4669e-03 | validation loss: 2.2717e-04\n",
      "Epoch: 7950 | training loss: 1.4661e-03 | validation loss: 2.2684e-04\n",
      "Epoch: 7960 | training loss: 1.4653e-03 | validation loss: 2.2652e-04\n",
      "Epoch: 7970 | training loss: 1.4644e-03 | validation loss: 2.2621e-04\n",
      "Epoch: 7980 | training loss: 1.4636e-03 | validation loss: 2.2627e-04\n",
      "Epoch: 7990 | training loss: 1.4674e-03 | validation loss: 2.3743e-04\n",
      "Epoch: 8000 | training loss: 1.4627e-03 | validation loss: 2.2873e-04\n",
      "Epoch: 8010 | training loss: 1.4618e-03 | validation loss: 2.2823e-04\n",
      "Epoch: 8020 | training loss: 1.4605e-03 | validation loss: 2.2544e-04\n",
      "Epoch: 8030 | training loss: 1.4597e-03 | validation loss: 2.2376e-04\n",
      "Epoch: 8040 | training loss: 1.4589e-03 | validation loss: 2.2339e-04\n",
      "Epoch: 8050 | training loss: 1.4581e-03 | validation loss: 2.2340e-04\n",
      "Epoch: 8060 | training loss: 1.4573e-03 | validation loss: 2.2330e-04\n",
      "Epoch: 8070 | training loss: 1.4565e-03 | validation loss: 2.2305e-04\n",
      "Epoch: 8080 | training loss: 1.4557e-03 | validation loss: 2.2273e-04\n",
      "Epoch: 8090 | training loss: 1.4549e-03 | validation loss: 2.2239e-04\n",
      "Epoch: 8100 | training loss: 1.4541e-03 | validation loss: 2.2206e-04\n",
      "Epoch: 8110 | training loss: 1.4533e-03 | validation loss: 2.2173e-04\n",
      "Epoch: 8120 | training loss: 1.4525e-03 | validation loss: 2.2140e-04\n",
      "Epoch: 8130 | training loss: 1.4517e-03 | validation loss: 2.2108e-04\n",
      "Epoch: 8140 | training loss: 1.4509e-03 | validation loss: 2.2076e-04\n",
      "Epoch: 8150 | training loss: 1.4501e-03 | validation loss: 2.2044e-04\n",
      "Epoch: 8160 | training loss: 1.4493e-03 | validation loss: 2.2012e-04\n",
      "Epoch: 8170 | training loss: 1.4485e-03 | validation loss: 2.1980e-04\n",
      "Epoch: 8180 | training loss: 1.4477e-03 | validation loss: 2.1948e-04\n",
      "Epoch: 8190 | training loss: 1.4468e-03 | validation loss: 2.1916e-04\n",
      "Epoch: 8200 | training loss: 1.4460e-03 | validation loss: 2.1884e-04\n",
      "Epoch: 8210 | training loss: 1.4452e-03 | validation loss: 2.1852e-04\n",
      "Epoch: 8220 | training loss: 1.4444e-03 | validation loss: 2.1820e-04\n",
      "Epoch: 8230 | training loss: 1.4435e-03 | validation loss: 2.1788e-04\n",
      "Epoch: 8240 | training loss: 1.4427e-03 | validation loss: 2.1756e-04\n",
      "Epoch: 8250 | training loss: 1.4419e-03 | validation loss: 2.1723e-04\n",
      "Epoch: 8260 | training loss: 1.4411e-03 | validation loss: 2.1691e-04\n",
      "Epoch: 8270 | training loss: 1.4402e-03 | validation loss: 2.1659e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8280 | training loss: 1.4394e-03 | validation loss: 2.1627e-04\n",
      "Epoch: 8290 | training loss: 1.4386e-03 | validation loss: 2.1595e-04\n",
      "Epoch: 8300 | training loss: 1.4377e-03 | validation loss: 2.1563e-04\n",
      "Epoch: 8310 | training loss: 1.4369e-03 | validation loss: 2.1530e-04\n",
      "Epoch: 8320 | training loss: 1.4360e-03 | validation loss: 2.1498e-04\n",
      "Epoch: 8330 | training loss: 1.4352e-03 | validation loss: 2.1468e-04\n",
      "Epoch: 8340 | training loss: 1.4343e-03 | validation loss: 2.1484e-04\n",
      "Epoch: 8350 | training loss: 1.4409e-03 | validation loss: 2.3051e-04\n",
      "Epoch: 8360 | training loss: 1.4345e-03 | validation loss: 2.2026e-04\n",
      "Epoch: 8370 | training loss: 1.4320e-03 | validation loss: 2.1223e-04\n",
      "Epoch: 8380 | training loss: 1.4312e-03 | validation loss: 2.1181e-04\n",
      "Epoch: 8390 | training loss: 1.4303e-03 | validation loss: 2.1368e-04\n",
      "Epoch: 8400 | training loss: 1.4294e-03 | validation loss: 2.1276e-04\n",
      "Epoch: 8410 | training loss: 1.4286e-03 | validation loss: 2.1179e-04\n",
      "Epoch: 8420 | training loss: 1.4278e-03 | validation loss: 2.1169e-04\n",
      "Epoch: 8430 | training loss: 1.4270e-03 | validation loss: 2.1159e-04\n",
      "Epoch: 8440 | training loss: 1.4262e-03 | validation loss: 2.1126e-04\n",
      "Epoch: 8450 | training loss: 1.4253e-03 | validation loss: 2.1088e-04\n",
      "Epoch: 8460 | training loss: 1.4245e-03 | validation loss: 2.1053e-04\n",
      "Epoch: 8470 | training loss: 1.4237e-03 | validation loss: 2.1022e-04\n",
      "Epoch: 8480 | training loss: 1.4229e-03 | validation loss: 2.0991e-04\n",
      "Epoch: 8490 | training loss: 1.4220e-03 | validation loss: 2.0960e-04\n",
      "Epoch: 8500 | training loss: 1.4212e-03 | validation loss: 2.0929e-04\n",
      "Epoch: 8510 | training loss: 1.4204e-03 | validation loss: 2.0898e-04\n",
      "Epoch: 8520 | training loss: 1.4195e-03 | validation loss: 2.0867e-04\n",
      "Epoch: 8530 | training loss: 1.4187e-03 | validation loss: 2.0835e-04\n",
      "Epoch: 8540 | training loss: 1.4179e-03 | validation loss: 2.0804e-04\n",
      "Epoch: 8550 | training loss: 1.4170e-03 | validation loss: 2.0772e-04\n",
      "Epoch: 8560 | training loss: 1.4162e-03 | validation loss: 2.0741e-04\n",
      "Epoch: 8570 | training loss: 1.4153e-03 | validation loss: 2.0710e-04\n",
      "Epoch: 8580 | training loss: 1.4145e-03 | validation loss: 2.0678e-04\n",
      "Epoch: 8590 | training loss: 1.4136e-03 | validation loss: 2.0647e-04\n",
      "Epoch: 8600 | training loss: 1.4128e-03 | validation loss: 2.0615e-04\n",
      "Epoch: 8610 | training loss: 1.4119e-03 | validation loss: 2.0584e-04\n",
      "Epoch: 8620 | training loss: 1.4111e-03 | validation loss: 2.0552e-04\n",
      "Epoch: 8630 | training loss: 1.4102e-03 | validation loss: 2.0521e-04\n",
      "Epoch: 8640 | training loss: 1.4093e-03 | validation loss: 2.0490e-04\n",
      "Epoch: 8650 | training loss: 1.4085e-03 | validation loss: 2.0458e-04\n",
      "Epoch: 8660 | training loss: 1.4076e-03 | validation loss: 2.0427e-04\n",
      "Epoch: 8670 | training loss: 1.4068e-03 | validation loss: 2.0395e-04\n",
      "Epoch: 8680 | training loss: 1.4059e-03 | validation loss: 2.0364e-04\n",
      "Epoch: 8690 | training loss: 1.4050e-03 | validation loss: 2.0332e-04\n",
      "Epoch: 8700 | training loss: 1.4042e-03 | validation loss: 2.0301e-04\n",
      "Epoch: 8710 | training loss: 1.4033e-03 | validation loss: 2.0269e-04\n",
      "Epoch: 8720 | training loss: 1.4024e-03 | validation loss: 2.0238e-04\n",
      "Epoch: 8730 | training loss: 1.4015e-03 | validation loss: 2.0206e-04\n",
      "Epoch: 8740 | training loss: 1.4007e-03 | validation loss: 2.0175e-04\n",
      "Epoch: 8750 | training loss: 1.3998e-03 | validation loss: 2.0138e-04\n",
      "Epoch: 8760 | training loss: 1.3992e-03 | validation loss: 1.9947e-04\n",
      "Epoch: 8770 | training loss: 1.4001e-03 | validation loss: 2.0785e-04\n",
      "Epoch: 8780 | training loss: 1.3987e-03 | validation loss: 2.0611e-04\n",
      "Epoch: 8790 | training loss: 1.3966e-03 | validation loss: 1.9863e-04\n",
      "Epoch: 8800 | training loss: 1.3955e-03 | validation loss: 1.9932e-04\n",
      "Epoch: 8810 | training loss: 1.3948e-03 | validation loss: 2.0061e-04\n",
      "Epoch: 8820 | training loss: 1.3938e-03 | validation loss: 1.9911e-04\n",
      "Epoch: 8830 | training loss: 1.3930e-03 | validation loss: 1.9868e-04\n",
      "Epoch: 8840 | training loss: 1.3922e-03 | validation loss: 1.9877e-04\n",
      "Epoch: 8850 | training loss: 1.3913e-03 | validation loss: 1.9847e-04\n",
      "Epoch: 8860 | training loss: 1.3905e-03 | validation loss: 1.9804e-04\n",
      "Epoch: 8870 | training loss: 1.3896e-03 | validation loss: 1.9771e-04\n",
      "Epoch: 8880 | training loss: 1.3888e-03 | validation loss: 1.9743e-04\n",
      "Epoch: 8890 | training loss: 1.3879e-03 | validation loss: 1.9714e-04\n",
      "Epoch: 8900 | training loss: 1.3871e-03 | validation loss: 1.9684e-04\n",
      "Epoch: 8910 | training loss: 1.3862e-03 | validation loss: 1.9654e-04\n",
      "Epoch: 8920 | training loss: 1.3854e-03 | validation loss: 1.9623e-04\n",
      "Epoch: 8930 | training loss: 1.3845e-03 | validation loss: 1.9593e-04\n",
      "Epoch: 8940 | training loss: 1.3836e-03 | validation loss: 1.9562e-04\n",
      "Epoch: 8950 | training loss: 1.3828e-03 | validation loss: 1.9532e-04\n",
      "Epoch: 8960 | training loss: 1.3819e-03 | validation loss: 1.9501e-04\n",
      "Epoch: 8970 | training loss: 1.3811e-03 | validation loss: 1.9471e-04\n",
      "Epoch: 8980 | training loss: 1.3802e-03 | validation loss: 1.9440e-04\n",
      "Epoch: 8990 | training loss: 1.3793e-03 | validation loss: 1.9410e-04\n",
      "Epoch: 9000 | training loss: 1.3785e-03 | validation loss: 1.9380e-04\n",
      "Epoch: 9010 | training loss: 1.3776e-03 | validation loss: 1.9349e-04\n",
      "Epoch: 9020 | training loss: 1.3767e-03 | validation loss: 1.9319e-04\n",
      "Epoch: 9030 | training loss: 1.3758e-03 | validation loss: 1.9289e-04\n",
      "Epoch: 9040 | training loss: 1.3749e-03 | validation loss: 1.9258e-04\n",
      "Epoch: 9050 | training loss: 1.3741e-03 | validation loss: 1.9228e-04\n",
      "Epoch: 9060 | training loss: 1.3732e-03 | validation loss: 1.9198e-04\n",
      "Epoch: 9070 | training loss: 1.3723e-03 | validation loss: 1.9167e-04\n",
      "Epoch: 9080 | training loss: 1.3714e-03 | validation loss: 1.9137e-04\n",
      "Epoch: 9090 | training loss: 1.3705e-03 | validation loss: 1.9107e-04\n",
      "Epoch: 9100 | training loss: 1.3696e-03 | validation loss: 1.9076e-04\n",
      "Epoch: 9110 | training loss: 1.3688e-03 | validation loss: 1.9046e-04\n",
      "Epoch: 9120 | training loss: 1.3679e-03 | validation loss: 1.9016e-04\n",
      "Epoch: 9130 | training loss: 1.3670e-03 | validation loss: 1.8985e-04\n",
      "Epoch: 9140 | training loss: 1.3661e-03 | validation loss: 1.8955e-04\n",
      "Epoch: 9150 | training loss: 1.3652e-03 | validation loss: 1.8925e-04\n",
      "Epoch: 9160 | training loss: 1.3643e-03 | validation loss: 1.8895e-04\n",
      "Epoch: 9170 | training loss: 1.3634e-03 | validation loss: 1.8865e-04\n",
      "Epoch: 9180 | training loss: 1.3625e-03 | validation loss: 1.8845e-04\n",
      "Epoch: 9190 | training loss: 1.3626e-03 | validation loss: 1.9249e-04\n",
      "Epoch: 9200 | training loss: 1.3655e-03 | validation loss: 1.8465e-04\n",
      "Epoch: 9210 | training loss: 1.3600e-03 | validation loss: 1.8616e-04\n",
      "Epoch: 9220 | training loss: 1.3594e-03 | validation loss: 1.8998e-04\n",
      "Epoch: 9230 | training loss: 1.3581e-03 | validation loss: 1.8730e-04\n",
      "Epoch: 9240 | training loss: 1.3573e-03 | validation loss: 1.8580e-04\n",
      "Epoch: 9250 | training loss: 1.3564e-03 | validation loss: 1.8612e-04\n",
      "Epoch: 9260 | training loss: 1.3556e-03 | validation loss: 1.8627e-04\n",
      "Epoch: 9270 | training loss: 1.3547e-03 | validation loss: 1.8586e-04\n",
      "Epoch: 9280 | training loss: 1.3538e-03 | validation loss: 1.8540e-04\n",
      "Epoch: 9290 | training loss: 1.3530e-03 | validation loss: 1.8507e-04\n",
      "Epoch: 9300 | training loss: 1.3521e-03 | validation loss: 1.8480e-04\n",
      "Epoch: 9310 | training loss: 1.3512e-03 | validation loss: 1.8453e-04\n",
      "Epoch: 9320 | training loss: 1.3504e-03 | validation loss: 1.8425e-04\n",
      "Epoch: 9330 | training loss: 1.3495e-03 | validation loss: 1.8396e-04\n",
      "Epoch: 9340 | training loss: 1.3486e-03 | validation loss: 1.8367e-04\n",
      "Epoch: 9350 | training loss: 1.3478e-03 | validation loss: 1.8338e-04\n",
      "Epoch: 9360 | training loss: 1.3469e-03 | validation loss: 1.8309e-04\n",
      "Epoch: 9370 | training loss: 1.3460e-03 | validation loss: 1.8280e-04\n",
      "Epoch: 9380 | training loss: 1.3451e-03 | validation loss: 1.8251e-04\n",
      "Epoch: 9390 | training loss: 1.3443e-03 | validation loss: 1.8222e-04\n",
      "Epoch: 9400 | training loss: 1.3434e-03 | validation loss: 1.8193e-04\n",
      "Epoch: 9410 | training loss: 1.3425e-03 | validation loss: 1.8164e-04\n",
      "Epoch: 9420 | training loss: 1.3416e-03 | validation loss: 1.8135e-04\n",
      "Epoch: 9430 | training loss: 1.3407e-03 | validation loss: 1.8106e-04\n",
      "Epoch: 9440 | training loss: 1.3398e-03 | validation loss: 1.8078e-04\n",
      "Epoch: 9450 | training loss: 1.3389e-03 | validation loss: 1.8049e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9460 | training loss: 1.3381e-03 | validation loss: 1.8020e-04\n",
      "Epoch: 9470 | training loss: 1.3372e-03 | validation loss: 1.7991e-04\n",
      "Epoch: 9480 | training loss: 1.3363e-03 | validation loss: 1.7962e-04\n",
      "Epoch: 9490 | training loss: 1.3354e-03 | validation loss: 1.7933e-04\n",
      "Epoch: 9500 | training loss: 1.3345e-03 | validation loss: 1.7904e-04\n",
      "Epoch: 9510 | training loss: 1.3336e-03 | validation loss: 1.7876e-04\n",
      "Epoch: 9520 | training loss: 1.3327e-03 | validation loss: 1.7847e-04\n",
      "Epoch: 9530 | training loss: 1.3318e-03 | validation loss: 1.7818e-04\n",
      "Epoch: 9540 | training loss: 1.3309e-03 | validation loss: 1.7789e-04\n",
      "Epoch: 9550 | training loss: 1.3300e-03 | validation loss: 1.7760e-04\n",
      "Epoch: 9560 | training loss: 1.3290e-03 | validation loss: 1.7732e-04\n",
      "Epoch: 9570 | training loss: 1.3281e-03 | validation loss: 1.7703e-04\n",
      "Epoch: 9580 | training loss: 1.3272e-03 | validation loss: 1.7680e-04\n",
      "Epoch: 9590 | training loss: 1.3266e-03 | validation loss: 1.7850e-04\n",
      "Epoch: 9600 | training loss: 1.3256e-03 | validation loss: 1.7491e-04\n",
      "Epoch: 9610 | training loss: 1.3259e-03 | validation loss: 1.7327e-04\n",
      "Epoch: 9620 | training loss: 1.3243e-03 | validation loss: 1.7374e-04\n",
      "Epoch: 9630 | training loss: 1.3229e-03 | validation loss: 1.7605e-04\n",
      "Epoch: 9640 | training loss: 1.3221e-03 | validation loss: 1.7620e-04\n",
      "Epoch: 9650 | training loss: 1.3212e-03 | validation loss: 1.7499e-04\n",
      "Epoch: 9660 | training loss: 1.3203e-03 | validation loss: 1.7432e-04\n",
      "Epoch: 9670 | training loss: 1.3195e-03 | validation loss: 1.7410e-04\n",
      "Epoch: 9680 | training loss: 1.3186e-03 | validation loss: 1.7396e-04\n",
      "Epoch: 9690 | training loss: 1.3177e-03 | validation loss: 1.7377e-04\n",
      "Epoch: 9700 | training loss: 1.3169e-03 | validation loss: 1.7351e-04\n",
      "Epoch: 9710 | training loss: 1.3160e-03 | validation loss: 1.7324e-04\n",
      "Epoch: 9720 | training loss: 1.3152e-03 | validation loss: 1.7296e-04\n",
      "Epoch: 9730 | training loss: 1.3143e-03 | validation loss: 1.7269e-04\n",
      "Epoch: 9740 | training loss: 1.3134e-03 | validation loss: 1.7242e-04\n",
      "Epoch: 9750 | training loss: 1.3126e-03 | validation loss: 1.7214e-04\n",
      "Epoch: 9760 | training loss: 1.3117e-03 | validation loss: 1.7187e-04\n",
      "Epoch: 9770 | training loss: 1.3108e-03 | validation loss: 1.7160e-04\n",
      "Epoch: 9780 | training loss: 1.3100e-03 | validation loss: 1.7133e-04\n",
      "Epoch: 9790 | training loss: 1.3091e-03 | validation loss: 1.7107e-04\n",
      "Epoch: 9800 | training loss: 1.3082e-03 | validation loss: 1.7080e-04\n",
      "Epoch: 9810 | training loss: 1.3073e-03 | validation loss: 1.7053e-04\n",
      "Epoch: 9820 | training loss: 1.3065e-03 | validation loss: 1.7026e-04\n",
      "Epoch: 9830 | training loss: 1.3056e-03 | validation loss: 1.6999e-04\n",
      "Epoch: 9840 | training loss: 1.3047e-03 | validation loss: 1.6972e-04\n",
      "Epoch: 9850 | training loss: 1.3038e-03 | validation loss: 1.6946e-04\n",
      "Epoch: 9860 | training loss: 1.3029e-03 | validation loss: 1.6919e-04\n",
      "Epoch: 9870 | training loss: 1.3021e-03 | validation loss: 1.6892e-04\n",
      "Epoch: 9880 | training loss: 1.3012e-03 | validation loss: 1.6865e-04\n",
      "Epoch: 9890 | training loss: 1.3003e-03 | validation loss: 1.6839e-04\n",
      "Epoch: 9900 | training loss: 1.2994e-03 | validation loss: 1.6812e-04\n",
      "Epoch: 9910 | training loss: 1.2985e-03 | validation loss: 1.6785e-04\n",
      "Epoch: 9920 | training loss: 1.2976e-03 | validation loss: 1.6759e-04\n",
      "Epoch: 9930 | training loss: 1.2967e-03 | validation loss: 1.6732e-04\n",
      "Epoch: 9940 | training loss: 1.2958e-03 | validation loss: 1.6705e-04\n",
      "Epoch: 9950 | training loss: 1.2949e-03 | validation loss: 1.6678e-04\n",
      "Epoch: 9960 | training loss: 1.2940e-03 | validation loss: 1.6648e-04\n",
      "Epoch: 9970 | training loss: 1.2932e-03 | validation loss: 1.6537e-04\n",
      "Epoch: 9980 | training loss: 1.3023e-03 | validation loss: 1.6513e-04\n",
      "Epoch: 9990 | training loss: 1.2940e-03 | validation loss: 1.6292e-04\n",
      "Epoch: 10000 | training loss: 1.2906e-03 | validation loss: 1.6544e-04\n",
      "Epoch: 10010 | training loss: 1.2901e-03 | validation loss: 1.6734e-04\n",
      "Epoch: 10020 | training loss: 1.2891e-03 | validation loss: 1.6612e-04\n",
      "Epoch: 10030 | training loss: 1.2881e-03 | validation loss: 1.6487e-04\n",
      "Epoch: 10040 | training loss: 1.2873e-03 | validation loss: 1.6430e-04\n",
      "Epoch: 10050 | training loss: 1.2865e-03 | validation loss: 1.6410e-04\n",
      "Epoch: 10060 | training loss: 1.2857e-03 | validation loss: 1.6397e-04\n",
      "Epoch: 10070 | training loss: 1.2848e-03 | validation loss: 1.6379e-04\n",
      "Epoch: 10080 | training loss: 1.2840e-03 | validation loss: 1.6358e-04\n",
      "Epoch: 10090 | training loss: 1.2832e-03 | validation loss: 1.6336e-04\n",
      "Epoch: 10100 | training loss: 1.2823e-03 | validation loss: 1.6312e-04\n",
      "Epoch: 10110 | training loss: 1.2815e-03 | validation loss: 1.6288e-04\n",
      "Epoch: 10120 | training loss: 1.2807e-03 | validation loss: 1.6265e-04\n",
      "Epoch: 10130 | training loss: 1.2798e-03 | validation loss: 1.6241e-04\n",
      "Epoch: 10140 | training loss: 1.2790e-03 | validation loss: 1.6217e-04\n",
      "Epoch: 10150 | training loss: 1.2782e-03 | validation loss: 1.6193e-04\n",
      "Epoch: 10160 | training loss: 1.2773e-03 | validation loss: 1.6169e-04\n",
      "Epoch: 10170 | training loss: 1.2765e-03 | validation loss: 1.6145e-04\n",
      "Epoch: 10180 | training loss: 1.2756e-03 | validation loss: 1.6121e-04\n",
      "Epoch: 10190 | training loss: 1.2748e-03 | validation loss: 1.6097e-04\n",
      "Epoch: 10200 | training loss: 1.2739e-03 | validation loss: 1.6073e-04\n",
      "Epoch: 10210 | training loss: 1.2731e-03 | validation loss: 1.6049e-04\n",
      "Epoch: 10220 | training loss: 1.2722e-03 | validation loss: 1.6025e-04\n",
      "Epoch: 10230 | training loss: 1.2714e-03 | validation loss: 1.6001e-04\n",
      "Epoch: 10240 | training loss: 1.2705e-03 | validation loss: 1.5977e-04\n",
      "Epoch: 10250 | training loss: 1.2697e-03 | validation loss: 1.5953e-04\n",
      "Epoch: 10260 | training loss: 1.2688e-03 | validation loss: 1.5930e-04\n",
      "Epoch: 10270 | training loss: 1.2680e-03 | validation loss: 1.5906e-04\n",
      "Epoch: 10280 | training loss: 1.2671e-03 | validation loss: 1.5882e-04\n",
      "Epoch: 10290 | training loss: 1.2663e-03 | validation loss: 1.5858e-04\n",
      "Epoch: 10300 | training loss: 1.2654e-03 | validation loss: 1.5834e-04\n",
      "Epoch: 10310 | training loss: 1.2645e-03 | validation loss: 1.5810e-04\n",
      "Epoch: 10320 | training loss: 1.2637e-03 | validation loss: 1.5781e-04\n",
      "Epoch: 10330 | training loss: 1.2630e-03 | validation loss: 1.5657e-04\n",
      "Epoch: 10340 | training loss: 1.2717e-03 | validation loss: 1.5695e-04\n",
      "Epoch: 10350 | training loss: 1.2643e-03 | validation loss: 1.5473e-04\n",
      "Epoch: 10360 | training loss: 1.2605e-03 | validation loss: 1.5617e-04\n",
      "Epoch: 10370 | training loss: 1.2597e-03 | validation loss: 1.5798e-04\n",
      "Epoch: 10380 | training loss: 1.2590e-03 | validation loss: 1.5774e-04\n",
      "Epoch: 10390 | training loss: 1.2581e-03 | validation loss: 1.5693e-04\n",
      "Epoch: 10400 | training loss: 1.2573e-03 | validation loss: 1.5638e-04\n",
      "Epoch: 10410 | training loss: 1.2565e-03 | validation loss: 1.5600e-04\n",
      "Epoch: 10420 | training loss: 1.2557e-03 | validation loss: 1.5572e-04\n",
      "Epoch: 10430 | training loss: 1.2549e-03 | validation loss: 1.5550e-04\n",
      "Epoch: 10440 | training loss: 1.2541e-03 | validation loss: 1.5530e-04\n",
      "Epoch: 10450 | training loss: 1.2533e-03 | validation loss: 1.5509e-04\n",
      "Epoch: 10460 | training loss: 1.2525e-03 | validation loss: 1.5489e-04\n",
      "Epoch: 10470 | training loss: 1.2517e-03 | validation loss: 1.5467e-04\n",
      "Epoch: 10480 | training loss: 1.2509e-03 | validation loss: 1.5445e-04\n",
      "Epoch: 10490 | training loss: 1.2501e-03 | validation loss: 1.5423e-04\n",
      "Epoch: 10500 | training loss: 1.2493e-03 | validation loss: 1.5402e-04\n",
      "Epoch: 10510 | training loss: 1.2486e-03 | validation loss: 1.5381e-04\n",
      "Epoch: 10520 | training loss: 1.2478e-03 | validation loss: 1.5360e-04\n",
      "Epoch: 10530 | training loss: 1.2470e-03 | validation loss: 1.5338e-04\n",
      "Epoch: 10540 | training loss: 1.2461e-03 | validation loss: 1.5317e-04\n",
      "Epoch: 10550 | training loss: 1.2453e-03 | validation loss: 1.5296e-04\n",
      "Epoch: 10560 | training loss: 1.2445e-03 | validation loss: 1.5275e-04\n",
      "Epoch: 10570 | training loss: 1.2437e-03 | validation loss: 1.5253e-04\n",
      "Epoch: 10580 | training loss: 1.2429e-03 | validation loss: 1.5232e-04\n",
      "Epoch: 10590 | training loss: 1.2421e-03 | validation loss: 1.5211e-04\n",
      "Epoch: 10600 | training loss: 1.2413e-03 | validation loss: 1.5190e-04\n",
      "Epoch: 10610 | training loss: 1.2405e-03 | validation loss: 1.5168e-04\n",
      "Epoch: 10620 | training loss: 1.2397e-03 | validation loss: 1.5147e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10630 | training loss: 1.2389e-03 | validation loss: 1.5126e-04\n",
      "Epoch: 10640 | training loss: 1.2380e-03 | validation loss: 1.5105e-04\n",
      "Epoch: 10650 | training loss: 1.2372e-03 | validation loss: 1.5083e-04\n",
      "Epoch: 10660 | training loss: 1.2364e-03 | validation loss: 1.5059e-04\n",
      "Epoch: 10670 | training loss: 1.2356e-03 | validation loss: 1.4986e-04\n",
      "Epoch: 10680 | training loss: 1.2524e-03 | validation loss: 1.5327e-04\n",
      "Epoch: 10690 | training loss: 1.2355e-03 | validation loss: 1.4859e-04\n",
      "Epoch: 10700 | training loss: 1.2350e-03 | validation loss: 1.4806e-04\n",
      "Epoch: 10710 | training loss: 1.2332e-03 | validation loss: 1.4810e-04\n",
      "Epoch: 10720 | training loss: 1.2319e-03 | validation loss: 1.4899e-04\n",
      "Epoch: 10730 | training loss: 1.2311e-03 | validation loss: 1.4947e-04\n",
      "Epoch: 10740 | training loss: 1.2303e-03 | validation loss: 1.4938e-04\n",
      "Epoch: 10750 | training loss: 1.2296e-03 | validation loss: 1.4913e-04\n",
      "Epoch: 10760 | training loss: 1.2288e-03 | validation loss: 1.4886e-04\n",
      "Epoch: 10770 | training loss: 1.2281e-03 | validation loss: 1.4861e-04\n",
      "Epoch: 10780 | training loss: 1.2273e-03 | validation loss: 1.4839e-04\n",
      "Epoch: 10790 | training loss: 1.2266e-03 | validation loss: 1.4817e-04\n",
      "Epoch: 10800 | training loss: 1.2259e-03 | validation loss: 1.4796e-04\n",
      "Epoch: 10810 | training loss: 1.2251e-03 | validation loss: 1.4776e-04\n",
      "Epoch: 10820 | training loss: 1.2244e-03 | validation loss: 1.4756e-04\n",
      "Epoch: 10830 | training loss: 1.2236e-03 | validation loss: 1.4738e-04\n",
      "Epoch: 10840 | training loss: 1.2228e-03 | validation loss: 1.4719e-04\n",
      "Epoch: 10850 | training loss: 1.2221e-03 | validation loss: 1.4700e-04\n",
      "Epoch: 10860 | training loss: 1.2213e-03 | validation loss: 1.4681e-04\n",
      "Epoch: 10870 | training loss: 1.2206e-03 | validation loss: 1.4662e-04\n",
      "Epoch: 10880 | training loss: 1.2198e-03 | validation loss: 1.4643e-04\n",
      "Epoch: 10890 | training loss: 1.2191e-03 | validation loss: 1.4624e-04\n",
      "Epoch: 10900 | training loss: 1.2183e-03 | validation loss: 1.4605e-04\n",
      "Epoch: 10910 | training loss: 1.2175e-03 | validation loss: 1.4586e-04\n",
      "Epoch: 10920 | training loss: 1.2168e-03 | validation loss: 1.4567e-04\n",
      "Epoch: 10930 | training loss: 1.2160e-03 | validation loss: 1.4548e-04\n",
      "Epoch: 10940 | training loss: 1.2152e-03 | validation loss: 1.4529e-04\n",
      "Epoch: 10950 | training loss: 1.2145e-03 | validation loss: 1.4510e-04\n",
      "Epoch: 10960 | training loss: 1.2137e-03 | validation loss: 1.4491e-04\n",
      "Epoch: 10970 | training loss: 1.2129e-03 | validation loss: 1.4472e-04\n",
      "Epoch: 10980 | training loss: 1.2122e-03 | validation loss: 1.4453e-04\n",
      "Epoch: 10990 | training loss: 1.2114e-03 | validation loss: 1.4434e-04\n",
      "Epoch: 11000 | training loss: 1.2106e-03 | validation loss: 1.4416e-04\n",
      "Epoch: 11010 | training loss: 1.2098e-03 | validation loss: 1.4424e-04\n",
      "Epoch: 11020 | training loss: 1.2158e-03 | validation loss: 1.5485e-04\n",
      "Epoch: 11030 | training loss: 1.2162e-03 | validation loss: 1.4327e-04\n",
      "Epoch: 11040 | training loss: 1.2084e-03 | validation loss: 1.4207e-04\n",
      "Epoch: 11050 | training loss: 1.2071e-03 | validation loss: 1.4430e-04\n",
      "Epoch: 11060 | training loss: 1.2066e-03 | validation loss: 1.4483e-04\n",
      "Epoch: 11070 | training loss: 1.2056e-03 | validation loss: 1.4371e-04\n",
      "Epoch: 11080 | training loss: 1.2049e-03 | validation loss: 1.4283e-04\n",
      "Epoch: 11090 | training loss: 1.2042e-03 | validation loss: 1.4247e-04\n",
      "Epoch: 11100 | training loss: 1.2035e-03 | validation loss: 1.4228e-04\n",
      "Epoch: 11110 | training loss: 1.2028e-03 | validation loss: 1.4215e-04\n",
      "Epoch: 11120 | training loss: 1.2021e-03 | validation loss: 1.4202e-04\n",
      "Epoch: 11130 | training loss: 1.2014e-03 | validation loss: 1.4187e-04\n",
      "Epoch: 11140 | training loss: 1.2007e-03 | validation loss: 1.4171e-04\n",
      "Epoch: 11150 | training loss: 1.2000e-03 | validation loss: 1.4155e-04\n",
      "Epoch: 11160 | training loss: 1.1992e-03 | validation loss: 1.4139e-04\n",
      "Epoch: 11170 | training loss: 1.1985e-03 | validation loss: 1.4122e-04\n",
      "Epoch: 11180 | training loss: 1.1978e-03 | validation loss: 1.4105e-04\n",
      "Epoch: 11190 | training loss: 1.1971e-03 | validation loss: 1.4087e-04\n",
      "Epoch: 11200 | training loss: 1.1964e-03 | validation loss: 1.4070e-04\n",
      "Epoch: 11210 | training loss: 1.1957e-03 | validation loss: 1.4053e-04\n",
      "Epoch: 11220 | training loss: 1.1950e-03 | validation loss: 1.4036e-04\n",
      "Epoch: 11230 | training loss: 1.1943e-03 | validation loss: 1.4019e-04\n",
      "Epoch: 11240 | training loss: 1.1936e-03 | validation loss: 1.4002e-04\n",
      "Epoch: 11250 | training loss: 1.1928e-03 | validation loss: 1.3984e-04\n",
      "Epoch: 11260 | training loss: 1.1921e-03 | validation loss: 1.3967e-04\n",
      "Epoch: 11270 | training loss: 1.1914e-03 | validation loss: 1.3950e-04\n",
      "Epoch: 11280 | training loss: 1.1907e-03 | validation loss: 1.3933e-04\n",
      "Epoch: 11290 | training loss: 1.1900e-03 | validation loss: 1.3916e-04\n",
      "Epoch: 11300 | training loss: 1.1892e-03 | validation loss: 1.3899e-04\n",
      "Epoch: 11310 | training loss: 1.1885e-03 | validation loss: 1.3882e-04\n",
      "Epoch: 11320 | training loss: 1.1878e-03 | validation loss: 1.3865e-04\n",
      "Epoch: 11330 | training loss: 1.1870e-03 | validation loss: 1.3848e-04\n",
      "Epoch: 11340 | training loss: 1.1863e-03 | validation loss: 1.3831e-04\n",
      "Epoch: 11350 | training loss: 1.1856e-03 | validation loss: 1.3814e-04\n",
      "Epoch: 11360 | training loss: 1.1849e-03 | validation loss: 1.3804e-04\n",
      "Epoch: 11370 | training loss: 1.1847e-03 | validation loss: 1.3990e-04\n",
      "Epoch: 11380 | training loss: 1.1866e-03 | validation loss: 1.4336e-04\n",
      "Epoch: 11390 | training loss: 1.1831e-03 | validation loss: 1.3898e-04\n",
      "Epoch: 11400 | training loss: 1.1828e-03 | validation loss: 1.3606e-04\n",
      "Epoch: 11410 | training loss: 1.1821e-03 | validation loss: 1.3591e-04\n",
      "Epoch: 11420 | training loss: 1.1809e-03 | validation loss: 1.3657e-04\n",
      "Epoch: 11430 | training loss: 1.1801e-03 | validation loss: 1.3703e-04\n",
      "Epoch: 11440 | training loss: 1.1795e-03 | validation loss: 1.3704e-04\n",
      "Epoch: 11450 | training loss: 1.1788e-03 | validation loss: 1.3680e-04\n",
      "Epoch: 11460 | training loss: 1.1782e-03 | validation loss: 1.3653e-04\n",
      "Epoch: 11470 | training loss: 1.1775e-03 | validation loss: 1.3631e-04\n",
      "Epoch: 11480 | training loss: 1.1768e-03 | validation loss: 1.3612e-04\n",
      "Epoch: 11490 | training loss: 1.1762e-03 | validation loss: 1.3595e-04\n",
      "Epoch: 11500 | training loss: 1.1755e-03 | validation loss: 1.3579e-04\n",
      "Epoch: 11510 | training loss: 1.1749e-03 | validation loss: 1.3562e-04\n",
      "Epoch: 11520 | training loss: 1.1742e-03 | validation loss: 1.3546e-04\n",
      "Epoch: 11530 | training loss: 1.1735e-03 | validation loss: 1.3530e-04\n",
      "Epoch: 11540 | training loss: 1.1729e-03 | validation loss: 1.3514e-04\n",
      "Epoch: 11550 | training loss: 1.1722e-03 | validation loss: 1.3498e-04\n",
      "Epoch: 11560 | training loss: 1.1715e-03 | validation loss: 1.3483e-04\n",
      "Epoch: 11570 | training loss: 1.1708e-03 | validation loss: 1.3467e-04\n",
      "Epoch: 11580 | training loss: 1.1702e-03 | validation loss: 1.3452e-04\n",
      "Epoch: 11590 | training loss: 1.1695e-03 | validation loss: 1.3436e-04\n",
      "Epoch: 11600 | training loss: 1.1688e-03 | validation loss: 1.3421e-04\n",
      "Epoch: 11610 | training loss: 1.1681e-03 | validation loss: 1.3405e-04\n",
      "Epoch: 11620 | training loss: 1.1675e-03 | validation loss: 1.3389e-04\n",
      "Epoch: 11630 | training loss: 1.1668e-03 | validation loss: 1.3374e-04\n",
      "Epoch: 11640 | training loss: 1.1661e-03 | validation loss: 1.3358e-04\n",
      "Epoch: 11650 | training loss: 1.1654e-03 | validation loss: 1.3343e-04\n",
      "Epoch: 11660 | training loss: 1.1647e-03 | validation loss: 1.3327e-04\n",
      "Epoch: 11670 | training loss: 1.1640e-03 | validation loss: 1.3312e-04\n",
      "Epoch: 11680 | training loss: 1.1634e-03 | validation loss: 1.3296e-04\n",
      "Epoch: 11690 | training loss: 1.1627e-03 | validation loss: 1.3281e-04\n",
      "Epoch: 11700 | training loss: 1.1620e-03 | validation loss: 1.3265e-04\n",
      "Epoch: 11710 | training loss: 1.1613e-03 | validation loss: 1.3249e-04\n",
      "Epoch: 11720 | training loss: 1.1606e-03 | validation loss: 1.3221e-04\n",
      "Epoch: 11730 | training loss: 1.1603e-03 | validation loss: 1.3123e-04\n",
      "Epoch: 11740 | training loss: 1.1701e-03 | validation loss: 1.3345e-04\n",
      "Epoch: 11750 | training loss: 1.1610e-03 | validation loss: 1.3078e-04\n",
      "Epoch: 11760 | training loss: 1.1582e-03 | validation loss: 1.3274e-04\n",
      "Epoch: 11770 | training loss: 1.1582e-03 | validation loss: 1.3394e-04\n",
      "Epoch: 11780 | training loss: 1.1569e-03 | validation loss: 1.3231e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11790 | training loss: 1.1562e-03 | validation loss: 1.3128e-04\n",
      "Epoch: 11800 | training loss: 1.1556e-03 | validation loss: 1.3096e-04\n",
      "Epoch: 11810 | training loss: 1.1550e-03 | validation loss: 1.3086e-04\n",
      "Epoch: 11820 | training loss: 1.1544e-03 | validation loss: 1.3081e-04\n",
      "Epoch: 11830 | training loss: 1.1537e-03 | validation loss: 1.3073e-04\n",
      "Epoch: 11840 | training loss: 1.1531e-03 | validation loss: 1.3061e-04\n",
      "Epoch: 11850 | training loss: 1.1525e-03 | validation loss: 1.3049e-04\n",
      "Epoch: 11860 | training loss: 1.1519e-03 | validation loss: 1.3035e-04\n",
      "Epoch: 11870 | training loss: 1.1513e-03 | validation loss: 1.3021e-04\n",
      "Epoch: 11880 | training loss: 1.1507e-03 | validation loss: 1.3007e-04\n",
      "Epoch: 11890 | training loss: 1.1500e-03 | validation loss: 1.2994e-04\n",
      "Epoch: 11900 | training loss: 1.1494e-03 | validation loss: 1.2980e-04\n",
      "Epoch: 11910 | training loss: 1.1488e-03 | validation loss: 1.2966e-04\n",
      "Epoch: 11920 | training loss: 1.1482e-03 | validation loss: 1.2952e-04\n",
      "Epoch: 11930 | training loss: 1.1475e-03 | validation loss: 1.2937e-04\n",
      "Epoch: 11940 | training loss: 1.1469e-03 | validation loss: 1.2923e-04\n",
      "Epoch: 11950 | training loss: 1.1463e-03 | validation loss: 1.2909e-04\n",
      "Epoch: 11960 | training loss: 1.1457e-03 | validation loss: 1.2895e-04\n",
      "Epoch: 11970 | training loss: 1.1450e-03 | validation loss: 1.2882e-04\n",
      "Epoch: 11980 | training loss: 1.1444e-03 | validation loss: 1.2868e-04\n",
      "Epoch: 11990 | training loss: 1.1438e-03 | validation loss: 1.2854e-04\n",
      "Epoch: 12000 | training loss: 1.1431e-03 | validation loss: 1.2840e-04\n",
      "Epoch: 12010 | training loss: 1.1425e-03 | validation loss: 1.2826e-04\n",
      "Epoch: 12020 | training loss: 1.1419e-03 | validation loss: 1.2812e-04\n",
      "Epoch: 12030 | training loss: 1.1412e-03 | validation loss: 1.2798e-04\n",
      "Epoch: 12040 | training loss: 1.1406e-03 | validation loss: 1.2784e-04\n",
      "Epoch: 12050 | training loss: 1.1399e-03 | validation loss: 1.2770e-04\n",
      "Epoch: 12060 | training loss: 1.1393e-03 | validation loss: 1.2756e-04\n",
      "Epoch: 12070 | training loss: 1.1387e-03 | validation loss: 1.2743e-04\n",
      "Epoch: 12080 | training loss: 1.1380e-03 | validation loss: 1.2728e-04\n",
      "Epoch: 12090 | training loss: 1.1374e-03 | validation loss: 1.2692e-04\n",
      "Epoch: 12100 | training loss: 1.1449e-03 | validation loss: 1.2727e-04\n",
      "Epoch: 12110 | training loss: 1.1458e-03 | validation loss: 1.3942e-04\n",
      "Epoch: 12120 | training loss: 1.1359e-03 | validation loss: 1.2816e-04\n",
      "Epoch: 12130 | training loss: 1.1358e-03 | validation loss: 1.2561e-04\n",
      "Epoch: 12140 | training loss: 1.1348e-03 | validation loss: 1.2556e-04\n",
      "Epoch: 12150 | training loss: 1.1338e-03 | validation loss: 1.2634e-04\n",
      "Epoch: 12160 | training loss: 1.1333e-03 | validation loss: 1.2665e-04\n",
      "Epoch: 12170 | training loss: 1.1327e-03 | validation loss: 1.2643e-04\n",
      "Epoch: 12180 | training loss: 1.1321e-03 | validation loss: 1.2613e-04\n",
      "Epoch: 12190 | training loss: 1.1315e-03 | validation loss: 1.2589e-04\n",
      "Epoch: 12200 | training loss: 1.1309e-03 | validation loss: 1.2573e-04\n",
      "Epoch: 12210 | training loss: 1.1304e-03 | validation loss: 1.2559e-04\n",
      "Epoch: 12220 | training loss: 1.1298e-03 | validation loss: 1.2547e-04\n",
      "Epoch: 12230 | training loss: 1.1292e-03 | validation loss: 1.2534e-04\n",
      "Epoch: 12240 | training loss: 1.1286e-03 | validation loss: 1.2522e-04\n",
      "Epoch: 12250 | training loss: 1.1280e-03 | validation loss: 1.2509e-04\n",
      "Epoch: 12260 | training loss: 1.1275e-03 | validation loss: 1.2496e-04\n",
      "Epoch: 12270 | training loss: 1.1269e-03 | validation loss: 1.2483e-04\n",
      "Epoch: 12280 | training loss: 1.1263e-03 | validation loss: 1.2470e-04\n",
      "Epoch: 12290 | training loss: 1.1257e-03 | validation loss: 1.2458e-04\n",
      "Epoch: 12300 | training loss: 1.1251e-03 | validation loss: 1.2445e-04\n",
      "Epoch: 12310 | training loss: 1.1245e-03 | validation loss: 1.2432e-04\n",
      "Epoch: 12320 | training loss: 1.1239e-03 | validation loss: 1.2419e-04\n",
      "Epoch: 12330 | training loss: 1.1233e-03 | validation loss: 1.2407e-04\n",
      "Epoch: 12340 | training loss: 1.1227e-03 | validation loss: 1.2394e-04\n",
      "Epoch: 12350 | training loss: 1.1221e-03 | validation loss: 1.2381e-04\n",
      "Epoch: 12360 | training loss: 1.1215e-03 | validation loss: 1.2369e-04\n",
      "Epoch: 12370 | training loss: 1.1209e-03 | validation loss: 1.2356e-04\n",
      "Epoch: 12380 | training loss: 1.1203e-03 | validation loss: 1.2344e-04\n",
      "Epoch: 12390 | training loss: 1.1197e-03 | validation loss: 1.2331e-04\n",
      "Epoch: 12400 | training loss: 1.1191e-03 | validation loss: 1.2319e-04\n",
      "Epoch: 12410 | training loss: 1.1185e-03 | validation loss: 1.2306e-04\n",
      "Epoch: 12420 | training loss: 1.1179e-03 | validation loss: 1.2294e-04\n",
      "Epoch: 12430 | training loss: 1.1173e-03 | validation loss: 1.2281e-04\n",
      "Epoch: 12440 | training loss: 1.1167e-03 | validation loss: 1.2269e-04\n",
      "Epoch: 12450 | training loss: 1.1161e-03 | validation loss: 1.2256e-04\n",
      "Epoch: 12460 | training loss: 1.1155e-03 | validation loss: 1.2237e-04\n",
      "Epoch: 12470 | training loss: 1.1156e-03 | validation loss: 1.2126e-04\n",
      "Epoch: 12480 | training loss: 1.1169e-03 | validation loss: 1.2142e-04\n",
      "Epoch: 12490 | training loss: 1.1138e-03 | validation loss: 1.2229e-04\n",
      "Epoch: 12500 | training loss: 1.1152e-03 | validation loss: 1.2599e-04\n",
      "Epoch: 12510 | training loss: 1.1131e-03 | validation loss: 1.2327e-04\n",
      "Epoch: 12520 | training loss: 1.1122e-03 | validation loss: 1.2137e-04\n",
      "Epoch: 12530 | training loss: 1.1117e-03 | validation loss: 1.2115e-04\n",
      "Epoch: 12540 | training loss: 1.1111e-03 | validation loss: 1.2129e-04\n",
      "Epoch: 12550 | training loss: 1.1105e-03 | validation loss: 1.2142e-04\n",
      "Epoch: 12560 | training loss: 1.1100e-03 | validation loss: 1.2136e-04\n",
      "Epoch: 12570 | training loss: 1.1095e-03 | validation loss: 1.2123e-04\n",
      "Epoch: 12580 | training loss: 1.1089e-03 | validation loss: 1.2108e-04\n",
      "Epoch: 12590 | training loss: 1.1084e-03 | validation loss: 1.2095e-04\n",
      "Epoch: 12600 | training loss: 1.1078e-03 | validation loss: 1.2082e-04\n",
      "Epoch: 12610 | training loss: 1.1073e-03 | validation loss: 1.2069e-04\n",
      "Epoch: 12620 | training loss: 1.1067e-03 | validation loss: 1.2057e-04\n",
      "Epoch: 12630 | training loss: 1.1062e-03 | validation loss: 1.2046e-04\n",
      "Epoch: 12640 | training loss: 1.1056e-03 | validation loss: 1.2034e-04\n",
      "Epoch: 12650 | training loss: 1.1051e-03 | validation loss: 1.2022e-04\n",
      "Epoch: 12660 | training loss: 1.1045e-03 | validation loss: 1.2010e-04\n",
      "Epoch: 12670 | training loss: 1.1040e-03 | validation loss: 1.1999e-04\n",
      "Epoch: 12680 | training loss: 1.1034e-03 | validation loss: 1.1988e-04\n",
      "Epoch: 12690 | training loss: 1.1029e-03 | validation loss: 1.1976e-04\n",
      "Epoch: 12700 | training loss: 1.1023e-03 | validation loss: 1.1965e-04\n",
      "Epoch: 12710 | training loss: 1.1018e-03 | validation loss: 1.1953e-04\n",
      "Epoch: 12720 | training loss: 1.1012e-03 | validation loss: 1.1942e-04\n",
      "Epoch: 12730 | training loss: 1.1006e-03 | validation loss: 1.1931e-04\n",
      "Epoch: 12740 | training loss: 1.1001e-03 | validation loss: 1.1919e-04\n",
      "Epoch: 12750 | training loss: 1.0995e-03 | validation loss: 1.1908e-04\n",
      "Epoch: 12760 | training loss: 1.0989e-03 | validation loss: 1.1897e-04\n",
      "Epoch: 12770 | training loss: 1.0984e-03 | validation loss: 1.1885e-04\n",
      "Epoch: 12780 | training loss: 1.0978e-03 | validation loss: 1.1874e-04\n",
      "Epoch: 12790 | training loss: 1.0972e-03 | validation loss: 1.1863e-04\n",
      "Epoch: 12800 | training loss: 1.0967e-03 | validation loss: 1.1852e-04\n",
      "Epoch: 12810 | training loss: 1.0961e-03 | validation loss: 1.1841e-04\n",
      "Epoch: 12820 | training loss: 1.0955e-03 | validation loss: 1.1830e-04\n",
      "Epoch: 12830 | training loss: 1.0950e-03 | validation loss: 1.1819e-04\n",
      "Epoch: 12840 | training loss: 1.0944e-03 | validation loss: 1.1806e-04\n",
      "Epoch: 12850 | training loss: 1.0938e-03 | validation loss: 1.1771e-04\n",
      "Epoch: 12860 | training loss: 1.1105e-03 | validation loss: 1.2244e-04\n",
      "Epoch: 12870 | training loss: 1.0932e-03 | validation loss: 1.1853e-04\n",
      "Epoch: 12880 | training loss: 1.0961e-03 | validation loss: 1.1727e-04\n",
      "Epoch: 12890 | training loss: 1.0921e-03 | validation loss: 1.1703e-04\n",
      "Epoch: 12900 | training loss: 1.0917e-03 | validation loss: 1.1881e-04\n",
      "Epoch: 12910 | training loss: 1.0908e-03 | validation loss: 1.1780e-04\n",
      "Epoch: 12920 | training loss: 1.0903e-03 | validation loss: 1.1696e-04\n",
      "Epoch: 12930 | training loss: 1.0898e-03 | validation loss: 1.1692e-04\n",
      "Epoch: 12940 | training loss: 1.0893e-03 | validation loss: 1.1702e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12950 | training loss: 1.0888e-03 | validation loss: 1.1700e-04\n",
      "Epoch: 12960 | training loss: 1.0883e-03 | validation loss: 1.1687e-04\n",
      "Epoch: 12970 | training loss: 1.0878e-03 | validation loss: 1.1673e-04\n",
      "Epoch: 12980 | training loss: 1.0873e-03 | validation loss: 1.1661e-04\n",
      "Epoch: 12990 | training loss: 1.0868e-03 | validation loss: 1.1650e-04\n",
      "Epoch: 13000 | training loss: 1.0863e-03 | validation loss: 1.1639e-04\n",
      "Epoch: 13010 | training loss: 1.0858e-03 | validation loss: 1.1629e-04\n",
      "Epoch: 13020 | training loss: 1.0853e-03 | validation loss: 1.1619e-04\n",
      "Epoch: 13030 | training loss: 1.0848e-03 | validation loss: 1.1608e-04\n",
      "Epoch: 13040 | training loss: 1.0843e-03 | validation loss: 1.1598e-04\n",
      "Epoch: 13050 | training loss: 1.0838e-03 | validation loss: 1.1588e-04\n",
      "Epoch: 13060 | training loss: 1.0833e-03 | validation loss: 1.1578e-04\n",
      "Epoch: 13070 | training loss: 1.0828e-03 | validation loss: 1.1568e-04\n",
      "Epoch: 13080 | training loss: 1.0823e-03 | validation loss: 1.1557e-04\n",
      "Epoch: 13090 | training loss: 1.0818e-03 | validation loss: 1.1547e-04\n",
      "Epoch: 13100 | training loss: 1.0812e-03 | validation loss: 1.1537e-04\n",
      "Epoch: 13110 | training loss: 1.0807e-03 | validation loss: 1.1527e-04\n",
      "Epoch: 13120 | training loss: 1.0802e-03 | validation loss: 1.1517e-04\n",
      "Epoch: 13130 | training loss: 1.0797e-03 | validation loss: 1.1507e-04\n",
      "Epoch: 13140 | training loss: 1.0792e-03 | validation loss: 1.1497e-04\n",
      "Epoch: 13150 | training loss: 1.0787e-03 | validation loss: 1.1487e-04\n",
      "Epoch: 13160 | training loss: 1.0782e-03 | validation loss: 1.1477e-04\n",
      "Epoch: 13170 | training loss: 1.0776e-03 | validation loss: 1.1467e-04\n",
      "Epoch: 13180 | training loss: 1.0771e-03 | validation loss: 1.1457e-04\n",
      "Epoch: 13190 | training loss: 1.0766e-03 | validation loss: 1.1447e-04\n",
      "Epoch: 13200 | training loss: 1.0761e-03 | validation loss: 1.1438e-04\n",
      "Epoch: 13210 | training loss: 1.0755e-03 | validation loss: 1.1428e-04\n",
      "Epoch: 13220 | training loss: 1.0750e-03 | validation loss: 1.1418e-04\n",
      "Epoch: 13230 | training loss: 1.0745e-03 | validation loss: 1.1408e-04\n",
      "Epoch: 13240 | training loss: 1.0740e-03 | validation loss: 1.1399e-04\n",
      "Epoch: 13250 | training loss: 1.0734e-03 | validation loss: 1.1389e-04\n",
      "Epoch: 13260 | training loss: 1.0729e-03 | validation loss: 1.1377e-04\n",
      "Epoch: 13270 | training loss: 1.0726e-03 | validation loss: 1.1312e-04\n",
      "Epoch: 13280 | training loss: 1.1049e-03 | validation loss: 1.2732e-04\n",
      "Epoch: 13290 | training loss: 1.0749e-03 | validation loss: 1.1310e-04\n",
      "Epoch: 13300 | training loss: 1.0738e-03 | validation loss: 1.1823e-04\n",
      "Epoch: 13310 | training loss: 1.0706e-03 | validation loss: 1.1376e-04\n",
      "Epoch: 13320 | training loss: 1.0705e-03 | validation loss: 1.1247e-04\n",
      "Epoch: 13330 | training loss: 1.0696e-03 | validation loss: 1.1338e-04\n",
      "Epoch: 13340 | training loss: 1.0692e-03 | validation loss: 1.1344e-04\n",
      "Epoch: 13350 | training loss: 1.0687e-03 | validation loss: 1.1287e-04\n",
      "Epoch: 13360 | training loss: 1.0682e-03 | validation loss: 1.1274e-04\n",
      "Epoch: 13370 | training loss: 1.0678e-03 | validation loss: 1.1277e-04\n",
      "Epoch: 13380 | training loss: 1.0673e-03 | validation loss: 1.1272e-04\n",
      "Epoch: 13390 | training loss: 1.0669e-03 | validation loss: 1.1261e-04\n",
      "Epoch: 13400 | training loss: 1.0664e-03 | validation loss: 1.1249e-04\n",
      "Epoch: 13410 | training loss: 1.0659e-03 | validation loss: 1.1239e-04\n",
      "Epoch: 13420 | training loss: 1.0655e-03 | validation loss: 1.1229e-04\n",
      "Epoch: 13430 | training loss: 1.0650e-03 | validation loss: 1.1220e-04\n",
      "Epoch: 13440 | training loss: 1.0646e-03 | validation loss: 1.1211e-04\n",
      "Epoch: 13450 | training loss: 1.0641e-03 | validation loss: 1.1202e-04\n",
      "Epoch: 13460 | training loss: 1.0636e-03 | validation loss: 1.1193e-04\n",
      "Epoch: 13470 | training loss: 1.0632e-03 | validation loss: 1.1184e-04\n",
      "Epoch: 13480 | training loss: 1.0627e-03 | validation loss: 1.1175e-04\n",
      "Epoch: 13490 | training loss: 1.0622e-03 | validation loss: 1.1166e-04\n",
      "Epoch: 13500 | training loss: 1.0618e-03 | validation loss: 1.1157e-04\n",
      "Epoch: 13510 | training loss: 1.0613e-03 | validation loss: 1.1148e-04\n",
      "Epoch: 13520 | training loss: 1.0608e-03 | validation loss: 1.1139e-04\n",
      "Epoch: 13530 | training loss: 1.0603e-03 | validation loss: 1.1130e-04\n",
      "Epoch: 13540 | training loss: 1.0599e-03 | validation loss: 1.1121e-04\n",
      "Epoch: 13550 | training loss: 1.0594e-03 | validation loss: 1.1112e-04\n",
      "Epoch: 13560 | training loss: 1.0589e-03 | validation loss: 1.1103e-04\n",
      "Epoch: 13570 | training loss: 1.0584e-03 | validation loss: 1.1094e-04\n",
      "Epoch: 13580 | training loss: 1.0580e-03 | validation loss: 1.1085e-04\n",
      "Epoch: 13590 | training loss: 1.0575e-03 | validation loss: 1.1076e-04\n",
      "Epoch: 13600 | training loss: 1.0570e-03 | validation loss: 1.1068e-04\n",
      "Epoch: 13610 | training loss: 1.0565e-03 | validation loss: 1.1059e-04\n",
      "Epoch: 13620 | training loss: 1.0560e-03 | validation loss: 1.1050e-04\n",
      "Epoch: 13630 | training loss: 1.0556e-03 | validation loss: 1.1041e-04\n",
      "Epoch: 13640 | training loss: 1.0551e-03 | validation loss: 1.1033e-04\n",
      "Epoch: 13650 | training loss: 1.0546e-03 | validation loss: 1.1024e-04\n",
      "Epoch: 13660 | training loss: 1.0541e-03 | validation loss: 1.1015e-04\n",
      "Epoch: 13670 | training loss: 1.0536e-03 | validation loss: 1.1007e-04\n",
      "Epoch: 13680 | training loss: 1.0531e-03 | validation loss: 1.0998e-04\n",
      "Epoch: 13690 | training loss: 1.0526e-03 | validation loss: 1.0990e-04\n",
      "Epoch: 13700 | training loss: 1.0521e-03 | validation loss: 1.0981e-04\n",
      "Epoch: 13710 | training loss: 1.0517e-03 | validation loss: 1.0962e-04\n",
      "Epoch: 13720 | training loss: 1.0565e-03 | validation loss: 1.0964e-04\n",
      "Epoch: 13730 | training loss: 1.0692e-03 | validation loss: 1.2912e-04\n",
      "Epoch: 13740 | training loss: 1.0505e-03 | validation loss: 1.0911e-04\n",
      "Epoch: 13750 | training loss: 1.0516e-03 | validation loss: 1.0865e-04\n",
      "Epoch: 13760 | training loss: 1.0500e-03 | validation loss: 1.1082e-04\n",
      "Epoch: 13770 | training loss: 1.0491e-03 | validation loss: 1.0949e-04\n",
      "Epoch: 13780 | training loss: 1.0487e-03 | validation loss: 1.0874e-04\n",
      "Epoch: 13790 | training loss: 1.0482e-03 | validation loss: 1.0911e-04\n",
      "Epoch: 13800 | training loss: 1.0478e-03 | validation loss: 1.0917e-04\n",
      "Epoch: 13810 | training loss: 1.0474e-03 | validation loss: 1.0889e-04\n",
      "Epoch: 13820 | training loss: 1.0470e-03 | validation loss: 1.0875e-04\n",
      "Epoch: 13830 | training loss: 1.0466e-03 | validation loss: 1.0871e-04\n",
      "Epoch: 13840 | training loss: 1.0461e-03 | validation loss: 1.0865e-04\n",
      "Epoch: 13850 | training loss: 1.0457e-03 | validation loss: 1.0857e-04\n",
      "Epoch: 13860 | training loss: 1.0453e-03 | validation loss: 1.0848e-04\n",
      "Epoch: 13870 | training loss: 1.0449e-03 | validation loss: 1.0839e-04\n",
      "Epoch: 13880 | training loss: 1.0445e-03 | validation loss: 1.0831e-04\n",
      "Epoch: 13890 | training loss: 1.0440e-03 | validation loss: 1.0822e-04\n",
      "Epoch: 13900 | training loss: 1.0436e-03 | validation loss: 1.0814e-04\n",
      "Epoch: 13910 | training loss: 1.0432e-03 | validation loss: 1.0806e-04\n",
      "Epoch: 13920 | training loss: 1.0428e-03 | validation loss: 1.0798e-04\n",
      "Epoch: 13930 | training loss: 1.0423e-03 | validation loss: 1.0790e-04\n",
      "Epoch: 13940 | training loss: 1.0419e-03 | validation loss: 1.0781e-04\n",
      "Epoch: 13950 | training loss: 1.0415e-03 | validation loss: 1.0773e-04\n",
      "Epoch: 13960 | training loss: 1.0410e-03 | validation loss: 1.0765e-04\n",
      "Epoch: 13970 | training loss: 1.0406e-03 | validation loss: 1.0757e-04\n",
      "Epoch: 13980 | training loss: 1.0402e-03 | validation loss: 1.0749e-04\n",
      "Epoch: 13990 | training loss: 1.0397e-03 | validation loss: 1.0741e-04\n",
      "Epoch: 14000 | training loss: 1.0393e-03 | validation loss: 1.0733e-04\n",
      "Epoch: 14010 | training loss: 1.0389e-03 | validation loss: 1.0725e-04\n",
      "Epoch: 14020 | training loss: 1.0384e-03 | validation loss: 1.0718e-04\n",
      "Epoch: 14030 | training loss: 1.0380e-03 | validation loss: 1.0710e-04\n",
      "Epoch: 14040 | training loss: 1.0376e-03 | validation loss: 1.0702e-04\n",
      "Epoch: 14050 | training loss: 1.0371e-03 | validation loss: 1.0694e-04\n",
      "Epoch: 14060 | training loss: 1.0367e-03 | validation loss: 1.0686e-04\n",
      "Epoch: 14070 | training loss: 1.0362e-03 | validation loss: 1.0678e-04\n",
      "Epoch: 14080 | training loss: 1.0358e-03 | validation loss: 1.0671e-04\n",
      "Epoch: 14090 | training loss: 1.0354e-03 | validation loss: 1.0663e-04\n",
      "Epoch: 14100 | training loss: 1.0349e-03 | validation loss: 1.0655e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14110 | training loss: 1.0345e-03 | validation loss: 1.0648e-04\n",
      "Epoch: 14120 | training loss: 1.0340e-03 | validation loss: 1.0640e-04\n",
      "Epoch: 14130 | training loss: 1.0336e-03 | validation loss: 1.0632e-04\n",
      "Epoch: 14140 | training loss: 1.0331e-03 | validation loss: 1.0625e-04\n",
      "Epoch: 14150 | training loss: 1.0327e-03 | validation loss: 1.0617e-04\n",
      "Epoch: 14160 | training loss: 1.0322e-03 | validation loss: 1.0604e-04\n",
      "Epoch: 14170 | training loss: 1.0331e-03 | validation loss: 1.0519e-04\n",
      "Epoch: 14180 | training loss: 1.0359e-03 | validation loss: 1.1228e-04\n",
      "Epoch: 14190 | training loss: 1.0389e-03 | validation loss: 1.1573e-04\n",
      "Epoch: 14200 | training loss: 1.0306e-03 | validation loss: 1.0579e-04\n",
      "Epoch: 14210 | training loss: 1.0311e-03 | validation loss: 1.0498e-04\n",
      "Epoch: 14220 | training loss: 1.0299e-03 | validation loss: 1.0612e-04\n",
      "Epoch: 14230 | training loss: 1.0295e-03 | validation loss: 1.0607e-04\n",
      "Epoch: 14240 | training loss: 1.0291e-03 | validation loss: 1.0533e-04\n",
      "Epoch: 14250 | training loss: 1.0287e-03 | validation loss: 1.0528e-04\n",
      "Epoch: 14260 | training loss: 1.0283e-03 | validation loss: 1.0538e-04\n",
      "Epoch: 14270 | training loss: 1.0279e-03 | validation loss: 1.0534e-04\n",
      "Epoch: 14280 | training loss: 1.0275e-03 | validation loss: 1.0522e-04\n",
      "Epoch: 14290 | training loss: 1.0272e-03 | validation loss: 1.0511e-04\n",
      "Epoch: 14300 | training loss: 1.0268e-03 | validation loss: 1.0503e-04\n",
      "Epoch: 14310 | training loss: 1.0264e-03 | validation loss: 1.0496e-04\n",
      "Epoch: 14320 | training loss: 1.0260e-03 | validation loss: 1.0489e-04\n",
      "Epoch: 14330 | training loss: 1.0256e-03 | validation loss: 1.0481e-04\n",
      "Epoch: 14340 | training loss: 1.0252e-03 | validation loss: 1.0474e-04\n",
      "Epoch: 14350 | training loss: 1.0248e-03 | validation loss: 1.0467e-04\n",
      "Epoch: 14360 | training loss: 1.0244e-03 | validation loss: 1.0459e-04\n",
      "Epoch: 14370 | training loss: 1.0240e-03 | validation loss: 1.0452e-04\n",
      "Epoch: 14380 | training loss: 1.0237e-03 | validation loss: 1.0444e-04\n",
      "Epoch: 14390 | training loss: 1.0233e-03 | validation loss: 1.0437e-04\n",
      "Epoch: 14400 | training loss: 1.0229e-03 | validation loss: 1.0430e-04\n",
      "Epoch: 14410 | training loss: 1.0225e-03 | validation loss: 1.0422e-04\n",
      "Epoch: 14420 | training loss: 1.0221e-03 | validation loss: 1.0415e-04\n",
      "Epoch: 14430 | training loss: 1.0217e-03 | validation loss: 1.0408e-04\n",
      "Epoch: 14440 | training loss: 1.0213e-03 | validation loss: 1.0401e-04\n",
      "Epoch: 14450 | training loss: 1.0209e-03 | validation loss: 1.0394e-04\n",
      "Epoch: 14460 | training loss: 1.0205e-03 | validation loss: 1.0386e-04\n",
      "Epoch: 14470 | training loss: 1.0201e-03 | validation loss: 1.0379e-04\n",
      "Epoch: 14480 | training loss: 1.0197e-03 | validation loss: 1.0372e-04\n",
      "Epoch: 14490 | training loss: 1.0193e-03 | validation loss: 1.0365e-04\n",
      "Epoch: 14500 | training loss: 1.0189e-03 | validation loss: 1.0358e-04\n",
      "Epoch: 14510 | training loss: 1.0185e-03 | validation loss: 1.0351e-04\n",
      "Epoch: 14520 | training loss: 1.0181e-03 | validation loss: 1.0344e-04\n",
      "Epoch: 14530 | training loss: 1.0176e-03 | validation loss: 1.0337e-04\n",
      "Epoch: 14540 | training loss: 1.0172e-03 | validation loss: 1.0330e-04\n",
      "Epoch: 14550 | training loss: 1.0168e-03 | validation loss: 1.0323e-04\n",
      "Epoch: 14560 | training loss: 1.0164e-03 | validation loss: 1.0316e-04\n",
      "Epoch: 14570 | training loss: 1.0160e-03 | validation loss: 1.0309e-04\n",
      "Epoch: 14580 | training loss: 1.0156e-03 | validation loss: 1.0302e-04\n",
      "Epoch: 14590 | training loss: 1.0152e-03 | validation loss: 1.0296e-04\n",
      "Epoch: 14600 | training loss: 1.0148e-03 | validation loss: 1.0287e-04\n",
      "Epoch: 14610 | training loss: 1.0146e-03 | validation loss: 1.0229e-04\n",
      "Epoch: 14620 | training loss: 1.0425e-03 | validation loss: 1.1491e-04\n",
      "Epoch: 14630 | training loss: 1.0143e-03 | validation loss: 1.0241e-04\n",
      "Epoch: 14640 | training loss: 1.0171e-03 | validation loss: 1.0825e-04\n",
      "Epoch: 14650 | training loss: 1.0133e-03 | validation loss: 1.0201e-04\n",
      "Epoch: 14660 | training loss: 1.0127e-03 | validation loss: 1.0212e-04\n",
      "Epoch: 14670 | training loss: 1.0124e-03 | validation loss: 1.0305e-04\n",
      "Epoch: 14680 | training loss: 1.0119e-03 | validation loss: 1.0237e-04\n",
      "Epoch: 14690 | training loss: 1.0116e-03 | validation loss: 1.0209e-04\n",
      "Epoch: 14700 | training loss: 1.0112e-03 | validation loss: 1.0222e-04\n",
      "Epoch: 14710 | training loss: 1.0108e-03 | validation loss: 1.0221e-04\n",
      "Epoch: 14720 | training loss: 1.0105e-03 | validation loss: 1.0207e-04\n",
      "Epoch: 14730 | training loss: 1.0101e-03 | validation loss: 1.0197e-04\n",
      "Epoch: 14740 | training loss: 1.0098e-03 | validation loss: 1.0191e-04\n",
      "Epoch: 14750 | training loss: 1.0094e-03 | validation loss: 1.0185e-04\n",
      "Epoch: 14760 | training loss: 1.0091e-03 | validation loss: 1.0178e-04\n",
      "Epoch: 14770 | training loss: 1.0087e-03 | validation loss: 1.0171e-04\n",
      "Epoch: 14780 | training loss: 1.0084e-03 | validation loss: 1.0164e-04\n",
      "Epoch: 14790 | training loss: 1.0080e-03 | validation loss: 1.0157e-04\n",
      "Epoch: 14800 | training loss: 1.0077e-03 | validation loss: 1.0150e-04\n",
      "Epoch: 14810 | training loss: 1.0073e-03 | validation loss: 1.0144e-04\n",
      "Epoch: 14820 | training loss: 1.0070e-03 | validation loss: 1.0137e-04\n",
      "Epoch: 14830 | training loss: 1.0066e-03 | validation loss: 1.0130e-04\n",
      "Epoch: 14840 | training loss: 1.0063e-03 | validation loss: 1.0123e-04\n",
      "Epoch: 14850 | training loss: 1.0059e-03 | validation loss: 1.0117e-04\n",
      "Epoch: 14860 | training loss: 1.0055e-03 | validation loss: 1.0110e-04\n",
      "Epoch: 14870 | training loss: 1.0052e-03 | validation loss: 1.0103e-04\n",
      "Epoch: 14880 | training loss: 1.0048e-03 | validation loss: 1.0097e-04\n",
      "Epoch: 14890 | training loss: 1.0045e-03 | validation loss: 1.0090e-04\n",
      "Epoch: 14900 | training loss: 1.0041e-03 | validation loss: 1.0083e-04\n",
      "Epoch: 14910 | training loss: 1.0037e-03 | validation loss: 1.0077e-04\n",
      "Epoch: 14920 | training loss: 1.0034e-03 | validation loss: 1.0070e-04\n",
      "Epoch: 14930 | training loss: 1.0030e-03 | validation loss: 1.0064e-04\n",
      "Epoch: 14940 | training loss: 1.0026e-03 | validation loss: 1.0057e-04\n",
      "Epoch: 14950 | training loss: 1.0023e-03 | validation loss: 1.0051e-04\n",
      "Epoch: 14960 | training loss: 1.0019e-03 | validation loss: 1.0044e-04\n",
      "Epoch: 14970 | training loss: 1.0015e-03 | validation loss: 1.0038e-04\n",
      "Epoch: 14980 | training loss: 1.0011e-03 | validation loss: 1.0032e-04\n",
      "Epoch: 14990 | training loss: 1.0008e-03 | validation loss: 1.0025e-04\n",
      "Epoch: 15000 | training loss: 1.0004e-03 | validation loss: 1.0019e-04\n",
      "Epoch: 15010 | training loss: 1.0000e-03 | validation loss: 1.0013e-04\n",
      "Epoch: 15020 | training loss: 9.9965e-04 | validation loss: 1.0006e-04\n",
      "Epoch: 15030 | training loss: 9.9928e-04 | validation loss: 1.0000e-04\n",
      "Epoch: 15040 | training loss: 9.9890e-04 | validation loss: 9.9938e-05\n",
      "Epoch: 15050 | training loss: 9.9852e-04 | validation loss: 9.9877e-05\n",
      "Epoch: 15060 | training loss: 9.9814e-04 | validation loss: 9.9852e-05\n",
      "Epoch: 15070 | training loss: 9.9852e-04 | validation loss: 1.0145e-04\n",
      "Epoch: 15080 | training loss: 9.9779e-04 | validation loss: 1.0063e-04\n",
      "Epoch: 15090 | training loss: 1.0030e-03 | validation loss: 1.0040e-04\n",
      "Epoch: 15100 | training loss: 9.9735e-04 | validation loss: 9.8944e-05\n",
      "Epoch: 15110 | training loss: 9.9751e-04 | validation loss: 1.0170e-04\n",
      "Epoch: 15120 | training loss: 9.9615e-04 | validation loss: 9.9305e-05\n",
      "Epoch: 15130 | training loss: 9.9594e-04 | validation loss: 9.9004e-05\n",
      "Epoch: 15140 | training loss: 9.9552e-04 | validation loss: 9.9466e-05\n",
      "Epoch: 15150 | training loss: 9.9521e-04 | validation loss: 9.9414e-05\n",
      "Epoch: 15160 | training loss: 9.9487e-04 | validation loss: 9.9159e-05\n",
      "Epoch: 15170 | training loss: 9.9456e-04 | validation loss: 9.9054e-05\n",
      "Epoch: 15180 | training loss: 9.9424e-04 | validation loss: 9.9033e-05\n",
      "Epoch: 15190 | training loss: 9.9392e-04 | validation loss: 9.8997e-05\n",
      "Epoch: 15200 | training loss: 9.9360e-04 | validation loss: 9.8932e-05\n",
      "Epoch: 15210 | training loss: 9.9328e-04 | validation loss: 9.8862e-05\n",
      "Epoch: 15220 | training loss: 9.9296e-04 | validation loss: 9.8792e-05\n",
      "Epoch: 15230 | training loss: 9.9263e-04 | validation loss: 9.8726e-05\n",
      "Epoch: 15240 | training loss: 9.9231e-04 | validation loss: 9.8662e-05\n",
      "Epoch: 15250 | training loss: 9.9198e-04 | validation loss: 9.8599e-05\n",
      "Epoch: 15260 | training loss: 9.9166e-04 | validation loss: 9.8536e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15270 | training loss: 9.9133e-04 | validation loss: 9.8473e-05\n",
      "Epoch: 15280 | training loss: 9.9101e-04 | validation loss: 9.8411e-05\n",
      "Epoch: 15290 | training loss: 9.9068e-04 | validation loss: 9.8349e-05\n",
      "Epoch: 15300 | training loss: 9.9035e-04 | validation loss: 9.8287e-05\n",
      "Epoch: 15310 | training loss: 9.9002e-04 | validation loss: 9.8225e-05\n",
      "Epoch: 15320 | training loss: 9.8969e-04 | validation loss: 9.8163e-05\n",
      "Epoch: 15330 | training loss: 9.8936e-04 | validation loss: 9.8102e-05\n",
      "Epoch: 15340 | training loss: 9.8902e-04 | validation loss: 9.8041e-05\n",
      "Epoch: 15350 | training loss: 9.8869e-04 | validation loss: 9.7981e-05\n",
      "Epoch: 15360 | training loss: 9.8836e-04 | validation loss: 9.7920e-05\n",
      "Epoch: 15370 | training loss: 9.8802e-04 | validation loss: 9.7860e-05\n",
      "Epoch: 15380 | training loss: 9.8769e-04 | validation loss: 9.7800e-05\n",
      "Epoch: 15390 | training loss: 9.8735e-04 | validation loss: 9.7740e-05\n",
      "Epoch: 15400 | training loss: 9.8701e-04 | validation loss: 9.7681e-05\n",
      "Epoch: 15410 | training loss: 9.8667e-04 | validation loss: 9.7621e-05\n",
      "Epoch: 15420 | training loss: 9.8633e-04 | validation loss: 9.7562e-05\n",
      "Epoch: 15430 | training loss: 9.8599e-04 | validation loss: 9.7503e-05\n",
      "Epoch: 15440 | training loss: 9.8565e-04 | validation loss: 9.7445e-05\n",
      "Epoch: 15450 | training loss: 9.8531e-04 | validation loss: 9.7386e-05\n",
      "Epoch: 15460 | training loss: 9.8497e-04 | validation loss: 9.7328e-05\n",
      "Epoch: 15470 | training loss: 9.8462e-04 | validation loss: 9.7270e-05\n",
      "Epoch: 15480 | training loss: 9.8428e-04 | validation loss: 9.7212e-05\n",
      "Epoch: 15490 | training loss: 9.8393e-04 | validation loss: 9.7155e-05\n",
      "Epoch: 15500 | training loss: 9.8359e-04 | validation loss: 9.7101e-05\n",
      "Epoch: 15510 | training loss: 9.8324e-04 | validation loss: 9.7126e-05\n",
      "Epoch: 15520 | training loss: 9.8699e-04 | validation loss: 1.0240e-04\n",
      "Epoch: 15530 | training loss: 1.0044e-03 | validation loss: 1.0570e-04\n",
      "Epoch: 15540 | training loss: 9.8343e-04 | validation loss: 9.6530e-05\n",
      "Epoch: 15550 | training loss: 9.8437e-04 | validation loss: 1.0056e-04\n",
      "Epoch: 15560 | training loss: 9.8173e-04 | validation loss: 9.6660e-05\n",
      "Epoch: 15570 | training loss: 9.8169e-04 | validation loss: 9.6267e-05\n",
      "Epoch: 15580 | training loss: 9.8115e-04 | validation loss: 9.6802e-05\n",
      "Epoch: 15590 | training loss: 9.8087e-04 | validation loss: 9.6849e-05\n",
      "Epoch: 15600 | training loss: 9.8055e-04 | validation loss: 9.6459e-05\n",
      "Epoch: 15610 | training loss: 9.8026e-04 | validation loss: 9.6372e-05\n",
      "Epoch: 15620 | training loss: 9.7996e-04 | validation loss: 9.6376e-05\n",
      "Epoch: 15630 | training loss: 9.7967e-04 | validation loss: 9.6352e-05\n",
      "Epoch: 15640 | training loss: 9.7938e-04 | validation loss: 9.6289e-05\n",
      "Epoch: 15650 | training loss: 9.7908e-04 | validation loss: 9.6215e-05\n",
      "Epoch: 15660 | training loss: 9.7879e-04 | validation loss: 9.6150e-05\n",
      "Epoch: 15670 | training loss: 9.7849e-04 | validation loss: 9.6087e-05\n",
      "Epoch: 15680 | training loss: 9.7820e-04 | validation loss: 9.6028e-05\n",
      "Epoch: 15690 | training loss: 9.7790e-04 | validation loss: 9.5969e-05\n",
      "Epoch: 15700 | training loss: 9.7760e-04 | validation loss: 9.5911e-05\n",
      "Epoch: 15710 | training loss: 9.7730e-04 | validation loss: 9.5853e-05\n",
      "Epoch: 15720 | training loss: 9.7700e-04 | validation loss: 9.5795e-05\n",
      "Epoch: 15730 | training loss: 9.7670e-04 | validation loss: 9.5737e-05\n",
      "Epoch: 15740 | training loss: 9.7640e-04 | validation loss: 9.5679e-05\n",
      "Epoch: 15750 | training loss: 9.7610e-04 | validation loss: 9.5622e-05\n",
      "Epoch: 15760 | training loss: 9.7579e-04 | validation loss: 9.5564e-05\n",
      "Epoch: 15770 | training loss: 9.7549e-04 | validation loss: 9.5507e-05\n",
      "Epoch: 15780 | training loss: 9.7518e-04 | validation loss: 9.5451e-05\n",
      "Epoch: 15790 | training loss: 9.7488e-04 | validation loss: 9.5394e-05\n",
      "Epoch: 15800 | training loss: 9.7457e-04 | validation loss: 9.5338e-05\n",
      "Epoch: 15810 | training loss: 9.7426e-04 | validation loss: 9.5282e-05\n",
      "Epoch: 15820 | training loss: 9.7396e-04 | validation loss: 9.5226e-05\n",
      "Epoch: 15830 | training loss: 9.7365e-04 | validation loss: 9.5171e-05\n",
      "Epoch: 15840 | training loss: 9.7334e-04 | validation loss: 9.5115e-05\n",
      "Epoch: 15850 | training loss: 9.7303e-04 | validation loss: 9.5060e-05\n",
      "Epoch: 15860 | training loss: 9.7271e-04 | validation loss: 9.5005e-05\n",
      "Epoch: 15870 | training loss: 9.7240e-04 | validation loss: 9.4950e-05\n",
      "Epoch: 15880 | training loss: 9.7209e-04 | validation loss: 9.4896e-05\n",
      "Epoch: 15890 | training loss: 9.7177e-04 | validation loss: 9.4842e-05\n",
      "Epoch: 15900 | training loss: 9.7146e-04 | validation loss: 9.4787e-05\n",
      "Epoch: 15910 | training loss: 9.7114e-04 | validation loss: 9.4733e-05\n",
      "Epoch: 15920 | training loss: 9.7083e-04 | validation loss: 9.4680e-05\n",
      "Epoch: 15930 | training loss: 9.7051e-04 | validation loss: 9.4626e-05\n",
      "Epoch: 15940 | training loss: 9.7019e-04 | validation loss: 9.4572e-05\n",
      "Epoch: 15950 | training loss: 9.6987e-04 | validation loss: 9.4496e-05\n",
      "Epoch: 15960 | training loss: 9.6984e-04 | validation loss: 9.3993e-05\n",
      "Epoch: 15970 | training loss: 9.9337e-04 | validation loss: 1.0491e-04\n",
      "Epoch: 15980 | training loss: 9.7002e-04 | validation loss: 9.4171e-05\n",
      "Epoch: 15990 | training loss: 9.7219e-04 | validation loss: 9.9130e-05\n",
      "Epoch: 16000 | training loss: 9.6849e-04 | validation loss: 9.4372e-05\n",
      "Epoch: 16010 | training loss: 9.6862e-04 | validation loss: 9.3732e-05\n",
      "Epoch: 16020 | training loss: 9.6792e-04 | validation loss: 9.4254e-05\n",
      "Epoch: 16030 | training loss: 9.6769e-04 | validation loss: 9.4442e-05\n",
      "Epoch: 16040 | training loss: 9.6738e-04 | validation loss: 9.3996e-05\n",
      "Epoch: 16050 | training loss: 9.6711e-04 | validation loss: 9.3882e-05\n",
      "Epoch: 16060 | training loss: 9.6684e-04 | validation loss: 9.3886e-05\n",
      "Epoch: 16070 | training loss: 9.6657e-04 | validation loss: 9.3876e-05\n",
      "Epoch: 16080 | training loss: 9.6630e-04 | validation loss: 9.3825e-05\n",
      "Epoch: 16090 | training loss: 9.6603e-04 | validation loss: 9.3755e-05\n",
      "Epoch: 16100 | training loss: 9.6576e-04 | validation loss: 9.3692e-05\n",
      "Epoch: 16110 | training loss: 9.6549e-04 | validation loss: 9.3632e-05\n",
      "Epoch: 16120 | training loss: 9.6522e-04 | validation loss: 9.3576e-05\n",
      "Epoch: 16130 | training loss: 9.6494e-04 | validation loss: 9.3521e-05\n",
      "Epoch: 16140 | training loss: 9.6467e-04 | validation loss: 9.3467e-05\n",
      "Epoch: 16150 | training loss: 9.6439e-04 | validation loss: 9.3413e-05\n",
      "Epoch: 16160 | training loss: 9.6412e-04 | validation loss: 9.3358e-05\n",
      "Epoch: 16170 | training loss: 9.6384e-04 | validation loss: 9.3304e-05\n",
      "Epoch: 16180 | training loss: 9.6356e-04 | validation loss: 9.3251e-05\n",
      "Epoch: 16190 | training loss: 9.6328e-04 | validation loss: 9.3197e-05\n",
      "Epoch: 16200 | training loss: 9.6300e-04 | validation loss: 9.3143e-05\n",
      "Epoch: 16210 | training loss: 9.6272e-04 | validation loss: 9.3090e-05\n",
      "Epoch: 16220 | training loss: 9.6244e-04 | validation loss: 9.3037e-05\n",
      "Epoch: 16230 | training loss: 9.6216e-04 | validation loss: 9.2985e-05\n",
      "Epoch: 16240 | training loss: 9.6188e-04 | validation loss: 9.2932e-05\n",
      "Epoch: 16250 | training loss: 9.6160e-04 | validation loss: 9.2880e-05\n",
      "Epoch: 16260 | training loss: 9.6131e-04 | validation loss: 9.2828e-05\n",
      "Epoch: 16270 | training loss: 9.6103e-04 | validation loss: 9.2776e-05\n",
      "Epoch: 16280 | training loss: 9.6074e-04 | validation loss: 9.2724e-05\n",
      "Epoch: 16290 | training loss: 9.6045e-04 | validation loss: 9.2673e-05\n",
      "Epoch: 16300 | training loss: 9.6017e-04 | validation loss: 9.2622e-05\n",
      "Epoch: 16310 | training loss: 9.5988e-04 | validation loss: 9.2571e-05\n",
      "Epoch: 16320 | training loss: 9.5959e-04 | validation loss: 9.2520e-05\n",
      "Epoch: 16330 | training loss: 9.5930e-04 | validation loss: 9.2469e-05\n",
      "Epoch: 16340 | training loss: 9.5901e-04 | validation loss: 9.2419e-05\n",
      "Epoch: 16350 | training loss: 9.5872e-04 | validation loss: 9.2368e-05\n",
      "Epoch: 16360 | training loss: 9.5842e-04 | validation loss: 9.2318e-05\n",
      "Epoch: 16370 | training loss: 9.5813e-04 | validation loss: 9.2268e-05\n",
      "Epoch: 16380 | training loss: 9.5784e-04 | validation loss: 9.2216e-05\n",
      "Epoch: 16390 | training loss: 9.5754e-04 | validation loss: 9.2131e-05\n",
      "Epoch: 16400 | training loss: 9.5811e-04 | validation loss: 9.1609e-05\n",
      "Epoch: 16410 | training loss: 9.5733e-04 | validation loss: 9.1908e-05\n",
      "Epoch: 16420 | training loss: 9.6098e-04 | validation loss: 9.7539e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16430 | training loss: 9.5806e-04 | validation loss: 9.4636e-05\n",
      "Epoch: 16440 | training loss: 9.5686e-04 | validation loss: 9.1386e-05\n",
      "Epoch: 16450 | training loss: 9.5610e-04 | validation loss: 9.1606e-05\n",
      "Epoch: 16460 | training loss: 9.5583e-04 | validation loss: 9.2218e-05\n",
      "Epoch: 16470 | training loss: 9.5551e-04 | validation loss: 9.1960e-05\n",
      "Epoch: 16480 | training loss: 9.5525e-04 | validation loss: 9.1633e-05\n",
      "Epoch: 16490 | training loss: 9.5500e-04 | validation loss: 9.1575e-05\n",
      "Epoch: 16500 | training loss: 9.5475e-04 | validation loss: 9.1591e-05\n",
      "Epoch: 16510 | training loss: 9.5450e-04 | validation loss: 9.1570e-05\n",
      "Epoch: 16520 | training loss: 9.5425e-04 | validation loss: 9.1519e-05\n",
      "Epoch: 16530 | training loss: 9.5400e-04 | validation loss: 9.1456e-05\n",
      "Epoch: 16540 | training loss: 9.5375e-04 | validation loss: 9.1398e-05\n",
      "Epoch: 16550 | training loss: 9.5349e-04 | validation loss: 9.1343e-05\n",
      "Epoch: 16560 | training loss: 9.5324e-04 | validation loss: 9.1290e-05\n",
      "Epoch: 16570 | training loss: 9.5299e-04 | validation loss: 9.1239e-05\n",
      "Epoch: 16580 | training loss: 9.5273e-04 | validation loss: 9.1188e-05\n",
      "Epoch: 16590 | training loss: 9.5248e-04 | validation loss: 9.1137e-05\n",
      "Epoch: 16600 | training loss: 9.5222e-04 | validation loss: 9.1087e-05\n",
      "Epoch: 16610 | training loss: 9.5197e-04 | validation loss: 9.1036e-05\n",
      "Epoch: 16620 | training loss: 9.5171e-04 | validation loss: 9.0986e-05\n",
      "Epoch: 16630 | training loss: 9.5145e-04 | validation loss: 9.0936e-05\n",
      "Epoch: 16640 | training loss: 9.5119e-04 | validation loss: 9.0887e-05\n",
      "Epoch: 16650 | training loss: 9.5093e-04 | validation loss: 9.0837e-05\n",
      "Epoch: 16660 | training loss: 9.5067e-04 | validation loss: 9.0788e-05\n",
      "Epoch: 16670 | training loss: 9.5041e-04 | validation loss: 9.0739e-05\n",
      "Epoch: 16680 | training loss: 9.5015e-04 | validation loss: 9.0690e-05\n",
      "Epoch: 16690 | training loss: 9.4988e-04 | validation loss: 9.0642e-05\n",
      "Epoch: 16700 | training loss: 9.4962e-04 | validation loss: 9.0593e-05\n",
      "Epoch: 16710 | training loss: 9.4936e-04 | validation loss: 9.0545e-05\n",
      "Epoch: 16720 | training loss: 9.4909e-04 | validation loss: 9.0497e-05\n",
      "Epoch: 16730 | training loss: 9.4882e-04 | validation loss: 9.0449e-05\n",
      "Epoch: 16740 | training loss: 9.4856e-04 | validation loss: 9.0401e-05\n",
      "Epoch: 16750 | training loss: 9.4829e-04 | validation loss: 9.0354e-05\n",
      "Epoch: 16760 | training loss: 9.4802e-04 | validation loss: 9.0306e-05\n",
      "Epoch: 16770 | training loss: 9.4775e-04 | validation loss: 9.0259e-05\n",
      "Epoch: 16780 | training loss: 9.4748e-04 | validation loss: 9.0212e-05\n",
      "Epoch: 16790 | training loss: 9.4721e-04 | validation loss: 9.0165e-05\n",
      "Epoch: 16800 | training loss: 9.4694e-04 | validation loss: 9.0119e-05\n",
      "Epoch: 16810 | training loss: 9.4666e-04 | validation loss: 9.0071e-05\n",
      "Epoch: 16820 | training loss: 9.4639e-04 | validation loss: 8.9999e-05\n",
      "Epoch: 16830 | training loss: 9.4657e-04 | validation loss: 8.9537e-05\n",
      "Epoch: 16840 | training loss: 9.5852e-04 | validation loss: 9.4720e-05\n",
      "Epoch: 16850 | training loss: 9.4577e-04 | validation loss: 9.0071e-05\n",
      "Epoch: 16860 | training loss: 9.4878e-04 | validation loss: 9.4338e-05\n",
      "Epoch: 16870 | training loss: 9.4521e-04 | validation loss: 8.9856e-05\n",
      "Epoch: 16880 | training loss: 9.4536e-04 | validation loss: 8.9373e-05\n",
      "Epoch: 16890 | training loss: 9.4471e-04 | validation loss: 8.9670e-05\n",
      "Epoch: 16900 | training loss: 9.4453e-04 | validation loss: 8.9948e-05\n",
      "Epoch: 16910 | training loss: 9.4425e-04 | validation loss: 8.9678e-05\n",
      "Epoch: 16920 | training loss: 9.4402e-04 | validation loss: 8.9495e-05\n",
      "Epoch: 16930 | training loss: 9.4379e-04 | validation loss: 8.9454e-05\n",
      "Epoch: 16940 | training loss: 9.4356e-04 | validation loss: 8.9434e-05\n",
      "Epoch: 16950 | training loss: 9.4333e-04 | validation loss: 8.9407e-05\n",
      "Epoch: 16960 | training loss: 9.4309e-04 | validation loss: 8.9362e-05\n",
      "Epoch: 16970 | training loss: 9.4286e-04 | validation loss: 8.9312e-05\n",
      "Epoch: 16980 | training loss: 9.4263e-04 | validation loss: 8.9262e-05\n",
      "Epoch: 16990 | training loss: 9.4239e-04 | validation loss: 8.9212e-05\n",
      "Epoch: 17000 | training loss: 9.4216e-04 | validation loss: 8.9163e-05\n",
      "Epoch: 17010 | training loss: 9.4192e-04 | validation loss: 8.9115e-05\n",
      "Epoch: 17020 | training loss: 9.4168e-04 | validation loss: 8.9068e-05\n",
      "Epoch: 17030 | training loss: 9.4144e-04 | validation loss: 8.9020e-05\n",
      "Epoch: 17040 | training loss: 9.4121e-04 | validation loss: 8.8973e-05\n",
      "Epoch: 17050 | training loss: 9.4097e-04 | validation loss: 8.8927e-05\n",
      "Epoch: 17060 | training loss: 9.4073e-04 | validation loss: 8.8881e-05\n",
      "Epoch: 17070 | training loss: 9.4048e-04 | validation loss: 8.8835e-05\n",
      "Epoch: 17080 | training loss: 9.4024e-04 | validation loss: 8.8789e-05\n",
      "Epoch: 17090 | training loss: 9.4000e-04 | validation loss: 8.8743e-05\n",
      "Epoch: 17100 | training loss: 9.3976e-04 | validation loss: 8.8698e-05\n",
      "Epoch: 17110 | training loss: 9.3951e-04 | validation loss: 8.8653e-05\n",
      "Epoch: 17120 | training loss: 9.3927e-04 | validation loss: 8.8607e-05\n",
      "Epoch: 17130 | training loss: 9.3902e-04 | validation loss: 8.8563e-05\n",
      "Epoch: 17140 | training loss: 9.3877e-04 | validation loss: 8.8518e-05\n",
      "Epoch: 17150 | training loss: 9.3853e-04 | validation loss: 8.8473e-05\n",
      "Epoch: 17160 | training loss: 9.3828e-04 | validation loss: 8.8429e-05\n",
      "Epoch: 17170 | training loss: 9.3803e-04 | validation loss: 8.8385e-05\n",
      "Epoch: 17180 | training loss: 9.3778e-04 | validation loss: 8.8341e-05\n",
      "Epoch: 17190 | training loss: 9.3753e-04 | validation loss: 8.8297e-05\n",
      "Epoch: 17200 | training loss: 9.3728e-04 | validation loss: 8.8253e-05\n",
      "Epoch: 17210 | training loss: 9.3702e-04 | validation loss: 8.8210e-05\n",
      "Epoch: 17220 | training loss: 9.3677e-04 | validation loss: 8.8166e-05\n",
      "Epoch: 17230 | training loss: 9.3652e-04 | validation loss: 8.8123e-05\n",
      "Epoch: 17240 | training loss: 9.3626e-04 | validation loss: 8.8083e-05\n",
      "Epoch: 17250 | training loss: 9.3601e-04 | validation loss: 8.8099e-05\n",
      "Epoch: 17260 | training loss: 9.3842e-04 | validation loss: 9.1490e-05\n",
      "Epoch: 17270 | training loss: 9.5063e-04 | validation loss: 9.3951e-05\n",
      "Epoch: 17280 | training loss: 9.4213e-04 | validation loss: 9.0197e-05\n",
      "Epoch: 17290 | training loss: 9.3540e-04 | validation loss: 8.8641e-05\n",
      "Epoch: 17300 | training loss: 9.3567e-04 | validation loss: 8.9276e-05\n",
      "Epoch: 17310 | training loss: 9.3471e-04 | validation loss: 8.7638e-05\n",
      "Epoch: 17320 | training loss: 9.3455e-04 | validation loss: 8.7465e-05\n",
      "Epoch: 17330 | training loss: 9.3424e-04 | validation loss: 8.7708e-05\n",
      "Epoch: 17340 | training loss: 9.3404e-04 | validation loss: 8.7784e-05\n",
      "Epoch: 17350 | training loss: 9.3381e-04 | validation loss: 8.7649e-05\n",
      "Epoch: 17360 | training loss: 9.3359e-04 | validation loss: 8.7547e-05\n",
      "Epoch: 17370 | training loss: 9.3338e-04 | validation loss: 8.7487e-05\n",
      "Epoch: 17380 | training loss: 9.3316e-04 | validation loss: 8.7448e-05\n",
      "Epoch: 17390 | training loss: 9.3294e-04 | validation loss: 8.7412e-05\n",
      "Epoch: 17400 | training loss: 9.3273e-04 | validation loss: 8.7372e-05\n",
      "Epoch: 17410 | training loss: 9.3251e-04 | validation loss: 8.7329e-05\n",
      "Epoch: 17420 | training loss: 9.3229e-04 | validation loss: 8.7286e-05\n",
      "Epoch: 17430 | training loss: 9.3207e-04 | validation loss: 8.7242e-05\n",
      "Epoch: 17440 | training loss: 9.3185e-04 | validation loss: 8.7199e-05\n",
      "Epoch: 17450 | training loss: 9.3162e-04 | validation loss: 8.7156e-05\n",
      "Epoch: 17460 | training loss: 9.3140e-04 | validation loss: 8.7113e-05\n",
      "Epoch: 17470 | training loss: 9.3118e-04 | validation loss: 8.7071e-05\n",
      "Epoch: 17480 | training loss: 9.3095e-04 | validation loss: 8.7028e-05\n",
      "Epoch: 17490 | training loss: 9.3073e-04 | validation loss: 8.6986e-05\n",
      "Epoch: 17500 | training loss: 9.3050e-04 | validation loss: 8.6944e-05\n",
      "Epoch: 17510 | training loss: 9.3028e-04 | validation loss: 8.6902e-05\n",
      "Epoch: 17520 | training loss: 9.3005e-04 | validation loss: 8.6860e-05\n",
      "Epoch: 17530 | training loss: 9.2982e-04 | validation loss: 8.6819e-05\n",
      "Epoch: 17540 | training loss: 9.2959e-04 | validation loss: 8.6778e-05\n",
      "Epoch: 17550 | training loss: 9.2936e-04 | validation loss: 8.6737e-05\n",
      "Epoch: 17560 | training loss: 9.2913e-04 | validation loss: 8.6696e-05\n",
      "Epoch: 17570 | training loss: 9.2890e-04 | validation loss: 8.6655e-05\n",
      "Epoch: 17580 | training loss: 9.2867e-04 | validation loss: 8.6614e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17590 | training loss: 9.2844e-04 | validation loss: 8.6574e-05\n",
      "Epoch: 17600 | training loss: 9.2820e-04 | validation loss: 8.6534e-05\n",
      "Epoch: 17610 | training loss: 9.2797e-04 | validation loss: 8.6493e-05\n",
      "Epoch: 17620 | training loss: 9.2773e-04 | validation loss: 8.6454e-05\n",
      "Epoch: 17630 | training loss: 9.2750e-04 | validation loss: 8.6414e-05\n",
      "Epoch: 17640 | training loss: 9.2726e-04 | validation loss: 8.6374e-05\n",
      "Epoch: 17650 | training loss: 9.2702e-04 | validation loss: 8.6334e-05\n",
      "Epoch: 17660 | training loss: 9.2678e-04 | validation loss: 8.6294e-05\n",
      "Epoch: 17670 | training loss: 9.2654e-04 | validation loss: 8.6245e-05\n",
      "Epoch: 17680 | training loss: 9.2637e-04 | validation loss: 8.6007e-05\n",
      "Epoch: 17690 | training loss: 9.6247e-04 | validation loss: 1.0486e-04\n",
      "Epoch: 17700 | training loss: 9.3440e-04 | validation loss: 8.8975e-05\n",
      "Epoch: 17710 | training loss: 9.2871e-04 | validation loss: 8.6602e-05\n",
      "Epoch: 17720 | training loss: 9.2599e-04 | validation loss: 8.7222e-05\n",
      "Epoch: 17730 | training loss: 9.2568e-04 | validation loss: 8.6899e-05\n",
      "Epoch: 17740 | training loss: 9.2510e-04 | validation loss: 8.5837e-05\n",
      "Epoch: 17750 | training loss: 9.2493e-04 | validation loss: 8.5750e-05\n",
      "Epoch: 17760 | training loss: 9.2466e-04 | validation loss: 8.5887e-05\n",
      "Epoch: 17770 | training loss: 9.2446e-04 | validation loss: 8.5943e-05\n",
      "Epoch: 17780 | training loss: 9.2426e-04 | validation loss: 8.5877e-05\n",
      "Epoch: 17790 | training loss: 9.2405e-04 | validation loss: 8.5794e-05\n",
      "Epoch: 17800 | training loss: 9.2385e-04 | validation loss: 8.5735e-05\n",
      "Epoch: 17810 | training loss: 9.2365e-04 | validation loss: 8.5691e-05\n",
      "Epoch: 17820 | training loss: 9.2344e-04 | validation loss: 8.5653e-05\n",
      "Epoch: 17830 | training loss: 9.2324e-04 | validation loss: 8.5616e-05\n",
      "Epoch: 17840 | training loss: 9.2303e-04 | validation loss: 8.5577e-05\n",
      "Epoch: 17850 | training loss: 9.2282e-04 | validation loss: 8.5539e-05\n",
      "Epoch: 17860 | training loss: 9.2261e-04 | validation loss: 8.5501e-05\n",
      "Epoch: 17870 | training loss: 9.2240e-04 | validation loss: 8.5462e-05\n",
      "Epoch: 17880 | training loss: 9.2220e-04 | validation loss: 8.5424e-05\n",
      "Epoch: 17890 | training loss: 9.2199e-04 | validation loss: 8.5386e-05\n",
      "Epoch: 17900 | training loss: 9.2177e-04 | validation loss: 8.5348e-05\n",
      "Epoch: 17910 | training loss: 9.2156e-04 | validation loss: 8.5310e-05\n",
      "Epoch: 17920 | training loss: 9.2135e-04 | validation loss: 8.5272e-05\n",
      "Epoch: 17930 | training loss: 9.2114e-04 | validation loss: 8.5234e-05\n",
      "Epoch: 17940 | training loss: 9.2092e-04 | validation loss: 8.5197e-05\n",
      "Epoch: 17950 | training loss: 9.2071e-04 | validation loss: 8.5160e-05\n",
      "Epoch: 17960 | training loss: 9.2049e-04 | validation loss: 8.5123e-05\n",
      "Epoch: 17970 | training loss: 9.2028e-04 | validation loss: 8.5086e-05\n",
      "Epoch: 17980 | training loss: 9.2006e-04 | validation loss: 8.5049e-05\n",
      "Epoch: 17990 | training loss: 9.1984e-04 | validation loss: 8.5013e-05\n",
      "Epoch: 18000 | training loss: 9.1962e-04 | validation loss: 8.4977e-05\n",
      "Epoch: 18010 | training loss: 9.1940e-04 | validation loss: 8.4940e-05\n",
      "Epoch: 18020 | training loss: 9.1918e-04 | validation loss: 8.4904e-05\n",
      "Epoch: 18030 | training loss: 9.1896e-04 | validation loss: 8.4869e-05\n",
      "Epoch: 18040 | training loss: 9.1874e-04 | validation loss: 8.4833e-05\n",
      "Epoch: 18050 | training loss: 9.1852e-04 | validation loss: 8.4797e-05\n",
      "Epoch: 18060 | training loss: 9.1830e-04 | validation loss: 8.4762e-05\n",
      "Epoch: 18070 | training loss: 9.1807e-04 | validation loss: 8.4727e-05\n",
      "Epoch: 18080 | training loss: 9.1785e-04 | validation loss: 8.4692e-05\n",
      "Epoch: 18090 | training loss: 9.1762e-04 | validation loss: 8.4661e-05\n",
      "Epoch: 18100 | training loss: 9.1741e-04 | validation loss: 8.4744e-05\n",
      "Epoch: 18110 | training loss: 9.3078e-04 | validation loss: 9.7078e-05\n",
      "Epoch: 18120 | training loss: 9.2152e-04 | validation loss: 8.6065e-05\n",
      "Epoch: 18130 | training loss: 9.1967e-04 | validation loss: 8.8073e-05\n",
      "Epoch: 18140 | training loss: 9.1749e-04 | validation loss: 8.5987e-05\n",
      "Epoch: 18150 | training loss: 9.1690e-04 | validation loss: 8.4180e-05\n",
      "Epoch: 18160 | training loss: 9.1629e-04 | validation loss: 8.4227e-05\n",
      "Epoch: 18170 | training loss: 9.1610e-04 | validation loss: 8.4656e-05\n",
      "Epoch: 18180 | training loss: 9.1586e-04 | validation loss: 8.4478e-05\n",
      "Epoch: 18190 | training loss: 9.1566e-04 | validation loss: 8.4266e-05\n",
      "Epoch: 18200 | training loss: 9.1547e-04 | validation loss: 8.4223e-05\n",
      "Epoch: 18210 | training loss: 9.1528e-04 | validation loss: 8.4228e-05\n",
      "Epoch: 18220 | training loss: 9.1509e-04 | validation loss: 8.4215e-05\n",
      "Epoch: 18230 | training loss: 9.1490e-04 | validation loss: 8.4181e-05\n",
      "Epoch: 18240 | training loss: 9.1471e-04 | validation loss: 8.4142e-05\n",
      "Epoch: 18250 | training loss: 9.1451e-04 | validation loss: 8.4104e-05\n",
      "Epoch: 18260 | training loss: 9.1432e-04 | validation loss: 8.4067e-05\n",
      "Epoch: 18270 | training loss: 9.1413e-04 | validation loss: 8.4032e-05\n",
      "Epoch: 18280 | training loss: 9.1393e-04 | validation loss: 8.3998e-05\n",
      "Epoch: 18290 | training loss: 9.1374e-04 | validation loss: 8.3964e-05\n",
      "Epoch: 18300 | training loss: 9.1354e-04 | validation loss: 8.3930e-05\n",
      "Epoch: 18310 | training loss: 9.1334e-04 | validation loss: 8.3896e-05\n",
      "Epoch: 18320 | training loss: 9.1315e-04 | validation loss: 8.3863e-05\n",
      "Epoch: 18330 | training loss: 9.1295e-04 | validation loss: 8.3830e-05\n",
      "Epoch: 18340 | training loss: 9.1275e-04 | validation loss: 8.3797e-05\n",
      "Epoch: 18350 | training loss: 9.1255e-04 | validation loss: 8.3764e-05\n",
      "Epoch: 18360 | training loss: 9.1235e-04 | validation loss: 8.3732e-05\n",
      "Epoch: 18370 | training loss: 9.1215e-04 | validation loss: 8.3699e-05\n",
      "Epoch: 18380 | training loss: 9.1194e-04 | validation loss: 8.3667e-05\n",
      "Epoch: 18390 | training loss: 9.1174e-04 | validation loss: 8.3635e-05\n",
      "Epoch: 18400 | training loss: 9.1154e-04 | validation loss: 8.3604e-05\n",
      "Epoch: 18410 | training loss: 9.1133e-04 | validation loss: 8.3572e-05\n",
      "Epoch: 18420 | training loss: 9.1113e-04 | validation loss: 8.3540e-05\n",
      "Epoch: 18430 | training loss: 9.1092e-04 | validation loss: 8.3509e-05\n",
      "Epoch: 18440 | training loss: 9.1072e-04 | validation loss: 8.3478e-05\n",
      "Epoch: 18450 | training loss: 9.1051e-04 | validation loss: 8.3447e-05\n",
      "Epoch: 18460 | training loss: 9.1030e-04 | validation loss: 8.3416e-05\n",
      "Epoch: 18470 | training loss: 9.1009e-04 | validation loss: 8.3385e-05\n",
      "Epoch: 18480 | training loss: 9.0988e-04 | validation loss: 8.3355e-05\n",
      "Epoch: 18490 | training loss: 9.0967e-04 | validation loss: 8.3324e-05\n",
      "Epoch: 18500 | training loss: 9.0946e-04 | validation loss: 8.3294e-05\n",
      "Epoch: 18510 | training loss: 9.0925e-04 | validation loss: 8.3264e-05\n",
      "Epoch: 18520 | training loss: 9.0904e-04 | validation loss: 8.3233e-05\n",
      "Epoch: 18530 | training loss: 9.0883e-04 | validation loss: 8.3183e-05\n",
      "Epoch: 18540 | training loss: 9.0911e-04 | validation loss: 8.2926e-05\n",
      "Epoch: 18550 | training loss: 9.1923e-04 | validation loss: 8.7985e-05\n",
      "Epoch: 18560 | training loss: 9.0846e-04 | validation loss: 8.3670e-05\n",
      "Epoch: 18570 | training loss: 9.1158e-04 | validation loss: 8.7205e-05\n",
      "Epoch: 18580 | training loss: 9.0798e-04 | validation loss: 8.2918e-05\n",
      "Epoch: 18590 | training loss: 9.0810e-04 | validation loss: 8.2783e-05\n",
      "Epoch: 18600 | training loss: 9.0755e-04 | validation loss: 8.3142e-05\n",
      "Epoch: 18610 | training loss: 9.0740e-04 | validation loss: 8.3209e-05\n",
      "Epoch: 18620 | training loss: 9.0717e-04 | validation loss: 8.2940e-05\n",
      "Epoch: 18630 | training loss: 9.0700e-04 | validation loss: 8.2858e-05\n",
      "Epoch: 18640 | training loss: 9.0681e-04 | validation loss: 8.2860e-05\n",
      "Epoch: 18650 | training loss: 9.0663e-04 | validation loss: 8.2859e-05\n",
      "Epoch: 18660 | training loss: 9.0645e-04 | validation loss: 8.2838e-05\n",
      "Epoch: 18670 | training loss: 9.0627e-04 | validation loss: 8.2805e-05\n",
      "Epoch: 18680 | training loss: 9.0609e-04 | validation loss: 8.2771e-05\n",
      "Epoch: 18690 | training loss: 9.0591e-04 | validation loss: 8.2740e-05\n",
      "Epoch: 18700 | training loss: 9.0572e-04 | validation loss: 8.2710e-05\n",
      "Epoch: 18710 | training loss: 9.0554e-04 | validation loss: 8.2681e-05\n",
      "Epoch: 18720 | training loss: 9.0535e-04 | validation loss: 8.2653e-05\n",
      "Epoch: 18730 | training loss: 9.0517e-04 | validation loss: 8.2624e-05\n",
      "Epoch: 18740 | training loss: 9.0498e-04 | validation loss: 8.2596e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18750 | training loss: 9.0480e-04 | validation loss: 8.2568e-05\n",
      "Epoch: 18760 | training loss: 9.0461e-04 | validation loss: 8.2541e-05\n",
      "Epoch: 18770 | training loss: 9.0442e-04 | validation loss: 8.2513e-05\n",
      "Epoch: 18780 | training loss: 9.0423e-04 | validation loss: 8.2486e-05\n",
      "Epoch: 18790 | training loss: 9.0404e-04 | validation loss: 8.2459e-05\n",
      "Epoch: 18800 | training loss: 9.0385e-04 | validation loss: 8.2432e-05\n",
      "Epoch: 18810 | training loss: 9.0366e-04 | validation loss: 8.2405e-05\n",
      "Epoch: 18820 | training loss: 9.0347e-04 | validation loss: 8.2379e-05\n",
      "Epoch: 18830 | training loss: 9.0328e-04 | validation loss: 8.2352e-05\n",
      "Epoch: 18840 | training loss: 9.0309e-04 | validation loss: 8.2326e-05\n",
      "Epoch: 18850 | training loss: 9.0289e-04 | validation loss: 8.2300e-05\n",
      "Epoch: 18860 | training loss: 9.0270e-04 | validation loss: 8.2274e-05\n",
      "Epoch: 18870 | training loss: 9.0250e-04 | validation loss: 8.2248e-05\n",
      "Epoch: 18880 | training loss: 9.0231e-04 | validation loss: 8.2222e-05\n",
      "Epoch: 18890 | training loss: 9.0211e-04 | validation loss: 8.2197e-05\n",
      "Epoch: 18900 | training loss: 9.0192e-04 | validation loss: 8.2171e-05\n",
      "Epoch: 18910 | training loss: 9.0172e-04 | validation loss: 8.2146e-05\n",
      "Epoch: 18920 | training loss: 9.0152e-04 | validation loss: 8.2121e-05\n",
      "Epoch: 18930 | training loss: 9.0132e-04 | validation loss: 8.2096e-05\n",
      "Epoch: 18940 | training loss: 9.0112e-04 | validation loss: 8.2071e-05\n",
      "Epoch: 18950 | training loss: 9.0092e-04 | validation loss: 8.2046e-05\n",
      "Epoch: 18960 | training loss: 9.0072e-04 | validation loss: 8.2026e-05\n",
      "Epoch: 18970 | training loss: 9.0054e-04 | validation loss: 8.2124e-05\n",
      "Epoch: 18980 | training loss: 9.1819e-04 | validation loss: 9.7159e-05\n",
      "Epoch: 18990 | training loss: 9.0130e-04 | validation loss: 8.2233e-05\n",
      "Epoch: 19000 | training loss: 9.0403e-04 | validation loss: 8.6294e-05\n",
      "Epoch: 19010 | training loss: 9.0037e-04 | validation loss: 8.2841e-05\n",
      "Epoch: 19020 | training loss: 9.0015e-04 | validation loss: 8.1719e-05\n",
      "Epoch: 19030 | training loss: 8.9953e-04 | validation loss: 8.1729e-05\n",
      "Epoch: 19040 | training loss: 8.9935e-04 | validation loss: 8.2047e-05\n",
      "Epoch: 19050 | training loss: 8.9915e-04 | validation loss: 8.1937e-05\n",
      "Epoch: 19060 | training loss: 8.9896e-04 | validation loss: 8.1777e-05\n",
      "Epoch: 19070 | training loss: 8.9879e-04 | validation loss: 8.1728e-05\n",
      "Epoch: 19080 | training loss: 8.9862e-04 | validation loss: 8.1724e-05\n",
      "Epoch: 19090 | training loss: 8.9845e-04 | validation loss: 8.1720e-05\n",
      "Epoch: 19100 | training loss: 8.9828e-04 | validation loss: 8.1703e-05\n",
      "Epoch: 19110 | training loss: 8.9810e-04 | validation loss: 8.1679e-05\n",
      "Epoch: 19120 | training loss: 8.9793e-04 | validation loss: 8.1655e-05\n",
      "Epoch: 19130 | training loss: 8.9776e-04 | validation loss: 8.1630e-05\n",
      "Epoch: 19140 | training loss: 8.9758e-04 | validation loss: 8.1607e-05\n",
      "Epoch: 19150 | training loss: 8.9741e-04 | validation loss: 8.1584e-05\n",
      "Epoch: 19160 | training loss: 8.9723e-04 | validation loss: 8.1561e-05\n",
      "Epoch: 19170 | training loss: 8.9705e-04 | validation loss: 8.1538e-05\n",
      "Epoch: 19180 | training loss: 8.9688e-04 | validation loss: 8.1516e-05\n",
      "Epoch: 19190 | training loss: 8.9670e-04 | validation loss: 8.1494e-05\n",
      "Epoch: 19200 | training loss: 8.9652e-04 | validation loss: 8.1472e-05\n",
      "Epoch: 19210 | training loss: 8.9634e-04 | validation loss: 8.1451e-05\n",
      "Epoch: 19220 | training loss: 8.9616e-04 | validation loss: 8.1429e-05\n",
      "Epoch: 19230 | training loss: 8.9598e-04 | validation loss: 8.1408e-05\n",
      "Epoch: 19240 | training loss: 8.9580e-04 | validation loss: 8.1387e-05\n",
      "Epoch: 19250 | training loss: 8.9562e-04 | validation loss: 8.1366e-05\n",
      "Epoch: 19260 | training loss: 8.9544e-04 | validation loss: 8.1345e-05\n",
      "Epoch: 19270 | training loss: 8.9525e-04 | validation loss: 8.1324e-05\n",
      "Epoch: 19280 | training loss: 8.9507e-04 | validation loss: 8.1304e-05\n",
      "Epoch: 19290 | training loss: 8.9488e-04 | validation loss: 8.1283e-05\n",
      "Epoch: 19300 | training loss: 8.9470e-04 | validation loss: 8.1263e-05\n",
      "Epoch: 19310 | training loss: 8.9451e-04 | validation loss: 8.1242e-05\n",
      "Epoch: 19320 | training loss: 8.9433e-04 | validation loss: 8.1222e-05\n",
      "Epoch: 19330 | training loss: 8.9414e-04 | validation loss: 8.1202e-05\n",
      "Epoch: 19340 | training loss: 8.9395e-04 | validation loss: 8.1183e-05\n",
      "Epoch: 19350 | training loss: 8.9376e-04 | validation loss: 8.1163e-05\n",
      "Epoch: 19360 | training loss: 8.9357e-04 | validation loss: 8.1143e-05\n",
      "Epoch: 19370 | training loss: 8.9338e-04 | validation loss: 8.1124e-05\n",
      "Epoch: 19380 | training loss: 8.9319e-04 | validation loss: 8.1104e-05\n",
      "Epoch: 19390 | training loss: 8.9300e-04 | validation loss: 8.1075e-05\n",
      "Epoch: 19400 | training loss: 8.9295e-04 | validation loss: 8.0908e-05\n",
      "Epoch: 19410 | training loss: 9.3610e-04 | validation loss: 1.0611e-04\n",
      "Epoch: 19420 | training loss: 9.0496e-04 | validation loss: 8.7428e-05\n",
      "Epoch: 19430 | training loss: 8.9239e-04 | validation loss: 8.0845e-05\n",
      "Epoch: 19440 | training loss: 8.9355e-04 | validation loss: 8.2786e-05\n",
      "Epoch: 19450 | training loss: 8.9200e-04 | validation loss: 8.1115e-05\n",
      "Epoch: 19460 | training loss: 8.9199e-04 | validation loss: 8.0819e-05\n",
      "Epoch: 19470 | training loss: 8.9167e-04 | validation loss: 8.0879e-05\n",
      "Epoch: 19480 | training loss: 8.9151e-04 | validation loss: 8.1009e-05\n",
      "Epoch: 19490 | training loss: 8.9134e-04 | validation loss: 8.0983e-05\n",
      "Epoch: 19500 | training loss: 8.9117e-04 | validation loss: 8.0915e-05\n",
      "Epoch: 19510 | training loss: 8.9101e-04 | validation loss: 8.0873e-05\n",
      "Epoch: 19520 | training loss: 8.9085e-04 | validation loss: 8.0852e-05\n",
      "Epoch: 19530 | training loss: 8.9068e-04 | validation loss: 8.0838e-05\n",
      "Epoch: 19540 | training loss: 8.9052e-04 | validation loss: 8.0824e-05\n",
      "Epoch: 19550 | training loss: 8.9035e-04 | validation loss: 8.0808e-05\n",
      "Epoch: 19560 | training loss: 8.9019e-04 | validation loss: 8.0792e-05\n",
      "Epoch: 19570 | training loss: 8.9002e-04 | validation loss: 8.0776e-05\n",
      "Epoch: 19580 | training loss: 8.8985e-04 | validation loss: 8.0759e-05\n",
      "Epoch: 19590 | training loss: 8.8969e-04 | validation loss: 8.0742e-05\n",
      "Epoch: 19600 | training loss: 8.8952e-04 | validation loss: 8.0726e-05\n",
      "Epoch: 19610 | training loss: 8.8935e-04 | validation loss: 8.0710e-05\n",
      "Epoch: 19620 | training loss: 8.8918e-04 | validation loss: 8.0693e-05\n",
      "Epoch: 19630 | training loss: 8.8901e-04 | validation loss: 8.0677e-05\n",
      "Epoch: 19640 | training loss: 8.8884e-04 | validation loss: 8.0661e-05\n",
      "Epoch: 19650 | training loss: 8.8867e-04 | validation loss: 8.0645e-05\n",
      "Epoch: 19660 | training loss: 8.8849e-04 | validation loss: 8.0629e-05\n",
      "Epoch: 19670 | training loss: 8.8832e-04 | validation loss: 8.0614e-05\n",
      "Epoch: 19680 | training loss: 8.8815e-04 | validation loss: 8.0598e-05\n",
      "Epoch: 19690 | training loss: 8.8797e-04 | validation loss: 8.0583e-05\n",
      "Epoch: 19700 | training loss: 8.8780e-04 | validation loss: 8.0568e-05\n",
      "Epoch: 19710 | training loss: 8.8762e-04 | validation loss: 8.0552e-05\n",
      "Epoch: 19720 | training loss: 8.8745e-04 | validation loss: 8.0537e-05\n",
      "Epoch: 19730 | training loss: 8.8727e-04 | validation loss: 8.0522e-05\n",
      "Epoch: 19740 | training loss: 8.8709e-04 | validation loss: 8.0507e-05\n",
      "Epoch: 19750 | training loss: 8.8692e-04 | validation loss: 8.0493e-05\n",
      "Epoch: 19760 | training loss: 8.8674e-04 | validation loss: 8.0478e-05\n",
      "Epoch: 19770 | training loss: 8.8656e-04 | validation loss: 8.0463e-05\n",
      "Epoch: 19780 | training loss: 8.8638e-04 | validation loss: 8.0449e-05\n",
      "Epoch: 19790 | training loss: 8.8620e-04 | validation loss: 8.0435e-05\n",
      "Epoch: 19800 | training loss: 8.8602e-04 | validation loss: 8.0420e-05\n",
      "Epoch: 19810 | training loss: 8.8583e-04 | validation loss: 8.0402e-05\n",
      "Epoch: 19820 | training loss: 8.8568e-04 | validation loss: 8.0308e-05\n",
      "Epoch: 19830 | training loss: 9.0678e-04 | validation loss: 9.1929e-05\n",
      "Epoch: 19840 | training loss: 8.8562e-04 | validation loss: 8.0762e-05\n",
      "Epoch: 19850 | training loss: 8.8944e-04 | validation loss: 8.2182e-05\n",
      "Epoch: 19860 | training loss: 8.8558e-04 | validation loss: 8.0302e-05\n",
      "Epoch: 19870 | training loss: 8.8527e-04 | validation loss: 8.1015e-05\n",
      "Epoch: 19880 | training loss: 8.8481e-04 | validation loss: 8.0608e-05\n",
      "Epoch: 19890 | training loss: 8.8457e-04 | validation loss: 8.0245e-05\n",
      "Epoch: 19900 | training loss: 8.8442e-04 | validation loss: 8.0229e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19910 | training loss: 8.8424e-04 | validation loss: 8.0277e-05\n",
      "Epoch: 19920 | training loss: 8.8409e-04 | validation loss: 8.0302e-05\n",
      "Epoch: 19930 | training loss: 8.8393e-04 | validation loss: 8.0290e-05\n",
      "Epoch: 19940 | training loss: 8.8378e-04 | validation loss: 8.0268e-05\n",
      "Epoch: 19950 | training loss: 8.8362e-04 | validation loss: 8.0248e-05\n",
      "Epoch: 19960 | training loss: 8.8346e-04 | validation loss: 8.0232e-05\n",
      "Epoch: 19970 | training loss: 8.8331e-04 | validation loss: 8.0219e-05\n",
      "Epoch: 19980 | training loss: 8.8315e-04 | validation loss: 8.0207e-05\n",
      "Epoch: 19990 | training loss: 8.8299e-04 | validation loss: 8.0195e-05\n",
      "Epoch: 20000 | training loss: 8.8283e-04 | validation loss: 8.0183e-05\n",
      "Epoch: 20010 | training loss: 8.8267e-04 | validation loss: 8.0171e-05\n",
      "Epoch: 20020 | training loss: 8.8251e-04 | validation loss: 8.0160e-05\n",
      "Epoch: 20030 | training loss: 8.8235e-04 | validation loss: 8.0148e-05\n",
      "Epoch: 20040 | training loss: 8.8219e-04 | validation loss: 8.0137e-05\n",
      "Epoch: 20050 | training loss: 8.8203e-04 | validation loss: 8.0126e-05\n",
      "Epoch: 20060 | training loss: 8.8186e-04 | validation loss: 8.0114e-05\n",
      "Epoch: 20070 | training loss: 8.8170e-04 | validation loss: 8.0103e-05\n",
      "Epoch: 20080 | training loss: 8.8154e-04 | validation loss: 8.0092e-05\n",
      "Epoch: 20090 | training loss: 8.8137e-04 | validation loss: 8.0082e-05\n",
      "Epoch: 20100 | training loss: 8.8121e-04 | validation loss: 8.0071e-05\n",
      "Epoch: 20110 | training loss: 8.8104e-04 | validation loss: 8.0060e-05\n",
      "Epoch: 20120 | training loss: 8.8087e-04 | validation loss: 8.0050e-05\n",
      "Epoch: 20130 | training loss: 8.8071e-04 | validation loss: 8.0039e-05\n",
      "Epoch: 20140 | training loss: 8.8054e-04 | validation loss: 8.0029e-05\n",
      "Epoch: 20150 | training loss: 8.8037e-04 | validation loss: 8.0018e-05\n",
      "Epoch: 20160 | training loss: 8.8020e-04 | validation loss: 8.0008e-05\n",
      "Epoch: 20170 | training loss: 8.8003e-04 | validation loss: 7.9998e-05\n",
      "Epoch: 20180 | training loss: 8.7986e-04 | validation loss: 7.9988e-05\n",
      "Epoch: 20190 | training loss: 8.7969e-04 | validation loss: 7.9978e-05\n",
      "Epoch: 20200 | training loss: 8.7952e-04 | validation loss: 7.9968e-05\n",
      "Epoch: 20210 | training loss: 8.7935e-04 | validation loss: 7.9958e-05\n",
      "Epoch: 20220 | training loss: 8.7917e-04 | validation loss: 7.9949e-05\n",
      "Epoch: 20230 | training loss: 8.7900e-04 | validation loss: 7.9946e-05\n",
      "Epoch: 20240 | training loss: 8.7893e-04 | validation loss: 8.0173e-05\n",
      "Epoch: 20250 | training loss: 9.2025e-04 | validation loss: 1.1157e-04\n",
      "Epoch: 20260 | training loss: 8.9001e-04 | validation loss: 8.9661e-05\n",
      "Epoch: 20270 | training loss: 8.8022e-04 | validation loss: 8.1975e-05\n",
      "Epoch: 20280 | training loss: 8.7899e-04 | validation loss: 8.0020e-05\n",
      "Epoch: 20290 | training loss: 8.7843e-04 | validation loss: 7.9832e-05\n",
      "Epoch: 20300 | training loss: 8.7797e-04 | validation loss: 8.0025e-05\n",
      "Epoch: 20310 | training loss: 8.7785e-04 | validation loss: 8.0072e-05\n",
      "Epoch: 20320 | training loss: 8.7763e-04 | validation loss: 7.9905e-05\n",
      "Epoch: 20330 | training loss: 8.7749e-04 | validation loss: 7.9844e-05\n",
      "Epoch: 20340 | training loss: 8.7734e-04 | validation loss: 7.9838e-05\n",
      "Epoch: 20350 | training loss: 8.7719e-04 | validation loss: 7.9844e-05\n",
      "Epoch: 20360 | training loss: 8.7704e-04 | validation loss: 7.9846e-05\n",
      "Epoch: 20370 | training loss: 8.7689e-04 | validation loss: 7.9841e-05\n",
      "Epoch: 20380 | training loss: 8.7674e-04 | validation loss: 7.9833e-05\n",
      "Epoch: 20390 | training loss: 8.7659e-04 | validation loss: 7.9825e-05\n",
      "Epoch: 20400 | training loss: 8.7644e-04 | validation loss: 7.9817e-05\n",
      "Epoch: 20410 | training loss: 8.7629e-04 | validation loss: 7.9808e-05\n",
      "Epoch: 20420 | training loss: 8.7614e-04 | validation loss: 7.9800e-05\n",
      "Epoch: 20430 | training loss: 8.7599e-04 | validation loss: 7.9793e-05\n",
      "Epoch: 20440 | training loss: 8.7583e-04 | validation loss: 7.9785e-05\n",
      "Epoch: 20450 | training loss: 8.7568e-04 | validation loss: 7.9778e-05\n",
      "Epoch: 20460 | training loss: 8.7553e-04 | validation loss: 7.9771e-05\n",
      "Epoch: 20470 | training loss: 8.7537e-04 | validation loss: 7.9763e-05\n",
      "Epoch: 20480 | training loss: 8.7522e-04 | validation loss: 7.9756e-05\n",
      "Epoch: 20490 | training loss: 8.7506e-04 | validation loss: 7.9749e-05\n",
      "Epoch: 20500 | training loss: 8.7490e-04 | validation loss: 7.9742e-05\n",
      "Epoch: 20510 | training loss: 8.7475e-04 | validation loss: 7.9735e-05\n",
      "Epoch: 20520 | training loss: 8.7459e-04 | validation loss: 7.9728e-05\n",
      "Epoch: 20530 | training loss: 8.7443e-04 | validation loss: 7.9721e-05\n",
      "Epoch: 20540 | training loss: 8.7427e-04 | validation loss: 7.9714e-05\n",
      "Epoch: 20550 | training loss: 8.7411e-04 | validation loss: 7.9707e-05\n",
      "Epoch: 20560 | training loss: 8.7395e-04 | validation loss: 7.9701e-05\n",
      "Epoch: 20570 | training loss: 8.7379e-04 | validation loss: 7.9694e-05\n",
      "Epoch: 20580 | training loss: 8.7363e-04 | validation loss: 7.9687e-05\n",
      "Epoch: 20590 | training loss: 8.7347e-04 | validation loss: 7.9681e-05\n",
      "Epoch: 20600 | training loss: 8.7330e-04 | validation loss: 7.9674e-05\n",
      "Epoch: 20610 | training loss: 8.7314e-04 | validation loss: 7.9668e-05\n",
      "Epoch: 20620 | training loss: 8.7298e-04 | validation loss: 7.9661e-05\n",
      "Epoch: 20630 | training loss: 8.7281e-04 | validation loss: 7.9655e-05\n",
      "Epoch: 20640 | training loss: 8.7265e-04 | validation loss: 7.9649e-05\n",
      "Epoch: 20650 | training loss: 8.7248e-04 | validation loss: 7.9660e-05\n",
      "Epoch: 20660 | training loss: 8.7325e-04 | validation loss: 8.0713e-05\n",
      "Epoch: 20670 | training loss: 8.7309e-04 | validation loss: 8.0795e-05\n",
      "Epoch: 20680 | training loss: 8.7432e-04 | validation loss: 8.0478e-05\n",
      "Epoch: 20690 | training loss: 8.7474e-04 | validation loss: 8.0845e-05\n",
      "Epoch: 20700 | training loss: 8.7187e-04 | validation loss: 7.9884e-05\n",
      "Epoch: 20710 | training loss: 8.7199e-04 | validation loss: 8.0160e-05\n",
      "Epoch: 20720 | training loss: 8.7147e-04 | validation loss: 7.9623e-05\n",
      "Epoch: 20730 | training loss: 8.7137e-04 | validation loss: 7.9554e-05\n",
      "Epoch: 20740 | training loss: 8.7119e-04 | validation loss: 7.9584e-05\n",
      "Epoch: 20750 | training loss: 8.7105e-04 | validation loss: 7.9625e-05\n",
      "Epoch: 20760 | training loss: 8.7091e-04 | validation loss: 7.9628e-05\n",
      "Epoch: 20770 | training loss: 8.7077e-04 | validation loss: 7.9614e-05\n",
      "Epoch: 20780 | training loss: 8.7063e-04 | validation loss: 7.9600e-05\n",
      "Epoch: 20790 | training loss: 8.7048e-04 | validation loss: 7.9592e-05\n",
      "Epoch: 20800 | training loss: 8.7034e-04 | validation loss: 7.9586e-05\n",
      "Epoch: 20810 | training loss: 8.7020e-04 | validation loss: 7.9581e-05\n",
      "Epoch: 20820 | training loss: 8.7005e-04 | validation loss: 7.9576e-05\n",
      "Epoch: 20830 | training loss: 8.6991e-04 | validation loss: 7.9572e-05\n",
      "Epoch: 20840 | training loss: 8.6977e-04 | validation loss: 7.9567e-05\n",
      "Epoch: 20850 | training loss: 8.6962e-04 | validation loss: 7.9563e-05\n",
      "Epoch: 20860 | training loss: 8.6947e-04 | validation loss: 7.9558e-05\n",
      "Epoch: 20870 | training loss: 8.6933e-04 | validation loss: 7.9554e-05\n",
      "Epoch: 20880 | training loss: 8.6918e-04 | validation loss: 7.9549e-05\n",
      "Epoch: 20890 | training loss: 8.6903e-04 | validation loss: 7.9544e-05\n",
      "Epoch: 20900 | training loss: 8.6889e-04 | validation loss: 7.9540e-05\n",
      "Epoch: 20910 | training loss: 8.6874e-04 | validation loss: 7.9535e-05\n",
      "Epoch: 20920 | training loss: 8.6859e-04 | validation loss: 7.9531e-05\n",
      "Epoch: 20930 | training loss: 8.6844e-04 | validation loss: 7.9527e-05\n",
      "Epoch: 20940 | training loss: 8.6829e-04 | validation loss: 7.9522e-05\n",
      "Epoch: 20950 | training loss: 8.6814e-04 | validation loss: 7.9518e-05\n",
      "Epoch: 20960 | training loss: 8.6798e-04 | validation loss: 7.9514e-05\n",
      "Epoch: 20970 | training loss: 8.6783e-04 | validation loss: 7.9509e-05\n",
      "Epoch: 20980 | training loss: 8.6768e-04 | validation loss: 7.9505e-05\n",
      "Epoch: 20990 | training loss: 8.6753e-04 | validation loss: 7.9501e-05\n",
      "Epoch: 21000 | training loss: 8.6737e-04 | validation loss: 7.9496e-05\n",
      "Epoch: 21010 | training loss: 8.6722e-04 | validation loss: 7.9492e-05\n",
      "Epoch: 21020 | training loss: 8.6706e-04 | validation loss: 7.9488e-05\n",
      "Epoch: 21030 | training loss: 8.6691e-04 | validation loss: 7.9484e-05\n",
      "Epoch: 21040 | training loss: 8.6675e-04 | validation loss: 7.9480e-05\n",
      "Epoch: 21050 | training loss: 8.6659e-04 | validation loss: 7.9475e-05\n",
      "Epoch: 21060 | training loss: 8.6644e-04 | validation loss: 7.9471e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21070 | training loss: 8.6628e-04 | validation loss: 7.9464e-05\n",
      "Epoch: 21080 | training loss: 8.6617e-04 | validation loss: 7.9405e-05\n",
      "Epoch: 21090 | training loss: 8.9988e-04 | validation loss: 9.9891e-05\n",
      "Epoch: 21100 | training loss: 8.7189e-04 | validation loss: 8.2472e-05\n",
      "Epoch: 21110 | training loss: 8.7002e-04 | validation loss: 8.1697e-05\n",
      "Epoch: 21120 | training loss: 8.6623e-04 | validation loss: 8.0280e-05\n",
      "Epoch: 21130 | training loss: 8.6581e-04 | validation loss: 7.9948e-05\n",
      "Epoch: 21140 | training loss: 8.6544e-04 | validation loss: 7.9413e-05\n",
      "Epoch: 21150 | training loss: 8.6522e-04 | validation loss: 7.9415e-05\n",
      "Epoch: 21160 | training loss: 8.6505e-04 | validation loss: 7.9512e-05\n",
      "Epoch: 21170 | training loss: 8.6492e-04 | validation loss: 7.9504e-05\n",
      "Epoch: 21180 | training loss: 8.6478e-04 | validation loss: 7.9457e-05\n",
      "Epoch: 21190 | training loss: 8.6464e-04 | validation loss: 7.9441e-05\n",
      "Epoch: 21200 | training loss: 8.6451e-04 | validation loss: 7.9441e-05\n",
      "Epoch: 21210 | training loss: 8.6438e-04 | validation loss: 7.9444e-05\n",
      "Epoch: 21220 | training loss: 8.6424e-04 | validation loss: 7.9444e-05\n",
      "Epoch: 21230 | training loss: 8.6411e-04 | validation loss: 7.9441e-05\n",
      "Epoch: 21240 | training loss: 8.6397e-04 | validation loss: 7.9438e-05\n",
      "Epoch: 21250 | training loss: 8.6383e-04 | validation loss: 7.9435e-05\n",
      "Epoch: 21260 | training loss: 8.6370e-04 | validation loss: 7.9432e-05\n",
      "Epoch: 21270 | training loss: 8.6356e-04 | validation loss: 7.9429e-05\n",
      "Epoch: 21280 | training loss: 8.6342e-04 | validation loss: 7.9426e-05\n",
      "Epoch: 21290 | training loss: 8.6328e-04 | validation loss: 7.9423e-05\n",
      "Epoch: 21300 | training loss: 8.6315e-04 | validation loss: 7.9420e-05\n",
      "Epoch: 21310 | training loss: 8.6301e-04 | validation loss: 7.9417e-05\n",
      "Epoch: 21320 | training loss: 8.6287e-04 | validation loss: 7.9414e-05\n",
      "Epoch: 21330 | training loss: 8.6273e-04 | validation loss: 7.9411e-05\n",
      "Epoch: 21340 | training loss: 8.6258e-04 | validation loss: 7.9408e-05\n",
      "Epoch: 21350 | training loss: 8.6244e-04 | validation loss: 7.9405e-05\n",
      "Epoch: 21360 | training loss: 8.6230e-04 | validation loss: 7.9402e-05\n",
      "Epoch: 21370 | training loss: 8.6216e-04 | validation loss: 7.9399e-05\n",
      "Epoch: 21380 | training loss: 8.6201e-04 | validation loss: 7.9396e-05\n",
      "Epoch: 21390 | training loss: 8.6187e-04 | validation loss: 7.9393e-05\n",
      "Epoch: 21400 | training loss: 8.6173e-04 | validation loss: 7.9390e-05\n",
      "Epoch: 21410 | training loss: 8.6158e-04 | validation loss: 7.9387e-05\n",
      "Epoch: 21420 | training loss: 8.6144e-04 | validation loss: 7.9384e-05\n",
      "Epoch: 21430 | training loss: 8.6129e-04 | validation loss: 7.9381e-05\n",
      "Epoch: 21440 | training loss: 8.6114e-04 | validation loss: 7.9378e-05\n",
      "Epoch: 21450 | training loss: 8.6100e-04 | validation loss: 7.9375e-05\n",
      "Epoch: 21460 | training loss: 8.6085e-04 | validation loss: 7.9372e-05\n",
      "Epoch: 21470 | training loss: 8.6070e-04 | validation loss: 7.9369e-05\n",
      "Epoch: 21480 | training loss: 8.6055e-04 | validation loss: 7.9366e-05\n",
      "Epoch: 21490 | training loss: 8.6040e-04 | validation loss: 7.9363e-05\n",
      "Epoch: 21500 | training loss: 8.6025e-04 | validation loss: 7.9360e-05\n",
      "Epoch: 21510 | training loss: 8.6010e-04 | validation loss: 7.9354e-05\n",
      "Epoch: 21520 | training loss: 8.5998e-04 | validation loss: 7.9312e-05\n",
      "Epoch: 21530 | training loss: 8.8283e-04 | validation loss: 9.3081e-05\n",
      "Epoch: 21540 | training loss: 8.5999e-04 | validation loss: 7.9455e-05\n",
      "Epoch: 21550 | training loss: 8.6552e-04 | validation loss: 8.2725e-05\n",
      "Epoch: 21560 | training loss: 8.5948e-04 | validation loss: 7.9411e-05\n",
      "Epoch: 21570 | training loss: 8.6001e-04 | validation loss: 8.0130e-05\n",
      "Epoch: 21580 | training loss: 8.5919e-04 | validation loss: 7.9333e-05\n",
      "Epoch: 21590 | training loss: 8.5913e-04 | validation loss: 7.9318e-05\n",
      "Epoch: 21600 | training loss: 8.5892e-04 | validation loss: 7.9370e-05\n",
      "Epoch: 21610 | training loss: 8.5880e-04 | validation loss: 7.9403e-05\n",
      "Epoch: 21620 | training loss: 8.5867e-04 | validation loss: 7.9366e-05\n",
      "Epoch: 21630 | training loss: 8.5854e-04 | validation loss: 7.9346e-05\n",
      "Epoch: 21640 | training loss: 8.5841e-04 | validation loss: 7.9343e-05\n",
      "Epoch: 21650 | training loss: 8.5829e-04 | validation loss: 7.9345e-05\n",
      "Epoch: 21660 | training loss: 8.5816e-04 | validation loss: 7.9345e-05\n",
      "Epoch: 21670 | training loss: 8.5803e-04 | validation loss: 7.9344e-05\n",
      "Epoch: 21680 | training loss: 8.5790e-04 | validation loss: 7.9342e-05\n",
      "Epoch: 21690 | training loss: 8.5777e-04 | validation loss: 7.9339e-05\n",
      "Epoch: 21700 | training loss: 8.5764e-04 | validation loss: 7.9337e-05\n",
      "Epoch: 21710 | training loss: 8.5751e-04 | validation loss: 7.9334e-05\n",
      "Epoch: 21720 | training loss: 8.5738e-04 | validation loss: 7.9332e-05\n",
      "Epoch: 21730 | training loss: 8.5725e-04 | validation loss: 7.9329e-05\n",
      "Epoch: 21740 | training loss: 8.5711e-04 | validation loss: 7.9327e-05\n",
      "Epoch: 21750 | training loss: 8.5698e-04 | validation loss: 7.9324e-05\n",
      "Epoch: 21760 | training loss: 8.5685e-04 | validation loss: 7.9322e-05\n",
      "Epoch: 21770 | training loss: 8.5671e-04 | validation loss: 7.9319e-05\n",
      "Epoch: 21780 | training loss: 8.5658e-04 | validation loss: 7.9316e-05\n",
      "Epoch: 21790 | training loss: 8.5644e-04 | validation loss: 7.9314e-05\n",
      "Epoch: 21800 | training loss: 8.5631e-04 | validation loss: 7.9311e-05\n",
      "Epoch: 21810 | training loss: 8.5617e-04 | validation loss: 7.9309e-05\n",
      "Epoch: 21820 | training loss: 8.5604e-04 | validation loss: 7.9306e-05\n",
      "Epoch: 21830 | training loss: 8.5590e-04 | validation loss: 7.9303e-05\n",
      "Epoch: 21840 | training loss: 8.5576e-04 | validation loss: 7.9301e-05\n",
      "Epoch: 21850 | training loss: 8.5562e-04 | validation loss: 7.9298e-05\n",
      "Epoch: 21860 | training loss: 8.5548e-04 | validation loss: 7.9295e-05\n",
      "Epoch: 21870 | training loss: 8.5534e-04 | validation loss: 7.9292e-05\n",
      "Epoch: 21880 | training loss: 8.5520e-04 | validation loss: 7.9289e-05\n",
      "Epoch: 21890 | training loss: 8.5506e-04 | validation loss: 7.9287e-05\n",
      "Epoch: 21900 | training loss: 8.5492e-04 | validation loss: 7.9284e-05\n",
      "Epoch: 21910 | training loss: 8.5478e-04 | validation loss: 7.9281e-05\n",
      "Epoch: 21920 | training loss: 8.5464e-04 | validation loss: 7.9278e-05\n",
      "Epoch: 21930 | training loss: 8.5450e-04 | validation loss: 7.9275e-05\n",
      "Epoch: 21940 | training loss: 8.5435e-04 | validation loss: 7.9272e-05\n",
      "Epoch: 21950 | training loss: 8.5421e-04 | validation loss: 7.9268e-05\n",
      "Epoch: 21960 | training loss: 8.5407e-04 | validation loss: 7.9243e-05\n",
      "Epoch: 21970 | training loss: 8.6102e-04 | validation loss: 8.3169e-05\n",
      "Epoch: 21980 | training loss: 8.7464e-04 | validation loss: 9.5176e-05\n",
      "Epoch: 21990 | training loss: 8.5387e-04 | validation loss: 7.9545e-05\n",
      "Epoch: 22000 | training loss: 8.5606e-04 | validation loss: 8.0519e-05\n",
      "Epoch: 22010 | training loss: 8.5346e-04 | validation loss: 7.9299e-05\n",
      "Epoch: 22020 | training loss: 8.5363e-04 | validation loss: 7.9649e-05\n",
      "Epoch: 22030 | training loss: 8.5320e-04 | validation loss: 7.9271e-05\n",
      "Epoch: 22040 | training loss: 8.5312e-04 | validation loss: 7.9234e-05\n",
      "Epoch: 22050 | training loss: 8.5296e-04 | validation loss: 7.9255e-05\n",
      "Epoch: 22060 | training loss: 8.5284e-04 | validation loss: 7.9282e-05\n",
      "Epoch: 22070 | training loss: 8.5272e-04 | validation loss: 7.9277e-05\n",
      "Epoch: 22080 | training loss: 8.5259e-04 | validation loss: 7.9264e-05\n",
      "Epoch: 22090 | training loss: 8.5247e-04 | validation loss: 7.9256e-05\n",
      "Epoch: 22100 | training loss: 8.5235e-04 | validation loss: 7.9253e-05\n",
      "Epoch: 22110 | training loss: 8.5222e-04 | validation loss: 7.9251e-05\n",
      "Epoch: 22120 | training loss: 8.5210e-04 | validation loss: 7.9249e-05\n",
      "Epoch: 22130 | training loss: 8.5198e-04 | validation loss: 7.9247e-05\n",
      "Epoch: 22140 | training loss: 8.5185e-04 | validation loss: 7.9244e-05\n",
      "Epoch: 22150 | training loss: 8.5173e-04 | validation loss: 7.9242e-05\n",
      "Epoch: 22160 | training loss: 8.5160e-04 | validation loss: 7.9239e-05\n",
      "Epoch: 22170 | training loss: 8.5147e-04 | validation loss: 7.9236e-05\n",
      "Epoch: 22180 | training loss: 8.5135e-04 | validation loss: 7.9233e-05\n",
      "Epoch: 22190 | training loss: 8.5122e-04 | validation loss: 7.9231e-05\n",
      "Epoch: 22200 | training loss: 8.5109e-04 | validation loss: 7.9228e-05\n",
      "Epoch: 22210 | training loss: 8.5096e-04 | validation loss: 7.9225e-05\n",
      "Epoch: 22220 | training loss: 8.5084e-04 | validation loss: 7.9222e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22230 | training loss: 8.5071e-04 | validation loss: 7.9219e-05\n",
      "Epoch: 22240 | training loss: 8.5058e-04 | validation loss: 7.9216e-05\n",
      "Epoch: 22250 | training loss: 8.5045e-04 | validation loss: 7.9213e-05\n",
      "Epoch: 22260 | training loss: 8.5032e-04 | validation loss: 7.9209e-05\n",
      "Epoch: 22270 | training loss: 8.5018e-04 | validation loss: 7.9206e-05\n",
      "Epoch: 22280 | training loss: 8.5005e-04 | validation loss: 7.9203e-05\n",
      "Epoch: 22290 | training loss: 8.4992e-04 | validation loss: 7.9200e-05\n",
      "Epoch: 22300 | training loss: 8.4979e-04 | validation loss: 7.9197e-05\n",
      "Epoch: 22310 | training loss: 8.4965e-04 | validation loss: 7.9193e-05\n",
      "Epoch: 22320 | training loss: 8.4952e-04 | validation loss: 7.9190e-05\n",
      "Epoch: 22330 | training loss: 8.4939e-04 | validation loss: 7.9186e-05\n",
      "Epoch: 22340 | training loss: 8.4925e-04 | validation loss: 7.9183e-05\n",
      "Epoch: 22350 | training loss: 8.4912e-04 | validation loss: 7.9180e-05\n",
      "Epoch: 22360 | training loss: 8.4898e-04 | validation loss: 7.9176e-05\n",
      "Epoch: 22370 | training loss: 8.4884e-04 | validation loss: 7.9173e-05\n",
      "Epoch: 22380 | training loss: 8.4871e-04 | validation loss: 7.9169e-05\n",
      "Epoch: 22390 | training loss: 8.4857e-04 | validation loss: 7.9169e-05\n",
      "Epoch: 22400 | training loss: 8.4853e-04 | validation loss: 7.9324e-05\n",
      "Epoch: 22410 | training loss: 8.9358e-04 | validation loss: 1.1172e-04\n",
      "Epoch: 22420 | training loss: 8.6155e-04 | validation loss: 8.9418e-05\n",
      "Epoch: 22430 | training loss: 8.4900e-04 | validation loss: 8.0084e-05\n",
      "Epoch: 22440 | training loss: 8.4934e-04 | validation loss: 7.9802e-05\n",
      "Epoch: 22450 | training loss: 8.4794e-04 | validation loss: 7.9152e-05\n",
      "Epoch: 22460 | training loss: 8.4789e-04 | validation loss: 7.9391e-05\n",
      "Epoch: 22470 | training loss: 8.4763e-04 | validation loss: 7.9226e-05\n",
      "Epoch: 22480 | training loss: 8.4750e-04 | validation loss: 7.9138e-05\n",
      "Epoch: 22490 | training loss: 8.4738e-04 | validation loss: 7.9136e-05\n",
      "Epoch: 22500 | training loss: 8.4726e-04 | validation loss: 7.9149e-05\n",
      "Epoch: 22510 | training loss: 8.4714e-04 | validation loss: 7.9156e-05\n",
      "Epoch: 22520 | training loss: 8.4702e-04 | validation loss: 7.9153e-05\n",
      "Epoch: 22530 | training loss: 8.4691e-04 | validation loss: 7.9147e-05\n",
      "Epoch: 22540 | training loss: 8.4679e-04 | validation loss: 7.9141e-05\n",
      "Epoch: 22550 | training loss: 8.4667e-04 | validation loss: 7.9137e-05\n",
      "Epoch: 22560 | training loss: 8.4655e-04 | validation loss: 7.9133e-05\n",
      "Epoch: 22570 | training loss: 8.4643e-04 | validation loss: 7.9130e-05\n",
      "Epoch: 22580 | training loss: 8.4631e-04 | validation loss: 7.9126e-05\n",
      "Epoch: 22590 | training loss: 8.4619e-04 | validation loss: 7.9123e-05\n",
      "Epoch: 22600 | training loss: 8.4607e-04 | validation loss: 7.9119e-05\n",
      "Epoch: 22610 | training loss: 8.4595e-04 | validation loss: 7.9115e-05\n",
      "Epoch: 22620 | training loss: 8.4583e-04 | validation loss: 7.9112e-05\n",
      "Epoch: 22630 | training loss: 8.4570e-04 | validation loss: 7.9108e-05\n",
      "Epoch: 22640 | training loss: 8.4558e-04 | validation loss: 7.9104e-05\n",
      "Epoch: 22650 | training loss: 8.4546e-04 | validation loss: 7.9100e-05\n",
      "Epoch: 22660 | training loss: 8.4533e-04 | validation loss: 7.9097e-05\n",
      "Epoch: 22670 | training loss: 8.4521e-04 | validation loss: 7.9093e-05\n",
      "Epoch: 22680 | training loss: 8.4509e-04 | validation loss: 7.9089e-05\n",
      "Epoch: 22690 | training loss: 8.4496e-04 | validation loss: 7.9085e-05\n",
      "Epoch: 22700 | training loss: 8.4483e-04 | validation loss: 7.9081e-05\n",
      "Epoch: 22710 | training loss: 8.4471e-04 | validation loss: 7.9077e-05\n",
      "Epoch: 22720 | training loss: 8.4458e-04 | validation loss: 7.9072e-05\n",
      "Epoch: 22730 | training loss: 8.4445e-04 | validation loss: 7.9068e-05\n",
      "Epoch: 22740 | training loss: 8.4433e-04 | validation loss: 7.9064e-05\n",
      "Epoch: 22750 | training loss: 8.4420e-04 | validation loss: 7.9060e-05\n",
      "Epoch: 22760 | training loss: 8.4407e-04 | validation loss: 7.9056e-05\n",
      "Epoch: 22770 | training loss: 8.4394e-04 | validation loss: 7.9051e-05\n",
      "Epoch: 22780 | training loss: 8.4381e-04 | validation loss: 7.9047e-05\n",
      "Epoch: 22790 | training loss: 8.4368e-04 | validation loss: 7.9043e-05\n",
      "Epoch: 22800 | training loss: 8.4355e-04 | validation loss: 7.9038e-05\n",
      "Epoch: 22810 | training loss: 8.4342e-04 | validation loss: 7.9033e-05\n",
      "Epoch: 22820 | training loss: 8.4328e-04 | validation loss: 7.9027e-05\n",
      "Epoch: 22830 | training loss: 8.4320e-04 | validation loss: 7.9000e-05\n",
      "Epoch: 22840 | training loss: 8.7554e-04 | validation loss: 9.9436e-05\n",
      "Epoch: 22850 | training loss: 8.4715e-04 | validation loss: 8.1295e-05\n",
      "Epoch: 22860 | training loss: 8.4801e-04 | validation loss: 8.2152e-05\n",
      "Epoch: 22870 | training loss: 8.4301e-04 | validation loss: 7.9404e-05\n",
      "Epoch: 22880 | training loss: 8.4313e-04 | validation loss: 7.9563e-05\n",
      "Epoch: 22890 | training loss: 8.4253e-04 | validation loss: 7.9003e-05\n",
      "Epoch: 22900 | training loss: 8.4243e-04 | validation loss: 7.9001e-05\n",
      "Epoch: 22910 | training loss: 8.4225e-04 | validation loss: 7.9026e-05\n",
      "Epoch: 22920 | training loss: 8.4215e-04 | validation loss: 7.9045e-05\n",
      "Epoch: 22930 | training loss: 8.4203e-04 | validation loss: 7.9016e-05\n",
      "Epoch: 22940 | training loss: 8.4192e-04 | validation loss: 7.9000e-05\n",
      "Epoch: 22950 | training loss: 8.4180e-04 | validation loss: 7.8994e-05\n",
      "Epoch: 22960 | training loss: 8.4169e-04 | validation loss: 7.8992e-05\n",
      "Epoch: 22970 | training loss: 8.4158e-04 | validation loss: 7.8989e-05\n",
      "Epoch: 22980 | training loss: 8.4146e-04 | validation loss: 7.8986e-05\n",
      "Epoch: 22990 | training loss: 8.4135e-04 | validation loss: 7.8982e-05\n",
      "Epoch: 23000 | training loss: 8.4124e-04 | validation loss: 7.8978e-05\n",
      "Epoch: 23010 | training loss: 8.4112e-04 | validation loss: 7.8973e-05\n",
      "Epoch: 23020 | training loss: 8.4101e-04 | validation loss: 7.8969e-05\n",
      "Epoch: 23030 | training loss: 8.4089e-04 | validation loss: 7.8964e-05\n",
      "Epoch: 23040 | training loss: 8.4078e-04 | validation loss: 7.8960e-05\n",
      "Epoch: 23050 | training loss: 8.4066e-04 | validation loss: 7.8955e-05\n",
      "Epoch: 23060 | training loss: 8.4054e-04 | validation loss: 7.8950e-05\n",
      "Epoch: 23070 | training loss: 8.4042e-04 | validation loss: 7.8946e-05\n",
      "Epoch: 23080 | training loss: 8.4031e-04 | validation loss: 7.8941e-05\n",
      "Epoch: 23090 | training loss: 8.4019e-04 | validation loss: 7.8936e-05\n",
      "Epoch: 23100 | training loss: 8.4007e-04 | validation loss: 7.8931e-05\n",
      "Epoch: 23110 | training loss: 8.3995e-04 | validation loss: 7.8927e-05\n",
      "Epoch: 23120 | training loss: 8.3983e-04 | validation loss: 7.8922e-05\n",
      "Epoch: 23130 | training loss: 8.3971e-04 | validation loss: 7.8917e-05\n",
      "Epoch: 23140 | training loss: 8.3959e-04 | validation loss: 7.8912e-05\n",
      "Epoch: 23150 | training loss: 8.3947e-04 | validation loss: 7.8907e-05\n",
      "Epoch: 23160 | training loss: 8.3934e-04 | validation loss: 7.8901e-05\n",
      "Epoch: 23170 | training loss: 8.3922e-04 | validation loss: 7.8896e-05\n",
      "Epoch: 23180 | training loss: 8.3910e-04 | validation loss: 7.8891e-05\n",
      "Epoch: 23190 | training loss: 8.3897e-04 | validation loss: 7.8886e-05\n",
      "Epoch: 23200 | training loss: 8.3885e-04 | validation loss: 7.8881e-05\n",
      "Epoch: 23210 | training loss: 8.3873e-04 | validation loss: 7.8875e-05\n",
      "Epoch: 23220 | training loss: 8.3860e-04 | validation loss: 7.8870e-05\n",
      "Epoch: 23230 | training loss: 8.3848e-04 | validation loss: 7.8864e-05\n",
      "Epoch: 23240 | training loss: 8.3835e-04 | validation loss: 7.8859e-05\n",
      "Epoch: 23250 | training loss: 8.3822e-04 | validation loss: 7.8854e-05\n",
      "Epoch: 23260 | training loss: 8.3810e-04 | validation loss: 7.8855e-05\n",
      "Epoch: 23270 | training loss: 8.3854e-04 | validation loss: 7.9400e-05\n",
      "Epoch: 23280 | training loss: 8.4900e-04 | validation loss: 8.7479e-05\n",
      "Epoch: 23290 | training loss: 8.3793e-04 | validation loss: 7.8888e-05\n",
      "Epoch: 23300 | training loss: 8.4148e-04 | validation loss: 8.1098e-05\n",
      "Epoch: 23310 | training loss: 8.3759e-04 | validation loss: 7.8917e-05\n",
      "Epoch: 23320 | training loss: 8.3789e-04 | validation loss: 7.9290e-05\n",
      "Epoch: 23330 | training loss: 8.3733e-04 | validation loss: 7.8829e-05\n",
      "Epoch: 23340 | training loss: 8.3728e-04 | validation loss: 7.8815e-05\n",
      "Epoch: 23350 | training loss: 8.3711e-04 | validation loss: 7.8817e-05\n",
      "Epoch: 23360 | training loss: 8.3701e-04 | validation loss: 7.8839e-05\n",
      "Epoch: 23370 | training loss: 8.3690e-04 | validation loss: 7.8829e-05\n",
      "Epoch: 23380 | training loss: 8.3679e-04 | validation loss: 7.8814e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23390 | training loss: 8.3668e-04 | validation loss: 7.8805e-05\n",
      "Epoch: 23400 | training loss: 8.3657e-04 | validation loss: 7.8799e-05\n",
      "Epoch: 23410 | training loss: 8.3647e-04 | validation loss: 7.8795e-05\n",
      "Epoch: 23420 | training loss: 8.3636e-04 | validation loss: 7.8790e-05\n",
      "Epoch: 23430 | training loss: 8.3625e-04 | validation loss: 7.8785e-05\n",
      "Epoch: 23440 | training loss: 8.3614e-04 | validation loss: 7.8780e-05\n",
      "Epoch: 23450 | training loss: 8.3603e-04 | validation loss: 7.8775e-05\n",
      "Epoch: 23460 | training loss: 8.3592e-04 | validation loss: 7.8770e-05\n",
      "Epoch: 23470 | training loss: 8.3580e-04 | validation loss: 7.8765e-05\n",
      "Epoch: 23480 | training loss: 8.3569e-04 | validation loss: 7.8759e-05\n",
      "Epoch: 23490 | training loss: 8.3558e-04 | validation loss: 7.8754e-05\n",
      "Epoch: 23500 | training loss: 8.3547e-04 | validation loss: 7.8748e-05\n",
      "Epoch: 23510 | training loss: 8.3535e-04 | validation loss: 7.8743e-05\n",
      "Epoch: 23520 | training loss: 8.3524e-04 | validation loss: 7.8737e-05\n",
      "Epoch: 23530 | training loss: 8.3513e-04 | validation loss: 7.8731e-05\n",
      "Epoch: 23540 | training loss: 8.3501e-04 | validation loss: 7.8725e-05\n",
      "Epoch: 23550 | training loss: 8.3490e-04 | validation loss: 7.8720e-05\n",
      "Epoch: 23560 | training loss: 8.3478e-04 | validation loss: 7.8714e-05\n",
      "Epoch: 23570 | training loss: 8.3466e-04 | validation loss: 7.8708e-05\n",
      "Epoch: 23580 | training loss: 8.3455e-04 | validation loss: 7.8702e-05\n",
      "Epoch: 23590 | training loss: 8.3443e-04 | validation loss: 7.8696e-05\n",
      "Epoch: 23600 | training loss: 8.3431e-04 | validation loss: 7.8690e-05\n",
      "Epoch: 23610 | training loss: 8.3420e-04 | validation loss: 7.8684e-05\n",
      "Epoch: 23620 | training loss: 8.3408e-04 | validation loss: 7.8678e-05\n",
      "Epoch: 23630 | training loss: 8.3396e-04 | validation loss: 7.8672e-05\n",
      "Epoch: 23640 | training loss: 8.3384e-04 | validation loss: 7.8665e-05\n",
      "Epoch: 23650 | training loss: 8.3372e-04 | validation loss: 7.8659e-05\n",
      "Epoch: 23660 | training loss: 8.3360e-04 | validation loss: 7.8653e-05\n",
      "Epoch: 23670 | training loss: 8.3348e-04 | validation loss: 7.8647e-05\n",
      "Epoch: 23680 | training loss: 8.3336e-04 | validation loss: 7.8640e-05\n",
      "Epoch: 23690 | training loss: 8.3323e-04 | validation loss: 7.8635e-05\n",
      "Epoch: 23700 | training loss: 8.3313e-04 | validation loss: 7.8665e-05\n",
      "Epoch: 23710 | training loss: 8.4674e-04 | validation loss: 8.8621e-05\n",
      "Epoch: 23720 | training loss: 8.4009e-04 | validation loss: 8.3239e-05\n",
      "Epoch: 23730 | training loss: 8.3512e-04 | validation loss: 8.0549e-05\n",
      "Epoch: 23740 | training loss: 8.3402e-04 | validation loss: 7.9755e-05\n",
      "Epoch: 23750 | training loss: 8.3302e-04 | validation loss: 7.8773e-05\n",
      "Epoch: 23760 | training loss: 8.3259e-04 | validation loss: 7.8617e-05\n",
      "Epoch: 23770 | training loss: 8.3244e-04 | validation loss: 7.8697e-05\n",
      "Epoch: 23780 | training loss: 8.3230e-04 | validation loss: 7.8651e-05\n",
      "Epoch: 23790 | training loss: 8.3218e-04 | validation loss: 7.8591e-05\n",
      "Epoch: 23800 | training loss: 8.3208e-04 | validation loss: 7.8583e-05\n",
      "Epoch: 23810 | training loss: 8.3197e-04 | validation loss: 7.8584e-05\n",
      "Epoch: 23820 | training loss: 8.3187e-04 | validation loss: 7.8584e-05\n",
      "Epoch: 23830 | training loss: 8.3176e-04 | validation loss: 7.8579e-05\n",
      "Epoch: 23840 | training loss: 8.3166e-04 | validation loss: 7.8573e-05\n",
      "Epoch: 23850 | training loss: 8.3155e-04 | validation loss: 7.8566e-05\n",
      "Epoch: 23860 | training loss: 8.3145e-04 | validation loss: 7.8559e-05\n",
      "Epoch: 23870 | training loss: 8.3134e-04 | validation loss: 7.8553e-05\n",
      "Epoch: 23880 | training loss: 8.3124e-04 | validation loss: 7.8547e-05\n",
      "Epoch: 23890 | training loss: 8.3113e-04 | validation loss: 7.8541e-05\n",
      "Epoch: 23900 | training loss: 8.3102e-04 | validation loss: 7.8535e-05\n",
      "Epoch: 23910 | training loss: 8.3092e-04 | validation loss: 7.8529e-05\n",
      "Epoch: 23920 | training loss: 8.3081e-04 | validation loss: 7.8522e-05\n",
      "Epoch: 23930 | training loss: 8.3070e-04 | validation loss: 7.8516e-05\n",
      "Epoch: 23940 | training loss: 8.3059e-04 | validation loss: 7.8510e-05\n",
      "Epoch: 23950 | training loss: 8.3048e-04 | validation loss: 7.8503e-05\n",
      "Epoch: 23960 | training loss: 8.3037e-04 | validation loss: 7.8497e-05\n",
      "Epoch: 23970 | training loss: 8.3026e-04 | validation loss: 7.8490e-05\n",
      "Epoch: 23980 | training loss: 8.3015e-04 | validation loss: 7.8484e-05\n",
      "Epoch: 23990 | training loss: 8.3004e-04 | validation loss: 7.8477e-05\n",
      "Epoch: 24000 | training loss: 8.2993e-04 | validation loss: 7.8471e-05\n",
      "Epoch: 24010 | training loss: 8.2982e-04 | validation loss: 7.8464e-05\n",
      "Epoch: 24020 | training loss: 8.2971e-04 | validation loss: 7.8457e-05\n",
      "Epoch: 24030 | training loss: 8.2960e-04 | validation loss: 7.8451e-05\n",
      "Epoch: 24040 | training loss: 8.2948e-04 | validation loss: 7.8444e-05\n",
      "Epoch: 24050 | training loss: 8.2937e-04 | validation loss: 7.8437e-05\n",
      "Epoch: 24060 | training loss: 8.2925e-04 | validation loss: 7.8430e-05\n",
      "Epoch: 24070 | training loss: 8.2914e-04 | validation loss: 7.8423e-05\n",
      "Epoch: 24080 | training loss: 8.2903e-04 | validation loss: 7.8416e-05\n",
      "Epoch: 24090 | training loss: 8.2891e-04 | validation loss: 7.8409e-05\n",
      "Epoch: 24100 | training loss: 8.2879e-04 | validation loss: 7.8402e-05\n",
      "Epoch: 24110 | training loss: 8.2868e-04 | validation loss: 7.8395e-05\n",
      "Epoch: 24120 | training loss: 8.2856e-04 | validation loss: 7.8388e-05\n",
      "Epoch: 24130 | training loss: 8.2844e-04 | validation loss: 7.8379e-05\n",
      "Epoch: 24140 | training loss: 8.2838e-04 | validation loss: 7.8363e-05\n",
      "Epoch: 24150 | training loss: 8.6408e-04 | validation loss: 1.0129e-04\n",
      "Epoch: 24160 | training loss: 8.3422e-04 | validation loss: 8.1939e-05\n",
      "Epoch: 24170 | training loss: 8.3275e-04 | validation loss: 8.1306e-05\n",
      "Epoch: 24180 | training loss: 8.2839e-04 | validation loss: 7.8848e-05\n",
      "Epoch: 24190 | training loss: 8.2832e-04 | validation loss: 7.8815e-05\n",
      "Epoch: 24200 | training loss: 8.2780e-04 | validation loss: 7.8355e-05\n",
      "Epoch: 24210 | training loss: 8.2769e-04 | validation loss: 7.8348e-05\n",
      "Epoch: 24220 | training loss: 8.2753e-04 | validation loss: 7.8355e-05\n",
      "Epoch: 24230 | training loss: 8.2744e-04 | validation loss: 7.8367e-05\n",
      "Epoch: 24240 | training loss: 8.2733e-04 | validation loss: 7.8340e-05\n",
      "Epoch: 24250 | training loss: 8.2723e-04 | validation loss: 7.8325e-05\n",
      "Epoch: 24260 | training loss: 8.2713e-04 | validation loss: 7.8317e-05\n",
      "Epoch: 24270 | training loss: 8.2703e-04 | validation loss: 7.8312e-05\n",
      "Epoch: 24280 | training loss: 8.2693e-04 | validation loss: 7.8307e-05\n",
      "Epoch: 24290 | training loss: 8.2683e-04 | validation loss: 7.8301e-05\n",
      "Epoch: 24300 | training loss: 8.2673e-04 | validation loss: 7.8295e-05\n",
      "Epoch: 24310 | training loss: 8.2663e-04 | validation loss: 7.8288e-05\n",
      "Epoch: 24320 | training loss: 8.2652e-04 | validation loss: 7.8281e-05\n",
      "Epoch: 24330 | training loss: 8.2642e-04 | validation loss: 7.8275e-05\n",
      "Epoch: 24340 | training loss: 8.2632e-04 | validation loss: 7.8268e-05\n",
      "Epoch: 24350 | training loss: 8.2621e-04 | validation loss: 7.8261e-05\n",
      "Epoch: 24360 | training loss: 8.2611e-04 | validation loss: 7.8254e-05\n",
      "Epoch: 24370 | training loss: 8.2601e-04 | validation loss: 7.8247e-05\n",
      "Epoch: 24380 | training loss: 8.2590e-04 | validation loss: 7.8240e-05\n",
      "Epoch: 24390 | training loss: 8.2580e-04 | validation loss: 7.8233e-05\n",
      "Epoch: 24400 | training loss: 8.2569e-04 | validation loss: 7.8226e-05\n",
      "Epoch: 24410 | training loss: 8.2559e-04 | validation loss: 7.8219e-05\n",
      "Epoch: 24420 | training loss: 8.2548e-04 | validation loss: 7.8212e-05\n",
      "Epoch: 24430 | training loss: 8.2537e-04 | validation loss: 7.8204e-05\n",
      "Epoch: 24440 | training loss: 8.2526e-04 | validation loss: 7.8197e-05\n",
      "Epoch: 24450 | training loss: 8.2516e-04 | validation loss: 7.8190e-05\n",
      "Epoch: 24460 | training loss: 8.2505e-04 | validation loss: 7.8182e-05\n",
      "Epoch: 24470 | training loss: 8.2494e-04 | validation loss: 7.8175e-05\n",
      "Epoch: 24480 | training loss: 8.2483e-04 | validation loss: 7.8168e-05\n",
      "Epoch: 24490 | training loss: 8.2472e-04 | validation loss: 7.8160e-05\n",
      "Epoch: 24500 | training loss: 8.2461e-04 | validation loss: 7.8152e-05\n",
      "Epoch: 24510 | training loss: 8.2450e-04 | validation loss: 7.8145e-05\n",
      "Epoch: 24520 | training loss: 8.2439e-04 | validation loss: 7.8137e-05\n",
      "Epoch: 24530 | training loss: 8.2428e-04 | validation loss: 7.8129e-05\n",
      "Epoch: 24540 | training loss: 8.2417e-04 | validation loss: 7.8122e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24550 | training loss: 8.2405e-04 | validation loss: 7.8114e-05\n",
      "Epoch: 24560 | training loss: 8.2394e-04 | validation loss: 7.8106e-05\n",
      "Epoch: 24570 | training loss: 8.2383e-04 | validation loss: 7.8097e-05\n",
      "Epoch: 24580 | training loss: 8.2373e-04 | validation loss: 7.8078e-05\n",
      "Epoch: 24590 | training loss: 8.4091e-04 | validation loss: 8.8899e-05\n",
      "Epoch: 24600 | training loss: 8.2510e-04 | validation loss: 7.9542e-05\n",
      "Epoch: 24610 | training loss: 8.3047e-04 | validation loss: 8.2578e-05\n",
      "Epoch: 24620 | training loss: 8.2347e-04 | validation loss: 7.8248e-05\n",
      "Epoch: 24630 | training loss: 8.2383e-04 | validation loss: 7.8594e-05\n",
      "Epoch: 24640 | training loss: 8.2338e-04 | validation loss: 7.8140e-05\n",
      "Epoch: 24650 | training loss: 8.2305e-04 | validation loss: 7.8060e-05\n",
      "Epoch: 24660 | training loss: 8.2299e-04 | validation loss: 7.8125e-05\n",
      "Epoch: 24670 | training loss: 8.2286e-04 | validation loss: 7.8053e-05\n",
      "Epoch: 24680 | training loss: 8.2277e-04 | validation loss: 7.8039e-05\n",
      "Epoch: 24690 | training loss: 8.2267e-04 | validation loss: 7.8040e-05\n",
      "Epoch: 24700 | training loss: 8.2258e-04 | validation loss: 7.8039e-05\n",
      "Epoch: 24710 | training loss: 8.2248e-04 | validation loss: 7.8031e-05\n",
      "Epoch: 24720 | training loss: 8.2239e-04 | validation loss: 7.8021e-05\n",
      "Epoch: 24730 | training loss: 8.2229e-04 | validation loss: 7.8014e-05\n",
      "Epoch: 24740 | training loss: 8.2220e-04 | validation loss: 7.8007e-05\n",
      "Epoch: 24750 | training loss: 8.2210e-04 | validation loss: 7.8000e-05\n",
      "Epoch: 24760 | training loss: 8.2200e-04 | validation loss: 7.7993e-05\n",
      "Epoch: 24770 | training loss: 8.2191e-04 | validation loss: 7.7986e-05\n",
      "Epoch: 24780 | training loss: 8.2181e-04 | validation loss: 7.7979e-05\n",
      "Epoch: 24790 | training loss: 8.2171e-04 | validation loss: 7.7972e-05\n",
      "Epoch: 24800 | training loss: 8.2161e-04 | validation loss: 7.7965e-05\n",
      "Epoch: 24810 | training loss: 8.2151e-04 | validation loss: 7.7957e-05\n",
      "Epoch: 24820 | training loss: 8.2141e-04 | validation loss: 7.7950e-05\n",
      "Epoch: 24830 | training loss: 8.2131e-04 | validation loss: 7.7943e-05\n",
      "Epoch: 24840 | training loss: 8.2121e-04 | validation loss: 7.7935e-05\n",
      "Epoch: 24850 | training loss: 8.2111e-04 | validation loss: 7.7928e-05\n",
      "Epoch: 24860 | training loss: 8.2101e-04 | validation loss: 7.7920e-05\n",
      "Epoch: 24870 | training loss: 8.2091e-04 | validation loss: 7.7913e-05\n",
      "Epoch: 24880 | training loss: 8.2081e-04 | validation loss: 7.7905e-05\n",
      "Epoch: 24890 | training loss: 8.2071e-04 | validation loss: 7.7897e-05\n",
      "Epoch: 24900 | training loss: 8.2061e-04 | validation loss: 7.7889e-05\n",
      "Epoch: 24910 | training loss: 8.2050e-04 | validation loss: 7.7882e-05\n",
      "Epoch: 24920 | training loss: 8.2040e-04 | validation loss: 7.7874e-05\n",
      "Epoch: 24930 | training loss: 8.2030e-04 | validation loss: 7.7866e-05\n",
      "Epoch: 24940 | training loss: 8.2019e-04 | validation loss: 7.7858e-05\n",
      "Epoch: 24950 | training loss: 8.2009e-04 | validation loss: 7.7850e-05\n",
      "Epoch: 24960 | training loss: 8.1998e-04 | validation loss: 7.7842e-05\n",
      "Epoch: 24970 | training loss: 8.1988e-04 | validation loss: 7.7834e-05\n",
      "Epoch: 24980 | training loss: 8.1977e-04 | validation loss: 7.7826e-05\n",
      "Epoch: 24990 | training loss: 8.1966e-04 | validation loss: 7.7818e-05\n",
      "Epoch: 25000 | training loss: 8.1956e-04 | validation loss: 7.7810e-05\n",
      "Epoch: 25010 | training loss: 8.1945e-04 | validation loss: 7.7802e-05\n",
      "Epoch: 25020 | training loss: 8.1934e-04 | validation loss: 7.7793e-05\n",
      "Epoch: 25030 | training loss: 8.1923e-04 | validation loss: 7.7785e-05\n",
      "Epoch: 25040 | training loss: 8.1913e-04 | validation loss: 7.7777e-05\n",
      "Epoch: 25050 | training loss: 8.1903e-04 | validation loss: 7.7792e-05\n",
      "Epoch: 25060 | training loss: 8.2922e-04 | validation loss: 8.5211e-05\n",
      "Epoch: 25070 | training loss: 8.3325e-04 | validation loss: 8.7246e-05\n",
      "Epoch: 25080 | training loss: 8.2039e-04 | validation loss: 7.9103e-05\n",
      "Epoch: 25090 | training loss: 8.2010e-04 | validation loss: 7.8933e-05\n",
      "Epoch: 25100 | training loss: 8.1923e-04 | validation loss: 7.8060e-05\n",
      "Epoch: 25110 | training loss: 8.1848e-04 | validation loss: 7.7733e-05\n",
      "Epoch: 25120 | training loss: 8.1848e-04 | validation loss: 7.7876e-05\n",
      "Epoch: 25130 | training loss: 8.1828e-04 | validation loss: 7.7736e-05\n",
      "Epoch: 25140 | training loss: 8.1820e-04 | validation loss: 7.7716e-05\n",
      "Epoch: 25150 | training loss: 8.1810e-04 | validation loss: 7.7717e-05\n",
      "Epoch: 25160 | training loss: 8.1801e-04 | validation loss: 7.7720e-05\n",
      "Epoch: 25170 | training loss: 8.1792e-04 | validation loss: 7.7712e-05\n",
      "Epoch: 25180 | training loss: 8.1782e-04 | validation loss: 7.7700e-05\n",
      "Epoch: 25190 | training loss: 8.1773e-04 | validation loss: 7.7692e-05\n",
      "Epoch: 25200 | training loss: 8.1764e-04 | validation loss: 7.7684e-05\n",
      "Epoch: 25210 | training loss: 8.1755e-04 | validation loss: 7.7677e-05\n",
      "Epoch: 25220 | training loss: 8.1745e-04 | validation loss: 7.7670e-05\n",
      "Epoch: 25230 | training loss: 8.1736e-04 | validation loss: 7.7663e-05\n",
      "Epoch: 25240 | training loss: 8.1726e-04 | validation loss: 7.7656e-05\n",
      "Epoch: 25250 | training loss: 8.1717e-04 | validation loss: 7.7648e-05\n",
      "Epoch: 25260 | training loss: 8.1708e-04 | validation loss: 7.7640e-05\n",
      "Epoch: 25270 | training loss: 8.1698e-04 | validation loss: 7.7633e-05\n",
      "Epoch: 25280 | training loss: 8.1689e-04 | validation loss: 7.7625e-05\n",
      "Epoch: 25290 | training loss: 8.1679e-04 | validation loss: 7.7617e-05\n",
      "Epoch: 25300 | training loss: 8.1669e-04 | validation loss: 7.7610e-05\n",
      "Epoch: 25310 | training loss: 8.1660e-04 | validation loss: 7.7602e-05\n",
      "Epoch: 25320 | training loss: 8.1650e-04 | validation loss: 7.7594e-05\n",
      "Epoch: 25330 | training loss: 8.1640e-04 | validation loss: 7.7586e-05\n",
      "Epoch: 25340 | training loss: 8.1630e-04 | validation loss: 7.7578e-05\n",
      "Epoch: 25350 | training loss: 8.1620e-04 | validation loss: 7.7570e-05\n",
      "Epoch: 25360 | training loss: 8.1611e-04 | validation loss: 7.7562e-05\n",
      "Epoch: 25370 | training loss: 8.1601e-04 | validation loss: 7.7554e-05\n",
      "Epoch: 25380 | training loss: 8.1591e-04 | validation loss: 7.7545e-05\n",
      "Epoch: 25390 | training loss: 8.1581e-04 | validation loss: 7.7537e-05\n",
      "Epoch: 25400 | training loss: 8.1571e-04 | validation loss: 7.7529e-05\n",
      "Epoch: 25410 | training loss: 8.1561e-04 | validation loss: 7.7521e-05\n",
      "Epoch: 25420 | training loss: 8.1550e-04 | validation loss: 7.7512e-05\n",
      "Epoch: 25430 | training loss: 8.1540e-04 | validation loss: 7.7504e-05\n",
      "Epoch: 25440 | training loss: 8.1530e-04 | validation loss: 7.7495e-05\n",
      "Epoch: 25450 | training loss: 8.1520e-04 | validation loss: 7.7487e-05\n",
      "Epoch: 25460 | training loss: 8.1509e-04 | validation loss: 7.7478e-05\n",
      "Epoch: 25470 | training loss: 8.1499e-04 | validation loss: 7.7470e-05\n",
      "Epoch: 25480 | training loss: 8.1489e-04 | validation loss: 7.7461e-05\n",
      "Epoch: 25490 | training loss: 8.1478e-04 | validation loss: 7.7453e-05\n",
      "Epoch: 25500 | training loss: 8.1468e-04 | validation loss: 7.7444e-05\n",
      "Epoch: 25510 | training loss: 8.1457e-04 | validation loss: 7.7433e-05\n",
      "Epoch: 25520 | training loss: 8.1467e-04 | validation loss: 7.7486e-05\n",
      "Epoch: 25530 | training loss: 8.6199e-04 | validation loss: 1.0889e-04\n",
      "Epoch: 25540 | training loss: 8.1514e-04 | validation loss: 7.7981e-05\n",
      "Epoch: 25550 | training loss: 8.1911e-04 | validation loss: 8.1238e-05\n",
      "Epoch: 25560 | training loss: 8.1555e-04 | validation loss: 7.8217e-05\n",
      "Epoch: 25570 | training loss: 8.1406e-04 | validation loss: 7.7468e-05\n",
      "Epoch: 25580 | training loss: 8.1403e-04 | validation loss: 7.7516e-05\n",
      "Epoch: 25590 | training loss: 8.1394e-04 | validation loss: 7.7410e-05\n",
      "Epoch: 25600 | training loss: 8.1378e-04 | validation loss: 7.7408e-05\n",
      "Epoch: 25610 | training loss: 8.1370e-04 | validation loss: 7.7404e-05\n",
      "Epoch: 25620 | training loss: 8.1361e-04 | validation loss: 7.7375e-05\n",
      "Epoch: 25630 | training loss: 8.1352e-04 | validation loss: 7.7372e-05\n",
      "Epoch: 25640 | training loss: 8.1343e-04 | validation loss: 7.7371e-05\n",
      "Epoch: 25650 | training loss: 8.1334e-04 | validation loss: 7.7361e-05\n",
      "Epoch: 25660 | training loss: 8.1326e-04 | validation loss: 7.7352e-05\n",
      "Epoch: 25670 | training loss: 8.1317e-04 | validation loss: 7.7345e-05\n",
      "Epoch: 25680 | training loss: 8.1308e-04 | validation loss: 7.7338e-05\n",
      "Epoch: 25690 | training loss: 8.1299e-04 | validation loss: 7.7331e-05\n",
      "Epoch: 25700 | training loss: 8.1291e-04 | validation loss: 7.7323e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25710 | training loss: 8.1282e-04 | validation loss: 7.7316e-05\n",
      "Epoch: 25720 | training loss: 8.1273e-04 | validation loss: 7.7308e-05\n",
      "Epoch: 25730 | training loss: 8.1264e-04 | validation loss: 7.7300e-05\n",
      "Epoch: 25740 | training loss: 8.1255e-04 | validation loss: 7.7293e-05\n",
      "Epoch: 25750 | training loss: 8.1246e-04 | validation loss: 7.7285e-05\n",
      "Epoch: 25760 | training loss: 8.1237e-04 | validation loss: 7.7277e-05\n",
      "Epoch: 25770 | training loss: 8.1227e-04 | validation loss: 7.7269e-05\n",
      "Epoch: 25780 | training loss: 8.1218e-04 | validation loss: 7.7261e-05\n",
      "Epoch: 25790 | training loss: 8.1209e-04 | validation loss: 7.7253e-05\n",
      "Epoch: 25800 | training loss: 8.1200e-04 | validation loss: 7.7245e-05\n",
      "Epoch: 25810 | training loss: 8.1191e-04 | validation loss: 7.7237e-05\n",
      "Epoch: 25820 | training loss: 8.1181e-04 | validation loss: 7.7229e-05\n",
      "Epoch: 25830 | training loss: 8.1172e-04 | validation loss: 7.7221e-05\n",
      "Epoch: 25840 | training loss: 8.1162e-04 | validation loss: 7.7213e-05\n",
      "Epoch: 25850 | training loss: 8.1153e-04 | validation loss: 7.7205e-05\n",
      "Epoch: 25860 | training loss: 8.1144e-04 | validation loss: 7.7196e-05\n",
      "Epoch: 25870 | training loss: 8.1134e-04 | validation loss: 7.7188e-05\n",
      "Epoch: 25880 | training loss: 8.1124e-04 | validation loss: 7.7180e-05\n",
      "Epoch: 25890 | training loss: 8.1115e-04 | validation loss: 7.7171e-05\n",
      "Epoch: 25900 | training loss: 8.1105e-04 | validation loss: 7.7163e-05\n",
      "Epoch: 25910 | training loss: 8.1096e-04 | validation loss: 7.7154e-05\n",
      "Epoch: 25920 | training loss: 8.1086e-04 | validation loss: 7.7146e-05\n",
      "Epoch: 25930 | training loss: 8.1076e-04 | validation loss: 7.7137e-05\n",
      "Epoch: 25940 | training loss: 8.1066e-04 | validation loss: 7.7129e-05\n",
      "Epoch: 25950 | training loss: 8.1056e-04 | validation loss: 7.7120e-05\n",
      "Epoch: 25960 | training loss: 8.1046e-04 | validation loss: 7.7111e-05\n",
      "Epoch: 25970 | training loss: 8.1037e-04 | validation loss: 7.7103e-05\n",
      "Epoch: 25980 | training loss: 8.1027e-04 | validation loss: 7.7094e-05\n",
      "Epoch: 25990 | training loss: 8.1017e-04 | validation loss: 7.7085e-05\n",
      "Epoch: 26000 | training loss: 8.1007e-04 | validation loss: 7.7076e-05\n",
      "Epoch: 26010 | training loss: 8.0996e-04 | validation loss: 7.7068e-05\n",
      "Epoch: 26020 | training loss: 8.0987e-04 | validation loss: 7.7072e-05\n",
      "Epoch: 26030 | training loss: 8.1659e-04 | validation loss: 8.2024e-05\n",
      "Epoch: 26040 | training loss: 8.3408e-04 | validation loss: 9.3190e-05\n",
      "Epoch: 26050 | training loss: 8.1261e-04 | validation loss: 7.9372e-05\n",
      "Epoch: 26060 | training loss: 8.0982e-04 | validation loss: 7.7308e-05\n",
      "Epoch: 26070 | training loss: 8.1037e-04 | validation loss: 7.7492e-05\n",
      "Epoch: 26080 | training loss: 8.0971e-04 | validation loss: 7.7357e-05\n",
      "Epoch: 26090 | training loss: 8.0928e-04 | validation loss: 7.7025e-05\n",
      "Epoch: 26100 | training loss: 8.0921e-04 | validation loss: 7.7015e-05\n",
      "Epoch: 26110 | training loss: 8.0912e-04 | validation loss: 7.7047e-05\n",
      "Epoch: 26120 | training loss: 8.0902e-04 | validation loss: 7.7011e-05\n",
      "Epoch: 26130 | training loss: 8.0894e-04 | validation loss: 7.6997e-05\n",
      "Epoch: 26140 | training loss: 8.0886e-04 | validation loss: 7.6997e-05\n",
      "Epoch: 26150 | training loss: 8.0877e-04 | validation loss: 7.6992e-05\n",
      "Epoch: 26160 | training loss: 8.0869e-04 | validation loss: 7.6982e-05\n",
      "Epoch: 26170 | training loss: 8.0861e-04 | validation loss: 7.6973e-05\n",
      "Epoch: 26180 | training loss: 8.0852e-04 | validation loss: 7.6966e-05\n",
      "Epoch: 26190 | training loss: 8.0844e-04 | validation loss: 7.6959e-05\n",
      "Epoch: 26200 | training loss: 8.0835e-04 | validation loss: 7.6952e-05\n",
      "Epoch: 26210 | training loss: 8.0827e-04 | validation loss: 7.6944e-05\n",
      "Epoch: 26220 | training loss: 8.0818e-04 | validation loss: 7.6936e-05\n",
      "Epoch: 26230 | training loss: 8.0810e-04 | validation loss: 7.6928e-05\n",
      "Epoch: 26240 | training loss: 8.0801e-04 | validation loss: 7.6920e-05\n",
      "Epoch: 26250 | training loss: 8.0792e-04 | validation loss: 7.6913e-05\n",
      "Epoch: 26260 | training loss: 8.0784e-04 | validation loss: 7.6905e-05\n",
      "Epoch: 26270 | training loss: 8.0775e-04 | validation loss: 7.6897e-05\n",
      "Epoch: 26280 | training loss: 8.0766e-04 | validation loss: 7.6889e-05\n",
      "Epoch: 26290 | training loss: 8.0757e-04 | validation loss: 7.6881e-05\n",
      "Epoch: 26300 | training loss: 8.0748e-04 | validation loss: 7.6873e-05\n",
      "Epoch: 26310 | training loss: 8.0739e-04 | validation loss: 7.6864e-05\n",
      "Epoch: 26320 | training loss: 8.0730e-04 | validation loss: 7.6856e-05\n",
      "Epoch: 26330 | training loss: 8.0721e-04 | validation loss: 7.6848e-05\n",
      "Epoch: 26340 | training loss: 8.0712e-04 | validation loss: 7.6840e-05\n",
      "Epoch: 26350 | training loss: 8.0703e-04 | validation loss: 7.6831e-05\n",
      "Epoch: 26360 | training loss: 8.0694e-04 | validation loss: 7.6823e-05\n",
      "Epoch: 26370 | training loss: 8.0685e-04 | validation loss: 7.6815e-05\n",
      "Epoch: 26380 | training loss: 8.0676e-04 | validation loss: 7.6806e-05\n",
      "Epoch: 26390 | training loss: 8.0667e-04 | validation loss: 7.6798e-05\n",
      "Epoch: 26400 | training loss: 8.0657e-04 | validation loss: 7.6789e-05\n",
      "Epoch: 26410 | training loss: 8.0648e-04 | validation loss: 7.6781e-05\n",
      "Epoch: 26420 | training loss: 8.0639e-04 | validation loss: 7.6772e-05\n",
      "Epoch: 26430 | training loss: 8.0629e-04 | validation loss: 7.6764e-05\n",
      "Epoch: 26440 | training loss: 8.0620e-04 | validation loss: 7.6755e-05\n",
      "Epoch: 26450 | training loss: 8.0611e-04 | validation loss: 7.6746e-05\n",
      "Epoch: 26460 | training loss: 8.0601e-04 | validation loss: 7.6737e-05\n",
      "Epoch: 26470 | training loss: 8.0591e-04 | validation loss: 7.6729e-05\n",
      "Epoch: 26480 | training loss: 8.0582e-04 | validation loss: 7.6720e-05\n",
      "Epoch: 26490 | training loss: 8.0572e-04 | validation loss: 7.6711e-05\n",
      "Epoch: 26500 | training loss: 8.0563e-04 | validation loss: 7.6702e-05\n",
      "Epoch: 26510 | training loss: 8.0553e-04 | validation loss: 7.6693e-05\n",
      "Epoch: 26520 | training loss: 8.0543e-04 | validation loss: 7.6680e-05\n",
      "Epoch: 26530 | training loss: 8.0637e-04 | validation loss: 7.7206e-05\n",
      "Epoch: 26540 | training loss: 8.0579e-04 | validation loss: 7.7114e-05\n",
      "Epoch: 26550 | training loss: 8.1623e-04 | validation loss: 8.4855e-05\n",
      "Epoch: 26560 | training loss: 8.0516e-04 | validation loss: 7.6689e-05\n",
      "Epoch: 26570 | training loss: 8.0596e-04 | validation loss: 7.7132e-05\n",
      "Epoch: 26580 | training loss: 8.0535e-04 | validation loss: 7.7027e-05\n",
      "Epoch: 26590 | training loss: 8.0485e-04 | validation loss: 7.6657e-05\n",
      "Epoch: 26600 | training loss: 8.0483e-04 | validation loss: 7.6649e-05\n",
      "Epoch: 26610 | training loss: 8.0470e-04 | validation loss: 7.6654e-05\n",
      "Epoch: 26620 | training loss: 8.0462e-04 | validation loss: 7.6646e-05\n",
      "Epoch: 26630 | training loss: 8.0453e-04 | validation loss: 7.6621e-05\n",
      "Epoch: 26640 | training loss: 8.0445e-04 | validation loss: 7.6613e-05\n",
      "Epoch: 26650 | training loss: 8.0437e-04 | validation loss: 7.6610e-05\n",
      "Epoch: 26660 | training loss: 8.0429e-04 | validation loss: 7.6604e-05\n",
      "Epoch: 26670 | training loss: 8.0421e-04 | validation loss: 7.6595e-05\n",
      "Epoch: 26680 | training loss: 8.0412e-04 | validation loss: 7.6587e-05\n",
      "Epoch: 26690 | training loss: 8.0404e-04 | validation loss: 7.6579e-05\n",
      "Epoch: 26700 | training loss: 8.0396e-04 | validation loss: 7.6571e-05\n",
      "Epoch: 26710 | training loss: 8.0387e-04 | validation loss: 7.6563e-05\n",
      "Epoch: 26720 | training loss: 8.0379e-04 | validation loss: 7.6555e-05\n",
      "Epoch: 26730 | training loss: 8.0371e-04 | validation loss: 7.6547e-05\n",
      "Epoch: 26740 | training loss: 8.0362e-04 | validation loss: 7.6539e-05\n",
      "Epoch: 26750 | training loss: 8.0354e-04 | validation loss: 7.6531e-05\n",
      "Epoch: 26760 | training loss: 8.0345e-04 | validation loss: 7.6523e-05\n",
      "Epoch: 26770 | training loss: 8.0337e-04 | validation loss: 7.6515e-05\n",
      "Epoch: 26780 | training loss: 8.0328e-04 | validation loss: 7.6507e-05\n",
      "Epoch: 26790 | training loss: 8.0320e-04 | validation loss: 7.6499e-05\n",
      "Epoch: 26800 | training loss: 8.0311e-04 | validation loss: 7.6490e-05\n",
      "Epoch: 26810 | training loss: 8.0302e-04 | validation loss: 7.6482e-05\n",
      "Epoch: 26820 | training loss: 8.0294e-04 | validation loss: 7.6474e-05\n",
      "Epoch: 26830 | training loss: 8.0285e-04 | validation loss: 7.6465e-05\n",
      "Epoch: 26840 | training loss: 8.0276e-04 | validation loss: 7.6457e-05\n",
      "Epoch: 26850 | training loss: 8.0267e-04 | validation loss: 7.6448e-05\n",
      "Epoch: 26860 | training loss: 8.0258e-04 | validation loss: 7.6440e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26870 | training loss: 8.0249e-04 | validation loss: 7.6431e-05\n",
      "Epoch: 26880 | training loss: 8.0240e-04 | validation loss: 7.6423e-05\n",
      "Epoch: 26890 | training loss: 8.0231e-04 | validation loss: 7.6414e-05\n",
      "Epoch: 26900 | training loss: 8.0222e-04 | validation loss: 7.6405e-05\n",
      "Epoch: 26910 | training loss: 8.0213e-04 | validation loss: 7.6396e-05\n",
      "Epoch: 26920 | training loss: 8.0204e-04 | validation loss: 7.6388e-05\n",
      "Epoch: 26930 | training loss: 8.0195e-04 | validation loss: 7.6379e-05\n",
      "Epoch: 26940 | training loss: 8.0186e-04 | validation loss: 7.6370e-05\n",
      "Epoch: 26950 | training loss: 8.0177e-04 | validation loss: 7.6361e-05\n",
      "Epoch: 26960 | training loss: 8.0167e-04 | validation loss: 7.6352e-05\n",
      "Epoch: 26970 | training loss: 8.0158e-04 | validation loss: 7.6343e-05\n",
      "Epoch: 26980 | training loss: 8.0149e-04 | validation loss: 7.6334e-05\n",
      "Epoch: 26990 | training loss: 8.0139e-04 | validation loss: 7.6325e-05\n",
      "Epoch: 27000 | training loss: 8.0130e-04 | validation loss: 7.6323e-05\n",
      "Epoch: 27010 | training loss: 8.0306e-04 | validation loss: 7.7750e-05\n",
      "Epoch: 27020 | training loss: 8.1030e-04 | validation loss: 8.1899e-05\n",
      "Epoch: 27030 | training loss: 8.1161e-04 | validation loss: 8.3256e-05\n",
      "Epoch: 27040 | training loss: 8.0151e-04 | validation loss: 7.6805e-05\n",
      "Epoch: 27050 | training loss: 8.0160e-04 | validation loss: 7.6869e-05\n",
      "Epoch: 27060 | training loss: 8.0116e-04 | validation loss: 7.6431e-05\n",
      "Epoch: 27070 | training loss: 8.0075e-04 | validation loss: 7.6277e-05\n",
      "Epoch: 27080 | training loss: 8.0071e-04 | validation loss: 7.6349e-05\n",
      "Epoch: 27090 | training loss: 8.0058e-04 | validation loss: 7.6271e-05\n",
      "Epoch: 27100 | training loss: 8.0051e-04 | validation loss: 7.6253e-05\n",
      "Epoch: 27110 | training loss: 8.0042e-04 | validation loss: 7.6251e-05\n",
      "Epoch: 27120 | training loss: 8.0035e-04 | validation loss: 7.6250e-05\n",
      "Epoch: 27130 | training loss: 8.0027e-04 | validation loss: 7.6242e-05\n",
      "Epoch: 27140 | training loss: 8.0019e-04 | validation loss: 7.6232e-05\n",
      "Epoch: 27150 | training loss: 8.0011e-04 | validation loss: 7.6223e-05\n",
      "Epoch: 27160 | training loss: 8.0003e-04 | validation loss: 7.6215e-05\n",
      "Epoch: 27170 | training loss: 7.9995e-04 | validation loss: 7.6207e-05\n",
      "Epoch: 27180 | training loss: 7.9987e-04 | validation loss: 7.6199e-05\n",
      "Epoch: 27190 | training loss: 7.9978e-04 | validation loss: 7.6191e-05\n",
      "Epoch: 27200 | training loss: 7.9970e-04 | validation loss: 7.6183e-05\n",
      "Epoch: 27210 | training loss: 7.9962e-04 | validation loss: 7.6175e-05\n",
      "Epoch: 27220 | training loss: 7.9954e-04 | validation loss: 7.6167e-05\n",
      "Epoch: 27230 | training loss: 7.9946e-04 | validation loss: 7.6159e-05\n",
      "Epoch: 27240 | training loss: 7.9937e-04 | validation loss: 7.6151e-05\n",
      "Epoch: 27250 | training loss: 7.9929e-04 | validation loss: 7.6142e-05\n",
      "Epoch: 27260 | training loss: 7.9921e-04 | validation loss: 7.6134e-05\n",
      "Epoch: 27270 | training loss: 7.9912e-04 | validation loss: 7.6126e-05\n",
      "Epoch: 27280 | training loss: 7.9904e-04 | validation loss: 7.6117e-05\n",
      "Epoch: 27290 | training loss: 7.9895e-04 | validation loss: 7.6109e-05\n",
      "Epoch: 27300 | training loss: 7.9887e-04 | validation loss: 7.6100e-05\n",
      "Epoch: 27310 | training loss: 7.9878e-04 | validation loss: 7.6092e-05\n",
      "Epoch: 27320 | training loss: 7.9870e-04 | validation loss: 7.6083e-05\n",
      "Epoch: 27330 | training loss: 7.9861e-04 | validation loss: 7.6075e-05\n",
      "Epoch: 27340 | training loss: 7.9853e-04 | validation loss: 7.6066e-05\n",
      "Epoch: 27350 | training loss: 7.9844e-04 | validation loss: 7.6057e-05\n",
      "Epoch: 27360 | training loss: 7.9835e-04 | validation loss: 7.6048e-05\n",
      "Epoch: 27370 | training loss: 7.9826e-04 | validation loss: 7.6040e-05\n",
      "Epoch: 27380 | training loss: 7.9818e-04 | validation loss: 7.6031e-05\n",
      "Epoch: 27390 | training loss: 7.9809e-04 | validation loss: 7.6022e-05\n",
      "Epoch: 27400 | training loss: 7.9800e-04 | validation loss: 7.6013e-05\n",
      "Epoch: 27410 | training loss: 7.9791e-04 | validation loss: 7.6004e-05\n",
      "Epoch: 27420 | training loss: 7.9782e-04 | validation loss: 7.5995e-05\n",
      "Epoch: 27430 | training loss: 7.9773e-04 | validation loss: 7.5986e-05\n",
      "Epoch: 27440 | training loss: 7.9764e-04 | validation loss: 7.5977e-05\n",
      "Epoch: 27450 | training loss: 7.9755e-04 | validation loss: 7.5968e-05\n",
      "Epoch: 27460 | training loss: 7.9746e-04 | validation loss: 7.5956e-05\n",
      "Epoch: 27470 | training loss: 7.9770e-04 | validation loss: 7.6087e-05\n",
      "Epoch: 27480 | training loss: 8.3137e-04 | validation loss: 9.8509e-05\n",
      "Epoch: 27490 | training loss: 7.9931e-04 | validation loss: 7.7319e-05\n",
      "Epoch: 27500 | training loss: 8.0127e-04 | validation loss: 7.9149e-05\n",
      "Epoch: 27510 | training loss: 7.9712e-04 | validation loss: 7.5997e-05\n",
      "Epoch: 27520 | training loss: 7.9755e-04 | validation loss: 7.6194e-05\n",
      "Epoch: 27530 | training loss: 7.9692e-04 | validation loss: 7.5946e-05\n",
      "Epoch: 27540 | training loss: 7.9690e-04 | validation loss: 7.6001e-05\n",
      "Epoch: 27550 | training loss: 7.9675e-04 | validation loss: 7.5906e-05\n",
      "Epoch: 27560 | training loss: 7.9669e-04 | validation loss: 7.5895e-05\n",
      "Epoch: 27570 | training loss: 7.9660e-04 | validation loss: 7.5892e-05\n",
      "Epoch: 27580 | training loss: 7.9652e-04 | validation loss: 7.5891e-05\n",
      "Epoch: 27590 | training loss: 7.9645e-04 | validation loss: 7.5883e-05\n",
      "Epoch: 27600 | training loss: 7.9637e-04 | validation loss: 7.5873e-05\n",
      "Epoch: 27610 | training loss: 7.9629e-04 | validation loss: 7.5864e-05\n",
      "Epoch: 27620 | training loss: 7.9621e-04 | validation loss: 7.5856e-05\n",
      "Epoch: 27630 | training loss: 7.9614e-04 | validation loss: 7.5848e-05\n",
      "Epoch: 27640 | training loss: 7.9606e-04 | validation loss: 7.5840e-05\n",
      "Epoch: 27650 | training loss: 7.9598e-04 | validation loss: 7.5832e-05\n",
      "Epoch: 27660 | training loss: 7.9590e-04 | validation loss: 7.5824e-05\n",
      "Epoch: 27670 | training loss: 7.9582e-04 | validation loss: 7.5816e-05\n",
      "Epoch: 27680 | training loss: 7.9574e-04 | validation loss: 7.5808e-05\n",
      "Epoch: 27690 | training loss: 7.9566e-04 | validation loss: 7.5800e-05\n",
      "Epoch: 27700 | training loss: 7.9558e-04 | validation loss: 7.5791e-05\n",
      "Epoch: 27710 | training loss: 7.9550e-04 | validation loss: 7.5783e-05\n",
      "Epoch: 27720 | training loss: 7.9542e-04 | validation loss: 7.5775e-05\n",
      "Epoch: 27730 | training loss: 7.9533e-04 | validation loss: 7.5766e-05\n",
      "Epoch: 27740 | training loss: 7.9525e-04 | validation loss: 7.5758e-05\n",
      "Epoch: 27750 | training loss: 7.9517e-04 | validation loss: 7.5749e-05\n",
      "Epoch: 27760 | training loss: 7.9509e-04 | validation loss: 7.5741e-05\n",
      "Epoch: 27770 | training loss: 7.9500e-04 | validation loss: 7.5732e-05\n",
      "Epoch: 27780 | training loss: 7.9492e-04 | validation loss: 7.5723e-05\n",
      "Epoch: 27790 | training loss: 7.9484e-04 | validation loss: 7.5715e-05\n",
      "Epoch: 27800 | training loss: 7.9475e-04 | validation loss: 7.5706e-05\n",
      "Epoch: 27810 | training loss: 7.9467e-04 | validation loss: 7.5697e-05\n",
      "Epoch: 27820 | training loss: 7.9458e-04 | validation loss: 7.5689e-05\n",
      "Epoch: 27830 | training loss: 7.9450e-04 | validation loss: 7.5680e-05\n",
      "Epoch: 27840 | training loss: 7.9441e-04 | validation loss: 7.5671e-05\n",
      "Epoch: 27850 | training loss: 7.9432e-04 | validation loss: 7.5662e-05\n",
      "Epoch: 27860 | training loss: 7.9424e-04 | validation loss: 7.5653e-05\n",
      "Epoch: 27870 | training loss: 7.9415e-04 | validation loss: 7.5644e-05\n",
      "Epoch: 27880 | training loss: 7.9406e-04 | validation loss: 7.5635e-05\n",
      "Epoch: 27890 | training loss: 7.9398e-04 | validation loss: 7.5626e-05\n",
      "Epoch: 27900 | training loss: 7.9389e-04 | validation loss: 7.5617e-05\n",
      "Epoch: 27910 | training loss: 7.9380e-04 | validation loss: 7.5605e-05\n",
      "Epoch: 27920 | training loss: 7.9403e-04 | validation loss: 7.5732e-05\n",
      "Epoch: 27930 | training loss: 8.3030e-04 | validation loss: 9.9850e-05\n",
      "Epoch: 27940 | training loss: 7.9755e-04 | validation loss: 7.8224e-05\n",
      "Epoch: 27950 | training loss: 7.9657e-04 | validation loss: 7.8014e-05\n",
      "Epoch: 27960 | training loss: 7.9378e-04 | validation loss: 7.5898e-05\n",
      "Epoch: 27970 | training loss: 7.9382e-04 | validation loss: 7.5802e-05\n",
      "Epoch: 27980 | training loss: 7.9328e-04 | validation loss: 7.5566e-05\n",
      "Epoch: 27990 | training loss: 7.9325e-04 | validation loss: 7.5640e-05\n",
      "Epoch: 28000 | training loss: 7.9312e-04 | validation loss: 7.5577e-05\n",
      "Epoch: 28010 | training loss: 7.9304e-04 | validation loss: 7.5546e-05\n",
      "Epoch: 28020 | training loss: 7.9297e-04 | validation loss: 7.5538e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28030 | training loss: 7.9289e-04 | validation loss: 7.5535e-05\n",
      "Epoch: 28040 | training loss: 7.9281e-04 | validation loss: 7.5530e-05\n",
      "Epoch: 28050 | training loss: 7.9274e-04 | validation loss: 7.5523e-05\n",
      "Epoch: 28060 | training loss: 7.9266e-04 | validation loss: 7.5515e-05\n",
      "Epoch: 28070 | training loss: 7.9259e-04 | validation loss: 7.5506e-05\n",
      "Epoch: 28080 | training loss: 7.9251e-04 | validation loss: 7.5498e-05\n",
      "Epoch: 28090 | training loss: 7.9243e-04 | validation loss: 7.5490e-05\n",
      "Epoch: 28100 | training loss: 7.9236e-04 | validation loss: 7.5482e-05\n",
      "Epoch: 28110 | training loss: 7.9228e-04 | validation loss: 7.5474e-05\n",
      "Epoch: 28120 | training loss: 7.9220e-04 | validation loss: 7.5466e-05\n",
      "Epoch: 28130 | training loss: 7.9212e-04 | validation loss: 7.5457e-05\n",
      "Epoch: 28140 | training loss: 7.9205e-04 | validation loss: 7.5449e-05\n",
      "Epoch: 28150 | training loss: 7.9197e-04 | validation loss: 7.5441e-05\n",
      "Epoch: 28160 | training loss: 7.9189e-04 | validation loss: 7.5433e-05\n",
      "Epoch: 28170 | training loss: 7.9181e-04 | validation loss: 7.5424e-05\n",
      "Epoch: 28180 | training loss: 7.9173e-04 | validation loss: 7.5416e-05\n",
      "Epoch: 28190 | training loss: 7.9165e-04 | validation loss: 7.5408e-05\n",
      "Epoch: 28200 | training loss: 7.9157e-04 | validation loss: 7.5399e-05\n",
      "Epoch: 28210 | training loss: 7.9149e-04 | validation loss: 7.5391e-05\n",
      "Epoch: 28220 | training loss: 7.9141e-04 | validation loss: 7.5382e-05\n",
      "Epoch: 28230 | training loss: 7.9132e-04 | validation loss: 7.5374e-05\n",
      "Epoch: 28240 | training loss: 7.9124e-04 | validation loss: 7.5365e-05\n",
      "Epoch: 28250 | training loss: 7.9116e-04 | validation loss: 7.5356e-05\n",
      "Epoch: 28260 | training loss: 7.9108e-04 | validation loss: 7.5348e-05\n",
      "Epoch: 28270 | training loss: 7.9099e-04 | validation loss: 7.5339e-05\n",
      "Epoch: 28280 | training loss: 7.9091e-04 | validation loss: 7.5330e-05\n",
      "Epoch: 28290 | training loss: 7.9083e-04 | validation loss: 7.5321e-05\n",
      "Epoch: 28300 | training loss: 7.9074e-04 | validation loss: 7.5313e-05\n",
      "Epoch: 28310 | training loss: 7.9066e-04 | validation loss: 7.5304e-05\n",
      "Epoch: 28320 | training loss: 7.9057e-04 | validation loss: 7.5295e-05\n",
      "Epoch: 28330 | training loss: 7.9049e-04 | validation loss: 7.5286e-05\n",
      "Epoch: 28340 | training loss: 7.9040e-04 | validation loss: 7.5277e-05\n",
      "Epoch: 28350 | training loss: 7.9032e-04 | validation loss: 7.5266e-05\n",
      "Epoch: 28360 | training loss: 7.9035e-04 | validation loss: 7.5290e-05\n",
      "Epoch: 28370 | training loss: 8.4135e-04 | validation loss: 1.0879e-04\n",
      "Epoch: 28380 | training loss: 8.0551e-04 | validation loss: 8.5305e-05\n",
      "Epoch: 28390 | training loss: 7.9051e-04 | validation loss: 7.5494e-05\n",
      "Epoch: 28400 | training loss: 7.9169e-04 | validation loss: 7.6639e-05\n",
      "Epoch: 28410 | training loss: 7.8989e-04 | validation loss: 7.5270e-05\n",
      "Epoch: 28420 | training loss: 7.9002e-04 | validation loss: 7.5316e-05\n",
      "Epoch: 28430 | training loss: 7.8973e-04 | validation loss: 7.5223e-05\n",
      "Epoch: 28440 | training loss: 7.8967e-04 | validation loss: 7.5257e-05\n",
      "Epoch: 28450 | training loss: 7.8958e-04 | validation loss: 7.5226e-05\n",
      "Epoch: 28460 | training loss: 7.8950e-04 | validation loss: 7.5203e-05\n",
      "Epoch: 28470 | training loss: 7.8943e-04 | validation loss: 7.5194e-05\n",
      "Epoch: 28480 | training loss: 7.8936e-04 | validation loss: 7.5188e-05\n",
      "Epoch: 28490 | training loss: 7.8928e-04 | validation loss: 7.5182e-05\n",
      "Epoch: 28500 | training loss: 7.8921e-04 | validation loss: 7.5175e-05\n",
      "Epoch: 28510 | training loss: 7.8914e-04 | validation loss: 7.5167e-05\n",
      "Epoch: 28520 | training loss: 7.8906e-04 | validation loss: 7.5159e-05\n",
      "Epoch: 28530 | training loss: 7.8899e-04 | validation loss: 7.5152e-05\n",
      "Epoch: 28540 | training loss: 7.8891e-04 | validation loss: 7.5144e-05\n",
      "Epoch: 28550 | training loss: 7.8884e-04 | validation loss: 7.5136e-05\n",
      "Epoch: 28560 | training loss: 7.8876e-04 | validation loss: 7.5128e-05\n",
      "Epoch: 28570 | training loss: 7.8869e-04 | validation loss: 7.5120e-05\n",
      "Epoch: 28580 | training loss: 7.8861e-04 | validation loss: 7.5111e-05\n",
      "Epoch: 28590 | training loss: 7.8853e-04 | validation loss: 7.5103e-05\n",
      "Epoch: 28600 | training loss: 7.8846e-04 | validation loss: 7.5095e-05\n",
      "Epoch: 28610 | training loss: 7.8838e-04 | validation loss: 7.5087e-05\n",
      "Epoch: 28620 | training loss: 7.8830e-04 | validation loss: 7.5079e-05\n",
      "Epoch: 28630 | training loss: 7.8822e-04 | validation loss: 7.5071e-05\n",
      "Epoch: 28640 | training loss: 7.8814e-04 | validation loss: 7.5062e-05\n",
      "Epoch: 28650 | training loss: 7.8807e-04 | validation loss: 7.5054e-05\n",
      "Epoch: 28660 | training loss: 7.8799e-04 | validation loss: 7.5046e-05\n",
      "Epoch: 28670 | training loss: 7.8791e-04 | validation loss: 7.5037e-05\n",
      "Epoch: 28680 | training loss: 7.8783e-04 | validation loss: 7.5029e-05\n",
      "Epoch: 28690 | training loss: 7.8775e-04 | validation loss: 7.5020e-05\n",
      "Epoch: 28700 | training loss: 7.8767e-04 | validation loss: 7.5012e-05\n",
      "Epoch: 28710 | training loss: 7.8759e-04 | validation loss: 7.5003e-05\n",
      "Epoch: 28720 | training loss: 7.8750e-04 | validation loss: 7.4995e-05\n",
      "Epoch: 28730 | training loss: 7.8742e-04 | validation loss: 7.4986e-05\n",
      "Epoch: 28740 | training loss: 7.8734e-04 | validation loss: 7.4978e-05\n",
      "Epoch: 28750 | training loss: 7.8726e-04 | validation loss: 7.4969e-05\n",
      "Epoch: 28760 | training loss: 7.8718e-04 | validation loss: 7.4960e-05\n",
      "Epoch: 28770 | training loss: 7.8709e-04 | validation loss: 7.4951e-05\n",
      "Epoch: 28780 | training loss: 7.8701e-04 | validation loss: 7.4943e-05\n",
      "Epoch: 28790 | training loss: 7.8693e-04 | validation loss: 7.4939e-05\n",
      "Epoch: 28800 | training loss: 7.8758e-04 | validation loss: 7.5536e-05\n",
      "Epoch: 28810 | training loss: 7.9429e-04 | validation loss: 8.0634e-05\n",
      "Epoch: 28820 | training loss: 7.8726e-04 | validation loss: 7.5169e-05\n",
      "Epoch: 28830 | training loss: 7.9062e-04 | validation loss: 7.7426e-05\n",
      "Epoch: 28840 | training loss: 7.8662e-04 | validation loss: 7.4992e-05\n",
      "Epoch: 28850 | training loss: 7.8697e-04 | validation loss: 7.5313e-05\n",
      "Epoch: 28860 | training loss: 7.8642e-04 | validation loss: 7.4898e-05\n",
      "Epoch: 28870 | training loss: 7.8641e-04 | validation loss: 7.4900e-05\n",
      "Epoch: 28880 | training loss: 7.8628e-04 | validation loss: 7.4880e-05\n",
      "Epoch: 28890 | training loss: 7.8621e-04 | validation loss: 7.4889e-05\n",
      "Epoch: 28900 | training loss: 7.8614e-04 | validation loss: 7.4881e-05\n",
      "Epoch: 28910 | training loss: 7.8607e-04 | validation loss: 7.4868e-05\n",
      "Epoch: 28920 | training loss: 7.8599e-04 | validation loss: 7.4857e-05\n",
      "Epoch: 28930 | training loss: 7.8592e-04 | validation loss: 7.4849e-05\n",
      "Epoch: 28940 | training loss: 7.8585e-04 | validation loss: 7.4842e-05\n",
      "Epoch: 28950 | training loss: 7.8578e-04 | validation loss: 7.4835e-05\n",
      "Epoch: 28960 | training loss: 7.8570e-04 | validation loss: 7.4827e-05\n",
      "Epoch: 28970 | training loss: 7.8563e-04 | validation loss: 7.4820e-05\n",
      "Epoch: 28980 | training loss: 7.8556e-04 | validation loss: 7.4812e-05\n",
      "Epoch: 28990 | training loss: 7.8548e-04 | validation loss: 7.4804e-05\n",
      "Epoch: 29000 | training loss: 7.8541e-04 | validation loss: 7.4796e-05\n",
      "Epoch: 29010 | training loss: 7.8534e-04 | validation loss: 7.4789e-05\n",
      "Epoch: 29020 | training loss: 7.8526e-04 | validation loss: 7.4781e-05\n",
      "Epoch: 29030 | training loss: 7.8519e-04 | validation loss: 7.4773e-05\n",
      "Epoch: 29040 | training loss: 7.8511e-04 | validation loss: 7.4765e-05\n",
      "Epoch: 29050 | training loss: 7.8504e-04 | validation loss: 7.4757e-05\n",
      "Epoch: 29060 | training loss: 7.8496e-04 | validation loss: 7.4749e-05\n",
      "Epoch: 29070 | training loss: 7.8488e-04 | validation loss: 7.4741e-05\n",
      "Epoch: 29080 | training loss: 7.8481e-04 | validation loss: 7.4733e-05\n",
      "Epoch: 29090 | training loss: 7.8473e-04 | validation loss: 7.4724e-05\n",
      "Epoch: 29100 | training loss: 7.8465e-04 | validation loss: 7.4716e-05\n",
      "Epoch: 29110 | training loss: 7.8458e-04 | validation loss: 7.4708e-05\n",
      "Epoch: 29120 | training loss: 7.8450e-04 | validation loss: 7.4700e-05\n",
      "Epoch: 29130 | training loss: 7.8442e-04 | validation loss: 7.4692e-05\n",
      "Epoch: 29140 | training loss: 7.8434e-04 | validation loss: 7.4683e-05\n",
      "Epoch: 29150 | training loss: 7.8426e-04 | validation loss: 7.4675e-05\n",
      "Epoch: 29160 | training loss: 7.8418e-04 | validation loss: 7.4667e-05\n",
      "Epoch: 29170 | training loss: 7.8410e-04 | validation loss: 7.4658e-05\n",
      "Epoch: 29180 | training loss: 7.8402e-04 | validation loss: 7.4650e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29190 | training loss: 7.8394e-04 | validation loss: 7.4641e-05\n",
      "Epoch: 29200 | training loss: 7.8386e-04 | validation loss: 7.4633e-05\n",
      "Epoch: 29210 | training loss: 7.8378e-04 | validation loss: 7.4624e-05\n",
      "Epoch: 29220 | training loss: 7.8370e-04 | validation loss: 7.4617e-05\n",
      "Epoch: 29230 | training loss: 7.8369e-04 | validation loss: 7.4692e-05\n",
      "Epoch: 29240 | training loss: 8.2611e-04 | validation loss: 1.0410e-04\n",
      "Epoch: 29250 | training loss: 7.9297e-04 | validation loss: 8.1385e-05\n",
      "Epoch: 29260 | training loss: 7.8740e-04 | validation loss: 7.7685e-05\n",
      "Epoch: 29270 | training loss: 7.8414e-04 | validation loss: 7.5053e-05\n",
      "Epoch: 29280 | training loss: 7.8369e-04 | validation loss: 7.4773e-05\n",
      "Epoch: 29290 | training loss: 7.8331e-04 | validation loss: 7.4698e-05\n",
      "Epoch: 29300 | training loss: 7.8319e-04 | validation loss: 7.4654e-05\n",
      "Epoch: 29310 | training loss: 7.8307e-04 | validation loss: 7.4563e-05\n",
      "Epoch: 29320 | training loss: 7.8301e-04 | validation loss: 7.4556e-05\n",
      "Epoch: 29330 | training loss: 7.8293e-04 | validation loss: 7.4552e-05\n",
      "Epoch: 29340 | training loss: 7.8286e-04 | validation loss: 7.4551e-05\n",
      "Epoch: 29350 | training loss: 7.8279e-04 | validation loss: 7.4545e-05\n",
      "Epoch: 29360 | training loss: 7.8272e-04 | validation loss: 7.4536e-05\n",
      "Epoch: 29370 | training loss: 7.8265e-04 | validation loss: 7.4528e-05\n",
      "Epoch: 29380 | training loss: 7.8258e-04 | validation loss: 7.4520e-05\n",
      "Epoch: 29390 | training loss: 7.8251e-04 | validation loss: 7.4513e-05\n",
      "Epoch: 29400 | training loss: 7.8244e-04 | validation loss: 7.4505e-05\n",
      "Epoch: 29410 | training loss: 7.8237e-04 | validation loss: 7.4498e-05\n",
      "Epoch: 29420 | training loss: 7.8229e-04 | validation loss: 7.4490e-05\n",
      "Epoch: 29430 | training loss: 7.8222e-04 | validation loss: 7.4483e-05\n",
      "Epoch: 29440 | training loss: 7.8215e-04 | validation loss: 7.4475e-05\n",
      "Epoch: 29450 | training loss: 7.8208e-04 | validation loss: 7.4467e-05\n",
      "Epoch: 29460 | training loss: 7.8201e-04 | validation loss: 7.4460e-05\n",
      "Epoch: 29470 | training loss: 7.8193e-04 | validation loss: 7.4452e-05\n",
      "Epoch: 29480 | training loss: 7.8186e-04 | validation loss: 7.4444e-05\n",
      "Epoch: 29490 | training loss: 7.8178e-04 | validation loss: 7.4436e-05\n",
      "Epoch: 29500 | training loss: 7.8171e-04 | validation loss: 7.4429e-05\n",
      "Epoch: 29510 | training loss: 7.8164e-04 | validation loss: 7.4421e-05\n",
      "Epoch: 29520 | training loss: 7.8156e-04 | validation loss: 7.4413e-05\n",
      "Epoch: 29530 | training loss: 7.8149e-04 | validation loss: 7.4405e-05\n",
      "Epoch: 29540 | training loss: 7.8141e-04 | validation loss: 7.4397e-05\n",
      "Epoch: 29550 | training loss: 7.8134e-04 | validation loss: 7.4389e-05\n",
      "Epoch: 29560 | training loss: 7.8126e-04 | validation loss: 7.4381e-05\n",
      "Epoch: 29570 | training loss: 7.8118e-04 | validation loss: 7.4373e-05\n",
      "Epoch: 29580 | training loss: 7.8111e-04 | validation loss: 7.4365e-05\n",
      "Epoch: 29590 | training loss: 7.8103e-04 | validation loss: 7.4357e-05\n",
      "Epoch: 29600 | training loss: 7.8095e-04 | validation loss: 7.4349e-05\n",
      "Epoch: 29610 | training loss: 7.8087e-04 | validation loss: 7.4341e-05\n",
      "Epoch: 29620 | training loss: 7.8080e-04 | validation loss: 7.4333e-05\n",
      "Epoch: 29630 | training loss: 7.8072e-04 | validation loss: 7.4324e-05\n",
      "Epoch: 29640 | training loss: 7.8064e-04 | validation loss: 7.4316e-05\n",
      "Epoch: 29650 | training loss: 7.8056e-04 | validation loss: 7.4308e-05\n",
      "Epoch: 29660 | training loss: 7.8048e-04 | validation loss: 7.4301e-05\n",
      "Epoch: 29670 | training loss: 7.8050e-04 | validation loss: 7.4402e-05\n",
      "Epoch: 29680 | training loss: 8.3434e-04 | validation loss: 1.1168e-04\n",
      "Epoch: 29690 | training loss: 7.9719e-04 | validation loss: 8.6493e-05\n",
      "Epoch: 29700 | training loss: 7.8026e-04 | validation loss: 7.4357e-05\n",
      "Epoch: 29710 | training loss: 7.8208e-04 | validation loss: 7.5416e-05\n",
      "Epoch: 29720 | training loss: 7.8018e-04 | validation loss: 7.4401e-05\n",
      "Epoch: 29730 | training loss: 7.8017e-04 | validation loss: 7.4435e-05\n",
      "Epoch: 29740 | training loss: 7.7997e-04 | validation loss: 7.4263e-05\n",
      "Epoch: 29750 | training loss: 7.7989e-04 | validation loss: 7.4251e-05\n",
      "Epoch: 29760 | training loss: 7.7980e-04 | validation loss: 7.4259e-05\n",
      "Epoch: 29770 | training loss: 7.7974e-04 | validation loss: 7.4254e-05\n",
      "Epoch: 29780 | training loss: 7.7967e-04 | validation loss: 7.4238e-05\n",
      "Epoch: 29790 | training loss: 7.7960e-04 | validation loss: 7.4228e-05\n",
      "Epoch: 29800 | training loss: 7.7953e-04 | validation loss: 7.4221e-05\n",
      "Epoch: 29810 | training loss: 7.7946e-04 | validation loss: 7.4215e-05\n",
      "Epoch: 29820 | training loss: 7.7940e-04 | validation loss: 7.4209e-05\n",
      "Epoch: 29830 | training loss: 7.7933e-04 | validation loss: 7.4202e-05\n",
      "Epoch: 29840 | training loss: 7.7926e-04 | validation loss: 7.4195e-05\n",
      "Epoch: 29850 | training loss: 7.7919e-04 | validation loss: 7.4188e-05\n",
      "Epoch: 29860 | training loss: 7.7912e-04 | validation loss: 7.4180e-05\n",
      "Epoch: 29870 | training loss: 7.7905e-04 | validation loss: 7.4173e-05\n",
      "Epoch: 29880 | training loss: 7.7898e-04 | validation loss: 7.4166e-05\n",
      "Epoch: 29890 | training loss: 7.7891e-04 | validation loss: 7.4159e-05\n",
      "Epoch: 29900 | training loss: 7.7884e-04 | validation loss: 7.4151e-05\n",
      "Epoch: 29910 | training loss: 7.7877e-04 | validation loss: 7.4144e-05\n",
      "Epoch: 29920 | training loss: 7.7870e-04 | validation loss: 7.4136e-05\n",
      "Epoch: 29930 | training loss: 7.7863e-04 | validation loss: 7.4129e-05\n",
      "Epoch: 29940 | training loss: 7.7856e-04 | validation loss: 7.4122e-05\n",
      "Epoch: 29950 | training loss: 7.7849e-04 | validation loss: 7.4114e-05\n",
      "Epoch: 29960 | training loss: 7.7841e-04 | validation loss: 7.4106e-05\n",
      "Epoch: 29970 | training loss: 7.7834e-04 | validation loss: 7.4099e-05\n",
      "Epoch: 29980 | training loss: 7.7827e-04 | validation loss: 7.4091e-05\n",
      "Epoch: 29990 | training loss: 7.7820e-04 | validation loss: 7.4084e-05\n",
      "Epoch: 30000 | training loss: 7.7812e-04 | validation loss: 7.4076e-05\n",
      "Epoch: 30010 | training loss: 7.7805e-04 | validation loss: 7.4068e-05\n",
      "Epoch: 30020 | training loss: 7.7797e-04 | validation loss: 7.4061e-05\n",
      "Epoch: 30030 | training loss: 7.7790e-04 | validation loss: 7.4053e-05\n",
      "Epoch: 30040 | training loss: 7.7782e-04 | validation loss: 7.4045e-05\n",
      "Epoch: 30050 | training loss: 7.7775e-04 | validation loss: 7.4037e-05\n",
      "Epoch: 30060 | training loss: 7.7767e-04 | validation loss: 7.4029e-05\n",
      "Epoch: 30070 | training loss: 7.7760e-04 | validation loss: 7.4022e-05\n",
      "Epoch: 30080 | training loss: 7.7752e-04 | validation loss: 7.4014e-05\n",
      "Epoch: 30090 | training loss: 7.7745e-04 | validation loss: 7.4006e-05\n",
      "Epoch: 30100 | training loss: 7.7737e-04 | validation loss: 7.3998e-05\n",
      "Epoch: 30110 | training loss: 7.7729e-04 | validation loss: 7.3989e-05\n",
      "Epoch: 30120 | training loss: 7.7722e-04 | validation loss: 7.3976e-05\n",
      "Epoch: 30130 | training loss: 7.8094e-04 | validation loss: 7.6248e-05\n",
      "Epoch: 30140 | training loss: 8.0028e-04 | validation loss: 9.0266e-05\n",
      "Epoch: 30150 | training loss: 7.8341e-04 | validation loss: 7.8838e-05\n",
      "Epoch: 30160 | training loss: 7.7837e-04 | validation loss: 7.4837e-05\n",
      "Epoch: 30170 | training loss: 7.7737e-04 | validation loss: 7.4186e-05\n",
      "Epoch: 30180 | training loss: 7.7711e-04 | validation loss: 7.4218e-05\n",
      "Epoch: 30190 | training loss: 7.7679e-04 | validation loss: 7.4007e-05\n",
      "Epoch: 30200 | training loss: 7.7672e-04 | validation loss: 7.3945e-05\n",
      "Epoch: 30210 | training loss: 7.7663e-04 | validation loss: 7.3934e-05\n",
      "Epoch: 30220 | training loss: 7.7656e-04 | validation loss: 7.3939e-05\n",
      "Epoch: 30230 | training loss: 7.7649e-04 | validation loss: 7.3936e-05\n",
      "Epoch: 30240 | training loss: 7.7642e-04 | validation loss: 7.3924e-05\n",
      "Epoch: 30250 | training loss: 7.7636e-04 | validation loss: 7.3914e-05\n",
      "Epoch: 30260 | training loss: 7.7629e-04 | validation loss: 7.3907e-05\n",
      "Epoch: 30270 | training loss: 7.7623e-04 | validation loss: 7.3901e-05\n",
      "Epoch: 30280 | training loss: 7.7616e-04 | validation loss: 7.3894e-05\n",
      "Epoch: 30290 | training loss: 7.7609e-04 | validation loss: 7.3887e-05\n",
      "Epoch: 30300 | training loss: 7.7603e-04 | validation loss: 7.3880e-05\n",
      "Epoch: 30310 | training loss: 7.7596e-04 | validation loss: 7.3874e-05\n",
      "Epoch: 30320 | training loss: 7.7589e-04 | validation loss: 7.3867e-05\n",
      "Epoch: 30330 | training loss: 7.7582e-04 | validation loss: 7.3860e-05\n",
      "Epoch: 30340 | training loss: 7.7575e-04 | validation loss: 7.3852e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30350 | training loss: 7.7569e-04 | validation loss: 7.3845e-05\n",
      "Epoch: 30360 | training loss: 7.7562e-04 | validation loss: 7.3838e-05\n",
      "Epoch: 30370 | training loss: 7.7555e-04 | validation loss: 7.3831e-05\n",
      "Epoch: 30380 | training loss: 7.7548e-04 | validation loss: 7.3824e-05\n",
      "Epoch: 30390 | training loss: 7.7541e-04 | validation loss: 7.3817e-05\n",
      "Epoch: 30400 | training loss: 7.7534e-04 | validation loss: 7.3809e-05\n",
      "Epoch: 30410 | training loss: 7.7527e-04 | validation loss: 7.3802e-05\n",
      "Epoch: 30420 | training loss: 7.7520e-04 | validation loss: 7.3795e-05\n",
      "Epoch: 30430 | training loss: 7.7512e-04 | validation loss: 7.3787e-05\n",
      "Epoch: 30440 | training loss: 7.7505e-04 | validation loss: 7.3780e-05\n",
      "Epoch: 30450 | training loss: 7.7498e-04 | validation loss: 7.3772e-05\n",
      "Epoch: 30460 | training loss: 7.7491e-04 | validation loss: 7.3765e-05\n",
      "Epoch: 30470 | training loss: 7.7484e-04 | validation loss: 7.3757e-05\n",
      "Epoch: 30480 | training loss: 7.7476e-04 | validation loss: 7.3750e-05\n",
      "Epoch: 30490 | training loss: 7.7469e-04 | validation loss: 7.3742e-05\n",
      "Epoch: 30500 | training loss: 7.7462e-04 | validation loss: 7.3735e-05\n",
      "Epoch: 30510 | training loss: 7.7454e-04 | validation loss: 7.3727e-05\n",
      "Epoch: 30520 | training loss: 7.7447e-04 | validation loss: 7.3719e-05\n",
      "Epoch: 30530 | training loss: 7.7440e-04 | validation loss: 7.3712e-05\n",
      "Epoch: 30540 | training loss: 7.7432e-04 | validation loss: 7.3704e-05\n",
      "Epoch: 30550 | training loss: 7.7425e-04 | validation loss: 7.3696e-05\n",
      "Epoch: 30560 | training loss: 7.7417e-04 | validation loss: 7.3690e-05\n",
      "Epoch: 30570 | training loss: 7.7413e-04 | validation loss: 7.3730e-05\n",
      "Epoch: 30580 | training loss: 8.0401e-04 | validation loss: 9.4489e-05\n",
      "Epoch: 30590 | training loss: 7.7535e-04 | validation loss: 7.4699e-05\n",
      "Epoch: 30600 | training loss: 7.8072e-04 | validation loss: 7.8794e-05\n",
      "Epoch: 30610 | training loss: 7.7402e-04 | validation loss: 7.3761e-05\n",
      "Epoch: 30620 | training loss: 7.7446e-04 | validation loss: 7.4003e-05\n",
      "Epoch: 30630 | training loss: 7.7379e-04 | validation loss: 7.3752e-05\n",
      "Epoch: 30640 | training loss: 7.7372e-04 | validation loss: 7.3740e-05\n",
      "Epoch: 30650 | training loss: 7.7359e-04 | validation loss: 7.3641e-05\n",
      "Epoch: 30660 | training loss: 7.7353e-04 | validation loss: 7.3636e-05\n",
      "Epoch: 30670 | training loss: 7.7346e-04 | validation loss: 7.3636e-05\n",
      "Epoch: 30680 | training loss: 7.7339e-04 | validation loss: 7.3635e-05\n",
      "Epoch: 30690 | training loss: 7.7333e-04 | validation loss: 7.3627e-05\n",
      "Epoch: 30700 | training loss: 7.7327e-04 | validation loss: 7.3618e-05\n",
      "Epoch: 30710 | training loss: 7.7320e-04 | validation loss: 7.3610e-05\n",
      "Epoch: 30720 | training loss: 7.7314e-04 | validation loss: 7.3604e-05\n",
      "Epoch: 30730 | training loss: 7.7307e-04 | validation loss: 7.3597e-05\n",
      "Epoch: 30740 | training loss: 7.7301e-04 | validation loss: 7.3590e-05\n",
      "Epoch: 30750 | training loss: 7.7294e-04 | validation loss: 7.3584e-05\n",
      "Epoch: 30760 | training loss: 7.7287e-04 | validation loss: 7.3577e-05\n",
      "Epoch: 30770 | training loss: 7.7281e-04 | validation loss: 7.3570e-05\n",
      "Epoch: 30780 | training loss: 7.7274e-04 | validation loss: 7.3563e-05\n",
      "Epoch: 30790 | training loss: 7.7268e-04 | validation loss: 7.3556e-05\n",
      "Epoch: 30800 | training loss: 7.7261e-04 | validation loss: 7.3549e-05\n",
      "Epoch: 30810 | training loss: 7.7254e-04 | validation loss: 7.3542e-05\n",
      "Epoch: 30820 | training loss: 7.7247e-04 | validation loss: 7.3535e-05\n",
      "Epoch: 30830 | training loss: 7.7241e-04 | validation loss: 7.3528e-05\n",
      "Epoch: 30840 | training loss: 7.7234e-04 | validation loss: 7.3521e-05\n",
      "Epoch: 30850 | training loss: 7.7227e-04 | validation loss: 7.3514e-05\n",
      "Epoch: 30860 | training loss: 7.7220e-04 | validation loss: 7.3507e-05\n",
      "Epoch: 30870 | training loss: 7.7213e-04 | validation loss: 7.3499e-05\n",
      "Epoch: 30880 | training loss: 7.7206e-04 | validation loss: 7.3492e-05\n",
      "Epoch: 30890 | training loss: 7.7199e-04 | validation loss: 7.3485e-05\n",
      "Epoch: 30900 | training loss: 7.7192e-04 | validation loss: 7.3478e-05\n",
      "Epoch: 30910 | training loss: 7.7185e-04 | validation loss: 7.3470e-05\n",
      "Epoch: 30920 | training loss: 7.7178e-04 | validation loss: 7.3463e-05\n",
      "Epoch: 30930 | training loss: 7.7171e-04 | validation loss: 7.3456e-05\n",
      "Epoch: 30940 | training loss: 7.7164e-04 | validation loss: 7.3448e-05\n",
      "Epoch: 30950 | training loss: 7.7157e-04 | validation loss: 7.3441e-05\n",
      "Epoch: 30960 | training loss: 7.7150e-04 | validation loss: 7.3433e-05\n",
      "Epoch: 30970 | training loss: 7.7142e-04 | validation loss: 7.3426e-05\n",
      "Epoch: 30980 | training loss: 7.7135e-04 | validation loss: 7.3418e-05\n",
      "Epoch: 30990 | training loss: 7.7128e-04 | validation loss: 7.3411e-05\n",
      "Epoch: 31000 | training loss: 7.7120e-04 | validation loss: 7.3403e-05\n",
      "Epoch: 31010 | training loss: 7.7113e-04 | validation loss: 7.3394e-05\n",
      "Epoch: 31020 | training loss: 7.7110e-04 | validation loss: 7.3390e-05\n",
      "Epoch: 31030 | training loss: 8.0643e-04 | validation loss: 9.6363e-05\n",
      "Epoch: 31040 | training loss: 7.7548e-04 | validation loss: 7.6084e-05\n",
      "Epoch: 31050 | training loss: 7.7675e-04 | validation loss: 7.7213e-05\n",
      "Epoch: 31060 | training loss: 7.7152e-04 | validation loss: 7.4007e-05\n",
      "Epoch: 31070 | training loss: 7.7117e-04 | validation loss: 7.3723e-05\n",
      "Epoch: 31080 | training loss: 7.7088e-04 | validation loss: 7.3438e-05\n",
      "Epoch: 31090 | training loss: 7.7065e-04 | validation loss: 7.3365e-05\n",
      "Epoch: 31100 | training loss: 7.7058e-04 | validation loss: 7.3393e-05\n",
      "Epoch: 31110 | training loss: 7.7051e-04 | validation loss: 7.3367e-05\n",
      "Epoch: 31120 | training loss: 7.7044e-04 | validation loss: 7.3342e-05\n",
      "Epoch: 31130 | training loss: 7.7038e-04 | validation loss: 7.3335e-05\n",
      "Epoch: 31140 | training loss: 7.7031e-04 | validation loss: 7.3332e-05\n",
      "Epoch: 31150 | training loss: 7.7025e-04 | validation loss: 7.3327e-05\n",
      "Epoch: 31160 | training loss: 7.7019e-04 | validation loss: 7.3321e-05\n",
      "Epoch: 31170 | training loss: 7.7013e-04 | validation loss: 7.3315e-05\n",
      "Epoch: 31180 | training loss: 7.7006e-04 | validation loss: 7.3308e-05\n",
      "Epoch: 31190 | training loss: 7.7000e-04 | validation loss: 7.3301e-05\n",
      "Epoch: 31200 | training loss: 7.6994e-04 | validation loss: 7.3294e-05\n",
      "Epoch: 31210 | training loss: 7.6987e-04 | validation loss: 7.3287e-05\n",
      "Epoch: 31220 | training loss: 7.6981e-04 | validation loss: 7.3280e-05\n",
      "Epoch: 31230 | training loss: 7.6974e-04 | validation loss: 7.3274e-05\n",
      "Epoch: 31240 | training loss: 7.6968e-04 | validation loss: 7.3267e-05\n",
      "Epoch: 31250 | training loss: 7.6961e-04 | validation loss: 7.3260e-05\n",
      "Epoch: 31260 | training loss: 7.6955e-04 | validation loss: 7.3253e-05\n",
      "Epoch: 31270 | training loss: 7.6948e-04 | validation loss: 7.3246e-05\n",
      "Epoch: 31280 | training loss: 7.6942e-04 | validation loss: 7.3239e-05\n",
      "Epoch: 31290 | training loss: 7.6935e-04 | validation loss: 7.3232e-05\n",
      "Epoch: 31300 | training loss: 7.6928e-04 | validation loss: 7.3225e-05\n",
      "Epoch: 31310 | training loss: 7.6922e-04 | validation loss: 7.3218e-05\n",
      "Epoch: 31320 | training loss: 7.6915e-04 | validation loss: 7.3211e-05\n",
      "Epoch: 31330 | training loss: 7.6908e-04 | validation loss: 7.3203e-05\n",
      "Epoch: 31340 | training loss: 7.6901e-04 | validation loss: 7.3196e-05\n",
      "Epoch: 31350 | training loss: 7.6895e-04 | validation loss: 7.3189e-05\n",
      "Epoch: 31360 | training loss: 7.6888e-04 | validation loss: 7.3182e-05\n",
      "Epoch: 31370 | training loss: 7.6881e-04 | validation loss: 7.3175e-05\n",
      "Epoch: 31380 | training loss: 7.6874e-04 | validation loss: 7.3167e-05\n",
      "Epoch: 31390 | training loss: 7.6867e-04 | validation loss: 7.3160e-05\n",
      "Epoch: 31400 | training loss: 7.6860e-04 | validation loss: 7.3153e-05\n",
      "Epoch: 31410 | training loss: 7.6853e-04 | validation loss: 7.3145e-05\n",
      "Epoch: 31420 | training loss: 7.6846e-04 | validation loss: 7.3138e-05\n",
      "Epoch: 31430 | training loss: 7.6839e-04 | validation loss: 7.3131e-05\n",
      "Epoch: 31440 | training loss: 7.6832e-04 | validation loss: 7.3123e-05\n",
      "Epoch: 31450 | training loss: 7.6825e-04 | validation loss: 7.3116e-05\n",
      "Epoch: 31460 | training loss: 7.6818e-04 | validation loss: 7.3108e-05\n",
      "Epoch: 31470 | training loss: 7.6813e-04 | validation loss: 7.3096e-05\n",
      "Epoch: 31480 | training loss: 7.8655e-04 | validation loss: 8.4923e-05\n",
      "Epoch: 31490 | training loss: 7.7167e-04 | validation loss: 7.6034e-05\n",
      "Epoch: 31500 | training loss: 7.7233e-04 | validation loss: 7.5888e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31510 | training loss: 7.6866e-04 | validation loss: 7.3500e-05\n",
      "Epoch: 31520 | training loss: 7.6852e-04 | validation loss: 7.3677e-05\n",
      "Epoch: 31530 | training loss: 7.6778e-04 | validation loss: 7.3137e-05\n",
      "Epoch: 31540 | training loss: 7.6777e-04 | validation loss: 7.3102e-05\n",
      "Epoch: 31550 | training loss: 7.6763e-04 | validation loss: 7.3065e-05\n",
      "Epoch: 31560 | training loss: 7.6757e-04 | validation loss: 7.3081e-05\n",
      "Epoch: 31570 | training loss: 7.6750e-04 | validation loss: 7.3071e-05\n",
      "Epoch: 31580 | training loss: 7.6744e-04 | validation loss: 7.3054e-05\n",
      "Epoch: 31590 | training loss: 7.6738e-04 | validation loss: 7.3045e-05\n",
      "Epoch: 31600 | training loss: 7.6732e-04 | validation loss: 7.3039e-05\n",
      "Epoch: 31610 | training loss: 7.6726e-04 | validation loss: 7.3034e-05\n",
      "Epoch: 31620 | training loss: 7.6719e-04 | validation loss: 7.3028e-05\n",
      "Epoch: 31630 | training loss: 7.6713e-04 | validation loss: 7.3022e-05\n",
      "Epoch: 31640 | training loss: 7.6707e-04 | validation loss: 7.3015e-05\n",
      "Epoch: 31650 | training loss: 7.6701e-04 | validation loss: 7.3009e-05\n",
      "Epoch: 31660 | training loss: 7.6695e-04 | validation loss: 7.3002e-05\n",
      "Epoch: 31670 | training loss: 7.6688e-04 | validation loss: 7.2995e-05\n",
      "Epoch: 31680 | training loss: 7.6682e-04 | validation loss: 7.2989e-05\n",
      "Epoch: 31690 | training loss: 7.6676e-04 | validation loss: 7.2982e-05\n",
      "Epoch: 31700 | training loss: 7.6669e-04 | validation loss: 7.2975e-05\n",
      "Epoch: 31710 | training loss: 7.6663e-04 | validation loss: 7.2968e-05\n",
      "Epoch: 31720 | training loss: 7.6656e-04 | validation loss: 7.2961e-05\n",
      "Epoch: 31730 | training loss: 7.6650e-04 | validation loss: 7.2954e-05\n",
      "Epoch: 31740 | training loss: 7.6644e-04 | validation loss: 7.2947e-05\n",
      "Epoch: 31750 | training loss: 7.6637e-04 | validation loss: 7.2941e-05\n",
      "Epoch: 31760 | training loss: 7.6630e-04 | validation loss: 7.2934e-05\n",
      "Epoch: 31770 | training loss: 7.6624e-04 | validation loss: 7.2927e-05\n",
      "Epoch: 31780 | training loss: 7.6617e-04 | validation loss: 7.2920e-05\n",
      "Epoch: 31790 | training loss: 7.6611e-04 | validation loss: 7.2913e-05\n",
      "Epoch: 31800 | training loss: 7.6604e-04 | validation loss: 7.2905e-05\n",
      "Epoch: 31810 | training loss: 7.6597e-04 | validation loss: 7.2898e-05\n",
      "Epoch: 31820 | training loss: 7.6591e-04 | validation loss: 7.2891e-05\n",
      "Epoch: 31830 | training loss: 7.6584e-04 | validation loss: 7.2884e-05\n",
      "Epoch: 31840 | training loss: 7.6577e-04 | validation loss: 7.2877e-05\n",
      "Epoch: 31850 | training loss: 7.6570e-04 | validation loss: 7.2870e-05\n",
      "Epoch: 31860 | training loss: 7.6563e-04 | validation loss: 7.2863e-05\n",
      "Epoch: 31870 | training loss: 7.6557e-04 | validation loss: 7.2855e-05\n",
      "Epoch: 31880 | training loss: 7.6550e-04 | validation loss: 7.2848e-05\n",
      "Epoch: 31890 | training loss: 7.6543e-04 | validation loss: 7.2841e-05\n",
      "Epoch: 31900 | training loss: 7.6536e-04 | validation loss: 7.2833e-05\n",
      "Epoch: 31910 | training loss: 7.6529e-04 | validation loss: 7.2822e-05\n",
      "Epoch: 31920 | training loss: 7.6701e-04 | validation loss: 7.3845e-05\n",
      "Epoch: 31930 | training loss: 7.6886e-04 | validation loss: 7.5457e-05\n",
      "Epoch: 31940 | training loss: 7.7453e-04 | validation loss: 7.9688e-05\n",
      "Epoch: 31950 | training loss: 7.6554e-04 | validation loss: 7.3227e-05\n",
      "Epoch: 31960 | training loss: 7.6617e-04 | validation loss: 7.3479e-05\n",
      "Epoch: 31970 | training loss: 7.6495e-04 | validation loss: 7.2806e-05\n",
      "Epoch: 31980 | training loss: 7.6502e-04 | validation loss: 7.2950e-05\n",
      "Epoch: 31990 | training loss: 7.6482e-04 | validation loss: 7.2817e-05\n",
      "Epoch: 32000 | training loss: 7.6476e-04 | validation loss: 7.2785e-05\n",
      "Epoch: 32010 | training loss: 7.6470e-04 | validation loss: 7.2780e-05\n",
      "Epoch: 32020 | training loss: 7.6463e-04 | validation loss: 7.2779e-05\n",
      "Epoch: 32030 | training loss: 7.6457e-04 | validation loss: 7.2777e-05\n",
      "Epoch: 32040 | training loss: 7.6451e-04 | validation loss: 7.2770e-05\n",
      "Epoch: 32050 | training loss: 7.6445e-04 | validation loss: 7.2763e-05\n",
      "Epoch: 32060 | training loss: 7.6439e-04 | validation loss: 7.2756e-05\n",
      "Epoch: 32070 | training loss: 7.6433e-04 | validation loss: 7.2749e-05\n",
      "Epoch: 32080 | training loss: 7.6427e-04 | validation loss: 7.2743e-05\n",
      "Epoch: 32090 | training loss: 7.6421e-04 | validation loss: 7.2736e-05\n",
      "Epoch: 32100 | training loss: 7.6415e-04 | validation loss: 7.2730e-05\n",
      "Epoch: 32110 | training loss: 7.6409e-04 | validation loss: 7.2723e-05\n",
      "Epoch: 32120 | training loss: 7.6403e-04 | validation loss: 7.2717e-05\n",
      "Epoch: 32130 | training loss: 7.6397e-04 | validation loss: 7.2710e-05\n",
      "Epoch: 32140 | training loss: 7.6391e-04 | validation loss: 7.2704e-05\n",
      "Epoch: 32150 | training loss: 7.6384e-04 | validation loss: 7.2697e-05\n",
      "Epoch: 32160 | training loss: 7.6378e-04 | validation loss: 7.2690e-05\n",
      "Epoch: 32170 | training loss: 7.6372e-04 | validation loss: 7.2684e-05\n",
      "Epoch: 32180 | training loss: 7.6365e-04 | validation loss: 7.2677e-05\n",
      "Epoch: 32190 | training loss: 7.6359e-04 | validation loss: 7.2670e-05\n",
      "Epoch: 32200 | training loss: 7.6353e-04 | validation loss: 7.2663e-05\n",
      "Epoch: 32210 | training loss: 7.6346e-04 | validation loss: 7.2657e-05\n",
      "Epoch: 32220 | training loss: 7.6340e-04 | validation loss: 7.2650e-05\n",
      "Epoch: 32230 | training loss: 7.6333e-04 | validation loss: 7.2643e-05\n",
      "Epoch: 32240 | training loss: 7.6327e-04 | validation loss: 7.2636e-05\n",
      "Epoch: 32250 | training loss: 7.6320e-04 | validation loss: 7.2629e-05\n",
      "Epoch: 32260 | training loss: 7.6314e-04 | validation loss: 7.2622e-05\n",
      "Epoch: 32270 | training loss: 7.6307e-04 | validation loss: 7.2616e-05\n",
      "Epoch: 32280 | training loss: 7.6301e-04 | validation loss: 7.2609e-05\n",
      "Epoch: 32290 | training loss: 7.6294e-04 | validation loss: 7.2602e-05\n",
      "Epoch: 32300 | training loss: 7.6287e-04 | validation loss: 7.2595e-05\n",
      "Epoch: 32310 | training loss: 7.6281e-04 | validation loss: 7.2588e-05\n",
      "Epoch: 32320 | training loss: 7.6274e-04 | validation loss: 7.2581e-05\n",
      "Epoch: 32330 | training loss: 7.6267e-04 | validation loss: 7.2574e-05\n",
      "Epoch: 32340 | training loss: 7.6260e-04 | validation loss: 7.2567e-05\n",
      "Epoch: 32350 | training loss: 7.6254e-04 | validation loss: 7.2570e-05\n",
      "Epoch: 32360 | training loss: 7.6533e-04 | validation loss: 7.4682e-05\n",
      "Epoch: 32370 | training loss: 7.7677e-04 | validation loss: 8.1633e-05\n",
      "Epoch: 32380 | training loss: 7.7225e-04 | validation loss: 7.9068e-05\n",
      "Epoch: 32390 | training loss: 7.6233e-04 | validation loss: 7.2590e-05\n",
      "Epoch: 32400 | training loss: 7.6340e-04 | validation loss: 7.3454e-05\n",
      "Epoch: 32410 | training loss: 7.6219e-04 | validation loss: 7.2537e-05\n",
      "Epoch: 32420 | training loss: 7.6227e-04 | validation loss: 7.2587e-05\n",
      "Epoch: 32430 | training loss: 7.6207e-04 | validation loss: 7.2529e-05\n",
      "Epoch: 32440 | training loss: 7.6202e-04 | validation loss: 7.2553e-05\n",
      "Epoch: 32450 | training loss: 7.6196e-04 | validation loss: 7.2532e-05\n",
      "Epoch: 32460 | training loss: 7.6189e-04 | validation loss: 7.2515e-05\n",
      "Epoch: 32470 | training loss: 7.6184e-04 | validation loss: 7.2507e-05\n",
      "Epoch: 32480 | training loss: 7.6178e-04 | validation loss: 7.2502e-05\n",
      "Epoch: 32490 | training loss: 7.6172e-04 | validation loss: 7.2497e-05\n",
      "Epoch: 32500 | training loss: 7.6166e-04 | validation loss: 7.2491e-05\n",
      "Epoch: 32510 | training loss: 7.6160e-04 | validation loss: 7.2486e-05\n",
      "Epoch: 32520 | training loss: 7.6154e-04 | validation loss: 7.2480e-05\n",
      "Epoch: 32530 | training loss: 7.6148e-04 | validation loss: 7.2473e-05\n",
      "Epoch: 32540 | training loss: 7.6142e-04 | validation loss: 7.2467e-05\n",
      "Epoch: 32550 | training loss: 7.6136e-04 | validation loss: 7.2461e-05\n",
      "Epoch: 32560 | training loss: 7.6130e-04 | validation loss: 7.2455e-05\n",
      "Epoch: 32570 | training loss: 7.6124e-04 | validation loss: 7.2448e-05\n",
      "Epoch: 32580 | training loss: 7.6118e-04 | validation loss: 7.2442e-05\n",
      "Epoch: 32590 | training loss: 7.6112e-04 | validation loss: 7.2436e-05\n",
      "Epoch: 32600 | training loss: 7.6106e-04 | validation loss: 7.2429e-05\n",
      "Epoch: 32610 | training loss: 7.6100e-04 | validation loss: 7.2423e-05\n",
      "Epoch: 32620 | training loss: 7.6094e-04 | validation loss: 7.2416e-05\n",
      "Epoch: 32630 | training loss: 7.6087e-04 | validation loss: 7.2410e-05\n",
      "Epoch: 32640 | training loss: 7.6081e-04 | validation loss: 7.2403e-05\n",
      "Epoch: 32650 | training loss: 7.6075e-04 | validation loss: 7.2397e-05\n",
      "Epoch: 32660 | training loss: 7.6068e-04 | validation loss: 7.2390e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32670 | training loss: 7.6062e-04 | validation loss: 7.2384e-05\n",
      "Epoch: 32680 | training loss: 7.6056e-04 | validation loss: 7.2377e-05\n",
      "Epoch: 32690 | training loss: 7.6049e-04 | validation loss: 7.2371e-05\n",
      "Epoch: 32700 | training loss: 7.6043e-04 | validation loss: 7.2364e-05\n",
      "Epoch: 32710 | training loss: 7.6037e-04 | validation loss: 7.2357e-05\n",
      "Epoch: 32720 | training loss: 7.6030e-04 | validation loss: 7.2351e-05\n",
      "Epoch: 32730 | training loss: 7.6024e-04 | validation loss: 7.2344e-05\n",
      "Epoch: 32740 | training loss: 7.6017e-04 | validation loss: 7.2337e-05\n",
      "Epoch: 32750 | training loss: 7.6010e-04 | validation loss: 7.2331e-05\n",
      "Epoch: 32760 | training loss: 7.6004e-04 | validation loss: 7.2324e-05\n",
      "Epoch: 32770 | training loss: 7.5997e-04 | validation loss: 7.2317e-05\n",
      "Epoch: 32780 | training loss: 7.5991e-04 | validation loss: 7.2311e-05\n",
      "Epoch: 32790 | training loss: 7.5986e-04 | validation loss: 7.2334e-05\n",
      "Epoch: 32800 | training loss: 7.7591e-04 | validation loss: 8.3605e-05\n",
      "Epoch: 32810 | training loss: 7.6716e-04 | validation loss: 7.7247e-05\n",
      "Epoch: 32820 | training loss: 7.6246e-04 | validation loss: 7.4412e-05\n",
      "Epoch: 32830 | training loss: 7.6109e-04 | validation loss: 7.3447e-05\n",
      "Epoch: 32840 | training loss: 7.6007e-04 | validation loss: 7.2556e-05\n",
      "Epoch: 32850 | training loss: 7.5963e-04 | validation loss: 7.2330e-05\n",
      "Epoch: 32860 | training loss: 7.5951e-04 | validation loss: 7.2360e-05\n",
      "Epoch: 32870 | training loss: 7.5941e-04 | validation loss: 7.2317e-05\n",
      "Epoch: 32880 | training loss: 7.5933e-04 | validation loss: 7.2269e-05\n",
      "Epoch: 32890 | training loss: 7.5928e-04 | validation loss: 7.2263e-05\n",
      "Epoch: 32900 | training loss: 7.5922e-04 | validation loss: 7.2260e-05\n",
      "Epoch: 32910 | training loss: 7.5916e-04 | validation loss: 7.2259e-05\n",
      "Epoch: 32920 | training loss: 7.5910e-04 | validation loss: 7.2254e-05\n",
      "Epoch: 32930 | training loss: 7.5905e-04 | validation loss: 7.2248e-05\n",
      "Epoch: 32940 | training loss: 7.5899e-04 | validation loss: 7.2241e-05\n",
      "Epoch: 32950 | training loss: 7.5893e-04 | validation loss: 7.2235e-05\n",
      "Epoch: 32960 | training loss: 7.5887e-04 | validation loss: 7.2229e-05\n",
      "Epoch: 32970 | training loss: 7.5882e-04 | validation loss: 7.2223e-05\n",
      "Epoch: 32980 | training loss: 7.5876e-04 | validation loss: 7.2217e-05\n",
      "Epoch: 32990 | training loss: 7.5870e-04 | validation loss: 7.2211e-05\n",
      "Epoch: 33000 | training loss: 7.5864e-04 | validation loss: 7.2205e-05\n",
      "Epoch: 33010 | training loss: 7.5858e-04 | validation loss: 7.2199e-05\n",
      "Epoch: 33020 | training loss: 7.5852e-04 | validation loss: 7.2193e-05\n",
      "Epoch: 33030 | training loss: 7.5846e-04 | validation loss: 7.2187e-05\n",
      "Epoch: 33040 | training loss: 7.5840e-04 | validation loss: 7.2181e-05\n",
      "Epoch: 33050 | training loss: 7.5834e-04 | validation loss: 7.2175e-05\n",
      "Epoch: 33060 | training loss: 7.5828e-04 | validation loss: 7.2169e-05\n",
      "Epoch: 33070 | training loss: 7.5822e-04 | validation loss: 7.2162e-05\n",
      "Epoch: 33080 | training loss: 7.5816e-04 | validation loss: 7.2156e-05\n",
      "Epoch: 33090 | training loss: 7.5810e-04 | validation loss: 7.2150e-05\n",
      "Epoch: 33100 | training loss: 7.5804e-04 | validation loss: 7.2144e-05\n",
      "Epoch: 33110 | training loss: 7.5797e-04 | validation loss: 7.2138e-05\n",
      "Epoch: 33120 | training loss: 7.5791e-04 | validation loss: 7.2131e-05\n",
      "Epoch: 33130 | training loss: 7.5785e-04 | validation loss: 7.2125e-05\n",
      "Epoch: 33140 | training loss: 7.5779e-04 | validation loss: 7.2119e-05\n",
      "Epoch: 33150 | training loss: 7.5772e-04 | validation loss: 7.2112e-05\n",
      "Epoch: 33160 | training loss: 7.5766e-04 | validation loss: 7.2106e-05\n",
      "Epoch: 33170 | training loss: 7.5760e-04 | validation loss: 7.2100e-05\n",
      "Epoch: 33180 | training loss: 7.5753e-04 | validation loss: 7.2093e-05\n",
      "Epoch: 33190 | training loss: 7.5747e-04 | validation loss: 7.2087e-05\n",
      "Epoch: 33200 | training loss: 7.5740e-04 | validation loss: 7.2081e-05\n",
      "Epoch: 33210 | training loss: 7.5734e-04 | validation loss: 7.2074e-05\n",
      "Epoch: 33220 | training loss: 7.5727e-04 | validation loss: 7.2068e-05\n",
      "Epoch: 33230 | training loss: 7.5721e-04 | validation loss: 7.2069e-05\n",
      "Epoch: 33240 | training loss: 7.5908e-04 | validation loss: 7.3520e-05\n",
      "Epoch: 33250 | training loss: 7.6262e-04 | validation loss: 7.5380e-05\n",
      "Epoch: 33260 | training loss: 7.6746e-04 | validation loss: 7.8860e-05\n",
      "Epoch: 33270 | training loss: 7.5712e-04 | validation loss: 7.2092e-05\n",
      "Epoch: 33280 | training loss: 7.5823e-04 | validation loss: 7.3065e-05\n",
      "Epoch: 33290 | training loss: 7.5687e-04 | validation loss: 7.2045e-05\n",
      "Epoch: 33300 | training loss: 7.5698e-04 | validation loss: 7.2100e-05\n",
      "Epoch: 33310 | training loss: 7.5676e-04 | validation loss: 7.2039e-05\n",
      "Epoch: 33320 | training loss: 7.5673e-04 | validation loss: 7.2064e-05\n",
      "Epoch: 33330 | training loss: 7.5665e-04 | validation loss: 7.2036e-05\n",
      "Epoch: 33340 | training loss: 7.5660e-04 | validation loss: 7.2021e-05\n",
      "Epoch: 33350 | training loss: 7.5654e-04 | validation loss: 7.2016e-05\n",
      "Epoch: 33360 | training loss: 7.5649e-04 | validation loss: 7.2012e-05\n",
      "Epoch: 33370 | training loss: 7.5643e-04 | validation loss: 7.2008e-05\n",
      "Epoch: 33380 | training loss: 7.5638e-04 | validation loss: 7.2003e-05\n",
      "Epoch: 33390 | training loss: 7.5632e-04 | validation loss: 7.1998e-05\n",
      "Epoch: 33400 | training loss: 7.5626e-04 | validation loss: 7.1992e-05\n",
      "Epoch: 33410 | training loss: 7.5621e-04 | validation loss: 7.1986e-05\n",
      "Epoch: 33420 | training loss: 7.5615e-04 | validation loss: 7.1981e-05\n",
      "Epoch: 33430 | training loss: 7.5609e-04 | validation loss: 7.1975e-05\n",
      "Epoch: 33440 | training loss: 7.5603e-04 | validation loss: 7.1969e-05\n",
      "Epoch: 33450 | training loss: 7.5598e-04 | validation loss: 7.1964e-05\n",
      "Epoch: 33460 | training loss: 7.5592e-04 | validation loss: 7.1958e-05\n",
      "Epoch: 33470 | training loss: 7.5586e-04 | validation loss: 7.1952e-05\n",
      "Epoch: 33480 | training loss: 7.5580e-04 | validation loss: 7.1946e-05\n",
      "Epoch: 33490 | training loss: 7.5574e-04 | validation loss: 7.1940e-05\n",
      "Epoch: 33500 | training loss: 7.5568e-04 | validation loss: 7.1935e-05\n",
      "Epoch: 33510 | training loss: 7.5562e-04 | validation loss: 7.1929e-05\n",
      "Epoch: 33520 | training loss: 7.5556e-04 | validation loss: 7.1923e-05\n",
      "Epoch: 33530 | training loss: 7.5551e-04 | validation loss: 7.1917e-05\n",
      "Epoch: 33540 | training loss: 7.5544e-04 | validation loss: 7.1911e-05\n",
      "Epoch: 33550 | training loss: 7.5538e-04 | validation loss: 7.1905e-05\n",
      "Epoch: 33560 | training loss: 7.5532e-04 | validation loss: 7.1899e-05\n",
      "Epoch: 33570 | training loss: 7.5526e-04 | validation loss: 7.1893e-05\n",
      "Epoch: 33580 | training loss: 7.5520e-04 | validation loss: 7.1887e-05\n",
      "Epoch: 33590 | training loss: 7.5514e-04 | validation loss: 7.1881e-05\n",
      "Epoch: 33600 | training loss: 7.5508e-04 | validation loss: 7.1875e-05\n",
      "Epoch: 33610 | training loss: 7.5502e-04 | validation loss: 7.1869e-05\n",
      "Epoch: 33620 | training loss: 7.5495e-04 | validation loss: 7.1863e-05\n",
      "Epoch: 33630 | training loss: 7.5489e-04 | validation loss: 7.1857e-05\n",
      "Epoch: 33640 | training loss: 7.5483e-04 | validation loss: 7.1851e-05\n",
      "Epoch: 33650 | training loss: 7.5477e-04 | validation loss: 7.1845e-05\n",
      "Epoch: 33660 | training loss: 7.5470e-04 | validation loss: 7.1839e-05\n",
      "Epoch: 33670 | training loss: 7.5464e-04 | validation loss: 7.1834e-05\n",
      "Epoch: 33680 | training loss: 7.5459e-04 | validation loss: 7.1857e-05\n",
      "Epoch: 33690 | training loss: 7.7258e-04 | validation loss: 8.4440e-05\n",
      "Epoch: 33700 | training loss: 7.5891e-04 | validation loss: 7.4791e-05\n",
      "Epoch: 33710 | training loss: 7.5919e-04 | validation loss: 7.5373e-05\n",
      "Epoch: 33720 | training loss: 7.5500e-04 | validation loss: 7.2339e-05\n",
      "Epoch: 33730 | training loss: 7.5516e-04 | validation loss: 7.2279e-05\n",
      "Epoch: 33740 | training loss: 7.5426e-04 | validation loss: 7.1810e-05\n",
      "Epoch: 33750 | training loss: 7.5431e-04 | validation loss: 7.1931e-05\n",
      "Epoch: 33760 | training loss: 7.5414e-04 | validation loss: 7.1814e-05\n",
      "Epoch: 33770 | training loss: 7.5410e-04 | validation loss: 7.1798e-05\n",
      "Epoch: 33780 | training loss: 7.5404e-04 | validation loss: 7.1794e-05\n",
      "Epoch: 33790 | training loss: 7.5398e-04 | validation loss: 7.1796e-05\n",
      "Epoch: 33800 | training loss: 7.5393e-04 | validation loss: 7.1792e-05\n",
      "Epoch: 33810 | training loss: 7.5387e-04 | validation loss: 7.1785e-05\n",
      "Epoch: 33820 | training loss: 7.5382e-04 | validation loss: 7.1779e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33830 | training loss: 7.5376e-04 | validation loss: 7.1773e-05\n",
      "Epoch: 33840 | training loss: 7.5371e-04 | validation loss: 7.1768e-05\n",
      "Epoch: 33850 | training loss: 7.5365e-04 | validation loss: 7.1763e-05\n",
      "Epoch: 33860 | training loss: 7.5360e-04 | validation loss: 7.1758e-05\n",
      "Epoch: 33870 | training loss: 7.5354e-04 | validation loss: 7.1752e-05\n",
      "Epoch: 33880 | training loss: 7.5348e-04 | validation loss: 7.1747e-05\n",
      "Epoch: 33890 | training loss: 7.5343e-04 | validation loss: 7.1742e-05\n",
      "Epoch: 33900 | training loss: 7.5337e-04 | validation loss: 7.1736e-05\n",
      "Epoch: 33910 | training loss: 7.5331e-04 | validation loss: 7.1731e-05\n",
      "Epoch: 33920 | training loss: 7.5326e-04 | validation loss: 7.1726e-05\n",
      "Epoch: 33930 | training loss: 7.5320e-04 | validation loss: 7.1720e-05\n",
      "Epoch: 33940 | training loss: 7.5314e-04 | validation loss: 7.1715e-05\n",
      "Epoch: 33950 | training loss: 7.5309e-04 | validation loss: 7.1709e-05\n",
      "Epoch: 33960 | training loss: 7.5303e-04 | validation loss: 7.1703e-05\n",
      "Epoch: 33970 | training loss: 7.5297e-04 | validation loss: 7.1698e-05\n",
      "Epoch: 33980 | training loss: 7.5291e-04 | validation loss: 7.1692e-05\n",
      "Epoch: 33990 | training loss: 7.5285e-04 | validation loss: 7.1687e-05\n",
      "Epoch: 34000 | training loss: 7.5279e-04 | validation loss: 7.1681e-05\n",
      "Epoch: 34010 | training loss: 7.5273e-04 | validation loss: 7.1675e-05\n",
      "Epoch: 34020 | training loss: 7.5267e-04 | validation loss: 7.1670e-05\n",
      "Epoch: 34030 | training loss: 7.5261e-04 | validation loss: 7.1664e-05\n",
      "Epoch: 34040 | training loss: 7.5255e-04 | validation loss: 7.1658e-05\n",
      "Epoch: 34050 | training loss: 7.5249e-04 | validation loss: 7.1653e-05\n",
      "Epoch: 34060 | training loss: 7.5243e-04 | validation loss: 7.1647e-05\n",
      "Epoch: 34070 | training loss: 7.5237e-04 | validation loss: 7.1641e-05\n",
      "Epoch: 34080 | training loss: 7.5231e-04 | validation loss: 7.1635e-05\n",
      "Epoch: 34090 | training loss: 7.5225e-04 | validation loss: 7.1630e-05\n",
      "Epoch: 34100 | training loss: 7.5219e-04 | validation loss: 7.1624e-05\n",
      "Epoch: 34110 | training loss: 7.5212e-04 | validation loss: 7.1618e-05\n",
      "Epoch: 34120 | training loss: 7.5206e-04 | validation loss: 7.1613e-05\n",
      "Epoch: 34130 | training loss: 7.5201e-04 | validation loss: 7.1623e-05\n",
      "Epoch: 34140 | training loss: 7.5899e-04 | validation loss: 7.6639e-05\n",
      "Epoch: 34150 | training loss: 7.7949e-04 | validation loss: 8.9770e-05\n",
      "Epoch: 34160 | training loss: 7.5278e-04 | validation loss: 7.2192e-05\n",
      "Epoch: 34170 | training loss: 7.5478e-04 | validation loss: 7.3900e-05\n",
      "Epoch: 34180 | training loss: 7.5176e-04 | validation loss: 7.1630e-05\n",
      "Epoch: 34190 | training loss: 7.5207e-04 | validation loss: 7.1776e-05\n",
      "Epoch: 34200 | training loss: 7.5163e-04 | validation loss: 7.1597e-05\n",
      "Epoch: 34210 | training loss: 7.5162e-04 | validation loss: 7.1647e-05\n",
      "Epoch: 34220 | training loss: 7.5152e-04 | validation loss: 7.1589e-05\n",
      "Epoch: 34230 | training loss: 7.5147e-04 | validation loss: 7.1575e-05\n",
      "Epoch: 34240 | training loss: 7.5142e-04 | validation loss: 7.1571e-05\n",
      "Epoch: 34250 | training loss: 7.5136e-04 | validation loss: 7.1570e-05\n",
      "Epoch: 34260 | training loss: 7.5131e-04 | validation loss: 7.1567e-05\n",
      "Epoch: 34270 | training loss: 7.5125e-04 | validation loss: 7.1562e-05\n",
      "Epoch: 34280 | training loss: 7.5120e-04 | validation loss: 7.1557e-05\n",
      "Epoch: 34290 | training loss: 7.5115e-04 | validation loss: 7.1552e-05\n",
      "Epoch: 34300 | training loss: 7.5109e-04 | validation loss: 7.1547e-05\n",
      "Epoch: 34310 | training loss: 7.5104e-04 | validation loss: 7.1542e-05\n",
      "Epoch: 34320 | training loss: 7.5098e-04 | validation loss: 7.1537e-05\n",
      "Epoch: 34330 | training loss: 7.5093e-04 | validation loss: 7.1531e-05\n",
      "Epoch: 34340 | training loss: 7.5087e-04 | validation loss: 7.1526e-05\n",
      "Epoch: 34350 | training loss: 7.5082e-04 | validation loss: 7.1521e-05\n",
      "Epoch: 34360 | training loss: 7.5076e-04 | validation loss: 7.1516e-05\n",
      "Epoch: 34370 | training loss: 7.5071e-04 | validation loss: 7.1511e-05\n",
      "Epoch: 34380 | training loss: 7.5065e-04 | validation loss: 7.1506e-05\n",
      "Epoch: 34390 | training loss: 7.5059e-04 | validation loss: 7.1500e-05\n",
      "Epoch: 34400 | training loss: 7.5054e-04 | validation loss: 7.1495e-05\n",
      "Epoch: 34410 | training loss: 7.5048e-04 | validation loss: 7.1490e-05\n",
      "Epoch: 34420 | training loss: 7.5042e-04 | validation loss: 7.1484e-05\n",
      "Epoch: 34430 | training loss: 7.5037e-04 | validation loss: 7.1479e-05\n",
      "Epoch: 34440 | training loss: 7.5031e-04 | validation loss: 7.1474e-05\n",
      "Epoch: 34450 | training loss: 7.5025e-04 | validation loss: 7.1468e-05\n",
      "Epoch: 34460 | training loss: 7.5019e-04 | validation loss: 7.1463e-05\n",
      "Epoch: 34470 | training loss: 7.5013e-04 | validation loss: 7.1457e-05\n",
      "Epoch: 34480 | training loss: 7.5007e-04 | validation loss: 7.1452e-05\n",
      "Epoch: 34490 | training loss: 7.5001e-04 | validation loss: 7.1447e-05\n",
      "Epoch: 34500 | training loss: 7.4996e-04 | validation loss: 7.1441e-05\n",
      "Epoch: 34510 | training loss: 7.4990e-04 | validation loss: 7.1436e-05\n",
      "Epoch: 34520 | training loss: 7.4984e-04 | validation loss: 7.1430e-05\n",
      "Epoch: 34530 | training loss: 7.4978e-04 | validation loss: 7.1425e-05\n",
      "Epoch: 34540 | training loss: 7.4972e-04 | validation loss: 7.1419e-05\n",
      "Epoch: 34550 | training loss: 7.4965e-04 | validation loss: 7.1414e-05\n",
      "Epoch: 34560 | training loss: 7.4959e-04 | validation loss: 7.1408e-05\n",
      "Epoch: 34570 | training loss: 7.4953e-04 | validation loss: 7.1402e-05\n",
      "Epoch: 34580 | training loss: 7.4951e-04 | validation loss: 7.1399e-05\n",
      "Epoch: 34590 | training loss: 7.8023e-04 | validation loss: 9.1385e-05\n",
      "Epoch: 34600 | training loss: 7.5044e-04 | validation loss: 7.1958e-05\n",
      "Epoch: 34610 | training loss: 7.5644e-04 | validation loss: 7.6061e-05\n",
      "Epoch: 34620 | training loss: 7.4933e-04 | validation loss: 7.1476e-05\n",
      "Epoch: 34630 | training loss: 7.4999e-04 | validation loss: 7.2010e-05\n",
      "Epoch: 34640 | training loss: 7.4920e-04 | validation loss: 7.1393e-05\n",
      "Epoch: 34650 | training loss: 7.4920e-04 | validation loss: 7.1410e-05\n",
      "Epoch: 34660 | training loss: 7.4906e-04 | validation loss: 7.1388e-05\n",
      "Epoch: 34670 | training loss: 7.4902e-04 | validation loss: 7.1399e-05\n",
      "Epoch: 34680 | training loss: 7.4895e-04 | validation loss: 7.1375e-05\n",
      "Epoch: 34690 | training loss: 7.4890e-04 | validation loss: 7.1365e-05\n",
      "Epoch: 34700 | training loss: 7.4885e-04 | validation loss: 7.1360e-05\n",
      "Epoch: 34710 | training loss: 7.4880e-04 | validation loss: 7.1357e-05\n",
      "Epoch: 34720 | training loss: 7.4874e-04 | validation loss: 7.1354e-05\n",
      "Epoch: 34730 | training loss: 7.4869e-04 | validation loss: 7.1349e-05\n",
      "Epoch: 34740 | training loss: 7.4864e-04 | validation loss: 7.1345e-05\n",
      "Epoch: 34750 | training loss: 7.4859e-04 | validation loss: 7.1340e-05\n",
      "Epoch: 34760 | training loss: 7.4853e-04 | validation loss: 7.1335e-05\n",
      "Epoch: 34770 | training loss: 7.4848e-04 | validation loss: 7.1330e-05\n",
      "Epoch: 34780 | training loss: 7.4842e-04 | validation loss: 7.1325e-05\n",
      "Epoch: 34790 | training loss: 7.4837e-04 | validation loss: 7.1320e-05\n",
      "Epoch: 34800 | training loss: 7.4832e-04 | validation loss: 7.1315e-05\n",
      "Epoch: 34810 | training loss: 7.4826e-04 | validation loss: 7.1310e-05\n",
      "Epoch: 34820 | training loss: 7.4821e-04 | validation loss: 7.1305e-05\n",
      "Epoch: 34830 | training loss: 7.4815e-04 | validation loss: 7.1300e-05\n",
      "Epoch: 34840 | training loss: 7.4810e-04 | validation loss: 7.1295e-05\n",
      "Epoch: 34850 | training loss: 7.4804e-04 | validation loss: 7.1289e-05\n",
      "Epoch: 34860 | training loss: 7.4798e-04 | validation loss: 7.1284e-05\n",
      "Epoch: 34870 | training loss: 7.4793e-04 | validation loss: 7.1279e-05\n",
      "Epoch: 34880 | training loss: 7.4787e-04 | validation loss: 7.1274e-05\n",
      "Epoch: 34890 | training loss: 7.4782e-04 | validation loss: 7.1269e-05\n",
      "Epoch: 34900 | training loss: 7.4776e-04 | validation loss: 7.1264e-05\n",
      "Epoch: 34910 | training loss: 7.4770e-04 | validation loss: 7.1258e-05\n",
      "Epoch: 34920 | training loss: 7.4764e-04 | validation loss: 7.1253e-05\n",
      "Epoch: 34930 | training loss: 7.4759e-04 | validation loss: 7.1248e-05\n",
      "Epoch: 34940 | training loss: 7.4753e-04 | validation loss: 7.1243e-05\n",
      "Epoch: 34950 | training loss: 7.4747e-04 | validation loss: 7.1237e-05\n",
      "Epoch: 34960 | training loss: 7.4741e-04 | validation loss: 7.1232e-05\n",
      "Epoch: 34970 | training loss: 7.4735e-04 | validation loss: 7.1227e-05\n",
      "Epoch: 34980 | training loss: 7.4729e-04 | validation loss: 7.1221e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34990 | training loss: 7.4724e-04 | validation loss: 7.1216e-05\n",
      "Epoch: 35000 | training loss: 7.4718e-04 | validation loss: 7.1211e-05\n",
      "Epoch: 35010 | training loss: 7.4712e-04 | validation loss: 7.1205e-05\n",
      "Epoch: 35020 | training loss: 7.4706e-04 | validation loss: 7.1202e-05\n",
      "Epoch: 35030 | training loss: 7.4722e-04 | validation loss: 7.1402e-05\n",
      "Epoch: 35040 | training loss: 8.0020e-04 | validation loss: 1.0815e-04\n",
      "Epoch: 35050 | training loss: 7.5931e-04 | validation loss: 8.0297e-05\n",
      "Epoch: 35060 | training loss: 7.4750e-04 | validation loss: 7.1547e-05\n",
      "Epoch: 35070 | training loss: 7.4827e-04 | validation loss: 7.2042e-05\n",
      "Epoch: 35080 | training loss: 7.4692e-04 | validation loss: 7.1367e-05\n",
      "Epoch: 35090 | training loss: 7.4686e-04 | validation loss: 7.1342e-05\n",
      "Epoch: 35100 | training loss: 7.4666e-04 | validation loss: 7.1179e-05\n",
      "Epoch: 35110 | training loss: 7.4662e-04 | validation loss: 7.1178e-05\n",
      "Epoch: 35120 | training loss: 7.4654e-04 | validation loss: 7.1173e-05\n",
      "Epoch: 35130 | training loss: 7.4649e-04 | validation loss: 7.1177e-05\n",
      "Epoch: 35140 | training loss: 7.4644e-04 | validation loss: 7.1170e-05\n",
      "Epoch: 35150 | training loss: 7.4639e-04 | validation loss: 7.1162e-05\n",
      "Epoch: 35160 | training loss: 7.4634e-04 | validation loss: 7.1156e-05\n",
      "Epoch: 35170 | training loss: 7.4628e-04 | validation loss: 7.1151e-05\n",
      "Epoch: 35180 | training loss: 7.4623e-04 | validation loss: 7.1146e-05\n",
      "Epoch: 35190 | training loss: 7.4618e-04 | validation loss: 7.1142e-05\n",
      "Epoch: 35200 | training loss: 7.4613e-04 | validation loss: 7.1137e-05\n",
      "Epoch: 35210 | training loss: 7.4608e-04 | validation loss: 7.1133e-05\n",
      "Epoch: 35220 | training loss: 7.4602e-04 | validation loss: 7.1128e-05\n",
      "Epoch: 35230 | training loss: 7.4597e-04 | validation loss: 7.1123e-05\n",
      "Epoch: 35240 | training loss: 7.4592e-04 | validation loss: 7.1118e-05\n",
      "Epoch: 35250 | training loss: 7.4586e-04 | validation loss: 7.1113e-05\n",
      "Epoch: 35260 | training loss: 7.4581e-04 | validation loss: 7.1109e-05\n",
      "Epoch: 35270 | training loss: 7.4576e-04 | validation loss: 7.1104e-05\n",
      "Epoch: 35280 | training loss: 7.4570e-04 | validation loss: 7.1099e-05\n",
      "Epoch: 35290 | training loss: 7.4565e-04 | validation loss: 7.1094e-05\n",
      "Epoch: 35300 | training loss: 7.4559e-04 | validation loss: 7.1089e-05\n",
      "Epoch: 35310 | training loss: 7.4554e-04 | validation loss: 7.1084e-05\n",
      "Epoch: 35320 | training loss: 7.4548e-04 | validation loss: 7.1078e-05\n",
      "Epoch: 35330 | training loss: 7.4543e-04 | validation loss: 7.1073e-05\n",
      "Epoch: 35340 | training loss: 7.4537e-04 | validation loss: 7.1068e-05\n",
      "Epoch: 35350 | training loss: 7.4532e-04 | validation loss: 7.1063e-05\n",
      "Epoch: 35360 | training loss: 7.4526e-04 | validation loss: 7.1058e-05\n",
      "Epoch: 35370 | training loss: 7.4520e-04 | validation loss: 7.1053e-05\n",
      "Epoch: 35380 | training loss: 7.4515e-04 | validation loss: 7.1048e-05\n",
      "Epoch: 35390 | training loss: 7.4509e-04 | validation loss: 7.1043e-05\n",
      "Epoch: 35400 | training loss: 7.4503e-04 | validation loss: 7.1037e-05\n",
      "Epoch: 35410 | training loss: 7.4498e-04 | validation loss: 7.1032e-05\n",
      "Epoch: 35420 | training loss: 7.4492e-04 | validation loss: 7.1027e-05\n",
      "Epoch: 35430 | training loss: 7.4486e-04 | validation loss: 7.1022e-05\n",
      "Epoch: 35440 | training loss: 7.4480e-04 | validation loss: 7.1016e-05\n",
      "Epoch: 35450 | training loss: 7.4474e-04 | validation loss: 7.1011e-05\n",
      "Epoch: 35460 | training loss: 7.4468e-04 | validation loss: 7.1006e-05\n",
      "Epoch: 35470 | training loss: 7.4463e-04 | validation loss: 7.1017e-05\n",
      "Epoch: 35480 | training loss: 7.5263e-04 | validation loss: 7.6717e-05\n",
      "Epoch: 35490 | training loss: 7.7115e-04 | validation loss: 8.8602e-05\n",
      "Epoch: 35500 | training loss: 7.4457e-04 | validation loss: 7.1056e-05\n",
      "Epoch: 35510 | training loss: 7.4759e-04 | validation loss: 7.3400e-05\n",
      "Epoch: 35520 | training loss: 7.4452e-04 | validation loss: 7.1069e-05\n",
      "Epoch: 35530 | training loss: 7.4462e-04 | validation loss: 7.1123e-05\n",
      "Epoch: 35540 | training loss: 7.4432e-04 | validation loss: 7.1047e-05\n",
      "Epoch: 35550 | training loss: 7.4426e-04 | validation loss: 7.1030e-05\n",
      "Epoch: 35560 | training loss: 7.4418e-04 | validation loss: 7.0976e-05\n",
      "Epoch: 35570 | training loss: 7.4413e-04 | validation loss: 7.0972e-05\n",
      "Epoch: 35580 | training loss: 7.4408e-04 | validation loss: 7.0971e-05\n",
      "Epoch: 35590 | training loss: 7.4403e-04 | validation loss: 7.0971e-05\n",
      "Epoch: 35600 | training loss: 7.4398e-04 | validation loss: 7.0966e-05\n",
      "Epoch: 35610 | training loss: 7.4393e-04 | validation loss: 7.0960e-05\n",
      "Epoch: 35620 | training loss: 7.4388e-04 | validation loss: 7.0955e-05\n",
      "Epoch: 35630 | training loss: 7.4383e-04 | validation loss: 7.0950e-05\n",
      "Epoch: 35640 | training loss: 7.4378e-04 | validation loss: 7.0945e-05\n",
      "Epoch: 35650 | training loss: 7.4373e-04 | validation loss: 7.0941e-05\n",
      "Epoch: 35660 | training loss: 7.4367e-04 | validation loss: 7.0936e-05\n",
      "Epoch: 35670 | training loss: 7.4362e-04 | validation loss: 7.0931e-05\n",
      "Epoch: 35680 | training loss: 7.4357e-04 | validation loss: 7.0927e-05\n",
      "Epoch: 35690 | training loss: 7.4352e-04 | validation loss: 7.0922e-05\n",
      "Epoch: 35700 | training loss: 7.4347e-04 | validation loss: 7.0917e-05\n",
      "Epoch: 35710 | training loss: 7.4341e-04 | validation loss: 7.0912e-05\n",
      "Epoch: 35720 | training loss: 7.4336e-04 | validation loss: 7.0908e-05\n",
      "Epoch: 35730 | training loss: 7.4331e-04 | validation loss: 7.0903e-05\n",
      "Epoch: 35740 | training loss: 7.4326e-04 | validation loss: 7.0898e-05\n",
      "Epoch: 35750 | training loss: 7.4320e-04 | validation loss: 7.0893e-05\n",
      "Epoch: 35760 | training loss: 7.4315e-04 | validation loss: 7.0888e-05\n",
      "Epoch: 35770 | training loss: 7.4310e-04 | validation loss: 7.0883e-05\n",
      "Epoch: 35780 | training loss: 7.4304e-04 | validation loss: 7.0878e-05\n",
      "Epoch: 35790 | training loss: 7.4299e-04 | validation loss: 7.0873e-05\n",
      "Epoch: 35800 | training loss: 7.4293e-04 | validation loss: 7.0868e-05\n",
      "Epoch: 35810 | training loss: 7.4288e-04 | validation loss: 7.0863e-05\n",
      "Epoch: 35820 | training loss: 7.4282e-04 | validation loss: 7.0858e-05\n",
      "Epoch: 35830 | training loss: 7.4277e-04 | validation loss: 7.0853e-05\n",
      "Epoch: 35840 | training loss: 7.4271e-04 | validation loss: 7.0848e-05\n",
      "Epoch: 35850 | training loss: 7.4266e-04 | validation loss: 7.0843e-05\n",
      "Epoch: 35860 | training loss: 7.4260e-04 | validation loss: 7.0838e-05\n",
      "Epoch: 35870 | training loss: 7.4254e-04 | validation loss: 7.0833e-05\n",
      "Epoch: 35880 | training loss: 7.4249e-04 | validation loss: 7.0827e-05\n",
      "Epoch: 35890 | training loss: 7.4243e-04 | validation loss: 7.0822e-05\n",
      "Epoch: 35900 | training loss: 7.4237e-04 | validation loss: 7.0817e-05\n",
      "Epoch: 35910 | training loss: 7.4232e-04 | validation loss: 7.0812e-05\n",
      "Epoch: 35920 | training loss: 7.4226e-04 | validation loss: 7.0806e-05\n",
      "Epoch: 35930 | training loss: 7.4226e-04 | validation loss: 7.0812e-05\n",
      "Epoch: 35940 | training loss: 7.8624e-04 | validation loss: 9.9639e-05\n",
      "Epoch: 35950 | training loss: 7.5245e-04 | validation loss: 7.7322e-05\n",
      "Epoch: 35960 | training loss: 7.4584e-04 | validation loss: 7.3270e-05\n",
      "Epoch: 35970 | training loss: 7.4379e-04 | validation loss: 7.2208e-05\n",
      "Epoch: 35980 | training loss: 7.4203e-04 | validation loss: 7.0871e-05\n",
      "Epoch: 35990 | training loss: 7.4221e-04 | validation loss: 7.0929e-05\n",
      "Epoch: 36000 | training loss: 7.4186e-04 | validation loss: 7.0799e-05\n",
      "Epoch: 36010 | training loss: 7.4185e-04 | validation loss: 7.0834e-05\n",
      "Epoch: 36020 | training loss: 7.4176e-04 | validation loss: 7.0783e-05\n",
      "Epoch: 36030 | training loss: 7.4172e-04 | validation loss: 7.0775e-05\n",
      "Epoch: 36040 | training loss: 7.4167e-04 | validation loss: 7.0773e-05\n",
      "Epoch: 36050 | training loss: 7.4162e-04 | validation loss: 7.0772e-05\n",
      "Epoch: 36060 | training loss: 7.4157e-04 | validation loss: 7.0768e-05\n",
      "Epoch: 36070 | training loss: 7.4152e-04 | validation loss: 7.0763e-05\n",
      "Epoch: 36080 | training loss: 7.4147e-04 | validation loss: 7.0758e-05\n",
      "Epoch: 36090 | training loss: 7.4142e-04 | validation loss: 7.0753e-05\n",
      "Epoch: 36100 | training loss: 7.4137e-04 | validation loss: 7.0749e-05\n",
      "Epoch: 36110 | training loss: 7.4132e-04 | validation loss: 7.0744e-05\n",
      "Epoch: 36120 | training loss: 7.4127e-04 | validation loss: 7.0740e-05\n",
      "Epoch: 36130 | training loss: 7.4122e-04 | validation loss: 7.0735e-05\n",
      "Epoch: 36140 | training loss: 7.4117e-04 | validation loss: 7.0730e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36150 | training loss: 7.4112e-04 | validation loss: 7.0726e-05\n",
      "Epoch: 36160 | training loss: 7.4107e-04 | validation loss: 7.0721e-05\n",
      "Epoch: 36170 | training loss: 7.4102e-04 | validation loss: 7.0716e-05\n",
      "Epoch: 36180 | training loss: 7.4097e-04 | validation loss: 7.0712e-05\n",
      "Epoch: 36190 | training loss: 7.4092e-04 | validation loss: 7.0707e-05\n",
      "Epoch: 36200 | training loss: 7.4086e-04 | validation loss: 7.0702e-05\n",
      "Epoch: 36210 | training loss: 7.4081e-04 | validation loss: 7.0697e-05\n",
      "Epoch: 36220 | training loss: 7.4076e-04 | validation loss: 7.0692e-05\n",
      "Epoch: 36230 | training loss: 7.4071e-04 | validation loss: 7.0688e-05\n",
      "Epoch: 36240 | training loss: 7.4065e-04 | validation loss: 7.0683e-05\n",
      "Epoch: 36250 | training loss: 7.4060e-04 | validation loss: 7.0678e-05\n",
      "Epoch: 36260 | training loss: 7.4055e-04 | validation loss: 7.0673e-05\n",
      "Epoch: 36270 | training loss: 7.4049e-04 | validation loss: 7.0668e-05\n",
      "Epoch: 36280 | training loss: 7.4044e-04 | validation loss: 7.0663e-05\n",
      "Epoch: 36290 | training loss: 7.4039e-04 | validation loss: 7.0658e-05\n",
      "Epoch: 36300 | training loss: 7.4033e-04 | validation loss: 7.0653e-05\n",
      "Epoch: 36310 | training loss: 7.4028e-04 | validation loss: 7.0648e-05\n",
      "Epoch: 36320 | training loss: 7.4022e-04 | validation loss: 7.0643e-05\n",
      "Epoch: 36330 | training loss: 7.4017e-04 | validation loss: 7.0638e-05\n",
      "Epoch: 36340 | training loss: 7.4011e-04 | validation loss: 7.0633e-05\n",
      "Epoch: 36350 | training loss: 7.4006e-04 | validation loss: 7.0628e-05\n",
      "Epoch: 36360 | training loss: 7.4000e-04 | validation loss: 7.0623e-05\n",
      "Epoch: 36370 | training loss: 7.3994e-04 | validation loss: 7.0618e-05\n",
      "Epoch: 36380 | training loss: 7.3989e-04 | validation loss: 7.0613e-05\n",
      "Epoch: 36390 | training loss: 7.3986e-04 | validation loss: 7.0607e-05\n",
      "Epoch: 36400 | training loss: 7.6197e-04 | validation loss: 8.4934e-05\n",
      "Epoch: 36410 | training loss: 7.4109e-04 | validation loss: 7.1766e-05\n",
      "Epoch: 36420 | training loss: 7.4616e-04 | validation loss: 7.4814e-05\n",
      "Epoch: 36430 | training loss: 7.3982e-04 | validation loss: 7.0669e-05\n",
      "Epoch: 36440 | training loss: 7.4053e-04 | validation loss: 7.1335e-05\n",
      "Epoch: 36450 | training loss: 7.3955e-04 | validation loss: 7.0597e-05\n",
      "Epoch: 36460 | training loss: 7.3961e-04 | validation loss: 7.0637e-05\n",
      "Epoch: 36470 | training loss: 7.3945e-04 | validation loss: 7.0597e-05\n",
      "Epoch: 36480 | training loss: 7.3942e-04 | validation loss: 7.0614e-05\n",
      "Epoch: 36490 | training loss: 7.3936e-04 | validation loss: 7.0590e-05\n",
      "Epoch: 36500 | training loss: 7.3931e-04 | validation loss: 7.0579e-05\n",
      "Epoch: 36510 | training loss: 7.3926e-04 | validation loss: 7.0575e-05\n",
      "Epoch: 36520 | training loss: 7.3921e-04 | validation loss: 7.0573e-05\n",
      "Epoch: 36530 | training loss: 7.3916e-04 | validation loss: 7.0569e-05\n",
      "Epoch: 36540 | training loss: 7.3912e-04 | validation loss: 7.0565e-05\n",
      "Epoch: 36550 | training loss: 7.3907e-04 | validation loss: 7.0561e-05\n",
      "Epoch: 36560 | training loss: 7.3902e-04 | validation loss: 7.0556e-05\n",
      "Epoch: 36570 | training loss: 7.3897e-04 | validation loss: 7.0552e-05\n",
      "Epoch: 36580 | training loss: 7.3892e-04 | validation loss: 7.0547e-05\n",
      "Epoch: 36590 | training loss: 7.3887e-04 | validation loss: 7.0543e-05\n",
      "Epoch: 36600 | training loss: 7.3882e-04 | validation loss: 7.0538e-05\n",
      "Epoch: 36610 | training loss: 7.3877e-04 | validation loss: 7.0534e-05\n",
      "Epoch: 36620 | training loss: 7.3872e-04 | validation loss: 7.0529e-05\n",
      "Epoch: 36630 | training loss: 7.3867e-04 | validation loss: 7.0525e-05\n",
      "Epoch: 36640 | training loss: 7.3862e-04 | validation loss: 7.0520e-05\n",
      "Epoch: 36650 | training loss: 7.3857e-04 | validation loss: 7.0515e-05\n",
      "Epoch: 36660 | training loss: 7.3852e-04 | validation loss: 7.0511e-05\n",
      "Epoch: 36670 | training loss: 7.3847e-04 | validation loss: 7.0506e-05\n",
      "Epoch: 36680 | training loss: 7.3842e-04 | validation loss: 7.0501e-05\n",
      "Epoch: 36690 | training loss: 7.3836e-04 | validation loss: 7.0496e-05\n",
      "Epoch: 36700 | training loss: 7.3831e-04 | validation loss: 7.0492e-05\n",
      "Epoch: 36710 | training loss: 7.3826e-04 | validation loss: 7.0487e-05\n",
      "Epoch: 36720 | training loss: 7.3821e-04 | validation loss: 7.0482e-05\n",
      "Epoch: 36730 | training loss: 7.3816e-04 | validation loss: 7.0477e-05\n",
      "Epoch: 36740 | training loss: 7.3810e-04 | validation loss: 7.0472e-05\n",
      "Epoch: 36750 | training loss: 7.3805e-04 | validation loss: 7.0468e-05\n",
      "Epoch: 36760 | training loss: 7.3800e-04 | validation loss: 7.0463e-05\n",
      "Epoch: 36770 | training loss: 7.3794e-04 | validation loss: 7.0458e-05\n",
      "Epoch: 36780 | training loss: 7.3789e-04 | validation loss: 7.0453e-05\n",
      "Epoch: 36790 | training loss: 7.3783e-04 | validation loss: 7.0448e-05\n",
      "Epoch: 36800 | training loss: 7.3778e-04 | validation loss: 7.0443e-05\n",
      "Epoch: 36810 | training loss: 7.3773e-04 | validation loss: 7.0438e-05\n",
      "Epoch: 36820 | training loss: 7.3767e-04 | validation loss: 7.0433e-05\n",
      "Epoch: 36830 | training loss: 7.3762e-04 | validation loss: 7.0428e-05\n",
      "Epoch: 36840 | training loss: 7.3756e-04 | validation loss: 7.0419e-05\n",
      "Epoch: 36850 | training loss: 7.4069e-04 | validation loss: 7.2352e-05\n",
      "Epoch: 36860 | training loss: 7.5577e-04 | validation loss: 8.3114e-05\n",
      "Epoch: 36870 | training loss: 7.4712e-04 | validation loss: 7.7520e-05\n",
      "Epoch: 36880 | training loss: 7.3782e-04 | validation loss: 7.0673e-05\n",
      "Epoch: 36890 | training loss: 7.3829e-04 | validation loss: 7.0954e-05\n",
      "Epoch: 36900 | training loss: 7.3742e-04 | validation loss: 7.0557e-05\n",
      "Epoch: 36910 | training loss: 7.3733e-04 | validation loss: 7.0516e-05\n",
      "Epoch: 36920 | training loss: 7.3720e-04 | validation loss: 7.0406e-05\n",
      "Epoch: 36930 | training loss: 7.3715e-04 | validation loss: 7.0404e-05\n",
      "Epoch: 36940 | training loss: 7.3709e-04 | validation loss: 7.0401e-05\n",
      "Epoch: 36950 | training loss: 7.3704e-04 | validation loss: 7.0403e-05\n",
      "Epoch: 36960 | training loss: 7.3700e-04 | validation loss: 7.0397e-05\n",
      "Epoch: 36970 | training loss: 7.3695e-04 | validation loss: 7.0390e-05\n",
      "Epoch: 36980 | training loss: 7.3690e-04 | validation loss: 7.0385e-05\n",
      "Epoch: 36990 | training loss: 7.3685e-04 | validation loss: 7.0380e-05\n",
      "Epoch: 37000 | training loss: 7.3681e-04 | validation loss: 7.0376e-05\n",
      "Epoch: 37010 | training loss: 7.3676e-04 | validation loss: 7.0372e-05\n",
      "Epoch: 37020 | training loss: 7.3671e-04 | validation loss: 7.0368e-05\n",
      "Epoch: 37030 | training loss: 7.3666e-04 | validation loss: 7.0364e-05\n",
      "Epoch: 37040 | training loss: 7.3661e-04 | validation loss: 7.0359e-05\n",
      "Epoch: 37050 | training loss: 7.3657e-04 | validation loss: 7.0355e-05\n",
      "Epoch: 37060 | training loss: 7.3652e-04 | validation loss: 7.0350e-05\n",
      "Epoch: 37070 | training loss: 7.3647e-04 | validation loss: 7.0346e-05\n",
      "Epoch: 37080 | training loss: 7.3642e-04 | validation loss: 7.0341e-05\n",
      "Epoch: 37090 | training loss: 7.3637e-04 | validation loss: 7.0337e-05\n",
      "Epoch: 37100 | training loss: 7.3632e-04 | validation loss: 7.0332e-05\n",
      "Epoch: 37110 | training loss: 7.3627e-04 | validation loss: 7.0328e-05\n",
      "Epoch: 37120 | training loss: 7.3622e-04 | validation loss: 7.0323e-05\n",
      "Epoch: 37130 | training loss: 7.3617e-04 | validation loss: 7.0318e-05\n",
      "Epoch: 37140 | training loss: 7.3612e-04 | validation loss: 7.0314e-05\n",
      "Epoch: 37150 | training loss: 7.3607e-04 | validation loss: 7.0309e-05\n",
      "Epoch: 37160 | training loss: 7.3602e-04 | validation loss: 7.0304e-05\n",
      "Epoch: 37170 | training loss: 7.3596e-04 | validation loss: 7.0300e-05\n",
      "Epoch: 37180 | training loss: 7.3591e-04 | validation loss: 7.0295e-05\n",
      "Epoch: 37190 | training loss: 7.3586e-04 | validation loss: 7.0290e-05\n",
      "Epoch: 37200 | training loss: 7.3581e-04 | validation loss: 7.0286e-05\n",
      "Epoch: 37210 | training loss: 7.3576e-04 | validation loss: 7.0281e-05\n",
      "Epoch: 37220 | training loss: 7.3570e-04 | validation loss: 7.0276e-05\n",
      "Epoch: 37230 | training loss: 7.3565e-04 | validation loss: 7.0271e-05\n",
      "Epoch: 37240 | training loss: 7.3560e-04 | validation loss: 7.0267e-05\n",
      "Epoch: 37250 | training loss: 7.3554e-04 | validation loss: 7.0262e-05\n",
      "Epoch: 37260 | training loss: 7.3549e-04 | validation loss: 7.0257e-05\n",
      "Epoch: 37270 | training loss: 7.3544e-04 | validation loss: 7.0252e-05\n",
      "Epoch: 37280 | training loss: 7.3538e-04 | validation loss: 7.0248e-05\n",
      "Epoch: 37290 | training loss: 7.3533e-04 | validation loss: 7.0256e-05\n",
      "Epoch: 37300 | training loss: 7.4155e-04 | validation loss: 7.4703e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37310 | training loss: 7.6533e-04 | validation loss: 9.0087e-05\n",
      "Epoch: 37320 | training loss: 7.3660e-04 | validation loss: 7.1169e-05\n",
      "Epoch: 37330 | training loss: 7.3841e-04 | validation loss: 7.2715e-05\n",
      "Epoch: 37340 | training loss: 7.3513e-04 | validation loss: 7.0261e-05\n",
      "Epoch: 37350 | training loss: 7.3545e-04 | validation loss: 7.0429e-05\n",
      "Epoch: 37360 | training loss: 7.3502e-04 | validation loss: 7.0259e-05\n",
      "Epoch: 37370 | training loss: 7.3501e-04 | validation loss: 7.0285e-05\n",
      "Epoch: 37380 | training loss: 7.3491e-04 | validation loss: 7.0223e-05\n",
      "Epoch: 37390 | training loss: 7.3488e-04 | validation loss: 7.0218e-05\n",
      "Epoch: 37400 | training loss: 7.3482e-04 | validation loss: 7.0216e-05\n",
      "Epoch: 37410 | training loss: 7.3478e-04 | validation loss: 7.0216e-05\n",
      "Epoch: 37420 | training loss: 7.3473e-04 | validation loss: 7.0213e-05\n",
      "Epoch: 37430 | training loss: 7.3469e-04 | validation loss: 7.0207e-05\n",
      "Epoch: 37440 | training loss: 7.3464e-04 | validation loss: 7.0203e-05\n",
      "Epoch: 37450 | training loss: 7.3459e-04 | validation loss: 7.0198e-05\n",
      "Epoch: 37460 | training loss: 7.3455e-04 | validation loss: 7.0194e-05\n",
      "Epoch: 37470 | training loss: 7.3450e-04 | validation loss: 7.0190e-05\n",
      "Epoch: 37480 | training loss: 7.3445e-04 | validation loss: 7.0186e-05\n",
      "Epoch: 37490 | training loss: 7.3440e-04 | validation loss: 7.0181e-05\n",
      "Epoch: 37500 | training loss: 7.3436e-04 | validation loss: 7.0177e-05\n",
      "Epoch: 37510 | training loss: 7.3431e-04 | validation loss: 7.0173e-05\n",
      "Epoch: 37520 | training loss: 7.3426e-04 | validation loss: 7.0168e-05\n",
      "Epoch: 37530 | training loss: 7.3421e-04 | validation loss: 7.0164e-05\n",
      "Epoch: 37540 | training loss: 7.3416e-04 | validation loss: 7.0160e-05\n",
      "Epoch: 37550 | training loss: 7.3412e-04 | validation loss: 7.0155e-05\n",
      "Epoch: 37560 | training loss: 7.3407e-04 | validation loss: 7.0151e-05\n",
      "Epoch: 37570 | training loss: 7.3402e-04 | validation loss: 7.0146e-05\n",
      "Epoch: 37580 | training loss: 7.3397e-04 | validation loss: 7.0142e-05\n",
      "Epoch: 37590 | training loss: 7.3392e-04 | validation loss: 7.0137e-05\n",
      "Epoch: 37600 | training loss: 7.3387e-04 | validation loss: 7.0133e-05\n",
      "Epoch: 37610 | training loss: 7.3382e-04 | validation loss: 7.0128e-05\n",
      "Epoch: 37620 | training loss: 7.3377e-04 | validation loss: 7.0124e-05\n",
      "Epoch: 37630 | training loss: 7.3372e-04 | validation loss: 7.0119e-05\n",
      "Epoch: 37640 | training loss: 7.3367e-04 | validation loss: 7.0115e-05\n",
      "Epoch: 37650 | training loss: 7.3362e-04 | validation loss: 7.0110e-05\n",
      "Epoch: 37660 | training loss: 7.3357e-04 | validation loss: 7.0105e-05\n",
      "Epoch: 37670 | training loss: 7.3351e-04 | validation loss: 7.0101e-05\n",
      "Epoch: 37680 | training loss: 7.3346e-04 | validation loss: 7.0096e-05\n",
      "Epoch: 37690 | training loss: 7.3341e-04 | validation loss: 7.0092e-05\n",
      "Epoch: 37700 | training loss: 7.3336e-04 | validation loss: 7.0087e-05\n",
      "Epoch: 37710 | training loss: 7.3331e-04 | validation loss: 7.0082e-05\n",
      "Epoch: 37720 | training loss: 7.3325e-04 | validation loss: 7.0078e-05\n",
      "Epoch: 37730 | training loss: 7.3320e-04 | validation loss: 7.0073e-05\n",
      "Epoch: 37740 | training loss: 7.3315e-04 | validation loss: 7.0067e-05\n",
      "Epoch: 37750 | training loss: 7.3318e-04 | validation loss: 7.0091e-05\n",
      "Epoch: 37760 | training loss: 7.8486e-04 | validation loss: 1.0418e-04\n",
      "Epoch: 37770 | training loss: 7.4772e-04 | validation loss: 7.9596e-05\n",
      "Epoch: 37780 | training loss: 7.3517e-04 | validation loss: 7.1491e-05\n",
      "Epoch: 37790 | training loss: 7.3474e-04 | validation loss: 7.1485e-05\n",
      "Epoch: 37800 | training loss: 7.3295e-04 | validation loss: 7.0141e-05\n",
      "Epoch: 37810 | training loss: 7.3310e-04 | validation loss: 7.0183e-05\n",
      "Epoch: 37820 | training loss: 7.3278e-04 | validation loss: 7.0055e-05\n",
      "Epoch: 37830 | training loss: 7.3277e-04 | validation loss: 7.0092e-05\n",
      "Epoch: 37840 | training loss: 7.3270e-04 | validation loss: 7.0057e-05\n",
      "Epoch: 37850 | training loss: 7.3265e-04 | validation loss: 7.0042e-05\n",
      "Epoch: 37860 | training loss: 7.3260e-04 | validation loss: 7.0038e-05\n",
      "Epoch: 37870 | training loss: 7.3256e-04 | validation loss: 7.0036e-05\n",
      "Epoch: 37880 | training loss: 7.3251e-04 | validation loss: 7.0034e-05\n",
      "Epoch: 37890 | training loss: 7.3247e-04 | validation loss: 7.0030e-05\n",
      "Epoch: 37900 | training loss: 7.3242e-04 | validation loss: 7.0026e-05\n",
      "Epoch: 37910 | training loss: 7.3238e-04 | validation loss: 7.0022e-05\n",
      "Epoch: 37920 | training loss: 7.3233e-04 | validation loss: 7.0018e-05\n",
      "Epoch: 37930 | training loss: 7.3228e-04 | validation loss: 7.0013e-05\n"
     ]
    }
   ],
   "source": [
    "par_svars0 = svars_tv[0]\n",
    "par_svars0.requires_grad=True\n",
    "\n",
    "for epoch in range(1,Nepochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    ueps_e_0 = NICE_network.DeNormalize(NICE_network.e0,NICE_network.prm_ee)\n",
    "    usvars0 = torch.cat((ueps_e_0,par_svars0),-1)\n",
    "    stress0 = NICE_network.stress([ueps_e_0,par_svars0[:,:1],par_svars0[:,1:]])\n",
    "    \n",
    "    pred = NICE_network.integrate(dstrain_tv, usvars0, t, np.hstack((ntrain,nval)))\n",
    "    pred_svars,pred_stress,pred_diss = pred\n",
    "    \n",
    "    training_loss_stress = MSE(NICE_network.Normalize(pred_stress[:,nval],prm_s),\n",
    "                                    NICE_network.Normalize(stress_tv[:,nval],prm_s))\n",
    "    training_loss_r0 = MSE(NICE_network.Normalize(stress0,prm_s),\n",
    "                                NICE_network.Normalize(stress_tv[0],prm_s))\n",
    "    training_loss_svars = (MSE(NICE_network.Normalize(pred_svars[:,ntrain,-1],prm_z),\n",
    "                               NICE_network.Normalize(svars_tv[:,ntrain,-1],prm_z)))\n",
    "\n",
    "    norm_d = torch.max(torch.abs(pred_diss)).detach()\n",
    "    training_loss_dissipation = MSE(NICE_network.relu(-pred_diss[:,ntrain])/norm_d, pred_diss[:,ntrain].detach()*0)\n",
    "\n",
    "    l_reg = torch.tensor(0.,requires_grad=True)\n",
    "    for name, param in NICE_network.named_parameters():\n",
    "        if 'weight' in name: l_reg = l_reg + pow(param,2).sum()\n",
    "\n",
    "    training_loss = (torch.mean(training_loss_stress)\n",
    "                     + torch.mean(training_loss_r0)\n",
    "                     + torch.mean(training_loss_svars)\n",
    "                     + torch.mean(training_loss_dissipation)\n",
    "                     + w_reg*l_reg\n",
    "                    )\n",
    "    training_loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    if scheduler.get_last_lr()[0]>1.e-4: scheduler.step()\n",
    "        \n",
    "    validation_loss_stress = MSE(NICE_network.Normalize(pred_stress[:,nval],prm_s),\n",
    "                                 NICE_network.Normalize(stress_tv[:,nval],prm_s))\n",
    "    validation_loss_svars = (MSE(NICE_network.Normalize(pred_svars[:,nval,-1],prm_z),\n",
    "                                 NICE_network.Normalize(svars_tv[:,nval,-1],prm_z)))\n",
    "    validation_loss_dissipation = MSE(NICE_network.relu(-pred_diss[:,nval])/norm_d,pred_diss[:,nval].detach()*0)\n",
    "    validation_loss = (torch.mean(validation_loss_stress)\n",
    "                       + torch.mean(validation_loss_svars)\n",
    "                       + torch.mean(validation_loss_dissipation))    \n",
    "    \n",
    "    training_loss_value = training_loss.item()\n",
    "    validation_loss_value = validation_loss.item()\n",
    "    training_loss_hist.append(training_loss_value)\n",
    "    validation_loss_value_hist.append(validation_loss_value)\n",
    "    \n",
    "    # Print loss\n",
    "    if not epoch % verbose_frequency:\n",
    "        print(f\"Epoch: {epoch}\"\n",
    "          + f\" | training loss: {training_loss_value:.4e}\"\n",
    "          + f\" | validation loss: {validation_loss_value:.4e}\")\n",
    "\n",
    "    # Check early_stopping criterion\n",
    "    early_stopping(validation_loss_value, NICE_network)\n",
    "    if early_stopping.early_stop :\n",
    "        print(\"Early stopping\")\n",
    "        NICE_network.load_state_dict(torch.load(checkpoint_path))\n",
    "        break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac38a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './checkpoint_benchmark2_dense.pt'\n",
    "NICE_network.load_state_dict(torch.load(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf71b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = prm_dt*torch.linspace(0., data_size-1, (data_size)).to(device)\n",
    "svars_tv.requires_grad=True\n",
    "sol = root(NICE_network.find_elastic_strain,\n",
    "           args=([svars_tv[0,:,:1].reshape(-1,1),\n",
    "                 svars_tv[0,:,-1:].reshape(-1,1)],\n",
    "                 stress_tv[0].reshape(-1,2)),\n",
    "           x0=np.zeros((len(ntrainval),2)),\n",
    "           tol=1e-12)\n",
    "eps_e_0 = torch.from_numpy(sol.x.reshape(-1,2))\n",
    "ueps_e_0 = NICE_network.DeNormalize(eps_e_0,NICE_network.prm_ee)\n",
    "usvars = torch.cat((ueps_e_0,svars_tv[0]),-1)\n",
    "\n",
    "pred_svars,pred_stress,pred_diss = NICE_network.integrate(dstrain_tv[:],usvars,t,np.hstack((ntrain,nval)))\n",
    "# Evaluate error\n",
    "loss = torch.nn.L1Loss()\n",
    "MAE_stress = loss(NICE_network.Normalize(pred_stress,prm_s),NICE_network.Normalize(stress_tv,prm_s))\n",
    "MAE_z = loss(NICE_network.Normalize(pred_svars[:,:,-1:],prm_z),NICE_network.Normalize(svars_tv[:,:,-1:],prm_z))\n",
    "print(\"MAE stress : \", MAE_stress,\n",
    "      \"\\nMAE z : \", MAE_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e702240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.subplots(1)\n",
    "for i in np.arange(len(ntrainval)):\n",
    "    ax.plot(strain_t_tv[:,i,1],svars_tv[:,i,1].detach().numpy(),linewidth=5,color='black',alpha=0.2,markersize=0,marker='.')\n",
    "    ax.plot(strain_t_tv[:,i,1],pred_svars[:,i,3].cpu().detach(),linewidth=2,color=colorb,markersize=0,marker='.')\n",
    "ax.set_ylabel('$\\phi$ (-)')\n",
    "ax.set_xlabel('$\\\\varepsilon_s$ (%)')\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots(1)\n",
    "for i in np.arange(len(ntrainval)):\n",
    "    ax.plot(strain_t_tv[:,i,1],stress_tv[:,i,1].cpu().detach(),linewidth=5,color='black',alpha=0.2,markersize=0,marker='.')\n",
    "    ax.plot(strain_t_tv[:,i,1],pred_stress[:,i,1].cpu().detach(),linewidth=2,color=colorb,markersize=0,marker='.')\n",
    "ax.set_ylabel('$q$ (MPa)')\n",
    "ax.set_xlabel('$\\\\varepsilon_s$ (-)')\n",
    "ax.set_ylim(0,30)\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots(1)\n",
    "for i in np.arange(len(ntrainval)):\n",
    "    ax.plot(strain_t_tv[:,i,0],stress_tv[:,i,0].cpu().detach(),linewidth=5,color='black',alpha=0.2,markersize=0,marker='.')\n",
    "    ax.plot(strain_t_tv[:,i,0],pred_stress[:,i,0].cpu().detach(),linewidth=2,color=colorb,markersize=0,marker='.')\n",
    "ax.set_ylabel('$p$ (MPa)')\n",
    "ax.set_xlabel('$\\\\varepsilon_v$ (-)')\n",
    "ax.set_ylim(0,20)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48e571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "svars_test.requires_grad=True\n",
    "\n",
    "t = prm_dt*torch.linspace(0., data_size-1, (data_size)).to(device)\n",
    "NICE_network.init_interp(dstrain_test,t)\n",
    "sol = root(NICE_network.find_elastic_strain,\n",
    "           args=([svars_test[0,:,:1].reshape(-1,1),\n",
    "                 svars_test[0,:,-1:].reshape(-1,1)],\n",
    "                 stress_test[0].reshape(-1,2)),\n",
    "           x0=np.zeros((stress_test.shape[1],2)),\n",
    "           tol=1e-12)\n",
    "eps_e_0 = torch.from_numpy(sol.x.reshape(-1,2))\n",
    "ueps_e_0 = NICE_network.DeNormalize(eps_e_0,NICE_network.prm_ee)\n",
    "usvars = torch.cat((ueps_e_0,svars_test[0]),-1)\n",
    "Ntest = np.arange(0,dstrain_test.shape[1])\n",
    "pred_svars,pred_stress,pred_diss = NICE_network.integrate(dstrain_test,usvars,t,Ntest)\n",
    "\n",
    "# Evaluate error\n",
    "loss = torch.nn.L1Loss()\n",
    "MAE_stress = loss(NICE_network.Normalize(pred_stress,prm_s),NICE_network.Normalize(stress_test,prm_s))\n",
    "MAE_z = loss(NICE_network.Normalize(pred_svars[:,:,-1:],prm_z),NICE_network.Normalize(svars_test[:,:,-1:],prm_z))\n",
    "print(\"MAE stress : \", MAE_stress,\n",
    "      \"\\nMAE z : \", MAE_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fe1294",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.subplots(1)\n",
    "for i in np.arange(len(ntest)):\n",
    "    ax.plot(strain_t_test[:,i,1],stress_test[:,i,1].cpu().detach(),linewidth=5,color='black',alpha=0.2,markersize=0,marker='.')\n",
    "    ax.plot(strain_t_test[:,i,1],pred_stress[:,i,1].cpu().detach(),linewidth=2,color=colorb,markersize=0,marker='.')\n",
    "ax.set_ylabel('$q$ (MPa)')\n",
    "ax.set_xlabel('$\\\\varepsilon_s$ (-)')\n",
    "ax.grid()\n",
    "ax.set_ylim(0,30)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots(1)\n",
    "for i in np.arange(len(ntest)):\n",
    "    ax.plot(strain_t_test[:,i,0],stress_test[:,i,0].cpu().detach(),linewidth=5,color='black',alpha=0.2,markersize=0,marker='.')\n",
    "    ax.plot(strain_t_test[:,i,0],pred_stress[:,i,0].cpu().detach(),linewidth=2,color=colorb,markersize=0,marker='.')\n",
    "ax.set_ylabel('$p$ (MPa)')\n",
    "ax.set_xlabel('$\\\\varepsilon_v$ (-)')\n",
    "ax.set_ylim(0,20)\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots(1)\n",
    "for i in np.arange(len(ntest)):\n",
    "    ax.plot(strain_t_test[:,i,1],svars_test[:,i,1].detach().numpy(),linewidth=5,color='black',alpha=0.2,markersize=0,marker='.')\n",
    "    ax.plot(strain_t_test[:,i,1],pred_svars[:,i,3].detach().numpy(),linewidth=2,color=colorb,markersize=0,marker='.')\n",
    "ax.set_ylabel('$\\phi$ (-)')\n",
    "ax.set_xlabel('$\\\\varepsilon_s$ (-)')\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots(1)\n",
    "for i in np.arange(len(ntest)):\n",
    "    ax.plot(pred_diss[:,i].cpu().detach(),linewidth=3,alpha=0.3,color=colorb,markersize=0,marker='.')\n",
    "ax.set_ylabel('$d$')\n",
    "ax.grid()\n",
    "# ax.set_ylim(0,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f325a1",
   "metadata": {},
   "source": [
    "### 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5042e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './dataset/benchmark2_inference_UndrainedTriaxial_phi08'\n",
    "with open(file, 'rb') as f_obj:\n",
    "    data = pickle.load(f_obj)\n",
    "[stress_t,strain_t,svars_e_t,epl,r_t,rs_t,stress_tdt,strain_tdt,svars_e_tdt,epl_tdt,r_tdt,rs_tdt,dt,n_reset]= data\n",
    "batch_time = n_reset\n",
    "data_size=n_reset\n",
    "\n",
    "# strain protocol\n",
    "dstrain = (strain_tdt - strain_t)/prm_dt\n",
    "\n",
    "# initial condition and data for comparison\n",
    "strain_t = np.reshape(strain_t,(batch_time,-1,dim),order='F')\n",
    "dstrain = np.reshape(dstrain,(batch_time,-1,dim),order='F')\n",
    "r_t = np.reshape(r_t,(batch_time,-1,1),order='F')\n",
    "rs_t = np.reshape(rs_t,(batch_time,-1,1),order='F')\n",
    "el_strain_t = np.reshape(svars_e_t,(batch_time,-1,dim),order='F')\n",
    "stress_t = np.reshape(stress_t,(batch_time,-1,dim),order='F')\n",
    "data_size = strain_t.shape[0]\n",
    "number_IC = strain_t.shape[1]\n",
    "\n",
    "svars = torch.cat((torch.from_numpy(np.float64(r_t)),torch.from_numpy(np.float64(rs_t))),-1).to(device)\n",
    "stress = torch.from_numpy(np.float64(stress_t)).to(device)\n",
    "dstrain = torch.from_numpy(np.float64(dstrain)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7d4382",
   "metadata": {},
   "outputs": [],
   "source": [
    "NICE_network.inference=True\n",
    "t = torch.arange(0,prm_dt*data_size,prm_dt)\n",
    "initial_conditions = torch.cat((svars[0,:,:1].reshape(-1,1),svars[0,:,-1:].reshape(-1,1),stress[0].reshape(-1,2)),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271b82b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(0,number_IC)\n",
    "initial_conditions.requires_grad=True\n",
    "NICE_network.init_interp(dstrain,t)\n",
    "sol = root(NICE_network.find_elastic_strain,\n",
    "           args=([initial_conditions[:,:1],initial_conditions[:,1:2]],initial_conditions[:,2:]),\n",
    "           x0=np.zeros((number_IC,dim)),\n",
    "           tol=1e-12)\n",
    "eps_e_0 = torch.from_numpy(sol.x.reshape(-1,2))\n",
    "ueps_e_0 = NICE_network.DeNormalize(eps_e_0,NICE_network.prm_ee)\n",
    "usvars = torch.cat((ueps_e_0,svars[0]),-1)\n",
    "\n",
    "pred = NICE_network.integrate(dstrain,usvars,t,idx)\n",
    "pred_svars,pred_stress,pred_diss = pred\n",
    "pred_svars = pred_svars.cpu().detach().numpy()\n",
    "pred_stress = pred_stress.cpu().detach().numpy()\n",
    "pred_diss = pred_diss.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f57dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_ratio = stress_t[:,:,1]/stress_t[:,:,0]\n",
    "pred_stress_ratio = pred_stress[:,:,1]/pred_stress[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nid=1\n",
    "for idd in range(6):\n",
    "    fig = plt.figure(dpi=100, figsize=(2.5, 1.5), tight_layout=True)\n",
    "    axes = fig.subplots(1)\n",
    "    axes.plot(strain_t[:,idd:idd+nid,1],stress_ratio[:,idd:idd+nid],marker='o',markerfacecolor='white',linestyle='-',\n",
    "        color='black',alpha=0.2,linewidth=5,markersize=0,label='ref')\n",
    "    axes.plot(strain_t[::2,idd:idd+nid,1],stress_ratio[::2,idd:idd+nid],marker='o',markerfacecolor='white',markeredgewidth=0.0,linestyle='-',\n",
    "        color='black',alpha=0.5,linewidth=0,markersize=2.5)\n",
    "    axes.plot(strain_t[:,idd:idd+nid,1],pred_stress_ratio[:,idd:idd+nid],alpha=1,linewidth=2,color=colorb,\n",
    "            markersize=0,markeredgewidth=0.0,marker='.')\n",
    "    axes.set_ylabel('$q/p$ (-)')\n",
    "    axes.set_xlabel('$\\\\varepsilon_s$ (%)')\n",
    "    every_nth=2\n",
    "    for n, label in enumerate(axes.xaxis.get_ticklabels()):\n",
    "        if n % every_nth == 0: label.set_visible(False)\n",
    "    axes.grid()    \n",
    "    axes.set_ylim(0,2.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff33555",
   "metadata": {},
   "outputs": [],
   "source": [
    "nid=1\n",
    "for idd in range(6):\n",
    "    fig = plt.figure(dpi=100, figsize=(2.5, 1.5), tight_layout=True)\n",
    "    axes = fig.subplots(1)\n",
    "    axes.plot(strain_t[:,idd:idd+nid,1],rs_t[:,idd:idd+nid,-1],marker='o',markerfacecolor='white',linestyle='-',\n",
    "        color='black',alpha=0.2,linewidth=5,markersize=0,label='ref')\n",
    "    axes.plot(strain_t[::2,idd:idd+nid,1],rs_t[::2,idd:idd+nid,-1],marker='o',markerfacecolor='white',markeredgewidth=0.0,linestyle='-',\n",
    "        color='black',alpha=0.5,linewidth=0,markersize=2.5)\n",
    "    axes.plot(strain_t[:,idd:idd+nid,1],pred_svars[:,idd:idd+nid,-1],alpha=1,linewidth=2,color=colorb,\n",
    "            markersize=0,markeredgewidth=0.0,marker='.')\n",
    "    axes.set_ylabel('$\\phi$ (-)')\n",
    "    axes.set_xlabel('$\\\\varepsilon_s$ (%)')\n",
    "    axes.grid()\n",
    "    every_nth=2\n",
    "    for n, label in enumerate(axes.xaxis.get_ticklabels()):\n",
    "        if n % every_nth == 0: label.set_visible(False)\n",
    "    axes.set_ylim(0.65,0.85)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f6471",
   "metadata": {},
   "source": [
    "### 6. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed912697",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './saved/[state]NICE_benchmark2_dense'\n",
    "torch.save(NICE_network.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31577006",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./saved/[params]NICE_benchmark2_dense', 'wb') as f_obj:\n",
    "     pickle.dump(norm_params, f_obj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
