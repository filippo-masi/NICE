{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e5ff77c",
   "metadata": {},
   "source": [
    "# Neural integration for constitutive equations \n",
    "\n",
    "### Benchmark #2: porous, elasto-plastic material\n",
    "\n",
    "Authors: Filippo Masi, Itai Einav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb870a3b",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a8d1509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 200x160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.optimize import root\n",
    "from torchdiffeq import odeint\n",
    "from nice_module import NICE, EarlyStopping, slice_data, get_params\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "np.random.seed(6)\n",
    "torch.manual_seed(6)\n",
    "\n",
    "# Plotting configurations\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# Using a classic style for plots\n",
    "plt.style.use('classic')\n",
    "\n",
    "# Updating plot configurations\n",
    "plt.rcParams.update({\"axes.grid\": False, \"grid.color\": 'black', \"grid.alpha\": 0.4})\n",
    "font = {'size': 11}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "# Setting background color for plots\n",
    "plt.rcParams['axes.facecolor'] = 'none'\n",
    "plt.rcParams['savefig.facecolor'] = 'none'\n",
    "plt.rcParams['figure.facecolor'] = 'none'\n",
    "\n",
    "# Setting figure size and layout parameters\n",
    "plt.rcParams[\"figure.figsize\"] = (2.5, 2)\n",
    "plt.tight_layout(pad=2.5, w_pad=3.5, h_pad=3.5)\n",
    "colorb = (0.2, 0.4, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5896503a",
   "metadata": {},
   "source": [
    "#### 1.1 Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7362a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the frequency in epochs for printing loss during training\n",
    "verbose_frequency = 10\n",
    "\n",
    "# Step size for training process. Set to 20 for reproducing training, else 1 for faster training.\n",
    "step_size = 1\n",
    "\n",
    "# Defining the device for training (CUDA if available, else CPU)\n",
    "device = torch.device('cuda:' + str(gpu) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Boolean flag for adding normally distributed noise to the training dataset\n",
    "corrupted_training_data = False\n",
    "\n",
    "# If corrupted_training_data is True, set the noise amplitude (percentage)\n",
    "delta = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caccd145",
   "metadata": {},
   "source": [
    "### 2. Import and prepare data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca061767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for the training dataset\n",
    "file = './dataset/benchmark2_data_training'\n",
    "\n",
    "# Loading data from the specified file using pickle\n",
    "with open(file, 'rb') as f_obj:\n",
    "    data = pickle.load(f_obj)\n",
    "\n",
    "# Unpacking the loaded data into individual variables\n",
    "strain_t, strain_tdt, r_t, r_tdt, z_t, z_tdt, stress_t, dt, n_reset = data\n",
    "\n",
    "# Setting batch_time to the value of n_reset\n",
    "batch_time = n_reset\n",
    "\n",
    "# Setting data_size to the value of n_reset\n",
    "data_size = n_reset\n",
    "\n",
    "# Dimensionality of the data\n",
    "dim = 2\n",
    "\n",
    "# Computing the time step parameter prm_dt\n",
    "prm_dt = 1 / data_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd063371",
   "metadata": {},
   "source": [
    "#### 3.1 Reshape \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20cad77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing finite differences to get rates of change\n",
    "dstrain = strain_tdt - strain_t\n",
    "dr = r_tdt - r_t\n",
    "dz = z_tdt - z_t\n",
    "\n",
    "# Normalizing the finite differences by the time step parameter prm_dt\n",
    "dstrain /= prm_dt\n",
    "dr /= prm_dt\n",
    "dz /= prm_dt\n",
    "\n",
    "# Reshaping arrays for compatibility with the model\n",
    "strain_t = np.reshape(strain_t, (batch_time, -1, dim), order='F')\n",
    "strain_tdt = np.reshape(strain_tdt, (batch_time, -1, dim), order='F')\n",
    "dstrain = np.reshape(dstrain, (batch_time, -1, dim), order='F')\n",
    "r_t = np.reshape(r_t, (batch_time, -1, 1), order='F')\n",
    "dr = np.reshape(dr, (batch_time, -1, 1), order='F')\n",
    "z_t = np.reshape(z_t, (batch_time, -1, 1), order='F')\n",
    "dz = np.reshape(dz, (batch_time, -1, 1), order='F')\n",
    "stress_t = np.reshape(stress_t, (batch_time, -1, dim), order='F')\n",
    "\n",
    "# Updating data_size based on the reshaped strain_t array\n",
    "data_size = strain_t.shape[0]\n",
    "\n",
    "# Determining the number of initial conditions (IC) based on the reshaped strain_t array\n",
    "number_IC = strain_t.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8076cbd9",
   "metadata": {},
   "source": [
    "#### 3.2 Split data in training, validation, and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a418e711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  52\n",
      "Training samples :  34\n",
      "Validation samples :  9\n",
      "Test samples :  9\n",
      "Total :  52\n"
     ]
    }
   ],
   "source": [
    "# Percentage of data reserved for training\n",
    "train_percentage = .65\n",
    "\n",
    "# Calculating the number of samples for training, validation, and testing\n",
    "train = int(round(number_IC * train_percentage))\n",
    "val = int(round(number_IC * 0.5 * (1. - train_percentage)))\n",
    "test = val\n",
    "\n",
    "# Displaying information about the dataset split\n",
    "print(\"Number of samples: \", number_IC)\n",
    "print(\"Training samples : \", train)\n",
    "print(\"Validation samples : \", val)\n",
    "print(\"Test samples : \", test)\n",
    "print(\"Total : \", test + val + train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79ab40b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating an array of consecutive numbers representing samples\n",
    "n = np.arange(0, number_IC, 1)\n",
    "\n",
    "# Creating a shuffled index array\n",
    "rnd = np.arange(len(n))\n",
    "np.random.shuffle(rnd[:-4])\n",
    "n = n[rnd]\n",
    "\n",
    "# Splitting the indices for training, validation, and test sets\n",
    "ntrain = n[:train-4]\n",
    "ntrain = np.hstack((ntrain, n[-4:]))\n",
    "cut = len(ntrain)\n",
    "nval = n[train:train+val]\n",
    "ntrainval = np.hstack((ntrain, nval))\n",
    "ntest = n[train+val:]\n",
    "\n",
    "# Shuffling the indices for the combined training and validation set\n",
    "rnd = np.arange(len(ntrainval))\n",
    "np.random.shuffle(rnd)\n",
    "\n",
    "# Creating separate indices for training and validation sets\n",
    "ntrain = rnd[:train]\n",
    "nval = rnd[val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae50299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing the data arrays based on the generated indices for training and validation sets (strain)\n",
    "strain_t_tv, strain_t_test = slice_data(strain_t, ntrainval, ntest)\n",
    "# (finite differences of strain)\n",
    "dstrain_tv, dstrain_test = slice_data(dstrain, ntrainval, ntest)\n",
    "\n",
    "# Slicing the data arrays based on the generated indices for training and validation sets (stress)\n",
    "stress_t_tv, stress_t_test = slice_data(stress_t, ntrainval, ntest)\n",
    "\n",
    "# Slicing the data arrays based on the generated indices for training and validation sets (r)\n",
    "r_t_tv, r_t_test = slice_data(r_t, ntrainval, ntest)\n",
    "# (finite differences of r)\n",
    "dr_tv, dr_test = slice_data(dr, ntrainval, ntest)\n",
    "\n",
    "# Slicing the data arrays based on the generated indices for training and validation sets (z)\n",
    "z_t_tv, z_t_test = slice_data(z_t, ntrainval, ntest)\n",
    "# (finite differences of z)\n",
    "dz_tv, dz_test = slice_data(dz, ntrainval, ntest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f1f3deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the time step index to stop loading for each training sequence\n",
    "stop_loading_train = []\n",
    "for i in range(ntrain.shape[0]):\n",
    "    listt = np.where(np.linalg.norm(dstrain_tv[:, ntrain[i]], axis=1) == np.linalg.norm(dstrain_tv[-1, ntrain[i]]))\n",
    "    stop_loading_train.append(listt[0][0])\n",
    "\n",
    "# Determine the time step index to stop loading for each validation sequence\n",
    "stop_loading_val = []\n",
    "for i in range(nval.shape[0]):\n",
    "    listt = np.where(np.linalg.norm(dstrain_tv[:, nval[i]], axis=1) == np.linalg.norm(dstrain_tv[-1, nval[i]]))\n",
    "    stop_loading_val.append(listt[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "345bd0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the change in state variables over time\n",
    "dsvars_tv = z_t_tv - z_t_tv[0]\n",
    "\n",
    "# Compute normalization parameters for the change in state variables\n",
    "prm_deltaz = get_params(dsvars_tv)\n",
    "\n",
    "# Calculate the change in stress over time for each component\n",
    "ddstress_tv = stress_t_tv[:, :, :dim] - stress_t_tv[0, :, :dim]\n",
    "\n",
    "# Initialize a list to store normalization parameters for the change in stress\n",
    "prm_deltas = [] \n",
    "\n",
    "# Iterate over each component and compute normalization parameters for the change in stress\n",
    "for i in range(dsvars_tv.shape[1]):\n",
    "    # Compute normalization parameters for the change in stress component\n",
    "    prm_ = get_params(ddstress_tv[:, i])\n",
    "    \n",
    "    # Append the computed parameters to the list\n",
    "    prm_deltas.append(prm_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89f6c50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters for training data and move to the specified device (strain)\n",
    "prm_e = get_params(strain_t_tv).to(device)\n",
    "# (finite differences of strain)\n",
    "prm_de = get_params(dstrain_tv).to(device)\n",
    "\n",
    "# Get parameters for training data (stress)\n",
    "prm_s = get_params(stress_t_tv).to(device)\n",
    "\n",
    "# Get parameters for training data (density)\n",
    "prm_r = get_params(r_t_tv).to(device)\n",
    "# (finite differences of density)\n",
    "prm_dr = get_params(dr_tv).to(device)\n",
    "\n",
    "# Get parameters for training data (dissipative state variables, z)\n",
    "prm_z = get_params(z_t_tv).to(device)\n",
    "# (finite differences of z)\n",
    "prm_dz = get_params(dz_tv).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ccee8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if corrupted_training_data:\n",
    "    # Stress noise vector\n",
    "    noise = delta/100 * np.random.normal(0, 1, ((data_size + 1, number_IC, dim)))\n",
    "    \n",
    "    # Corrupt stress with noise\n",
    "    noise_stress_t = stress_t_tv.copy()\n",
    "    noise_stress_t[:,:,0] = np.mean(stress_t_tv[:,:,0]) * noise[:-1,:,0]\n",
    "    noise_stress_t[:,:,1] = np.mean(stress_t_tv[:,:,1]) * noise[:-1,:,1]\n",
    "    noise_stress_t[0] *= 0.0\n",
    "    stress_t_tv += noise_stress_t\n",
    "    \n",
    "    # Dissipative state variable noise vector\n",
    "    noise = delta/100 * np.random.normal(0, 1, ((data_size + 1, number_IC, 1)))\n",
    "    \n",
    "    # Corrupt state variable with noise\n",
    "    noise_svars_z_t = z_t_tv.copy()\n",
    "    noise_svars_z_tdt = z_tdt_tv.copy()\n",
    "    noise_svars_z_t = np.mean(z_t_tv) * noise[:-1, :]\n",
    "    noise_svars_z_tdt = np.mean(z_tdt_tv) * noise[1:, :]\n",
    "    noise_svars_z_t[0] *= 0.0\n",
    "    z_t_tv[:, :] += noise_svars_z_t\n",
    "    z_tdt_tv[:, :] += noise_svars_z_tdt\n",
    "    \n",
    "    # Evaluate corrupted rate\n",
    "    dz_tv = (z_tdt_tv - z_t_tv) / prm_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ecebc",
   "metadata": {},
   "source": [
    "### 4. Neural integration for constitutive equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4906d0f4",
   "metadata": {},
   "source": [
    "#### 4.1 Construct neural net and set integration scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fb5fa08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NICE(\n",
       "  (NeuralNetEvolution): Sequential(\n",
       "    (0): Linear(in_features=6, out_features=42, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=42, out_features=42, bias=True)\n",
       "    (3): GELU(approximate='none')\n",
       "    (4): Linear(in_features=42, out_features=42, bias=True)\n",
       "    (5): GELU(approximate='none')\n",
       "    (6): Linear(in_features=42, out_features=3, bias=True)\n",
       "  )\n",
       "  (NeuralNetEnergy): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Softplus(beta=1, threshold=20)\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch.float64\n",
    "hidden_num = 0\n",
    "NNf_params = [2*dim+2, dim+1, [6*(2*dim+3), 6*(2*dim+3), 6*(2*dim+3)], 'gelu']\n",
    "NNu_params = [dim+2, 1, [2**6, 2**6], 'softplus']\n",
    "norm_params = [prm_e, prm_de, prm_r, prm_z, prm_dz, prm_s, prm_dt]\n",
    "\n",
    "# Creating a NICE network with specified parameters and moving it to the device\n",
    "NICE_network = NICE(NNf_params, NNu_params, len(ntrainval), norm_params, dim, dtype).to(device)\n",
    "NICE_network.to(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6a2216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating time points based on the time step parameter prm_dt\n",
    "t = torch.arange(0, 1.0, prm_dt)\n",
    "\n",
    "# Converting stress and strain data to PyTorch tensors and moving them to the specified device (training set)\n",
    "stress_tv = torch.from_numpy(np.float64(stress_t_tv)).to(device)\n",
    "dstrain_tv = torch.from_numpy(np.float64(dstrain_tv)).to(device)\n",
    "\n",
    "# Converting stress and strain data to PyTorch tensors and moving them to the specified device (test set)\n",
    "stress_test = torch.from_numpy(np.float64(stress_t_test)).to(device)\n",
    "dstrain_test = torch.from_numpy(np.float64(dstrain_test)).to(device)\n",
    "\n",
    "# Concatenating state variables and converting to PyTorch tensors (training set)\n",
    "svars_tv = torch.cat((torch.from_numpy(np.float64(r_t_tv)), torch.from_numpy(np.float64(z_t_tv))), -1).to(device)\n",
    "\n",
    "# Concatenating state variables and converting to PyTorch tensors (test set)\n",
    "svars_test = torch.cat((torch.from_numpy(np.float64(r_t_test)), torch.from_numpy(np.float64(z_t_test))), -1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed30eca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the NICE network parameters\n",
    "NICE_network.solver = \"midpoint\"\n",
    "NICE_network.scheme = \"forward\"\n",
    "NICE_network.step_size = prm_dt / step_size\n",
    "\n",
    "# Initializing interpolation for the NICE network\n",
    "NICE_network.init_interp(dstrain_tv, t)\n",
    "\n",
    "# Turning off the inference mode\n",
    "NICE_network.inference = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb25f454",
   "metadata": {},
   "source": [
    "#### 4.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ab2b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer, loss function, and hyperparameters\n",
    "learningRate = 1e-2\n",
    "optimizer = torch.optim.Adam(NICE_network.parameters(), lr=learningRate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "w_reg = 1.e-5  # L2 penalty weight\n",
    "Nepochs = 300000  # Number of epochs\n",
    "MSE = torch.nn.MSELoss()\n",
    "\n",
    "# Early stopping criterion\n",
    "checkpoint_path = './checkpoints/checkpoint_benchmark2_sparse.pt'\n",
    "early_stopping = EarlyStopping(patience=10000, delta=1.e-9, verbose=False, path=checkpoint_path)\n",
    "\n",
    "# Lists for storing training and validation loss\n",
    "training_loss_hist = []\n",
    "validation_loss_value_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5484ace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | training loss: 2.3319e+02 | validation loss: 5.0372e+01\n",
      "Epoch: 20 | training loss: 1.2907e+02 | validation loss: 3.8840e+01\n",
      "Epoch: 30 | training loss: 5.5367e+01 | validation loss: 1.8399e+01\n",
      "Epoch: 40 | training loss: 2.8783e+01 | validation loss: 1.0589e+01\n",
      "Epoch: 50 | training loss: 1.5365e+01 | validation loss: 5.4132e+00\n",
      "Epoch: 60 | training loss: 8.7022e+00 | validation loss: 3.5285e+00\n",
      "Epoch: 70 | training loss: 4.1675e+00 | validation loss: 2.5150e+00\n",
      "Epoch: 80 | training loss: 2.1579e+00 | validation loss: 1.8429e+00\n",
      "Epoch: 90 | training loss: 1.3320e+00 | validation loss: 1.2859e+00\n",
      "Epoch: 100 | training loss: 9.8365e-01 | validation loss: 9.0233e-01\n",
      "Epoch: 110 | training loss: 7.8975e-01 | validation loss: 5.8791e-01\n",
      "Epoch: 120 | training loss: 6.5747e-01 | validation loss: 4.4292e-01\n",
      "Epoch: 130 | training loss: 5.6296e-01 | validation loss: 3.7499e-01\n",
      "Epoch: 140 | training loss: 4.9206e-01 | validation loss: 3.4193e-01\n",
      "Epoch: 150 | training loss: 4.3642e-01 | validation loss: 3.1401e-01\n",
      "Epoch: 160 | training loss: 3.9122e-01 | validation loss: 2.8743e-01\n",
      "Epoch: 170 | training loss: 3.5345e-01 | validation loss: 2.6380e-01\n",
      "Epoch: 180 | training loss: 3.2140e-01 | validation loss: 2.4179e-01\n",
      "Epoch: 190 | training loss: 2.9400e-01 | validation loss: 2.2352e-01\n",
      "Epoch: 200 | training loss: 2.7059e-01 | validation loss: 2.0808e-01\n",
      "Epoch: 210 | training loss: 2.5065e-01 | validation loss: 1.9557e-01\n",
      "Epoch: 220 | training loss: 2.3372e-01 | validation loss: 1.8470e-01\n",
      "Epoch: 230 | training loss: 2.1929e-01 | validation loss: 1.7581e-01\n",
      "Epoch: 240 | training loss: 2.0689e-01 | validation loss: 1.6852e-01\n",
      "Epoch: 250 | training loss: 3.6904e-01 | validation loss: 2.2003e-01\n",
      "Epoch: 260 | training loss: 2.3662e-01 | validation loss: 1.7607e-01\n",
      "Epoch: 270 | training loss: 1.8235e-01 | validation loss: 1.4931e-01\n",
      "Epoch: 280 | training loss: 1.8139e-01 | validation loss: 1.4883e-01\n",
      "Epoch: 290 | training loss: 1.6631e-01 | validation loss: 1.4474e-01\n",
      "Epoch: 300 | training loss: 1.5904e-01 | validation loss: 1.4240e-01\n",
      "Epoch: 310 | training loss: 1.5302e-01 | validation loss: 1.3953e-01\n",
      "Epoch: 320 | training loss: 1.4731e-01 | validation loss: 1.3677e-01\n",
      "Epoch: 330 | training loss: 1.4207e-01 | validation loss: 1.3460e-01\n",
      "Epoch: 340 | training loss: 1.3724e-01 | validation loss: 1.3222e-01\n",
      "Epoch: 350 | training loss: 1.3274e-01 | validation loss: 1.3000e-01\n",
      "Epoch: 360 | training loss: 1.2852e-01 | validation loss: 1.2793e-01\n",
      "Epoch: 370 | training loss: 1.2455e-01 | validation loss: 1.2591e-01\n",
      "Epoch: 380 | training loss: 1.2081e-01 | validation loss: 1.2398e-01\n",
      "Epoch: 390 | training loss: 1.1728e-01 | validation loss: 1.2217e-01\n",
      "Epoch: 400 | training loss: 1.1393e-01 | validation loss: 1.2044e-01\n",
      "Epoch: 410 | training loss: 1.1076e-01 | validation loss: 1.1881e-01\n",
      "Epoch: 420 | training loss: 1.0774e-01 | validation loss: 1.1725e-01\n",
      "Epoch: 430 | training loss: 1.0486e-01 | validation loss: 1.1577e-01\n",
      "Epoch: 440 | training loss: 1.0212e-01 | validation loss: 1.1436e-01\n",
      "Epoch: 450 | training loss: 9.9499e-02 | validation loss: 1.1301e-01\n",
      "Epoch: 460 | training loss: 9.6995e-02 | validation loss: 1.1171e-01\n",
      "Epoch: 470 | training loss: 9.4600e-02 | validation loss: 1.1046e-01\n",
      "Epoch: 480 | training loss: 9.2305e-02 | validation loss: 1.0926e-01\n",
      "Epoch: 490 | training loss: 9.0105e-02 | validation loss: 1.0810e-01\n",
      "Epoch: 500 | training loss: 8.7993e-02 | validation loss: 1.0697e-01\n",
      "Epoch: 510 | training loss: 8.5966e-02 | validation loss: 1.0587e-01\n",
      "Epoch: 520 | training loss: 8.4017e-02 | validation loss: 1.0480e-01\n",
      "Epoch: 530 | training loss: 8.2143e-02 | validation loss: 1.0376e-01\n",
      "Epoch: 540 | training loss: 8.0339e-02 | validation loss: 1.0273e-01\n",
      "Epoch: 550 | training loss: 7.8603e-02 | validation loss: 1.0173e-01\n",
      "Epoch: 560 | training loss: 7.6929e-02 | validation loss: 1.0074e-01\n",
      "Epoch: 570 | training loss: 7.5316e-02 | validation loss: 9.9765e-02\n",
      "Epoch: 580 | training loss: 7.3760e-02 | validation loss: 9.8805e-02\n",
      "Epoch: 590 | training loss: 7.2259e-02 | validation loss: 9.7857e-02\n",
      "Epoch: 600 | training loss: 7.0810e-02 | validation loss: 9.6923e-02\n",
      "Epoch: 610 | training loss: 6.9412e-02 | validation loss: 9.6028e-02\n",
      "Epoch: 620 | training loss: 6.8950e-02 | validation loss: 9.5990e-02\n",
      "Epoch: 630 | training loss: 2.5571e-01 | validation loss: 1.5413e-01\n",
      "Epoch: 640 | training loss: 1.1340e-01 | validation loss: 1.1134e-01\n",
      "Epoch: 650 | training loss: 6.5114e-02 | validation loss: 9.3428e-02\n",
      "Epoch: 660 | training loss: 6.7990e-02 | validation loss: 9.2287e-02\n",
      "Epoch: 670 | training loss: 6.3535e-02 | validation loss: 9.2397e-02\n",
      "Epoch: 680 | training loss: 6.1522e-02 | validation loss: 9.1128e-02\n",
      "Epoch: 690 | training loss: 6.0556e-02 | validation loss: 8.9727e-02\n",
      "Epoch: 700 | training loss: 5.9298e-02 | validation loss: 8.9205e-02\n",
      "Epoch: 710 | training loss: 5.8320e-02 | validation loss: 8.8617e-02\n",
      "Epoch: 720 | training loss: 5.7323e-02 | validation loss: 8.7771e-02\n",
      "Epoch: 730 | training loss: 5.6388e-02 | validation loss: 8.6939e-02\n",
      "Epoch: 740 | training loss: 5.5479e-02 | validation loss: 8.6211e-02\n",
      "Epoch: 750 | training loss: 5.4601e-02 | validation loss: 8.5480e-02\n",
      "Epoch: 760 | training loss: 5.3752e-02 | validation loss: 8.4729e-02\n",
      "Epoch: 770 | training loss: 5.2930e-02 | validation loss: 8.3980e-02\n",
      "Epoch: 780 | training loss: 5.2134e-02 | validation loss: 8.3245e-02\n",
      "Epoch: 790 | training loss: 5.1363e-02 | validation loss: 8.2521e-02\n",
      "Epoch: 800 | training loss: 5.0616e-02 | validation loss: 8.1812e-02\n",
      "Epoch: 810 | training loss: 4.9893e-02 | validation loss: 8.1116e-02\n",
      "Epoch: 820 | training loss: 4.9191e-02 | validation loss: 8.0431e-02\n",
      "Epoch: 830 | training loss: 4.8511e-02 | validation loss: 7.9758e-02\n",
      "Epoch: 840 | training loss: 4.7851e-02 | validation loss: 7.9097e-02\n",
      "Epoch: 850 | training loss: 4.7212e-02 | validation loss: 7.8448e-02\n",
      "Epoch: 860 | training loss: 4.6591e-02 | validation loss: 7.7811e-02\n",
      "Epoch: 870 | training loss: 4.5989e-02 | validation loss: 7.7188e-02\n",
      "Epoch: 880 | training loss: 4.5404e-02 | validation loss: 7.6576e-02\n",
      "Epoch: 890 | training loss: 4.4837e-02 | validation loss: 7.5977e-02\n",
      "Epoch: 900 | training loss: 4.4286e-02 | validation loss: 7.5390e-02\n",
      "Epoch: 910 | training loss: 4.3750e-02 | validation loss: 7.4815e-02\n",
      "Epoch: 920 | training loss: 4.3230e-02 | validation loss: 7.4253e-02\n",
      "Epoch: 930 | training loss: 4.2724e-02 | validation loss: 7.3702e-02\n",
      "Epoch: 940 | training loss: 4.2233e-02 | validation loss: 7.3162e-02\n",
      "Epoch: 950 | training loss: 4.1755e-02 | validation loss: 7.2635e-02\n",
      "Epoch: 960 | training loss: 4.1290e-02 | validation loss: 7.2118e-02\n",
      "Epoch: 970 | training loss: 4.0837e-02 | validation loss: 7.1613e-02\n",
      "Epoch: 980 | training loss: 4.0397e-02 | validation loss: 7.1118e-02\n",
      "Epoch: 990 | training loss: 3.9968e-02 | validation loss: 7.0634e-02\n",
      "Epoch: 1000 | training loss: 3.9550e-02 | validation loss: 7.0161e-02\n",
      "Epoch: 1010 | training loss: 3.9143e-02 | validation loss: 6.9697e-02\n",
      "Epoch: 1020 | training loss: 3.8746e-02 | validation loss: 6.9244e-02\n",
      "Epoch: 1030 | training loss: 3.8359e-02 | validation loss: 6.8800e-02\n",
      "Epoch: 1040 | training loss: 3.7982e-02 | validation loss: 6.8365e-02\n",
      "Epoch: 1050 | training loss: 3.7614e-02 | validation loss: 6.7940e-02\n",
      "Epoch: 1060 | training loss: 3.7255e-02 | validation loss: 6.7524e-02\n",
      "Epoch: 1070 | training loss: 3.6904e-02 | validation loss: 6.7116e-02\n",
      "Epoch: 1080 | training loss: 3.6562e-02 | validation loss: 6.6716e-02\n",
      "Epoch: 1090 | training loss: 3.6227e-02 | validation loss: 6.6325e-02\n",
      "Epoch: 1100 | training loss: 3.5901e-02 | validation loss: 6.5942e-02\n",
      "Epoch: 1110 | training loss: 3.5581e-02 | validation loss: 6.5566e-02\n",
      "Epoch: 1120 | training loss: 3.5269e-02 | validation loss: 6.5198e-02\n",
      "Epoch: 1130 | training loss: 3.4964e-02 | validation loss: 6.4837e-02\n",
      "Epoch: 1140 | training loss: 3.4665e-02 | validation loss: 6.4483e-02\n",
      "Epoch: 1150 | training loss: 3.4373e-02 | validation loss: 6.4135e-02\n",
      "Epoch: 1160 | training loss: 3.4087e-02 | validation loss: 6.3795e-02\n",
      "Epoch: 1170 | training loss: 3.3807e-02 | validation loss: 6.3460e-02\n",
      "Epoch: 1180 | training loss: 3.3533e-02 | validation loss: 6.3132e-02\n",
      "Epoch: 1190 | training loss: 3.3264e-02 | validation loss: 6.2809e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1200 | training loss: 3.3001e-02 | validation loss: 6.2492e-02\n",
      "Epoch: 1210 | training loss: 3.2744e-02 | validation loss: 6.2181e-02\n",
      "Epoch: 1220 | training loss: 3.2491e-02 | validation loss: 6.1875e-02\n",
      "Epoch: 1230 | training loss: 3.2244e-02 | validation loss: 6.1574e-02\n",
      "Epoch: 1240 | training loss: 3.2001e-02 | validation loss: 6.1278e-02\n",
      "Epoch: 1250 | training loss: 3.1763e-02 | validation loss: 6.0987e-02\n",
      "Epoch: 1260 | training loss: 3.1529e-02 | validation loss: 6.0701e-02\n",
      "Epoch: 1270 | training loss: 3.1300e-02 | validation loss: 6.0419e-02\n",
      "Epoch: 1280 | training loss: 3.1075e-02 | validation loss: 6.0141e-02\n",
      "Epoch: 1290 | training loss: 3.0854e-02 | validation loss: 5.9868e-02\n",
      "Epoch: 1300 | training loss: 3.0638e-02 | validation loss: 5.9599e-02\n",
      "Epoch: 1310 | training loss: 3.0425e-02 | validation loss: 5.9334e-02\n",
      "Epoch: 1320 | training loss: 3.0216e-02 | validation loss: 5.9073e-02\n",
      "Epoch: 1330 | training loss: 3.0011e-02 | validation loss: 5.8815e-02\n",
      "Epoch: 1340 | training loss: 2.9810e-02 | validation loss: 5.8561e-02\n",
      "Epoch: 1350 | training loss: 2.9612e-02 | validation loss: 5.8311e-02\n",
      "Epoch: 1360 | training loss: 2.9417e-02 | validation loss: 5.8064e-02\n",
      "Epoch: 1370 | training loss: 2.9226e-02 | validation loss: 5.7820e-02\n",
      "Epoch: 1380 | training loss: 2.9038e-02 | validation loss: 5.7579e-02\n",
      "Epoch: 1390 | training loss: 2.8854e-02 | validation loss: 5.7342e-02\n",
      "Epoch: 1400 | training loss: 2.8672e-02 | validation loss: 5.7108e-02\n",
      "Epoch: 1410 | training loss: 2.8494e-02 | validation loss: 5.6876e-02\n",
      "Epoch: 1420 | training loss: 2.8318e-02 | validation loss: 5.6648e-02\n",
      "Epoch: 1430 | training loss: 2.8146e-02 | validation loss: 5.6422e-02\n",
      "Epoch: 1440 | training loss: 2.7976e-02 | validation loss: 5.6199e-02\n",
      "Epoch: 1450 | training loss: 2.7809e-02 | validation loss: 5.5979e-02\n",
      "Epoch: 1460 | training loss: 2.7645e-02 | validation loss: 5.5761e-02\n",
      "Epoch: 1470 | training loss: 2.7483e-02 | validation loss: 5.5546e-02\n",
      "Epoch: 1480 | training loss: 2.7324e-02 | validation loss: 5.5333e-02\n",
      "Epoch: 1490 | training loss: 2.7167e-02 | validation loss: 5.5123e-02\n",
      "Epoch: 1500 | training loss: 2.7013e-02 | validation loss: 5.4914e-02\n",
      "Epoch: 1510 | training loss: 2.6861e-02 | validation loss: 5.4709e-02\n",
      "Epoch: 1520 | training loss: 2.6712e-02 | validation loss: 5.4505e-02\n",
      "Epoch: 1530 | training loss: 2.6565e-02 | validation loss: 5.4304e-02\n",
      "Epoch: 1540 | training loss: 2.6420e-02 | validation loss: 5.4105e-02\n",
      "Epoch: 1550 | training loss: 2.6278e-02 | validation loss: 5.3908e-02\n",
      "Epoch: 1560 | training loss: 2.6137e-02 | validation loss: 5.3713e-02\n",
      "Epoch: 1570 | training loss: 2.5999e-02 | validation loss: 5.3520e-02\n",
      "Epoch: 1580 | training loss: 2.5863e-02 | validation loss: 5.3329e-02\n",
      "Epoch: 1590 | training loss: 2.5729e-02 | validation loss: 5.3140e-02\n",
      "Epoch: 1600 | training loss: 2.5596e-02 | validation loss: 5.2953e-02\n",
      "Epoch: 1610 | training loss: 2.5466e-02 | validation loss: 5.2767e-02\n",
      "Epoch: 1620 | training loss: 2.5338e-02 | validation loss: 5.2584e-02\n",
      "Epoch: 1630 | training loss: 2.5211e-02 | validation loss: 5.2402e-02\n",
      "Epoch: 1640 | training loss: 2.5086e-02 | validation loss: 5.2222e-02\n",
      "Epoch: 1650 | training loss: 2.4964e-02 | validation loss: 5.2044e-02\n",
      "Epoch: 1660 | training loss: 2.4842e-02 | validation loss: 5.1868e-02\n",
      "Epoch: 1670 | training loss: 2.4723e-02 | validation loss: 5.1693e-02\n",
      "Epoch: 1680 | training loss: 2.4605e-02 | validation loss: 5.1520e-02\n",
      "Epoch: 1690 | training loss: 2.4489e-02 | validation loss: 5.1348e-02\n",
      "Epoch: 1700 | training loss: 2.4375e-02 | validation loss: 5.1178e-02\n",
      "Epoch: 1710 | training loss: 2.4262e-02 | validation loss: 5.1010e-02\n",
      "Epoch: 1720 | training loss: 2.4151e-02 | validation loss: 5.0843e-02\n",
      "Epoch: 1730 | training loss: 2.4041e-02 | validation loss: 5.0678e-02\n",
      "Epoch: 1740 | training loss: 2.3932e-02 | validation loss: 5.0515e-02\n",
      "Epoch: 1750 | training loss: 2.3826e-02 | validation loss: 5.0352e-02\n",
      "Epoch: 1760 | training loss: 2.3720e-02 | validation loss: 5.0192e-02\n",
      "Epoch: 1770 | training loss: 2.3616e-02 | validation loss: 5.0032e-02\n",
      "Epoch: 1780 | training loss: 2.3514e-02 | validation loss: 4.9874e-02\n",
      "Epoch: 1790 | training loss: 2.3413e-02 | validation loss: 4.9718e-02\n",
      "Epoch: 1800 | training loss: 2.3313e-02 | validation loss: 4.9563e-02\n",
      "Epoch: 1810 | training loss: 2.3214e-02 | validation loss: 4.9409e-02\n",
      "Epoch: 1820 | training loss: 2.3117e-02 | validation loss: 4.9257e-02\n",
      "Epoch: 1830 | training loss: 2.3021e-02 | validation loss: 4.9106e-02\n",
      "Epoch: 1840 | training loss: 2.2926e-02 | validation loss: 4.8956e-02\n",
      "Epoch: 1850 | training loss: 2.2833e-02 | validation loss: 4.8808e-02\n",
      "Epoch: 1860 | training loss: 2.2740e-02 | validation loss: 4.8661e-02\n",
      "Epoch: 1870 | training loss: 2.2649e-02 | validation loss: 4.8515e-02\n",
      "Epoch: 1880 | training loss: 2.2559e-02 | validation loss: 4.8370e-02\n",
      "Epoch: 1890 | training loss: 2.2470e-02 | validation loss: 4.8227e-02\n",
      "Epoch: 1900 | training loss: 2.2383e-02 | validation loss: 4.8085e-02\n",
      "Epoch: 1910 | training loss: 2.2296e-02 | validation loss: 4.7944e-02\n",
      "Epoch: 1920 | training loss: 2.2210e-02 | validation loss: 4.7804e-02\n",
      "Epoch: 1930 | training loss: 2.2126e-02 | validation loss: 4.7666e-02\n",
      "Epoch: 1940 | training loss: 2.2042e-02 | validation loss: 4.7529e-02\n",
      "Epoch: 1950 | training loss: 2.1960e-02 | validation loss: 4.7393e-02\n",
      "Epoch: 1960 | training loss: 2.1879e-02 | validation loss: 4.7258e-02\n",
      "Epoch: 1970 | training loss: 2.1798e-02 | validation loss: 4.7124e-02\n",
      "Epoch: 1980 | training loss: 2.1719e-02 | validation loss: 4.6991e-02\n",
      "Epoch: 1990 | training loss: 2.1640e-02 | validation loss: 4.6859e-02\n",
      "Epoch: 2000 | training loss: 2.1563e-02 | validation loss: 4.6729e-02\n",
      "Epoch: 2010 | training loss: 2.1486e-02 | validation loss: 4.6600e-02\n",
      "Epoch: 2020 | training loss: 2.1410e-02 | validation loss: 4.6471e-02\n",
      "Epoch: 2030 | training loss: 2.1335e-02 | validation loss: 4.6344e-02\n",
      "Epoch: 2040 | training loss: 2.1261e-02 | validation loss: 4.6218e-02\n",
      "Epoch: 2050 | training loss: 2.1188e-02 | validation loss: 4.6093e-02\n",
      "Epoch: 2060 | training loss: 2.1116e-02 | validation loss: 4.5968e-02\n",
      "Epoch: 2070 | training loss: 2.1045e-02 | validation loss: 4.5845e-02\n",
      "Epoch: 2080 | training loss: 2.0974e-02 | validation loss: 4.5723e-02\n",
      "Epoch: 2090 | training loss: 2.0904e-02 | validation loss: 4.5602e-02\n",
      "Epoch: 2100 | training loss: 2.0835e-02 | validation loss: 4.5482e-02\n",
      "Epoch: 2110 | training loss: 2.0767e-02 | validation loss: 4.5363e-02\n",
      "Epoch: 2120 | training loss: 2.0700e-02 | validation loss: 4.5245e-02\n",
      "Epoch: 2130 | training loss: 2.0633e-02 | validation loss: 4.5128e-02\n",
      "Epoch: 2140 | training loss: 2.0567e-02 | validation loss: 4.5012e-02\n",
      "Epoch: 2150 | training loss: 2.0502e-02 | validation loss: 4.4896e-02\n",
      "Epoch: 2160 | training loss: 2.0437e-02 | validation loss: 4.4782e-02\n",
      "Epoch: 2170 | training loss: 2.0374e-02 | validation loss: 4.4669e-02\n",
      "Epoch: 2180 | training loss: 2.0310e-02 | validation loss: 4.4556e-02\n",
      "Epoch: 2190 | training loss: 2.0248e-02 | validation loss: 4.4445e-02\n",
      "Epoch: 2200 | training loss: 2.0186e-02 | validation loss: 4.4334e-02\n",
      "Epoch: 2210 | training loss: 2.0125e-02 | validation loss: 4.4225e-02\n",
      "Epoch: 2220 | training loss: 2.0065e-02 | validation loss: 4.4116e-02\n",
      "Epoch: 2230 | training loss: 2.0005e-02 | validation loss: 4.4008e-02\n",
      "Epoch: 2240 | training loss: 1.9946e-02 | validation loss: 4.3901e-02\n",
      "Epoch: 2250 | training loss: 1.9888e-02 | validation loss: 4.3794e-02\n",
      "Epoch: 2260 | training loss: 1.9830e-02 | validation loss: 4.3689e-02\n",
      "Epoch: 2270 | training loss: 1.9773e-02 | validation loss: 4.3585e-02\n",
      "Epoch: 2280 | training loss: 1.9716e-02 | validation loss: 4.3481e-02\n",
      "Epoch: 2290 | training loss: 1.9660e-02 | validation loss: 4.3378e-02\n",
      "Epoch: 2300 | training loss: 1.9605e-02 | validation loss: 4.3276e-02\n",
      "Epoch: 2310 | training loss: 1.9550e-02 | validation loss: 4.3175e-02\n",
      "Epoch: 2320 | training loss: 1.9496e-02 | validation loss: 4.3074e-02\n",
      "Epoch: 2330 | training loss: 1.9442e-02 | validation loss: 4.2975e-02\n",
      "Epoch: 2340 | training loss: 1.9389e-02 | validation loss: 4.2876e-02\n",
      "Epoch: 2350 | training loss: 1.9336e-02 | validation loss: 4.2778e-02\n",
      "Epoch: 2360 | training loss: 1.9284e-02 | validation loss: 4.2681e-02\n",
      "Epoch: 2370 | training loss: 1.9232e-02 | validation loss: 4.2584e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2380 | training loss: 1.9181e-02 | validation loss: 4.2489e-02\n",
      "Epoch: 2390 | training loss: 1.9131e-02 | validation loss: 4.2394e-02\n",
      "Epoch: 2400 | training loss: 1.9081e-02 | validation loss: 4.2300e-02\n",
      "Epoch: 2410 | training loss: 1.9031e-02 | validation loss: 4.2206e-02\n",
      "Epoch: 2420 | training loss: 1.8982e-02 | validation loss: 4.2113e-02\n",
      "Epoch: 2430 | training loss: 1.8934e-02 | validation loss: 4.2021e-02\n",
      "Epoch: 2440 | training loss: 1.8886e-02 | validation loss: 4.1930e-02\n",
      "Epoch: 2450 | training loss: 1.8838e-02 | validation loss: 4.1840e-02\n",
      "Epoch: 2460 | training loss: 1.8791e-02 | validation loss: 4.1750e-02\n",
      "Epoch: 2470 | training loss: 1.8744e-02 | validation loss: 4.1661e-02\n",
      "Epoch: 2480 | training loss: 1.8698e-02 | validation loss: 4.1572e-02\n",
      "Epoch: 2490 | training loss: 1.8652e-02 | validation loss: 4.1485e-02\n",
      "Epoch: 2500 | training loss: 1.8607e-02 | validation loss: 4.1398e-02\n",
      "Epoch: 2510 | training loss: 1.8562e-02 | validation loss: 4.1311e-02\n",
      "Epoch: 2520 | training loss: 1.8517e-02 | validation loss: 4.1226e-02\n",
      "Epoch: 2530 | training loss: 1.8473e-02 | validation loss: 4.1141e-02\n",
      "Epoch: 2540 | training loss: 1.8430e-02 | validation loss: 4.1056e-02\n",
      "Epoch: 2550 | training loss: 1.8386e-02 | validation loss: 4.0973e-02\n",
      "Epoch: 2560 | training loss: 1.8344e-02 | validation loss: 4.0890e-02\n",
      "Epoch: 2570 | training loss: 1.8301e-02 | validation loss: 4.0807e-02\n",
      "Epoch: 2580 | training loss: 1.8259e-02 | validation loss: 4.0726e-02\n",
      "Epoch: 2590 | training loss: 1.8218e-02 | validation loss: 4.0645e-02\n",
      "Epoch: 2600 | training loss: 1.8176e-02 | validation loss: 4.0564e-02\n",
      "Epoch: 2610 | training loss: 1.8136e-02 | validation loss: 4.0484e-02\n",
      "Epoch: 2620 | training loss: 1.8095e-02 | validation loss: 4.0405e-02\n",
      "Epoch: 2630 | training loss: 1.8055e-02 | validation loss: 4.0327e-02\n",
      "Epoch: 2640 | training loss: 1.8015e-02 | validation loss: 4.0249e-02\n",
      "Epoch: 2650 | training loss: 1.7976e-02 | validation loss: 4.0171e-02\n",
      "Epoch: 2660 | training loss: 1.7937e-02 | validation loss: 4.0094e-02\n",
      "Epoch: 2670 | training loss: 1.7898e-02 | validation loss: 4.0018e-02\n",
      "Epoch: 2680 | training loss: 1.7860e-02 | validation loss: 3.9943e-02\n",
      "Epoch: 2690 | training loss: 1.7822e-02 | validation loss: 3.9867e-02\n",
      "Epoch: 2700 | training loss: 1.7784e-02 | validation loss: 3.9793e-02\n",
      "Epoch: 2710 | training loss: 1.7747e-02 | validation loss: 3.9719e-02\n",
      "Epoch: 2720 | training loss: 1.7710e-02 | validation loss: 3.9646e-02\n",
      "Epoch: 2730 | training loss: 1.7673e-02 | validation loss: 3.9573e-02\n",
      "Epoch: 2740 | training loss: 1.7637e-02 | validation loss: 3.9501e-02\n",
      "Epoch: 2750 | training loss: 1.7601e-02 | validation loss: 3.9429e-02\n",
      "Epoch: 2760 | training loss: 1.7565e-02 | validation loss: 3.9358e-02\n",
      "Epoch: 2770 | training loss: 1.7530e-02 | validation loss: 3.9288e-02\n",
      "Epoch: 2780 | training loss: 1.7495e-02 | validation loss: 3.9218e-02\n",
      "Epoch: 2790 | training loss: 1.7460e-02 | validation loss: 3.9148e-02\n",
      "Epoch: 2800 | training loss: 1.7426e-02 | validation loss: 3.9079e-02\n",
      "Epoch: 2810 | training loss: 1.7392e-02 | validation loss: 3.9011e-02\n",
      "Epoch: 2820 | training loss: 1.7358e-02 | validation loss: 3.8943e-02\n",
      "Epoch: 2830 | training loss: 1.7324e-02 | validation loss: 3.8875e-02\n",
      "Epoch: 2840 | training loss: 1.7291e-02 | validation loss: 3.8808e-02\n",
      "Epoch: 2850 | training loss: 1.7258e-02 | validation loss: 3.8742e-02\n",
      "Epoch: 2860 | training loss: 1.7225e-02 | validation loss: 3.8676e-02\n",
      "Epoch: 2870 | training loss: 1.7193e-02 | validation loss: 3.8611e-02\n",
      "Epoch: 2880 | training loss: 1.7161e-02 | validation loss: 3.8546e-02\n",
      "Epoch: 2890 | training loss: 1.7129e-02 | validation loss: 3.8482e-02\n",
      "Epoch: 2900 | training loss: 1.7097e-02 | validation loss: 3.8418e-02\n",
      "Epoch: 2910 | training loss: 1.7066e-02 | validation loss: 3.8354e-02\n",
      "Epoch: 2920 | training loss: 1.7035e-02 | validation loss: 3.8291e-02\n",
      "Epoch: 2930 | training loss: 1.7004e-02 | validation loss: 3.8229e-02\n",
      "Epoch: 2940 | training loss: 1.6974e-02 | validation loss: 3.8167e-02\n",
      "Epoch: 2950 | training loss: 1.6943e-02 | validation loss: 3.8105e-02\n",
      "Epoch: 2960 | training loss: 1.6913e-02 | validation loss: 3.8044e-02\n",
      "Epoch: 2970 | training loss: 1.6883e-02 | validation loss: 3.7983e-02\n",
      "Epoch: 2980 | training loss: 1.6854e-02 | validation loss: 3.7923e-02\n",
      "Epoch: 2990 | training loss: 1.6825e-02 | validation loss: 3.7863e-02\n",
      "Epoch: 3000 | training loss: 1.6795e-02 | validation loss: 3.7804e-02\n",
      "Epoch: 3010 | training loss: 1.6767e-02 | validation loss: 3.7745e-02\n",
      "Epoch: 3020 | training loss: 1.6738e-02 | validation loss: 3.7687e-02\n",
      "Epoch: 3030 | training loss: 1.6710e-02 | validation loss: 3.7629e-02\n",
      "Epoch: 3040 | training loss: 1.6682e-02 | validation loss: 3.7571e-02\n",
      "Epoch: 3050 | training loss: 1.6654e-02 | validation loss: 3.7514e-02\n",
      "Epoch: 3060 | training loss: 1.6626e-02 | validation loss: 3.7458e-02\n",
      "Epoch: 3070 | training loss: 1.6598e-02 | validation loss: 3.7401e-02\n",
      "Epoch: 3080 | training loss: 1.6571e-02 | validation loss: 3.7345e-02\n",
      "Epoch: 3090 | training loss: 1.6544e-02 | validation loss: 3.7290e-02\n",
      "Epoch: 3100 | training loss: 1.6517e-02 | validation loss: 3.7235e-02\n",
      "Epoch: 3110 | training loss: 1.6491e-02 | validation loss: 3.7180e-02\n",
      "Epoch: 3120 | training loss: 1.6464e-02 | validation loss: 3.7126e-02\n",
      "Epoch: 3130 | training loss: 1.6438e-02 | validation loss: 3.7072e-02\n",
      "Epoch: 3140 | training loss: 1.6412e-02 | validation loss: 3.7019e-02\n",
      "Epoch: 3150 | training loss: 1.6387e-02 | validation loss: 3.6966e-02\n",
      "Epoch: 3160 | training loss: 1.6361e-02 | validation loss: 3.6913e-02\n",
      "Epoch: 3170 | training loss: 1.6336e-02 | validation loss: 3.6861e-02\n",
      "Epoch: 3180 | training loss: 1.6310e-02 | validation loss: 3.6809e-02\n",
      "Epoch: 3190 | training loss: 1.6286e-02 | validation loss: 3.6757e-02\n",
      "Epoch: 3200 | training loss: 1.6261e-02 | validation loss: 3.6706e-02\n",
      "Epoch: 3210 | training loss: 1.6236e-02 | validation loss: 3.6656e-02\n",
      "Epoch: 3220 | training loss: 1.6212e-02 | validation loss: 3.6605e-02\n",
      "Epoch: 3230 | training loss: 1.6188e-02 | validation loss: 3.6555e-02\n",
      "Epoch: 3240 | training loss: 1.6164e-02 | validation loss: 3.6506e-02\n",
      "Epoch: 3250 | training loss: 1.6140e-02 | validation loss: 3.6456e-02\n",
      "Epoch: 3260 | training loss: 1.6116e-02 | validation loss: 3.6407e-02\n",
      "Epoch: 3270 | training loss: 1.6093e-02 | validation loss: 3.6359e-02\n",
      "Epoch: 3280 | training loss: 1.6069e-02 | validation loss: 3.6310e-02\n",
      "Epoch: 3290 | training loss: 1.6046e-02 | validation loss: 3.6263e-02\n",
      "Epoch: 3300 | training loss: 1.6023e-02 | validation loss: 3.6215e-02\n",
      "Epoch: 3310 | training loss: 1.6001e-02 | validation loss: 3.6168e-02\n",
      "Epoch: 3320 | training loss: 1.5978e-02 | validation loss: 3.6121e-02\n",
      "Epoch: 3330 | training loss: 1.5956e-02 | validation loss: 3.6074e-02\n",
      "Epoch: 3340 | training loss: 1.5933e-02 | validation loss: 3.6028e-02\n",
      "Epoch: 3350 | training loss: 1.5911e-02 | validation loss: 3.5982e-02\n",
      "Epoch: 3360 | training loss: 1.5889e-02 | validation loss: 3.5937e-02\n",
      "Epoch: 3370 | training loss: 1.5868e-02 | validation loss: 3.5892e-02\n",
      "Epoch: 3380 | training loss: 1.5846e-02 | validation loss: 3.5847e-02\n",
      "Epoch: 3390 | training loss: 1.5825e-02 | validation loss: 3.5802e-02\n",
      "Epoch: 3400 | training loss: 1.5803e-02 | validation loss: 3.5758e-02\n",
      "Epoch: 3410 | training loss: 1.5782e-02 | validation loss: 3.5714e-02\n",
      "Epoch: 3420 | training loss: 1.5761e-02 | validation loss: 3.5671e-02\n",
      "Epoch: 3430 | training loss: 1.5741e-02 | validation loss: 3.5627e-02\n",
      "Epoch: 3440 | training loss: 1.5720e-02 | validation loss: 3.5584e-02\n",
      "Epoch: 3450 | training loss: 1.5700e-02 | validation loss: 3.5542e-02\n",
      "Epoch: 3460 | training loss: 1.5679e-02 | validation loss: 3.5499e-02\n",
      "Epoch: 3470 | training loss: 1.5659e-02 | validation loss: 3.5457e-02\n",
      "Epoch: 3480 | training loss: 1.5639e-02 | validation loss: 3.5415e-02\n",
      "Epoch: 3490 | training loss: 1.5619e-02 | validation loss: 3.5374e-02\n",
      "Epoch: 3500 | training loss: 1.5599e-02 | validation loss: 3.5333e-02\n",
      "Epoch: 3510 | training loss: 1.5580e-02 | validation loss: 3.5292e-02\n",
      "Epoch: 3520 | training loss: 1.5560e-02 | validation loss: 3.5251e-02\n",
      "Epoch: 3530 | training loss: 1.5541e-02 | validation loss: 3.5211e-02\n",
      "Epoch: 3540 | training loss: 1.5522e-02 | validation loss: 3.5171e-02\n",
      "Epoch: 3550 | training loss: 1.5503e-02 | validation loss: 3.5131e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3560 | training loss: 1.5484e-02 | validation loss: 3.5092e-02\n",
      "Epoch: 3570 | training loss: 1.5465e-02 | validation loss: 3.5052e-02\n",
      "Epoch: 3580 | training loss: 1.5447e-02 | validation loss: 3.5014e-02\n",
      "Epoch: 3590 | training loss: 1.5428e-02 | validation loss: 3.4975e-02\n",
      "Epoch: 3600 | training loss: 1.5410e-02 | validation loss: 3.4937e-02\n",
      "Epoch: 3610 | training loss: 1.5392e-02 | validation loss: 3.4898e-02\n",
      "Epoch: 3620 | training loss: 1.5373e-02 | validation loss: 3.4861e-02\n",
      "Epoch: 3630 | training loss: 1.5355e-02 | validation loss: 3.4823e-02\n",
      "Epoch: 3640 | training loss: 1.5338e-02 | validation loss: 3.4786e-02\n",
      "Epoch: 3650 | training loss: 1.5320e-02 | validation loss: 3.4749e-02\n",
      "Epoch: 3660 | training loss: 1.5302e-02 | validation loss: 3.4712e-02\n",
      "Epoch: 3670 | training loss: 1.5285e-02 | validation loss: 3.4675e-02\n",
      "Epoch: 3680 | training loss: 1.5268e-02 | validation loss: 3.4639e-02\n",
      "Epoch: 3690 | training loss: 1.5250e-02 | validation loss: 3.4603e-02\n",
      "Epoch: 3700 | training loss: 1.5233e-02 | validation loss: 3.4567e-02\n",
      "Epoch: 3710 | training loss: 1.5216e-02 | validation loss: 3.4532e-02\n",
      "Epoch: 3720 | training loss: 1.5199e-02 | validation loss: 3.4496e-02\n",
      "Epoch: 3730 | training loss: 1.5183e-02 | validation loss: 3.4461e-02\n",
      "Epoch: 3740 | training loss: 1.5166e-02 | validation loss: 3.4427e-02\n",
      "Epoch: 3750 | training loss: 1.5150e-02 | validation loss: 3.4392e-02\n",
      "Epoch: 3760 | training loss: 1.5133e-02 | validation loss: 3.4358e-02\n",
      "Epoch: 3770 | training loss: 1.5117e-02 | validation loss: 3.4324e-02\n",
      "Epoch: 3780 | training loss: 1.5101e-02 | validation loss: 3.4290e-02\n",
      "Epoch: 3790 | training loss: 1.5085e-02 | validation loss: 3.4256e-02\n",
      "Epoch: 3800 | training loss: 1.5069e-02 | validation loss: 3.4223e-02\n",
      "Epoch: 3810 | training loss: 1.5053e-02 | validation loss: 3.4190e-02\n",
      "Epoch: 3820 | training loss: 1.5037e-02 | validation loss: 3.4157e-02\n",
      "Epoch: 3830 | training loss: 1.5022e-02 | validation loss: 3.4124e-02\n",
      "Epoch: 3840 | training loss: 1.5006e-02 | validation loss: 3.4091e-02\n",
      "Epoch: 3850 | training loss: 1.4991e-02 | validation loss: 3.4059e-02\n",
      "Epoch: 3860 | training loss: 1.4975e-02 | validation loss: 3.4027e-02\n",
      "Epoch: 3870 | training loss: 1.4960e-02 | validation loss: 3.3995e-02\n",
      "Epoch: 3880 | training loss: 1.4945e-02 | validation loss: 3.3964e-02\n",
      "Epoch: 3890 | training loss: 1.4930e-02 | validation loss: 3.3932e-02\n",
      "Epoch: 3900 | training loss: 1.4915e-02 | validation loss: 3.3901e-02\n",
      "Epoch: 3910 | training loss: 1.4901e-02 | validation loss: 3.3870e-02\n",
      "Epoch: 3920 | training loss: 1.4886e-02 | validation loss: 3.3839e-02\n",
      "Epoch: 3930 | training loss: 1.4871e-02 | validation loss: 3.3809e-02\n",
      "Epoch: 3940 | training loss: 1.4857e-02 | validation loss: 3.3779e-02\n",
      "Epoch: 3950 | training loss: 1.4842e-02 | validation loss: 3.3748e-02\n",
      "Epoch: 3960 | training loss: 1.4828e-02 | validation loss: 3.3718e-02\n",
      "Epoch: 3970 | training loss: 1.4814e-02 | validation loss: 3.3689e-02\n",
      "Epoch: 3980 | training loss: 1.4800e-02 | validation loss: 3.3659e-02\n",
      "Epoch: 3990 | training loss: 1.4786e-02 | validation loss: 3.3630e-02\n",
      "Epoch: 4000 | training loss: 1.4772e-02 | validation loss: 3.3601e-02\n",
      "Epoch: 4010 | training loss: 1.4758e-02 | validation loss: 3.3572e-02\n",
      "Epoch: 4020 | training loss: 1.4744e-02 | validation loss: 3.3543e-02\n",
      "Epoch: 4030 | training loss: 1.4731e-02 | validation loss: 3.3514e-02\n",
      "Epoch: 4040 | training loss: 1.4717e-02 | validation loss: 3.3486e-02\n",
      "Epoch: 4050 | training loss: 1.4704e-02 | validation loss: 3.3458e-02\n",
      "Epoch: 4060 | training loss: 1.4691e-02 | validation loss: 3.3430e-02\n",
      "Epoch: 4070 | training loss: 1.4677e-02 | validation loss: 3.3402e-02\n",
      "Epoch: 4080 | training loss: 1.4664e-02 | validation loss: 3.3375e-02\n",
      "Epoch: 4090 | training loss: 1.4651e-02 | validation loss: 3.3347e-02\n",
      "Epoch: 4100 | training loss: 1.4638e-02 | validation loss: 3.3320e-02\n",
      "Epoch: 4110 | training loss: 1.4625e-02 | validation loss: 3.3293e-02\n",
      "Epoch: 4120 | training loss: 1.4612e-02 | validation loss: 3.3266e-02\n",
      "Epoch: 4130 | training loss: 1.4600e-02 | validation loss: 3.3239e-02\n",
      "Epoch: 4140 | training loss: 1.4587e-02 | validation loss: 3.3213e-02\n",
      "Epoch: 4150 | training loss: 1.4574e-02 | validation loss: 3.3186e-02\n",
      "Epoch: 4160 | training loss: 1.4562e-02 | validation loss: 3.3160e-02\n",
      "Epoch: 4170 | training loss: 1.4549e-02 | validation loss: 3.3134e-02\n",
      "Epoch: 4180 | training loss: 1.4537e-02 | validation loss: 3.3108e-02\n",
      "Epoch: 4190 | training loss: 1.4525e-02 | validation loss: 3.3083e-02\n",
      "Epoch: 4200 | training loss: 1.4513e-02 | validation loss: 3.3057e-02\n",
      "Epoch: 4210 | training loss: 1.4501e-02 | validation loss: 3.3032e-02\n",
      "Epoch: 4220 | training loss: 1.4489e-02 | validation loss: 3.3007e-02\n",
      "Epoch: 4230 | training loss: 1.4477e-02 | validation loss: 3.2982e-02\n",
      "Epoch: 4240 | training loss: 1.4465e-02 | validation loss: 3.2957e-02\n",
      "Epoch: 4250 | training loss: 1.4453e-02 | validation loss: 3.2932e-02\n",
      "Epoch: 4260 | training loss: 1.4441e-02 | validation loss: 3.2908e-02\n",
      "Epoch: 4270 | training loss: 1.4430e-02 | validation loss: 3.2884e-02\n",
      "Epoch: 4280 | training loss: 1.4418e-02 | validation loss: 3.2859e-02\n",
      "Epoch: 4290 | training loss: 1.4407e-02 | validation loss: 3.2835e-02\n",
      "Epoch: 4300 | training loss: 1.4395e-02 | validation loss: 3.2811e-02\n",
      "Epoch: 4310 | training loss: 1.4384e-02 | validation loss: 3.2788e-02\n",
      "Epoch: 4320 | training loss: 1.4373e-02 | validation loss: 3.2764e-02\n",
      "Epoch: 4330 | training loss: 1.4361e-02 | validation loss: 3.2741e-02\n",
      "Epoch: 4340 | training loss: 1.4350e-02 | validation loss: 3.2718e-02\n",
      "Epoch: 4350 | training loss: 1.4339e-02 | validation loss: 3.2695e-02\n",
      "Epoch: 4360 | training loss: 1.4328e-02 | validation loss: 3.2672e-02\n",
      "Epoch: 4370 | training loss: 1.4317e-02 | validation loss: 3.2649e-02\n",
      "Epoch: 4380 | training loss: 1.4307e-02 | validation loss: 3.2626e-02\n",
      "Epoch: 4390 | training loss: 1.4296e-02 | validation loss: 3.2604e-02\n",
      "Epoch: 4400 | training loss: 1.4285e-02 | validation loss: 3.2581e-02\n",
      "Epoch: 4410 | training loss: 1.4275e-02 | validation loss: 3.2559e-02\n",
      "Epoch: 4420 | training loss: 1.4264e-02 | validation loss: 3.2537e-02\n",
      "Epoch: 4430 | training loss: 1.4253e-02 | validation loss: 3.2515e-02\n",
      "Epoch: 4440 | training loss: 1.4243e-02 | validation loss: 3.2493e-02\n",
      "Epoch: 4450 | training loss: 1.4233e-02 | validation loss: 3.2472e-02\n",
      "Epoch: 4460 | training loss: 1.4222e-02 | validation loss: 3.2450e-02\n",
      "Epoch: 4470 | training loss: 1.4212e-02 | validation loss: 3.2429e-02\n",
      "Epoch: 4480 | training loss: 1.4202e-02 | validation loss: 3.2408e-02\n",
      "Epoch: 4490 | training loss: 1.4192e-02 | validation loss: 3.2387e-02\n",
      "Epoch: 4500 | training loss: 1.4182e-02 | validation loss: 3.2366e-02\n",
      "Epoch: 4510 | training loss: 1.4172e-02 | validation loss: 3.2345e-02\n",
      "Epoch: 4520 | training loss: 1.4162e-02 | validation loss: 3.2324e-02\n",
      "Epoch: 4530 | training loss: 1.4152e-02 | validation loss: 3.2304e-02\n",
      "Epoch: 4540 | training loss: 1.4142e-02 | validation loss: 3.2283e-02\n",
      "Epoch: 4550 | training loss: 1.4133e-02 | validation loss: 3.2263e-02\n",
      "Epoch: 4560 | training loss: 1.4123e-02 | validation loss: 3.2243e-02\n",
      "Epoch: 4570 | training loss: 1.4113e-02 | validation loss: 3.2223e-02\n",
      "Epoch: 4580 | training loss: 1.4104e-02 | validation loss: 3.2203e-02\n",
      "Epoch: 4590 | training loss: 1.4094e-02 | validation loss: 3.2183e-02\n",
      "Epoch: 4600 | training loss: 1.4085e-02 | validation loss: 3.2163e-02\n",
      "Epoch: 4610 | training loss: 1.4076e-02 | validation loss: 3.2144e-02\n",
      "Epoch: 4620 | training loss: 1.4066e-02 | validation loss: 3.2124e-02\n",
      "Epoch: 4630 | training loss: 1.4057e-02 | validation loss: 3.2104e-02\n",
      "Epoch: 4640 | training loss: 1.4047e-02 | validation loss: 3.2085e-02\n",
      "Epoch: 4650 | training loss: 1.4038e-02 | validation loss: 3.2065e-02\n",
      "Epoch: 4660 | training loss: 1.4028e-02 | validation loss: 3.2045e-02\n",
      "Epoch: 4670 | training loss: 1.4019e-02 | validation loss: 3.2025e-02\n",
      "Epoch: 4680 | training loss: 1.4009e-02 | validation loss: 3.2005e-02\n",
      "Epoch: 4690 | training loss: 1.3999e-02 | validation loss: 3.1985e-02\n",
      "Epoch: 4700 | training loss: 1.3990e-02 | validation loss: 3.1965e-02\n",
      "Epoch: 4710 | training loss: 1.3980e-02 | validation loss: 3.1944e-02\n",
      "Epoch: 4720 | training loss: 1.3970e-02 | validation loss: 3.1924e-02\n",
      "Epoch: 4730 | training loss: 1.3960e-02 | validation loss: 3.1904e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4740 | training loss: 1.3951e-02 | validation loss: 3.1883e-02\n",
      "Epoch: 4750 | training loss: 1.3941e-02 | validation loss: 3.1863e-02\n",
      "Epoch: 4760 | training loss: 1.3931e-02 | validation loss: 3.1842e-02\n",
      "Epoch: 4770 | training loss: 1.3921e-02 | validation loss: 3.1821e-02\n",
      "Epoch: 4780 | training loss: 1.3911e-02 | validation loss: 3.1801e-02\n",
      "Epoch: 4790 | training loss: 1.3901e-02 | validation loss: 3.1780e-02\n",
      "Epoch: 4800 | training loss: 1.3891e-02 | validation loss: 3.1759e-02\n",
      "Epoch: 4810 | training loss: 1.3881e-02 | validation loss: 3.1738e-02\n",
      "Epoch: 4820 | training loss: 1.3871e-02 | validation loss: 3.1717e-02\n",
      "Epoch: 4830 | training loss: 1.3860e-02 | validation loss: 3.1696e-02\n",
      "Epoch: 4840 | training loss: 1.3850e-02 | validation loss: 3.1675e-02\n",
      "Epoch: 4850 | training loss: 1.3840e-02 | validation loss: 3.1653e-02\n",
      "Epoch: 4860 | training loss: 1.3830e-02 | validation loss: 3.1632e-02\n",
      "Epoch: 4870 | training loss: 1.3819e-02 | validation loss: 3.1611e-02\n",
      "Epoch: 4880 | training loss: 1.3809e-02 | validation loss: 3.1589e-02\n",
      "Epoch: 4890 | training loss: 1.3799e-02 | validation loss: 3.1568e-02\n",
      "Epoch: 4900 | training loss: 1.3788e-02 | validation loss: 3.1546e-02\n",
      "Epoch: 4910 | training loss: 1.3778e-02 | validation loss: 3.1524e-02\n",
      "Epoch: 4920 | training loss: 1.3767e-02 | validation loss: 3.1503e-02\n",
      "Epoch: 4930 | training loss: 1.3757e-02 | validation loss: 3.1481e-02\n",
      "Epoch: 4940 | training loss: 1.3746e-02 | validation loss: 3.1459e-02\n",
      "Epoch: 4950 | training loss: 1.3736e-02 | validation loss: 3.1437e-02\n",
      "Epoch: 4960 | training loss: 1.3725e-02 | validation loss: 3.1415e-02\n",
      "Epoch: 4970 | training loss: 1.3714e-02 | validation loss: 3.1393e-02\n",
      "Epoch: 4980 | training loss: 1.3703e-02 | validation loss: 3.1370e-02\n",
      "Epoch: 4990 | training loss: 1.3693e-02 | validation loss: 3.1348e-02\n",
      "Epoch: 5000 | training loss: 1.3682e-02 | validation loss: 3.1326e-02\n",
      "Epoch: 5010 | training loss: 1.3671e-02 | validation loss: 3.1303e-02\n",
      "Epoch: 5020 | training loss: 1.3660e-02 | validation loss: 3.1281e-02\n",
      "Epoch: 5030 | training loss: 1.3649e-02 | validation loss: 3.1258e-02\n",
      "Epoch: 5040 | training loss: 1.3638e-02 | validation loss: 3.1236e-02\n",
      "Epoch: 5050 | training loss: 1.3627e-02 | validation loss: 3.1213e-02\n",
      "Epoch: 5060 | training loss: 1.3616e-02 | validation loss: 3.1190e-02\n",
      "Epoch: 5070 | training loss: 1.3605e-02 | validation loss: 3.1167e-02\n",
      "Epoch: 5080 | training loss: 1.3594e-02 | validation loss: 3.1144e-02\n",
      "Epoch: 5090 | training loss: 1.3583e-02 | validation loss: 3.1121e-02\n",
      "Epoch: 5100 | training loss: 1.3572e-02 | validation loss: 3.1098e-02\n",
      "Epoch: 5110 | training loss: 1.3561e-02 | validation loss: 3.1075e-02\n",
      "Epoch: 5120 | training loss: 1.3549e-02 | validation loss: 3.1052e-02\n",
      "Epoch: 5130 | training loss: 1.3538e-02 | validation loss: 3.1029e-02\n",
      "Epoch: 5140 | training loss: 1.3527e-02 | validation loss: 3.1005e-02\n",
      "Epoch: 5150 | training loss: 1.3515e-02 | validation loss: 3.0982e-02\n",
      "Epoch: 5160 | training loss: 1.3504e-02 | validation loss: 3.0958e-02\n",
      "Epoch: 5170 | training loss: 1.3492e-02 | validation loss: 3.0935e-02\n",
      "Epoch: 5180 | training loss: 1.3481e-02 | validation loss: 3.0911e-02\n",
      "Epoch: 5190 | training loss: 1.3469e-02 | validation loss: 3.0887e-02\n",
      "Epoch: 5200 | training loss: 1.3458e-02 | validation loss: 3.0864e-02\n",
      "Epoch: 5210 | training loss: 1.3446e-02 | validation loss: 3.0840e-02\n",
      "Epoch: 5220 | training loss: 1.3435e-02 | validation loss: 3.0816e-02\n",
      "Epoch: 5230 | training loss: 1.3423e-02 | validation loss: 3.0792e-02\n",
      "Epoch: 5240 | training loss: 1.3411e-02 | validation loss: 3.0768e-02\n",
      "Epoch: 5250 | training loss: 1.3399e-02 | validation loss: 3.0744e-02\n",
      "Epoch: 5260 | training loss: 1.3388e-02 | validation loss: 3.0719e-02\n",
      "Epoch: 5270 | training loss: 1.3376e-02 | validation loss: 3.0695e-02\n",
      "Epoch: 5280 | training loss: 1.3364e-02 | validation loss: 3.0671e-02\n",
      "Epoch: 5290 | training loss: 1.3352e-02 | validation loss: 3.0646e-02\n",
      "Epoch: 5300 | training loss: 1.3340e-02 | validation loss: 3.0622e-02\n",
      "Epoch: 5310 | training loss: 1.3328e-02 | validation loss: 3.0597e-02\n",
      "Epoch: 5320 | training loss: 1.3316e-02 | validation loss: 3.0572e-02\n",
      "Epoch: 5330 | training loss: 1.3304e-02 | validation loss: 3.0548e-02\n",
      "Epoch: 5340 | training loss: 1.3292e-02 | validation loss: 3.0523e-02\n",
      "Epoch: 5350 | training loss: 1.3280e-02 | validation loss: 3.0498e-02\n",
      "Epoch: 5360 | training loss: 1.3267e-02 | validation loss: 3.0473e-02\n",
      "Epoch: 5370 | training loss: 1.3255e-02 | validation loss: 3.0448e-02\n",
      "Epoch: 5380 | training loss: 1.3243e-02 | validation loss: 3.0423e-02\n",
      "Epoch: 5390 | training loss: 1.3231e-02 | validation loss: 3.0398e-02\n",
      "Epoch: 5400 | training loss: 1.3218e-02 | validation loss: 3.0372e-02\n",
      "Epoch: 5410 | training loss: 1.3206e-02 | validation loss: 3.0347e-02\n",
      "Epoch: 5420 | training loss: 1.3193e-02 | validation loss: 3.0322e-02\n",
      "Epoch: 5430 | training loss: 1.3181e-02 | validation loss: 3.0296e-02\n",
      "Epoch: 5440 | training loss: 1.3168e-02 | validation loss: 3.0271e-02\n",
      "Epoch: 5450 | training loss: 1.3156e-02 | validation loss: 3.0245e-02\n",
      "Epoch: 5460 | training loss: 1.3143e-02 | validation loss: 3.0220e-02\n",
      "Epoch: 5470 | training loss: 1.3131e-02 | validation loss: 3.0194e-02\n",
      "Epoch: 5480 | training loss: 1.3118e-02 | validation loss: 3.0168e-02\n",
      "Epoch: 5490 | training loss: 1.3105e-02 | validation loss: 3.0142e-02\n",
      "Epoch: 5500 | training loss: 1.3093e-02 | validation loss: 3.0116e-02\n",
      "Epoch: 5510 | training loss: 1.3080e-02 | validation loss: 3.0090e-02\n",
      "Epoch: 5520 | training loss: 1.3067e-02 | validation loss: 3.0064e-02\n",
      "Epoch: 5530 | training loss: 1.3054e-02 | validation loss: 3.0038e-02\n",
      "Epoch: 5540 | training loss: 1.3041e-02 | validation loss: 3.0012e-02\n",
      "Epoch: 5550 | training loss: 1.3028e-02 | validation loss: 2.9986e-02\n",
      "Epoch: 5560 | training loss: 1.3015e-02 | validation loss: 2.9959e-02\n",
      "Epoch: 5570 | training loss: 1.3002e-02 | validation loss: 2.9933e-02\n",
      "Epoch: 5580 | training loss: 1.2989e-02 | validation loss: 2.9906e-02\n",
      "Epoch: 5590 | training loss: 1.2976e-02 | validation loss: 2.9880e-02\n",
      "Epoch: 5600 | training loss: 1.2963e-02 | validation loss: 2.9853e-02\n",
      "Epoch: 5610 | training loss: 1.2950e-02 | validation loss: 2.9827e-02\n",
      "Epoch: 5620 | training loss: 1.2936e-02 | validation loss: 2.9800e-02\n",
      "Epoch: 5630 | training loss: 1.2923e-02 | validation loss: 2.9773e-02\n",
      "Epoch: 5640 | training loss: 1.2910e-02 | validation loss: 2.9746e-02\n",
      "Epoch: 5650 | training loss: 1.2896e-02 | validation loss: 2.9719e-02\n",
      "Epoch: 5660 | training loss: 1.2883e-02 | validation loss: 2.9692e-02\n",
      "Epoch: 5670 | training loss: 1.2870e-02 | validation loss: 2.9665e-02\n",
      "Epoch: 5680 | training loss: 1.2856e-02 | validation loss: 2.9638e-02\n",
      "Epoch: 5690 | training loss: 1.2843e-02 | validation loss: 2.9611e-02\n",
      "Epoch: 5700 | training loss: 1.2829e-02 | validation loss: 2.9584e-02\n",
      "Epoch: 5710 | training loss: 1.2816e-02 | validation loss: 2.9556e-02\n",
      "Epoch: 5720 | training loss: 1.2802e-02 | validation loss: 2.9529e-02\n",
      "Epoch: 5730 | training loss: 1.2788e-02 | validation loss: 2.9502e-02\n",
      "Epoch: 5740 | training loss: 1.2775e-02 | validation loss: 2.9474e-02\n",
      "Epoch: 5750 | training loss: 1.2761e-02 | validation loss: 2.9446e-02\n",
      "Epoch: 5760 | training loss: 1.2747e-02 | validation loss: 2.9419e-02\n",
      "Epoch: 5770 | training loss: 1.2733e-02 | validation loss: 2.9391e-02\n",
      "Epoch: 5780 | training loss: 1.2719e-02 | validation loss: 2.9363e-02\n",
      "Epoch: 5790 | training loss: 1.2705e-02 | validation loss: 2.9336e-02\n",
      "Epoch: 5800 | training loss: 1.2691e-02 | validation loss: 2.9308e-02\n",
      "Epoch: 5810 | training loss: 1.2677e-02 | validation loss: 2.9280e-02\n",
      "Epoch: 5820 | training loss: 1.2663e-02 | validation loss: 2.9252e-02\n",
      "Epoch: 5830 | training loss: 1.2649e-02 | validation loss: 2.9224e-02\n",
      "Epoch: 5840 | training loss: 1.2635e-02 | validation loss: 2.9196e-02\n",
      "Epoch: 5850 | training loss: 1.2621e-02 | validation loss: 2.9167e-02\n",
      "Epoch: 5860 | training loss: 1.2607e-02 | validation loss: 2.9139e-02\n",
      "Epoch: 5870 | training loss: 1.2593e-02 | validation loss: 2.9111e-02\n",
      "Epoch: 5880 | training loss: 1.2578e-02 | validation loss: 2.9083e-02\n",
      "Epoch: 5890 | training loss: 1.2564e-02 | validation loss: 2.9054e-02\n",
      "Epoch: 5900 | training loss: 1.2550e-02 | validation loss: 2.9026e-02\n",
      "Epoch: 5910 | training loss: 1.2535e-02 | validation loss: 2.8997e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5920 | training loss: 1.2521e-02 | validation loss: 2.8969e-02\n",
      "Epoch: 5930 | training loss: 1.2506e-02 | validation loss: 2.8940e-02\n",
      "Epoch: 5940 | training loss: 1.2492e-02 | validation loss: 2.8911e-02\n",
      "Epoch: 5950 | training loss: 1.2477e-02 | validation loss: 2.8882e-02\n",
      "Epoch: 5960 | training loss: 1.2463e-02 | validation loss: 2.8854e-02\n",
      "Epoch: 5970 | training loss: 1.2448e-02 | validation loss: 2.8825e-02\n",
      "Epoch: 5980 | training loss: 1.2434e-02 | validation loss: 2.8796e-02\n",
      "Epoch: 5990 | training loss: 1.2419e-02 | validation loss: 2.8767e-02\n",
      "Epoch: 6000 | training loss: 1.2404e-02 | validation loss: 2.8738e-02\n",
      "Epoch: 6010 | training loss: 1.2389e-02 | validation loss: 2.8709e-02\n",
      "Epoch: 6020 | training loss: 1.2374e-02 | validation loss: 2.8680e-02\n",
      "Epoch: 6030 | training loss: 1.2360e-02 | validation loss: 2.8651e-02\n",
      "Epoch: 6040 | training loss: 1.2345e-02 | validation loss: 2.8621e-02\n",
      "Epoch: 6050 | training loss: 1.2330e-02 | validation loss: 2.8592e-02\n",
      "Epoch: 6060 | training loss: 1.2315e-02 | validation loss: 2.8563e-02\n",
      "Epoch: 6070 | training loss: 1.2300e-02 | validation loss: 2.8533e-02\n",
      "Epoch: 6080 | training loss: 1.2285e-02 | validation loss: 2.8504e-02\n",
      "Epoch: 6090 | training loss: 1.2270e-02 | validation loss: 2.8474e-02\n",
      "Epoch: 6100 | training loss: 1.2254e-02 | validation loss: 2.8445e-02\n",
      "Epoch: 6110 | training loss: 1.2239e-02 | validation loss: 2.8415e-02\n",
      "Epoch: 6120 | training loss: 1.2224e-02 | validation loss: 2.8386e-02\n",
      "Epoch: 6130 | training loss: 1.2209e-02 | validation loss: 2.8356e-02\n",
      "Epoch: 6140 | training loss: 1.2194e-02 | validation loss: 2.8326e-02\n",
      "Epoch: 6150 | training loss: 1.2178e-02 | validation loss: 2.8297e-02\n",
      "Epoch: 6160 | training loss: 1.2163e-02 | validation loss: 2.8267e-02\n",
      "Epoch: 6170 | training loss: 1.2147e-02 | validation loss: 2.8237e-02\n",
      "Epoch: 6180 | training loss: 1.2132e-02 | validation loss: 2.8207e-02\n",
      "Epoch: 6190 | training loss: 1.2117e-02 | validation loss: 2.8177e-02\n",
      "Epoch: 6200 | training loss: 1.2101e-02 | validation loss: 2.8147e-02\n",
      "Epoch: 6210 | training loss: 1.2085e-02 | validation loss: 2.8117e-02\n",
      "Epoch: 6220 | training loss: 1.2070e-02 | validation loss: 2.8087e-02\n",
      "Epoch: 6230 | training loss: 1.2054e-02 | validation loss: 2.8057e-02\n",
      "Epoch: 6240 | training loss: 1.2039e-02 | validation loss: 2.8027e-02\n",
      "Epoch: 6250 | training loss: 1.2023e-02 | validation loss: 2.7997e-02\n",
      "Epoch: 6260 | training loss: 1.2007e-02 | validation loss: 2.7966e-02\n",
      "Epoch: 6270 | training loss: 1.1991e-02 | validation loss: 2.7936e-02\n",
      "Epoch: 6280 | training loss: 1.1976e-02 | validation loss: 2.7906e-02\n",
      "Epoch: 6290 | training loss: 1.1960e-02 | validation loss: 2.7876e-02\n",
      "Epoch: 6300 | training loss: 1.1944e-02 | validation loss: 2.7845e-02\n",
      "Epoch: 6310 | training loss: 1.1928e-02 | validation loss: 2.7815e-02\n",
      "Epoch: 6320 | training loss: 1.1912e-02 | validation loss: 2.7784e-02\n",
      "Epoch: 6330 | training loss: 1.1896e-02 | validation loss: 2.7754e-02\n",
      "Epoch: 6340 | training loss: 1.1880e-02 | validation loss: 2.7723e-02\n",
      "Epoch: 6350 | training loss: 1.1864e-02 | validation loss: 2.7693e-02\n",
      "Epoch: 6360 | training loss: 1.1848e-02 | validation loss: 2.7662e-02\n",
      "Epoch: 6370 | training loss: 1.1831e-02 | validation loss: 2.7632e-02\n",
      "Epoch: 6380 | training loss: 1.1815e-02 | validation loss: 2.7601e-02\n",
      "Epoch: 6390 | training loss: 1.1799e-02 | validation loss: 2.7570e-02\n",
      "Epoch: 6400 | training loss: 1.1783e-02 | validation loss: 2.7540e-02\n",
      "Epoch: 6410 | training loss: 1.1766e-02 | validation loss: 2.7509e-02\n",
      "Epoch: 6420 | training loss: 1.1750e-02 | validation loss: 2.7478e-02\n",
      "Epoch: 6430 | training loss: 1.1734e-02 | validation loss: 2.7447e-02\n",
      "Epoch: 6440 | training loss: 1.1717e-02 | validation loss: 2.7416e-02\n",
      "Epoch: 6450 | training loss: 1.1701e-02 | validation loss: 2.7386e-02\n",
      "Epoch: 6460 | training loss: 1.1684e-02 | validation loss: 2.7355e-02\n",
      "Epoch: 6470 | training loss: 1.1668e-02 | validation loss: 2.7324e-02\n",
      "Epoch: 6480 | training loss: 1.1651e-02 | validation loss: 2.7293e-02\n",
      "Epoch: 6490 | training loss: 1.1635e-02 | validation loss: 2.7262e-02\n",
      "Epoch: 6500 | training loss: 1.1618e-02 | validation loss: 2.7231e-02\n",
      "Epoch: 6510 | training loss: 1.1602e-02 | validation loss: 2.7200e-02\n",
      "Epoch: 6520 | training loss: 1.1585e-02 | validation loss: 2.7169e-02\n",
      "Epoch: 6530 | training loss: 1.1568e-02 | validation loss: 2.7138e-02\n",
      "Epoch: 6540 | training loss: 1.1551e-02 | validation loss: 2.7107e-02\n",
      "Epoch: 6550 | training loss: 1.1535e-02 | validation loss: 2.7076e-02\n",
      "Epoch: 6560 | training loss: 1.1518e-02 | validation loss: 2.7045e-02\n",
      "Epoch: 6570 | training loss: 1.1501e-02 | validation loss: 2.7014e-02\n",
      "Epoch: 6580 | training loss: 1.1484e-02 | validation loss: 2.6983e-02\n",
      "Epoch: 6590 | training loss: 1.1467e-02 | validation loss: 2.6951e-02\n",
      "Epoch: 6600 | training loss: 1.1450e-02 | validation loss: 2.6920e-02\n",
      "Epoch: 6610 | training loss: 1.1433e-02 | validation loss: 2.6889e-02\n",
      "Epoch: 6620 | training loss: 1.1416e-02 | validation loss: 2.6858e-02\n",
      "Epoch: 6630 | training loss: 1.1399e-02 | validation loss: 2.6827e-02\n",
      "Epoch: 6640 | training loss: 1.1382e-02 | validation loss: 2.6795e-02\n",
      "Epoch: 6650 | training loss: 1.1365e-02 | validation loss: 2.6764e-02\n",
      "Epoch: 6660 | training loss: 1.1348e-02 | validation loss: 2.6733e-02\n",
      "Epoch: 6670 | training loss: 1.1330e-02 | validation loss: 2.6702e-02\n",
      "Epoch: 6680 | training loss: 1.1313e-02 | validation loss: 2.6671e-02\n",
      "Epoch: 6690 | training loss: 1.1296e-02 | validation loss: 2.6639e-02\n",
      "Epoch: 6700 | training loss: 1.1279e-02 | validation loss: 2.6608e-02\n",
      "Epoch: 6710 | training loss: 1.1261e-02 | validation loss: 2.6577e-02\n",
      "Epoch: 6720 | training loss: 1.1244e-02 | validation loss: 2.6545e-02\n",
      "Epoch: 6730 | training loss: 1.1227e-02 | validation loss: 2.6514e-02\n",
      "Epoch: 6740 | training loss: 1.1209e-02 | validation loss: 2.6483e-02\n",
      "Epoch: 6750 | training loss: 1.1192e-02 | validation loss: 2.6452e-02\n",
      "Epoch: 6760 | training loss: 1.1174e-02 | validation loss: 2.6420e-02\n",
      "Epoch: 6770 | training loss: 1.1157e-02 | validation loss: 2.6389e-02\n",
      "Epoch: 6780 | training loss: 1.1139e-02 | validation loss: 2.6358e-02\n",
      "Epoch: 6790 | training loss: 1.1122e-02 | validation loss: 2.6326e-02\n",
      "Epoch: 6800 | training loss: 1.1104e-02 | validation loss: 2.6295e-02\n",
      "Epoch: 6810 | training loss: 1.1086e-02 | validation loss: 2.6264e-02\n",
      "Epoch: 6820 | training loss: 1.1069e-02 | validation loss: 2.6233e-02\n",
      "Epoch: 6830 | training loss: 1.1051e-02 | validation loss: 2.6201e-02\n",
      "Epoch: 6840 | training loss: 1.1033e-02 | validation loss: 2.6170e-02\n",
      "Epoch: 6850 | training loss: 1.1016e-02 | validation loss: 2.6139e-02\n",
      "Epoch: 6860 | training loss: 1.0998e-02 | validation loss: 2.6107e-02\n",
      "Epoch: 6870 | training loss: 1.0980e-02 | validation loss: 2.6076e-02\n",
      "Epoch: 6880 | training loss: 1.0962e-02 | validation loss: 2.6045e-02\n",
      "Epoch: 6890 | training loss: 1.0944e-02 | validation loss: 2.6014e-02\n",
      "Epoch: 6900 | training loss: 1.0926e-02 | validation loss: 2.5982e-02\n",
      "Epoch: 6910 | training loss: 1.0908e-02 | validation loss: 2.5951e-02\n",
      "Epoch: 6920 | training loss: 1.0891e-02 | validation loss: 2.5920e-02\n",
      "Epoch: 6930 | training loss: 1.0873e-02 | validation loss: 2.5889e-02\n",
      "Epoch: 6940 | training loss: 1.0855e-02 | validation loss: 2.5858e-02\n",
      "Epoch: 6950 | training loss: 1.0837e-02 | validation loss: 2.5832e-02\n",
      "Epoch: 6960 | training loss: 1.0840e-02 | validation loss: 2.6000e-02\n",
      "Epoch: 6970 | training loss: 1.0823e-02 | validation loss: 2.5618e-02\n",
      "Epoch: 6980 | training loss: 1.0791e-02 | validation loss: 2.5642e-02\n",
      "Epoch: 6990 | training loss: 1.0768e-02 | validation loss: 2.5643e-02\n",
      "Epoch: 7000 | training loss: 1.0748e-02 | validation loss: 2.5636e-02\n",
      "Epoch: 7010 | training loss: 1.0729e-02 | validation loss: 2.5622e-02\n",
      "Epoch: 7020 | training loss: 1.0711e-02 | validation loss: 2.5602e-02\n",
      "Epoch: 7030 | training loss: 1.0693e-02 | validation loss: 2.5580e-02\n",
      "Epoch: 7040 | training loss: 1.0675e-02 | validation loss: 2.5554e-02\n",
      "Epoch: 7050 | training loss: 1.0657e-02 | validation loss: 2.5523e-02\n",
      "Epoch: 7060 | training loss: 1.0639e-02 | validation loss: 2.5491e-02\n",
      "Epoch: 7070 | training loss: 1.0621e-02 | validation loss: 2.5459e-02\n",
      "Epoch: 7080 | training loss: 1.0603e-02 | validation loss: 2.5430e-02\n",
      "Epoch: 7090 | training loss: 1.0585e-02 | validation loss: 2.5400e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7100 | training loss: 1.0567e-02 | validation loss: 2.5370e-02\n",
      "Epoch: 7110 | training loss: 1.0549e-02 | validation loss: 2.5340e-02\n",
      "Epoch: 7120 | training loss: 1.0530e-02 | validation loss: 2.5310e-02\n",
      "Epoch: 7130 | training loss: 1.0512e-02 | validation loss: 2.5280e-02\n",
      "Epoch: 7140 | training loss: 1.0494e-02 | validation loss: 2.5250e-02\n",
      "Epoch: 7150 | training loss: 1.0476e-02 | validation loss: 2.5220e-02\n",
      "Epoch: 7160 | training loss: 1.0457e-02 | validation loss: 2.5190e-02\n",
      "Epoch: 7170 | training loss: 1.0439e-02 | validation loss: 2.5160e-02\n",
      "Epoch: 7180 | training loss: 1.0421e-02 | validation loss: 2.5130e-02\n",
      "Epoch: 7190 | training loss: 1.0402e-02 | validation loss: 2.5100e-02\n",
      "Epoch: 7200 | training loss: 1.0384e-02 | validation loss: 2.5069e-02\n",
      "Epoch: 7210 | training loss: 1.0366e-02 | validation loss: 2.5032e-02\n",
      "Epoch: 7220 | training loss: 1.0360e-02 | validation loss: 2.4905e-02\n",
      "Epoch: 7230 | training loss: 1.0338e-02 | validation loss: 2.5079e-02\n",
      "Epoch: 7240 | training loss: 1.0314e-02 | validation loss: 2.4902e-02\n",
      "Epoch: 7250 | training loss: 1.0297e-02 | validation loss: 2.4866e-02\n",
      "Epoch: 7260 | training loss: 1.0277e-02 | validation loss: 2.4850e-02\n",
      "Epoch: 7270 | training loss: 1.0257e-02 | validation loss: 2.4845e-02\n",
      "Epoch: 7280 | training loss: 1.0238e-02 | validation loss: 2.4835e-02\n",
      "Epoch: 7290 | training loss: 1.0220e-02 | validation loss: 2.4815e-02\n",
      "Epoch: 7300 | training loss: 1.0202e-02 | validation loss: 2.4782e-02\n",
      "Epoch: 7310 | training loss: 1.0184e-02 | validation loss: 2.4746e-02\n",
      "Epoch: 7320 | training loss: 1.0166e-02 | validation loss: 2.4719e-02\n",
      "Epoch: 7330 | training loss: 1.0147e-02 | validation loss: 2.4693e-02\n",
      "Epoch: 7340 | training loss: 1.0129e-02 | validation loss: 2.4662e-02\n",
      "Epoch: 7350 | training loss: 1.0111e-02 | validation loss: 2.4634e-02\n",
      "Epoch: 7360 | training loss: 1.0093e-02 | validation loss: 2.4605e-02\n",
      "Epoch: 7370 | training loss: 1.0074e-02 | validation loss: 2.4577e-02\n",
      "Epoch: 7380 | training loss: 1.0056e-02 | validation loss: 2.4548e-02\n",
      "Epoch: 7390 | training loss: 1.0037e-02 | validation loss: 2.4519e-02\n",
      "Epoch: 7400 | training loss: 1.0019e-02 | validation loss: 2.4491e-02\n",
      "Epoch: 7410 | training loss: 1.0001e-02 | validation loss: 2.4462e-02\n",
      "Epoch: 7420 | training loss: 9.9823e-03 | validation loss: 2.4433e-02\n",
      "Epoch: 7430 | training loss: 9.9639e-03 | validation loss: 2.4405e-02\n",
      "Epoch: 7440 | training loss: 9.9454e-03 | validation loss: 2.4374e-02\n",
      "Epoch: 7450 | training loss: 9.9276e-03 | validation loss: 2.4325e-02\n",
      "Epoch: 7460 | training loss: 1.0006e-02 | validation loss: 2.4117e-02\n",
      "Epoch: 7470 | training loss: 9.9266e-03 | validation loss: 2.4470e-02\n",
      "Epoch: 7480 | training loss: 9.8888e-03 | validation loss: 2.4386e-02\n",
      "Epoch: 7490 | training loss: 9.8565e-03 | validation loss: 2.4279e-02\n",
      "Epoch: 7500 | training loss: 9.8362e-03 | validation loss: 2.4204e-02\n",
      "Epoch: 7510 | training loss: 9.8187e-03 | validation loss: 2.4162e-02\n",
      "Epoch: 7520 | training loss: 9.8003e-03 | validation loss: 2.4144e-02\n",
      "Epoch: 7530 | training loss: 9.7820e-03 | validation loss: 2.4131e-02\n",
      "Epoch: 7540 | training loss: 9.7639e-03 | validation loss: 2.4105e-02\n",
      "Epoch: 7550 | training loss: 9.7457e-03 | validation loss: 2.4072e-02\n",
      "Epoch: 7560 | training loss: 9.7276e-03 | validation loss: 2.4045e-02\n",
      "Epoch: 7570 | training loss: 9.7095e-03 | validation loss: 2.4020e-02\n",
      "Epoch: 7580 | training loss: 9.6913e-03 | validation loss: 2.3991e-02\n",
      "Epoch: 7590 | training loss: 9.6731e-03 | validation loss: 2.3965e-02\n",
      "Epoch: 7600 | training loss: 9.6549e-03 | validation loss: 2.3937e-02\n",
      "Epoch: 7610 | training loss: 9.6367e-03 | validation loss: 2.3910e-02\n",
      "Epoch: 7620 | training loss: 9.6185e-03 | validation loss: 2.3882e-02\n",
      "Epoch: 7630 | training loss: 9.6003e-03 | validation loss: 2.3855e-02\n",
      "Epoch: 7640 | training loss: 9.5820e-03 | validation loss: 2.3828e-02\n",
      "Epoch: 7650 | training loss: 9.5637e-03 | validation loss: 2.3801e-02\n",
      "Epoch: 7660 | training loss: 9.5455e-03 | validation loss: 2.3774e-02\n",
      "Epoch: 7670 | training loss: 9.5272e-03 | validation loss: 2.3749e-02\n",
      "Epoch: 7680 | training loss: 9.5093e-03 | validation loss: 2.3736e-02\n",
      "Epoch: 7690 | training loss: 9.5603e-03 | validation loss: 2.3957e-02\n",
      "Epoch: 7700 | training loss: 9.5539e-03 | validation loss: 2.3506e-02\n",
      "Epoch: 7710 | training loss: 9.4635e-03 | validation loss: 2.3583e-02\n",
      "Epoch: 7720 | training loss: 9.4372e-03 | validation loss: 2.3618e-02\n",
      "Epoch: 7730 | training loss: 9.4205e-03 | validation loss: 2.3617e-02\n",
      "Epoch: 7740 | training loss: 9.4030e-03 | validation loss: 2.3592e-02\n",
      "Epoch: 7750 | training loss: 9.3844e-03 | validation loss: 2.3553e-02\n",
      "Epoch: 7760 | training loss: 9.3663e-03 | validation loss: 2.3512e-02\n",
      "Epoch: 7770 | training loss: 9.3486e-03 | validation loss: 2.3482e-02\n",
      "Epoch: 7780 | training loss: 9.3308e-03 | validation loss: 2.3461e-02\n",
      "Epoch: 7790 | training loss: 9.3130e-03 | validation loss: 2.3439e-02\n",
      "Epoch: 7800 | training loss: 9.2952e-03 | validation loss: 2.3410e-02\n",
      "Epoch: 7810 | training loss: 9.2774e-03 | validation loss: 2.3385e-02\n",
      "Epoch: 7820 | training loss: 9.2596e-03 | validation loss: 2.3360e-02\n",
      "Epoch: 7830 | training loss: 9.2418e-03 | validation loss: 2.3333e-02\n",
      "Epoch: 7840 | training loss: 9.2239e-03 | validation loss: 2.3308e-02\n",
      "Epoch: 7850 | training loss: 9.2061e-03 | validation loss: 2.3282e-02\n",
      "Epoch: 7860 | training loss: 9.1882e-03 | validation loss: 2.3257e-02\n",
      "Epoch: 7870 | training loss: 9.1704e-03 | validation loss: 2.3231e-02\n",
      "Epoch: 7880 | training loss: 9.1525e-03 | validation loss: 2.3206e-02\n",
      "Epoch: 7890 | training loss: 9.1346e-03 | validation loss: 2.3180e-02\n",
      "Epoch: 7900 | training loss: 9.1168e-03 | validation loss: 2.3154e-02\n",
      "Epoch: 7910 | training loss: 9.0989e-03 | validation loss: 2.3129e-02\n",
      "Epoch: 7920 | training loss: 9.0810e-03 | validation loss: 2.3100e-02\n",
      "Epoch: 7930 | training loss: 9.0665e-03 | validation loss: 2.3035e-02\n",
      "Epoch: 7940 | training loss: 9.3158e-03 | validation loss: 2.2860e-02\n",
      "Epoch: 7950 | training loss: 9.0558e-03 | validation loss: 2.2908e-02\n",
      "Epoch: 7960 | training loss: 9.0151e-03 | validation loss: 2.2951e-02\n",
      "Epoch: 7970 | training loss: 8.9943e-03 | validation loss: 2.2960e-02\n",
      "Epoch: 7980 | training loss: 8.9767e-03 | validation loss: 2.2957e-02\n",
      "Epoch: 7990 | training loss: 8.9597e-03 | validation loss: 2.2944e-02\n",
      "Epoch: 8000 | training loss: 8.9427e-03 | validation loss: 2.2922e-02\n",
      "Epoch: 8010 | training loss: 8.9253e-03 | validation loss: 2.2894e-02\n",
      "Epoch: 8020 | training loss: 8.9081e-03 | validation loss: 2.2864e-02\n",
      "Epoch: 8030 | training loss: 8.8909e-03 | validation loss: 2.2836e-02\n",
      "Epoch: 8040 | training loss: 8.8738e-03 | validation loss: 2.2814e-02\n",
      "Epoch: 8050 | training loss: 8.8566e-03 | validation loss: 2.2792e-02\n",
      "Epoch: 8060 | training loss: 8.8394e-03 | validation loss: 2.2768e-02\n",
      "Epoch: 8070 | training loss: 8.8223e-03 | validation loss: 2.2743e-02\n",
      "Epoch: 8080 | training loss: 8.8051e-03 | validation loss: 2.2720e-02\n",
      "Epoch: 8090 | training loss: 8.7879e-03 | validation loss: 2.2696e-02\n",
      "Epoch: 8100 | training loss: 8.7707e-03 | validation loss: 2.2672e-02\n",
      "Epoch: 8110 | training loss: 8.7535e-03 | validation loss: 2.2648e-02\n",
      "Epoch: 8120 | training loss: 8.7363e-03 | validation loss: 2.2624e-02\n",
      "Epoch: 8130 | training loss: 8.7191e-03 | validation loss: 2.2600e-02\n",
      "Epoch: 8140 | training loss: 8.7018e-03 | validation loss: 2.2576e-02\n",
      "Epoch: 8150 | training loss: 8.6846e-03 | validation loss: 2.2552e-02\n",
      "Epoch: 8160 | training loss: 8.6674e-03 | validation loss: 2.2528e-02\n",
      "Epoch: 8170 | training loss: 8.6501e-03 | validation loss: 2.2504e-02\n",
      "Epoch: 8180 | training loss: 8.6329e-03 | validation loss: 2.2481e-02\n",
      "Epoch: 8190 | training loss: 8.6158e-03 | validation loss: 2.2466e-02\n",
      "Epoch: 8200 | training loss: 8.6357e-03 | validation loss: 2.2607e-02\n",
      "Epoch: 8210 | training loss: 8.6215e-03 | validation loss: 2.2294e-02\n",
      "Epoch: 8220 | training loss: 8.5995e-03 | validation loss: 2.2389e-02\n",
      "Epoch: 8230 | training loss: 8.5597e-03 | validation loss: 2.2409e-02\n",
      "Epoch: 8240 | training loss: 8.5394e-03 | validation loss: 2.2421e-02\n",
      "Epoch: 8250 | training loss: 8.5178e-03 | validation loss: 2.2356e-02\n",
      "Epoch: 8260 | training loss: 8.5002e-03 | validation loss: 2.2297e-02\n",
      "Epoch: 8270 | training loss: 8.4838e-03 | validation loss: 2.2268e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8280 | training loss: 8.4675e-03 | validation loss: 2.2253e-02\n",
      "Epoch: 8290 | training loss: 8.4511e-03 | validation loss: 2.2238e-02\n",
      "Epoch: 8300 | training loss: 8.4348e-03 | validation loss: 2.2217e-02\n",
      "Epoch: 8310 | training loss: 8.4185e-03 | validation loss: 2.2191e-02\n",
      "Epoch: 8320 | training loss: 8.4022e-03 | validation loss: 2.2169e-02\n",
      "Epoch: 8330 | training loss: 8.3859e-03 | validation loss: 2.2147e-02\n",
      "Epoch: 8340 | training loss: 8.3696e-03 | validation loss: 2.2125e-02\n",
      "Epoch: 8350 | training loss: 8.3533e-03 | validation loss: 2.2103e-02\n",
      "Epoch: 8360 | training loss: 8.3370e-03 | validation loss: 2.2080e-02\n",
      "Epoch: 8370 | training loss: 8.3207e-03 | validation loss: 2.2058e-02\n",
      "Epoch: 8380 | training loss: 8.3044e-03 | validation loss: 2.2036e-02\n",
      "Epoch: 8390 | training loss: 8.2881e-03 | validation loss: 2.2014e-02\n",
      "Epoch: 8400 | training loss: 8.2717e-03 | validation loss: 2.1992e-02\n",
      "Epoch: 8410 | training loss: 8.2554e-03 | validation loss: 2.1969e-02\n",
      "Epoch: 8420 | training loss: 8.2391e-03 | validation loss: 2.1947e-02\n",
      "Epoch: 8430 | training loss: 8.2227e-03 | validation loss: 2.1925e-02\n",
      "Epoch: 8440 | training loss: 8.2065e-03 | validation loss: 2.1908e-02\n",
      "Epoch: 8450 | training loss: 8.2049e-03 | validation loss: 2.1964e-02\n",
      "Epoch: 8460 | training loss: 8.3167e-03 | validation loss: 2.2152e-02\n",
      "Epoch: 8470 | training loss: 8.2586e-03 | validation loss: 2.2068e-02\n",
      "Epoch: 8480 | training loss: 8.1775e-03 | validation loss: 2.1937e-02\n",
      "Epoch: 8490 | training loss: 8.1390e-03 | validation loss: 2.1858e-02\n",
      "Epoch: 8500 | training loss: 8.1152e-03 | validation loss: 2.1806e-02\n",
      "Epoch: 8510 | training loss: 8.0969e-03 | validation loss: 2.1766e-02\n",
      "Epoch: 8520 | training loss: 8.0807e-03 | validation loss: 2.1733e-02\n",
      "Epoch: 8530 | training loss: 8.0652e-03 | validation loss: 2.1707e-02\n",
      "Epoch: 8540 | training loss: 8.0497e-03 | validation loss: 2.1686e-02\n",
      "Epoch: 8550 | training loss: 8.0341e-03 | validation loss: 2.1669e-02\n",
      "Epoch: 8560 | training loss: 8.0186e-03 | validation loss: 2.1650e-02\n",
      "Epoch: 8570 | training loss: 8.0030e-03 | validation loss: 2.1628e-02\n",
      "Epoch: 8580 | training loss: 7.9875e-03 | validation loss: 2.1606e-02\n",
      "Epoch: 8590 | training loss: 7.9720e-03 | validation loss: 2.1585e-02\n",
      "Epoch: 8600 | training loss: 7.9564e-03 | validation loss: 2.1564e-02\n",
      "Epoch: 8610 | training loss: 7.9409e-03 | validation loss: 2.1542e-02\n",
      "Epoch: 8620 | training loss: 7.9253e-03 | validation loss: 2.1521e-02\n",
      "Epoch: 8630 | training loss: 7.9098e-03 | validation loss: 2.1499e-02\n",
      "Epoch: 8640 | training loss: 7.8942e-03 | validation loss: 2.1478e-02\n",
      "Epoch: 8650 | training loss: 7.8787e-03 | validation loss: 2.1456e-02\n",
      "Epoch: 8660 | training loss: 7.8632e-03 | validation loss: 2.1435e-02\n",
      "Epoch: 8670 | training loss: 7.8476e-03 | validation loss: 2.1413e-02\n",
      "Epoch: 8680 | training loss: 7.8321e-03 | validation loss: 2.1392e-02\n",
      "Epoch: 8690 | training loss: 7.8165e-03 | validation loss: 2.1370e-02\n",
      "Epoch: 8700 | training loss: 7.8009e-03 | validation loss: 2.1349e-02\n",
      "Epoch: 8710 | training loss: 7.7854e-03 | validation loss: 2.1328e-02\n",
      "Epoch: 8720 | training loss: 7.7709e-03 | validation loss: 2.1325e-02\n",
      "Epoch: 8730 | training loss: 8.0309e-03 | validation loss: 2.1779e-02\n",
      "Epoch: 8740 | training loss: 7.8770e-03 | validation loss: 2.1157e-02\n",
      "Epoch: 8750 | training loss: 7.7671e-03 | validation loss: 2.1217e-02\n",
      "Epoch: 8760 | training loss: 7.7354e-03 | validation loss: 2.1250e-02\n",
      "Epoch: 8770 | training loss: 7.7000e-03 | validation loss: 2.1172e-02\n",
      "Epoch: 8780 | training loss: 7.6842e-03 | validation loss: 2.1154e-02\n",
      "Epoch: 8790 | training loss: 7.6673e-03 | validation loss: 2.1164e-02\n",
      "Epoch: 8800 | training loss: 7.6529e-03 | validation loss: 2.1155e-02\n",
      "Epoch: 8810 | training loss: 7.6383e-03 | validation loss: 2.1131e-02\n",
      "Epoch: 8820 | training loss: 7.6239e-03 | validation loss: 2.1109e-02\n",
      "Epoch: 8830 | training loss: 7.6094e-03 | validation loss: 2.1088e-02\n",
      "Epoch: 8840 | training loss: 7.5950e-03 | validation loss: 2.1068e-02\n",
      "Epoch: 8850 | training loss: 7.5806e-03 | validation loss: 2.1049e-02\n",
      "Epoch: 8860 | training loss: 7.5662e-03 | validation loss: 2.1029e-02\n",
      "Epoch: 8870 | training loss: 7.5518e-03 | validation loss: 2.1009e-02\n",
      "Epoch: 8880 | training loss: 7.5374e-03 | validation loss: 2.0989e-02\n",
      "Epoch: 8890 | training loss: 7.5230e-03 | validation loss: 2.0969e-02\n",
      "Epoch: 8900 | training loss: 7.5085e-03 | validation loss: 2.0949e-02\n",
      "Epoch: 8910 | training loss: 7.4941e-03 | validation loss: 2.0929e-02\n",
      "Epoch: 8920 | training loss: 7.4797e-03 | validation loss: 2.0908e-02\n",
      "Epoch: 8930 | training loss: 7.4653e-03 | validation loss: 2.0888e-02\n",
      "Epoch: 8940 | training loss: 7.4509e-03 | validation loss: 2.0868e-02\n",
      "Epoch: 8950 | training loss: 7.4365e-03 | validation loss: 2.0848e-02\n",
      "Epoch: 8960 | training loss: 7.4221e-03 | validation loss: 2.0827e-02\n",
      "Epoch: 8970 | training loss: 7.4076e-03 | validation loss: 2.0807e-02\n",
      "Epoch: 8980 | training loss: 7.3932e-03 | validation loss: 2.0787e-02\n",
      "Epoch: 8990 | training loss: 7.3789e-03 | validation loss: 2.0770e-02\n",
      "Epoch: 9000 | training loss: 7.3769e-03 | validation loss: 2.0806e-02\n",
      "Epoch: 9010 | training loss: 7.8060e-03 | validation loss: 2.1295e-02\n",
      "Epoch: 9020 | training loss: 7.4896e-03 | validation loss: 2.0975e-02\n",
      "Epoch: 9030 | training loss: 7.3729e-03 | validation loss: 2.0821e-02\n",
      "Epoch: 9040 | training loss: 7.3274e-03 | validation loss: 2.0738e-02\n",
      "Epoch: 9050 | training loss: 7.3022e-03 | validation loss: 2.0686e-02\n",
      "Epoch: 9060 | training loss: 7.2844e-03 | validation loss: 2.0648e-02\n",
      "Epoch: 9070 | training loss: 7.2695e-03 | validation loss: 2.0618e-02\n",
      "Epoch: 9080 | training loss: 7.2558e-03 | validation loss: 2.0591e-02\n",
      "Epoch: 9090 | training loss: 7.2423e-03 | validation loss: 2.0568e-02\n",
      "Epoch: 9100 | training loss: 7.2289e-03 | validation loss: 2.0549e-02\n",
      "Epoch: 9110 | training loss: 7.2154e-03 | validation loss: 2.0532e-02\n",
      "Epoch: 9120 | training loss: 7.2019e-03 | validation loss: 2.0514e-02\n",
      "Epoch: 9130 | training loss: 7.1885e-03 | validation loss: 2.0494e-02\n",
      "Epoch: 9140 | training loss: 7.1750e-03 | validation loss: 2.0473e-02\n",
      "Epoch: 9150 | training loss: 7.1615e-03 | validation loss: 2.0454e-02\n",
      "Epoch: 9160 | training loss: 7.1481e-03 | validation loss: 2.0434e-02\n",
      "Epoch: 9170 | training loss: 7.1346e-03 | validation loss: 2.0414e-02\n",
      "Epoch: 9180 | training loss: 7.1212e-03 | validation loss: 2.0394e-02\n",
      "Epoch: 9190 | training loss: 7.1077e-03 | validation loss: 2.0374e-02\n",
      "Epoch: 9200 | training loss: 7.0943e-03 | validation loss: 2.0354e-02\n",
      "Epoch: 9210 | training loss: 7.0809e-03 | validation loss: 2.0334e-02\n",
      "Epoch: 9220 | training loss: 7.0674e-03 | validation loss: 2.0313e-02\n",
      "Epoch: 9230 | training loss: 7.0540e-03 | validation loss: 2.0293e-02\n",
      "Epoch: 9240 | training loss: 7.0405e-03 | validation loss: 2.0273e-02\n",
      "Epoch: 9250 | training loss: 7.0271e-03 | validation loss: 2.0253e-02\n",
      "Epoch: 9260 | training loss: 7.0137e-03 | validation loss: 2.0232e-02\n",
      "Epoch: 9270 | training loss: 7.0003e-03 | validation loss: 2.0213e-02\n",
      "Epoch: 9280 | training loss: 6.9875e-03 | validation loss: 2.0204e-02\n",
      "Epoch: 9290 | training loss: 7.1282e-03 | validation loss: 2.0461e-02\n",
      "Epoch: 9300 | training loss: 7.2215e-03 | validation loss: 2.0036e-02\n",
      "Epoch: 9310 | training loss: 6.9960e-03 | validation loss: 2.0056e-02\n",
      "Epoch: 9320 | training loss: 6.9380e-03 | validation loss: 2.0129e-02\n",
      "Epoch: 9330 | training loss: 6.9309e-03 | validation loss: 2.0144e-02\n",
      "Epoch: 9340 | training loss: 6.9123e-03 | validation loss: 2.0086e-02\n",
      "Epoch: 9350 | training loss: 6.9008e-03 | validation loss: 2.0061e-02\n",
      "Epoch: 9360 | training loss: 6.8871e-03 | validation loss: 2.0049e-02\n",
      "Epoch: 9370 | training loss: 6.8747e-03 | validation loss: 2.0028e-02\n",
      "Epoch: 9380 | training loss: 6.8624e-03 | validation loss: 2.0004e-02\n",
      "Epoch: 9390 | training loss: 6.8501e-03 | validation loss: 1.9984e-02\n",
      "Epoch: 9400 | training loss: 6.8379e-03 | validation loss: 1.9966e-02\n",
      "Epoch: 9410 | training loss: 6.8257e-03 | validation loss: 1.9947e-02\n",
      "Epoch: 9420 | training loss: 6.8135e-03 | validation loss: 1.9928e-02\n",
      "Epoch: 9430 | training loss: 6.8013e-03 | validation loss: 1.9909e-02\n",
      "Epoch: 9440 | training loss: 6.7890e-03 | validation loss: 1.9890e-02\n",
      "Epoch: 9450 | training loss: 6.7768e-03 | validation loss: 1.9871e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9460 | training loss: 6.7646e-03 | validation loss: 1.9852e-02\n",
      "Epoch: 9470 | training loss: 6.7524e-03 | validation loss: 1.9832e-02\n",
      "Epoch: 9480 | training loss: 6.7402e-03 | validation loss: 1.9813e-02\n",
      "Epoch: 9490 | training loss: 6.7280e-03 | validation loss: 1.9794e-02\n",
      "Epoch: 9500 | training loss: 6.7158e-03 | validation loss: 1.9774e-02\n",
      "Epoch: 9510 | training loss: 6.7036e-03 | validation loss: 1.9755e-02\n",
      "Epoch: 9520 | training loss: 6.6915e-03 | validation loss: 1.9735e-02\n",
      "Epoch: 9530 | training loss: 6.6793e-03 | validation loss: 1.9715e-02\n",
      "Epoch: 9540 | training loss: 6.6671e-03 | validation loss: 1.9696e-02\n",
      "Epoch: 9550 | training loss: 6.6550e-03 | validation loss: 1.9679e-02\n",
      "Epoch: 9560 | training loss: 6.6584e-03 | validation loss: 1.9716e-02\n",
      "Epoch: 9570 | training loss: 7.3816e-03 | validation loss: 2.0372e-02\n",
      "Epoch: 9580 | training loss: 6.8821e-03 | validation loss: 1.9954e-02\n",
      "Epoch: 9590 | training loss: 6.7014e-03 | validation loss: 1.9759e-02\n",
      "Epoch: 9600 | training loss: 6.6297e-03 | validation loss: 1.9659e-02\n",
      "Epoch: 9610 | training loss: 6.5964e-03 | validation loss: 1.9605e-02\n",
      "Epoch: 9620 | training loss: 6.5785e-03 | validation loss: 1.9571e-02\n",
      "Epoch: 9630 | training loss: 6.5652e-03 | validation loss: 1.9543e-02\n",
      "Epoch: 9640 | training loss: 6.5532e-03 | validation loss: 1.9518e-02\n",
      "Epoch: 9650 | training loss: 6.5417e-03 | validation loss: 1.9494e-02\n",
      "Epoch: 9660 | training loss: 6.5304e-03 | validation loss: 1.9470e-02\n",
      "Epoch: 9670 | training loss: 6.5193e-03 | validation loss: 1.9449e-02\n",
      "Epoch: 9680 | training loss: 6.5082e-03 | validation loss: 1.9429e-02\n",
      "Epoch: 9690 | training loss: 6.4971e-03 | validation loss: 1.9410e-02\n",
      "Epoch: 9700 | training loss: 6.4860e-03 | validation loss: 1.9391e-02\n",
      "Epoch: 9710 | training loss: 6.4749e-03 | validation loss: 1.9372e-02\n",
      "Epoch: 9720 | training loss: 6.4638e-03 | validation loss: 1.9352e-02\n",
      "Epoch: 9730 | training loss: 6.4527e-03 | validation loss: 1.9332e-02\n",
      "Epoch: 9740 | training loss: 6.4416e-03 | validation loss: 1.9313e-02\n",
      "Epoch: 9750 | training loss: 6.4305e-03 | validation loss: 1.9293e-02\n",
      "Epoch: 9760 | training loss: 6.4194e-03 | validation loss: 1.9273e-02\n",
      "Epoch: 9770 | training loss: 6.4083e-03 | validation loss: 1.9253e-02\n",
      "Epoch: 9780 | training loss: 6.3972e-03 | validation loss: 1.9233e-02\n",
      "Epoch: 9790 | training loss: 6.3861e-03 | validation loss: 1.9213e-02\n",
      "Epoch: 9800 | training loss: 6.3750e-03 | validation loss: 1.9193e-02\n",
      "Epoch: 9810 | training loss: 6.3640e-03 | validation loss: 1.9173e-02\n",
      "Epoch: 9820 | training loss: 6.3529e-03 | validation loss: 1.9153e-02\n",
      "Epoch: 9830 | training loss: 6.3418e-03 | validation loss: 1.9133e-02\n",
      "Epoch: 9840 | training loss: 6.3307e-03 | validation loss: 1.9112e-02\n",
      "Epoch: 9850 | training loss: 6.3197e-03 | validation loss: 1.9092e-02\n",
      "Epoch: 9860 | training loss: 6.3087e-03 | validation loss: 1.9069e-02\n",
      "Epoch: 9870 | training loss: 6.3072e-03 | validation loss: 1.9014e-02\n",
      "Epoch: 9880 | training loss: 7.6026e-03 | validation loss: 1.9231e-02\n",
      "Epoch: 9890 | training loss: 6.4836e-03 | validation loss: 1.8921e-02\n",
      "Epoch: 9900 | training loss: 6.3288e-03 | validation loss: 1.9000e-02\n",
      "Epoch: 9910 | training loss: 6.2891e-03 | validation loss: 1.8980e-02\n",
      "Epoch: 9920 | training loss: 6.2605e-03 | validation loss: 1.8912e-02\n",
      "Epoch: 9930 | training loss: 6.2408e-03 | validation loss: 1.8915e-02\n",
      "Epoch: 9940 | training loss: 6.2295e-03 | validation loss: 1.8919e-02\n",
      "Epoch: 9950 | training loss: 6.2183e-03 | validation loss: 1.8898e-02\n",
      "Epoch: 9960 | training loss: 6.2084e-03 | validation loss: 1.8878e-02\n",
      "Epoch: 9970 | training loss: 6.1986e-03 | validation loss: 1.8862e-02\n",
      "Epoch: 9980 | training loss: 6.1889e-03 | validation loss: 1.8846e-02\n",
      "Epoch: 9990 | training loss: 6.1792e-03 | validation loss: 1.8827e-02\n",
      "Epoch: 10000 | training loss: 6.1694e-03 | validation loss: 1.8807e-02\n",
      "Epoch: 10010 | training loss: 6.1597e-03 | validation loss: 1.8787e-02\n",
      "Epoch: 10020 | training loss: 6.1500e-03 | validation loss: 1.8768e-02\n",
      "Epoch: 10030 | training loss: 6.1403e-03 | validation loss: 1.8749e-02\n",
      "Epoch: 10040 | training loss: 6.1305e-03 | validation loss: 1.8729e-02\n",
      "Epoch: 10050 | training loss: 6.1208e-03 | validation loss: 1.8710e-02\n",
      "Epoch: 10060 | training loss: 6.1111e-03 | validation loss: 1.8691e-02\n",
      "Epoch: 10070 | training loss: 6.1014e-03 | validation loss: 1.8671e-02\n",
      "Epoch: 10080 | training loss: 6.0917e-03 | validation loss: 1.8652e-02\n",
      "Epoch: 10090 | training loss: 6.0820e-03 | validation loss: 1.8632e-02\n",
      "Epoch: 10100 | training loss: 6.0723e-03 | validation loss: 1.8612e-02\n",
      "Epoch: 10110 | training loss: 6.0626e-03 | validation loss: 1.8592e-02\n",
      "Epoch: 10120 | training loss: 6.0529e-03 | validation loss: 1.8573e-02\n",
      "Epoch: 10130 | training loss: 6.0432e-03 | validation loss: 1.8553e-02\n",
      "Epoch: 10140 | training loss: 6.0335e-03 | validation loss: 1.8533e-02\n",
      "Epoch: 10150 | training loss: 6.0238e-03 | validation loss: 1.8512e-02\n",
      "Epoch: 10160 | training loss: 6.0141e-03 | validation loss: 1.8492e-02\n",
      "Epoch: 10170 | training loss: 6.0044e-03 | validation loss: 1.8472e-02\n",
      "Epoch: 10180 | training loss: 5.9950e-03 | validation loss: 1.8458e-02\n",
      "Epoch: 10190 | training loss: 6.1143e-03 | validation loss: 1.8622e-02\n",
      "Epoch: 10200 | training loss: 6.2312e-03 | validation loss: 1.8329e-02\n",
      "Epoch: 10210 | training loss: 6.1077e-03 | validation loss: 1.8318e-02\n",
      "Epoch: 10220 | training loss: 6.0680e-03 | validation loss: 1.8321e-02\n",
      "Epoch: 10230 | training loss: 5.9861e-03 | validation loss: 1.8331e-02\n",
      "Epoch: 10240 | training loss: 5.9453e-03 | validation loss: 1.8329e-02\n",
      "Epoch: 10250 | training loss: 5.9325e-03 | validation loss: 1.8317e-02\n",
      "Epoch: 10260 | training loss: 5.9243e-03 | validation loss: 1.8299e-02\n",
      "Epoch: 10270 | training loss: 5.9156e-03 | validation loss: 1.8278e-02\n",
      "Epoch: 10280 | training loss: 5.9068e-03 | validation loss: 1.8257e-02\n",
      "Epoch: 10290 | training loss: 5.8981e-03 | validation loss: 1.8237e-02\n",
      "Epoch: 10300 | training loss: 5.8894e-03 | validation loss: 1.8216e-02\n",
      "Epoch: 10310 | training loss: 5.8808e-03 | validation loss: 1.8195e-02\n",
      "Epoch: 10320 | training loss: 5.8722e-03 | validation loss: 1.8175e-02\n",
      "Epoch: 10330 | training loss: 5.8636e-03 | validation loss: 1.8155e-02\n",
      "Epoch: 10340 | training loss: 5.8549e-03 | validation loss: 1.8135e-02\n",
      "Epoch: 10350 | training loss: 5.8463e-03 | validation loss: 1.8115e-02\n",
      "Epoch: 10360 | training loss: 5.8377e-03 | validation loss: 1.8095e-02\n",
      "Epoch: 10370 | training loss: 5.8291e-03 | validation loss: 1.8074e-02\n",
      "Epoch: 10380 | training loss: 5.8205e-03 | validation loss: 1.8054e-02\n",
      "Epoch: 10390 | training loss: 5.8118e-03 | validation loss: 1.8033e-02\n",
      "Epoch: 10400 | training loss: 5.8032e-03 | validation loss: 1.8013e-02\n",
      "Epoch: 10410 | training loss: 5.7946e-03 | validation loss: 1.7992e-02\n",
      "Epoch: 10420 | training loss: 5.7860e-03 | validation loss: 1.7972e-02\n",
      "Epoch: 10430 | training loss: 5.7774e-03 | validation loss: 1.7951e-02\n",
      "Epoch: 10440 | training loss: 5.7688e-03 | validation loss: 1.7930e-02\n",
      "Epoch: 10450 | training loss: 5.7602e-03 | validation loss: 1.7909e-02\n",
      "Epoch: 10460 | training loss: 5.7515e-03 | validation loss: 1.7888e-02\n",
      "Epoch: 10470 | training loss: 5.7429e-03 | validation loss: 1.7867e-02\n",
      "Epoch: 10480 | training loss: 5.7343e-03 | validation loss: 1.7846e-02\n",
      "Epoch: 10490 | training loss: 5.7257e-03 | validation loss: 1.7824e-02\n",
      "Epoch: 10500 | training loss: 5.7171e-03 | validation loss: 1.7803e-02\n",
      "Epoch: 10510 | training loss: 5.7085e-03 | validation loss: 1.7782e-02\n",
      "Epoch: 10520 | training loss: 5.6999e-03 | validation loss: 1.7760e-02\n",
      "Epoch: 10530 | training loss: 5.6916e-03 | validation loss: 1.7746e-02\n",
      "Epoch: 10540 | training loss: 5.8221e-03 | validation loss: 1.7939e-02\n",
      "Epoch: 10550 | training loss: 6.0513e-03 | validation loss: 1.7788e-02\n",
      "Epoch: 10560 | training loss: 5.7537e-03 | validation loss: 1.7715e-02\n",
      "Epoch: 10570 | training loss: 5.7223e-03 | validation loss: 1.7788e-02\n",
      "Epoch: 10580 | training loss: 5.6525e-03 | validation loss: 1.7638e-02\n",
      "Epoch: 10590 | training loss: 5.6520e-03 | validation loss: 1.7601e-02\n",
      "Epoch: 10600 | training loss: 5.6393e-03 | validation loss: 1.7609e-02\n",
      "Epoch: 10610 | training loss: 5.6302e-03 | validation loss: 1.7583e-02\n",
      "Epoch: 10620 | training loss: 5.6230e-03 | validation loss: 1.7556e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10630 | training loss: 5.6153e-03 | validation loss: 1.7540e-02\n",
      "Epoch: 10640 | training loss: 5.6079e-03 | validation loss: 1.7521e-02\n",
      "Epoch: 10650 | training loss: 5.6004e-03 | validation loss: 1.7498e-02\n",
      "Epoch: 10660 | training loss: 5.5930e-03 | validation loss: 1.7476e-02\n",
      "Epoch: 10670 | training loss: 5.5856e-03 | validation loss: 1.7456e-02\n",
      "Epoch: 10680 | training loss: 5.5782e-03 | validation loss: 1.7436e-02\n",
      "Epoch: 10690 | training loss: 5.5708e-03 | validation loss: 1.7416e-02\n",
      "Epoch: 10700 | training loss: 5.5634e-03 | validation loss: 1.7395e-02\n",
      "Epoch: 10710 | training loss: 5.5560e-03 | validation loss: 1.7375e-02\n",
      "Epoch: 10720 | training loss: 5.5486e-03 | validation loss: 1.7354e-02\n",
      "Epoch: 10730 | training loss: 5.5412e-03 | validation loss: 1.7334e-02\n",
      "Epoch: 10740 | training loss: 5.5338e-03 | validation loss: 1.7313e-02\n",
      "Epoch: 10750 | training loss: 5.5264e-03 | validation loss: 1.7292e-02\n",
      "Epoch: 10760 | training loss: 5.5190e-03 | validation loss: 1.7271e-02\n",
      "Epoch: 10770 | training loss: 5.5116e-03 | validation loss: 1.7250e-02\n",
      "Epoch: 10780 | training loss: 5.5042e-03 | validation loss: 1.7229e-02\n",
      "Epoch: 10790 | training loss: 5.4968e-03 | validation loss: 1.7208e-02\n",
      "Epoch: 10800 | training loss: 5.4894e-03 | validation loss: 1.7187e-02\n",
      "Epoch: 10810 | training loss: 5.4819e-03 | validation loss: 1.7166e-02\n",
      "Epoch: 10820 | training loss: 5.4745e-03 | validation loss: 1.7145e-02\n",
      "Epoch: 10830 | training loss: 5.4671e-03 | validation loss: 1.7124e-02\n",
      "Epoch: 10840 | training loss: 5.4597e-03 | validation loss: 1.7102e-02\n",
      "Epoch: 10850 | training loss: 5.4528e-03 | validation loss: 1.7074e-02\n",
      "Epoch: 10860 | training loss: 5.7065e-03 | validation loss: 1.7016e-02\n",
      "Epoch: 10870 | training loss: 6.3607e-03 | validation loss: 1.7770e-02\n",
      "Epoch: 10880 | training loss: 5.8828e-03 | validation loss: 1.7448e-02\n",
      "Epoch: 10890 | training loss: 5.5315e-03 | validation loss: 1.7149e-02\n",
      "Epoch: 10900 | training loss: 5.4186e-03 | validation loss: 1.6972e-02\n",
      "Epoch: 10910 | training loss: 5.4237e-03 | validation loss: 1.6925e-02\n",
      "Epoch: 10920 | training loss: 5.4121e-03 | validation loss: 1.6912e-02\n",
      "Epoch: 10930 | training loss: 5.3998e-03 | validation loss: 1.6903e-02\n",
      "Epoch: 10940 | training loss: 5.3918e-03 | validation loss: 1.6889e-02\n",
      "Epoch: 10950 | training loss: 5.3851e-03 | validation loss: 1.6870e-02\n",
      "Epoch: 10960 | training loss: 5.3785e-03 | validation loss: 1.6849e-02\n",
      "Epoch: 10970 | training loss: 5.3719e-03 | validation loss: 1.6827e-02\n",
      "Epoch: 10980 | training loss: 5.3653e-03 | validation loss: 1.6806e-02\n",
      "Epoch: 10990 | training loss: 5.3587e-03 | validation loss: 1.6784e-02\n",
      "Epoch: 11000 | training loss: 5.3520e-03 | validation loss: 1.6762e-02\n",
      "Epoch: 11010 | training loss: 5.3454e-03 | validation loss: 1.6741e-02\n",
      "Epoch: 11020 | training loss: 5.3388e-03 | validation loss: 1.6719e-02\n",
      "Epoch: 11030 | training loss: 5.3322e-03 | validation loss: 1.6698e-02\n",
      "Epoch: 11040 | training loss: 5.3256e-03 | validation loss: 1.6676e-02\n",
      "Epoch: 11050 | training loss: 5.3189e-03 | validation loss: 1.6654e-02\n",
      "Epoch: 11060 | training loss: 5.3123e-03 | validation loss: 1.6632e-02\n",
      "Epoch: 11070 | training loss: 5.3057e-03 | validation loss: 1.6611e-02\n",
      "Epoch: 11080 | training loss: 5.2991e-03 | validation loss: 1.6589e-02\n",
      "Epoch: 11090 | training loss: 5.2924e-03 | validation loss: 1.6566e-02\n",
      "Epoch: 11100 | training loss: 5.2858e-03 | validation loss: 1.6544e-02\n",
      "Epoch: 11110 | training loss: 5.2791e-03 | validation loss: 1.6522e-02\n",
      "Epoch: 11120 | training loss: 5.2725e-03 | validation loss: 1.6500e-02\n",
      "Epoch: 11130 | training loss: 5.2659e-03 | validation loss: 1.6477e-02\n",
      "Epoch: 11140 | training loss: 5.2592e-03 | validation loss: 1.6455e-02\n",
      "Epoch: 11150 | training loss: 5.2526e-03 | validation loss: 1.6432e-02\n",
      "Epoch: 11160 | training loss: 5.2459e-03 | validation loss: 1.6410e-02\n",
      "Epoch: 11170 | training loss: 5.2393e-03 | validation loss: 1.6387e-02\n",
      "Epoch: 11180 | training loss: 5.2326e-03 | validation loss: 1.6364e-02\n",
      "Epoch: 11190 | training loss: 5.2260e-03 | validation loss: 1.6341e-02\n",
      "Epoch: 11200 | training loss: 5.2193e-03 | validation loss: 1.6318e-02\n",
      "Epoch: 11210 | training loss: 5.2127e-03 | validation loss: 1.6295e-02\n",
      "Epoch: 11220 | training loss: 5.2075e-03 | validation loss: 1.6259e-02\n",
      "Epoch: 11230 | training loss: 5.8745e-03 | validation loss: 1.6352e-02\n",
      "Epoch: 11240 | training loss: 5.4545e-03 | validation loss: 1.6415e-02\n",
      "Epoch: 11250 | training loss: 5.3353e-03 | validation loss: 1.6247e-02\n",
      "Epoch: 11260 | training loss: 5.2669e-03 | validation loss: 1.6270e-02\n",
      "Epoch: 11270 | training loss: 5.1960e-03 | validation loss: 1.6136e-02\n",
      "Epoch: 11280 | training loss: 5.1769e-03 | validation loss: 1.6128e-02\n",
      "Epoch: 11290 | training loss: 5.1694e-03 | validation loss: 1.6126e-02\n",
      "Epoch: 11300 | training loss: 5.1606e-03 | validation loss: 1.6097e-02\n",
      "Epoch: 11310 | training loss: 5.1539e-03 | validation loss: 1.6086e-02\n",
      "Epoch: 11320 | training loss: 5.1484e-03 | validation loss: 1.6065e-02\n",
      "Epoch: 11330 | training loss: 5.1425e-03 | validation loss: 1.6037e-02\n",
      "Epoch: 11340 | training loss: 5.1368e-03 | validation loss: 1.6015e-02\n",
      "Epoch: 11350 | training loss: 5.1312e-03 | validation loss: 1.5996e-02\n",
      "Epoch: 11360 | training loss: 5.1255e-03 | validation loss: 1.5974e-02\n",
      "Epoch: 11370 | training loss: 5.1198e-03 | validation loss: 1.5951e-02\n",
      "Epoch: 11380 | training loss: 5.1142e-03 | validation loss: 1.5930e-02\n",
      "Epoch: 11390 | training loss: 5.1085e-03 | validation loss: 1.5908e-02\n",
      "Epoch: 11400 | training loss: 5.1028e-03 | validation loss: 1.5886e-02\n",
      "Epoch: 11410 | training loss: 5.0971e-03 | validation loss: 1.5864e-02\n",
      "Epoch: 11420 | training loss: 5.0914e-03 | validation loss: 1.5842e-02\n",
      "Epoch: 11430 | training loss: 5.0857e-03 | validation loss: 1.5820e-02\n",
      "Epoch: 11440 | training loss: 5.0800e-03 | validation loss: 1.5798e-02\n",
      "Epoch: 11450 | training loss: 5.0743e-03 | validation loss: 1.5776e-02\n",
      "Epoch: 11460 | training loss: 5.0686e-03 | validation loss: 1.5753e-02\n",
      "Epoch: 11470 | training loss: 5.0629e-03 | validation loss: 1.5731e-02\n",
      "Epoch: 11480 | training loss: 5.0572e-03 | validation loss: 1.5708e-02\n",
      "Epoch: 11490 | training loss: 5.0522e-03 | validation loss: 1.5680e-02\n",
      "Epoch: 11500 | training loss: 5.1955e-03 | validation loss: 1.5623e-02\n",
      "Epoch: 11510 | training loss: 5.0606e-03 | validation loss: 1.5608e-02\n",
      "Epoch: 11520 | training loss: 5.2022e-03 | validation loss: 1.5579e-02\n",
      "Epoch: 11530 | training loss: 5.0929e-03 | validation loss: 1.5560e-02\n",
      "Epoch: 11540 | training loss: 5.0459e-03 | validation loss: 1.5547e-02\n",
      "Epoch: 11550 | training loss: 5.0282e-03 | validation loss: 1.5531e-02\n",
      "Epoch: 11560 | training loss: 5.0188e-03 | validation loss: 1.5514e-02\n",
      "Epoch: 11570 | training loss: 5.0113e-03 | validation loss: 1.5497e-02\n",
      "Epoch: 11580 | training loss: 5.0046e-03 | validation loss: 1.5479e-02\n",
      "Epoch: 11590 | training loss: 4.9986e-03 | validation loss: 1.5462e-02\n",
      "Epoch: 11600 | training loss: 4.9932e-03 | validation loss: 1.5444e-02\n",
      "Epoch: 11610 | training loss: 4.9880e-03 | validation loss: 1.5422e-02\n",
      "Epoch: 11620 | training loss: 4.9827e-03 | validation loss: 1.5399e-02\n",
      "Epoch: 11630 | training loss: 4.9775e-03 | validation loss: 1.5375e-02\n",
      "Epoch: 11640 | training loss: 4.9722e-03 | validation loss: 1.5353e-02\n",
      "Epoch: 11650 | training loss: 4.9670e-03 | validation loss: 1.5331e-02\n",
      "Epoch: 11660 | training loss: 4.9617e-03 | validation loss: 1.5308e-02\n",
      "Epoch: 11670 | training loss: 4.9564e-03 | validation loss: 1.5286e-02\n",
      "Epoch: 11680 | training loss: 4.9512e-03 | validation loss: 1.5263e-02\n",
      "Epoch: 11690 | training loss: 4.9459e-03 | validation loss: 1.5240e-02\n",
      "Epoch: 11700 | training loss: 4.9406e-03 | validation loss: 1.5217e-02\n",
      "Epoch: 11710 | training loss: 4.9353e-03 | validation loss: 1.5194e-02\n",
      "Epoch: 11720 | training loss: 4.9300e-03 | validation loss: 1.5172e-02\n",
      "Epoch: 11730 | training loss: 4.9247e-03 | validation loss: 1.5149e-02\n",
      "Epoch: 11740 | training loss: 4.9194e-03 | validation loss: 1.5126e-02\n",
      "Epoch: 11750 | training loss: 4.9141e-03 | validation loss: 1.5102e-02\n",
      "Epoch: 11760 | training loss: 4.9088e-03 | validation loss: 1.5079e-02\n",
      "Epoch: 11770 | training loss: 4.9035e-03 | validation loss: 1.5056e-02\n",
      "Epoch: 11780 | training loss: 4.8982e-03 | validation loss: 1.5033e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11790 | training loss: 4.8929e-03 | validation loss: 1.5010e-02\n",
      "Epoch: 11800 | training loss: 4.8910e-03 | validation loss: 1.5004e-02\n",
      "Epoch: 11810 | training loss: 6.2115e-03 | validation loss: 1.5885e-02\n",
      "Epoch: 11820 | training loss: 5.4679e-03 | validation loss: 1.4999e-02\n",
      "Epoch: 11830 | training loss: 4.9287e-03 | validation loss: 1.4925e-02\n",
      "Epoch: 11840 | training loss: 4.9082e-03 | validation loss: 1.4991e-02\n",
      "Epoch: 11850 | training loss: 4.9025e-03 | validation loss: 1.4959e-02\n",
      "Epoch: 11860 | training loss: 4.8687e-03 | validation loss: 1.4885e-02\n",
      "Epoch: 11870 | training loss: 4.8554e-03 | validation loss: 1.4833e-02\n",
      "Epoch: 11880 | training loss: 4.8498e-03 | validation loss: 1.4801e-02\n",
      "Epoch: 11890 | training loss: 4.8448e-03 | validation loss: 1.4777e-02\n",
      "Epoch: 11900 | training loss: 4.8398e-03 | validation loss: 1.4755e-02\n",
      "Epoch: 11910 | training loss: 4.8349e-03 | validation loss: 1.4733e-02\n",
      "Epoch: 11920 | training loss: 4.8301e-03 | validation loss: 1.4711e-02\n",
      "Epoch: 11930 | training loss: 4.8253e-03 | validation loss: 1.4688e-02\n",
      "Epoch: 11940 | training loss: 4.8206e-03 | validation loss: 1.4666e-02\n",
      "Epoch: 11950 | training loss: 4.8158e-03 | validation loss: 1.4643e-02\n",
      "Epoch: 11960 | training loss: 4.8110e-03 | validation loss: 1.4620e-02\n",
      "Epoch: 11970 | training loss: 4.8063e-03 | validation loss: 1.4596e-02\n",
      "Epoch: 11980 | training loss: 4.8015e-03 | validation loss: 1.4573e-02\n",
      "Epoch: 11990 | training loss: 4.7967e-03 | validation loss: 1.4549e-02\n",
      "Epoch: 12000 | training loss: 4.7919e-03 | validation loss: 1.4526e-02\n",
      "Epoch: 12010 | training loss: 4.7871e-03 | validation loss: 1.4502e-02\n",
      "Epoch: 12020 | training loss: 4.7823e-03 | validation loss: 1.4479e-02\n",
      "Epoch: 12030 | training loss: 4.7775e-03 | validation loss: 1.4455e-02\n",
      "Epoch: 12040 | training loss: 4.7727e-03 | validation loss: 1.4432e-02\n",
      "Epoch: 12050 | training loss: 4.7679e-03 | validation loss: 1.4408e-02\n",
      "Epoch: 12060 | training loss: 4.7631e-03 | validation loss: 1.4384e-02\n",
      "Epoch: 12070 | training loss: 4.7583e-03 | validation loss: 1.4361e-02\n",
      "Epoch: 12080 | training loss: 4.7535e-03 | validation loss: 1.4337e-02\n",
      "Epoch: 12090 | training loss: 4.7487e-03 | validation loss: 1.4313e-02\n",
      "Epoch: 12100 | training loss: 4.7439e-03 | validation loss: 1.4289e-02\n",
      "Epoch: 12110 | training loss: 4.7390e-03 | validation loss: 1.4265e-02\n",
      "Epoch: 12120 | training loss: 4.7342e-03 | validation loss: 1.4241e-02\n",
      "Epoch: 12130 | training loss: 4.7294e-03 | validation loss: 1.4217e-02\n",
      "Epoch: 12140 | training loss: 4.7251e-03 | validation loss: 1.4195e-02\n",
      "Epoch: 12150 | training loss: 4.9720e-03 | validation loss: 1.4382e-02\n",
      "Epoch: 12160 | training loss: 5.6584e-03 | validation loss: 1.4491e-02\n",
      "Epoch: 12170 | training loss: 4.8331e-03 | validation loss: 1.4215e-02\n",
      "Epoch: 12180 | training loss: 4.7394e-03 | validation loss: 1.4159e-02\n",
      "Epoch: 12190 | training loss: 4.7263e-03 | validation loss: 1.4099e-02\n",
      "Epoch: 12200 | training loss: 4.7043e-03 | validation loss: 1.4074e-02\n",
      "Epoch: 12210 | training loss: 4.6990e-03 | validation loss: 1.4024e-02\n",
      "Epoch: 12220 | training loss: 4.6906e-03 | validation loss: 1.4022e-02\n",
      "Epoch: 12230 | training loss: 4.6860e-03 | validation loss: 1.3993e-02\n",
      "Epoch: 12240 | training loss: 4.6815e-03 | validation loss: 1.3973e-02\n",
      "Epoch: 12250 | training loss: 4.6771e-03 | validation loss: 1.3950e-02\n",
      "Epoch: 12260 | training loss: 4.6728e-03 | validation loss: 1.3925e-02\n",
      "Epoch: 12270 | training loss: 4.6686e-03 | validation loss: 1.3902e-02\n",
      "Epoch: 12280 | training loss: 4.6644e-03 | validation loss: 1.3877e-02\n",
      "Epoch: 12290 | training loss: 4.6619e-03 | validation loss: 1.3847e-02\n",
      "Epoch: 12300 | training loss: 4.7612e-03 | validation loss: 1.3804e-02\n",
      "Epoch: 12310 | training loss: 5.3501e-03 | validation loss: 1.3911e-02\n",
      "Epoch: 12320 | training loss: 4.6672e-03 | validation loss: 1.3766e-02\n",
      "Epoch: 12330 | training loss: 4.7458e-03 | validation loss: 1.3891e-02\n",
      "Epoch: 12340 | training loss: 4.6447e-03 | validation loss: 1.3765e-02\n",
      "Epoch: 12350 | training loss: 4.6489e-03 | validation loss: 1.3700e-02\n",
      "Epoch: 12360 | training loss: 4.6313e-03 | validation loss: 1.3700e-02\n",
      "Epoch: 12370 | training loss: 4.6288e-03 | validation loss: 1.3688e-02\n",
      "Epoch: 12380 | training loss: 4.6238e-03 | validation loss: 1.3649e-02\n",
      "Epoch: 12390 | training loss: 4.6192e-03 | validation loss: 1.3635e-02\n",
      "Epoch: 12400 | training loss: 4.6150e-03 | validation loss: 1.3609e-02\n",
      "Epoch: 12410 | training loss: 4.6109e-03 | validation loss: 1.3588e-02\n",
      "Epoch: 12420 | training loss: 4.6068e-03 | validation loss: 1.3564e-02\n",
      "Epoch: 12430 | training loss: 4.6028e-03 | validation loss: 1.3543e-02\n",
      "Epoch: 12440 | training loss: 4.5987e-03 | validation loss: 1.3519e-02\n",
      "Epoch: 12450 | training loss: 4.5946e-03 | validation loss: 1.3496e-02\n",
      "Epoch: 12460 | training loss: 4.5905e-03 | validation loss: 1.3474e-02\n",
      "Epoch: 12470 | training loss: 4.5864e-03 | validation loss: 1.3451e-02\n",
      "Epoch: 12480 | training loss: 4.5823e-03 | validation loss: 1.3427e-02\n",
      "Epoch: 12490 | training loss: 4.5802e-03 | validation loss: 1.3395e-02\n",
      "Epoch: 12500 | training loss: 4.7871e-03 | validation loss: 1.3366e-02\n",
      "Epoch: 12510 | training loss: 4.6008e-03 | validation loss: 1.3330e-02\n",
      "Epoch: 12520 | training loss: 4.9140e-03 | validation loss: 1.3350e-02\n",
      "Epoch: 12530 | training loss: 4.6435e-03 | validation loss: 1.3284e-02\n",
      "Epoch: 12540 | training loss: 4.5590e-03 | validation loss: 1.3289e-02\n",
      "Epoch: 12550 | training loss: 4.5639e-03 | validation loss: 1.3298e-02\n",
      "Epoch: 12560 | training loss: 4.5560e-03 | validation loss: 1.3268e-02\n",
      "Epoch: 12570 | training loss: 4.5471e-03 | validation loss: 1.3225e-02\n",
      "Epoch: 12580 | training loss: 4.5439e-03 | validation loss: 1.3197e-02\n",
      "Epoch: 12590 | training loss: 4.5393e-03 | validation loss: 1.3181e-02\n",
      "Epoch: 12600 | training loss: 4.5355e-03 | validation loss: 1.3160e-02\n",
      "Epoch: 12610 | training loss: 4.5316e-03 | validation loss: 1.3134e-02\n",
      "Epoch: 12620 | training loss: 4.5277e-03 | validation loss: 1.3113e-02\n",
      "Epoch: 12630 | training loss: 4.5239e-03 | validation loss: 1.3090e-02\n",
      "Epoch: 12640 | training loss: 4.5200e-03 | validation loss: 1.3067e-02\n",
      "Epoch: 12650 | training loss: 4.5161e-03 | validation loss: 1.3045e-02\n",
      "Epoch: 12660 | training loss: 4.5122e-03 | validation loss: 1.3022e-02\n",
      "Epoch: 12670 | training loss: 4.5083e-03 | validation loss: 1.2999e-02\n",
      "Epoch: 12680 | training loss: 4.5044e-03 | validation loss: 1.2976e-02\n",
      "Epoch: 12690 | training loss: 4.5005e-03 | validation loss: 1.2953e-02\n",
      "Epoch: 12700 | training loss: 4.4966e-03 | validation loss: 1.2931e-02\n",
      "Epoch: 12710 | training loss: 4.4927e-03 | validation loss: 1.2908e-02\n",
      "Epoch: 12720 | training loss: 4.4888e-03 | validation loss: 1.2886e-02\n",
      "Epoch: 12730 | training loss: 4.4877e-03 | validation loss: 1.2876e-02\n",
      "Epoch: 12740 | training loss: 4.8858e-03 | validation loss: 1.3180e-02\n",
      "Epoch: 12750 | training loss: 4.8024e-03 | validation loss: 1.2821e-02\n",
      "Epoch: 12760 | training loss: 4.5198e-03 | validation loss: 1.2843e-02\n",
      "Epoch: 12770 | training loss: 4.5188e-03 | validation loss: 1.2837e-02\n",
      "Epoch: 12780 | training loss: 4.4868e-03 | validation loss: 1.2795e-02\n",
      "Epoch: 12790 | training loss: 4.4712e-03 | validation loss: 1.2758e-02\n",
      "Epoch: 12800 | training loss: 4.4648e-03 | validation loss: 1.2724e-02\n",
      "Epoch: 12810 | training loss: 4.4572e-03 | validation loss: 1.2692e-02\n",
      "Epoch: 12820 | training loss: 4.4523e-03 | validation loss: 1.2663e-02\n",
      "Epoch: 12830 | training loss: 4.4487e-03 | validation loss: 1.2636e-02\n",
      "Epoch: 12840 | training loss: 4.4451e-03 | validation loss: 1.2614e-02\n",
      "Epoch: 12850 | training loss: 4.4414e-03 | validation loss: 1.2594e-02\n",
      "Epoch: 12860 | training loss: 4.4378e-03 | validation loss: 1.2572e-02\n",
      "Epoch: 12870 | training loss: 4.4341e-03 | validation loss: 1.2548e-02\n",
      "Epoch: 12880 | training loss: 4.4305e-03 | validation loss: 1.2526e-02\n",
      "Epoch: 12890 | training loss: 4.4269e-03 | validation loss: 1.2503e-02\n",
      "Epoch: 12900 | training loss: 4.4233e-03 | validation loss: 1.2480e-02\n",
      "Epoch: 12910 | training loss: 4.4196e-03 | validation loss: 1.2458e-02\n",
      "Epoch: 12920 | training loss: 4.4160e-03 | validation loss: 1.2435e-02\n",
      "Epoch: 12930 | training loss: 4.4124e-03 | validation loss: 1.2413e-02\n",
      "Epoch: 12940 | training loss: 4.4087e-03 | validation loss: 1.2390e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12950 | training loss: 4.4051e-03 | validation loss: 1.2367e-02\n",
      "Epoch: 12960 | training loss: 4.4014e-03 | validation loss: 1.2344e-02\n",
      "Epoch: 12970 | training loss: 4.3977e-03 | validation loss: 1.2322e-02\n",
      "Epoch: 12980 | training loss: 4.3942e-03 | validation loss: 1.2298e-02\n",
      "Epoch: 12990 | training loss: 4.4319e-03 | validation loss: 1.2288e-02\n",
      "Epoch: 13000 | training loss: 5.4061e-03 | validation loss: 1.2528e-02\n",
      "Epoch: 13010 | training loss: 4.5569e-03 | validation loss: 1.2421e-02\n",
      "Epoch: 13020 | training loss: 4.4182e-03 | validation loss: 1.2245e-02\n",
      "Epoch: 13030 | training loss: 4.4301e-03 | validation loss: 1.2203e-02\n",
      "Epoch: 13040 | training loss: 4.3743e-03 | validation loss: 1.2173e-02\n",
      "Epoch: 13050 | training loss: 4.3774e-03 | validation loss: 1.2171e-02\n",
      "Epoch: 13060 | training loss: 4.3690e-03 | validation loss: 1.2134e-02\n",
      "Epoch: 13070 | training loss: 4.3645e-03 | validation loss: 1.2113e-02\n",
      "Epoch: 13080 | training loss: 4.3614e-03 | validation loss: 1.2090e-02\n",
      "Epoch: 13090 | training loss: 4.3581e-03 | validation loss: 1.2070e-02\n",
      "Epoch: 13100 | training loss: 4.3548e-03 | validation loss: 1.2049e-02\n",
      "Epoch: 13110 | training loss: 4.3515e-03 | validation loss: 1.2028e-02\n",
      "Epoch: 13120 | training loss: 4.3483e-03 | validation loss: 1.2007e-02\n",
      "Epoch: 13130 | training loss: 4.3450e-03 | validation loss: 1.1986e-02\n",
      "Epoch: 13140 | training loss: 4.3418e-03 | validation loss: 1.1965e-02\n",
      "Epoch: 13150 | training loss: 4.3386e-03 | validation loss: 1.1944e-02\n",
      "Epoch: 13160 | training loss: 4.3353e-03 | validation loss: 1.1923e-02\n",
      "Epoch: 13170 | training loss: 4.3320e-03 | validation loss: 1.1902e-02\n",
      "Epoch: 13180 | training loss: 4.3288e-03 | validation loss: 1.1880e-02\n",
      "Epoch: 13190 | training loss: 4.3326e-03 | validation loss: 1.1846e-02\n",
      "Epoch: 13200 | training loss: 6.1489e-03 | validation loss: 1.2351e-02\n",
      "Epoch: 13210 | training loss: 4.5658e-03 | validation loss: 1.2041e-02\n",
      "Epoch: 13220 | training loss: 4.3515e-03 | validation loss: 1.1863e-02\n",
      "Epoch: 13230 | training loss: 4.3152e-03 | validation loss: 1.1774e-02\n",
      "Epoch: 13240 | training loss: 4.3226e-03 | validation loss: 1.1740e-02\n",
      "Epoch: 13250 | training loss: 4.3169e-03 | validation loss: 1.1721e-02\n",
      "Epoch: 13260 | training loss: 4.3090e-03 | validation loss: 1.1706e-02\n",
      "Epoch: 13270 | training loss: 4.3033e-03 | validation loss: 1.1690e-02\n",
      "Epoch: 13280 | training loss: 4.2992e-03 | validation loss: 1.1673e-02\n",
      "Epoch: 13290 | training loss: 4.2958e-03 | validation loss: 1.1655e-02\n",
      "Epoch: 13300 | training loss: 4.2927e-03 | validation loss: 1.1636e-02\n",
      "Epoch: 13310 | training loss: 4.2898e-03 | validation loss: 1.1617e-02\n",
      "Epoch: 13320 | training loss: 4.2868e-03 | validation loss: 1.1597e-02\n",
      "Epoch: 13330 | training loss: 4.2838e-03 | validation loss: 1.1576e-02\n",
      "Epoch: 13340 | training loss: 4.2808e-03 | validation loss: 1.1555e-02\n",
      "Epoch: 13350 | training loss: 4.2778e-03 | validation loss: 1.1535e-02\n",
      "Epoch: 13360 | training loss: 4.2748e-03 | validation loss: 1.1515e-02\n",
      "Epoch: 13370 | training loss: 4.2718e-03 | validation loss: 1.1494e-02\n",
      "Epoch: 13380 | training loss: 4.2688e-03 | validation loss: 1.1474e-02\n",
      "Epoch: 13390 | training loss: 4.2658e-03 | validation loss: 1.1453e-02\n",
      "Epoch: 13400 | training loss: 4.2627e-03 | validation loss: 1.1433e-02\n",
      "Epoch: 13410 | training loss: 4.2597e-03 | validation loss: 1.1412e-02\n",
      "Epoch: 13420 | training loss: 4.2567e-03 | validation loss: 1.1392e-02\n",
      "Epoch: 13430 | training loss: 4.2536e-03 | validation loss: 1.1371e-02\n",
      "Epoch: 13440 | training loss: 4.2506e-03 | validation loss: 1.1351e-02\n",
      "Epoch: 13450 | training loss: 4.2476e-03 | validation loss: 1.1330e-02\n",
      "Epoch: 13460 | training loss: 4.2445e-03 | validation loss: 1.1310e-02\n",
      "Epoch: 13470 | training loss: 4.2415e-03 | validation loss: 1.1289e-02\n",
      "Epoch: 13480 | training loss: 4.2384e-03 | validation loss: 1.1269e-02\n",
      "Epoch: 13490 | training loss: 4.2354e-03 | validation loss: 1.1248e-02\n",
      "Epoch: 13500 | training loss: 4.2323e-03 | validation loss: 1.1227e-02\n",
      "Epoch: 13510 | training loss: 4.2292e-03 | validation loss: 1.1206e-02\n",
      "Epoch: 13520 | training loss: 4.2273e-03 | validation loss: 1.1179e-02\n",
      "Epoch: 13530 | training loss: 4.6174e-03 | validation loss: 1.1211e-02\n",
      "Epoch: 13540 | training loss: 5.2030e-03 | validation loss: 1.1829e-02\n",
      "Epoch: 13550 | training loss: 4.5946e-03 | validation loss: 1.1462e-02\n",
      "Epoch: 13560 | training loss: 4.3631e-03 | validation loss: 1.1268e-02\n",
      "Epoch: 13570 | training loss: 4.2371e-03 | validation loss: 1.1129e-02\n",
      "Epoch: 13580 | training loss: 4.2189e-03 | validation loss: 1.1072e-02\n",
      "Epoch: 13590 | training loss: 4.2137e-03 | validation loss: 1.1044e-02\n",
      "Epoch: 13600 | training loss: 4.2070e-03 | validation loss: 1.1025e-02\n",
      "Epoch: 13610 | training loss: 4.2021e-03 | validation loss: 1.1007e-02\n",
      "Epoch: 13620 | training loss: 4.1987e-03 | validation loss: 1.0988e-02\n",
      "Epoch: 13630 | training loss: 4.1958e-03 | validation loss: 1.0969e-02\n",
      "Epoch: 13640 | training loss: 4.1931e-03 | validation loss: 1.0949e-02\n",
      "Epoch: 13650 | training loss: 4.1903e-03 | validation loss: 1.0930e-02\n",
      "Epoch: 13660 | training loss: 4.1875e-03 | validation loss: 1.0910e-02\n",
      "Epoch: 13670 | training loss: 4.1848e-03 | validation loss: 1.0891e-02\n",
      "Epoch: 13680 | training loss: 4.1820e-03 | validation loss: 1.0871e-02\n",
      "Epoch: 13690 | training loss: 4.1793e-03 | validation loss: 1.0852e-02\n",
      "Epoch: 13700 | training loss: 4.1765e-03 | validation loss: 1.0832e-02\n",
      "Epoch: 13710 | training loss: 4.1737e-03 | validation loss: 1.0812e-02\n",
      "Epoch: 13720 | training loss: 4.1709e-03 | validation loss: 1.0792e-02\n",
      "Epoch: 13730 | training loss: 4.1682e-03 | validation loss: 1.0773e-02\n",
      "Epoch: 13740 | training loss: 4.1654e-03 | validation loss: 1.0753e-02\n",
      "Epoch: 13750 | training loss: 4.1626e-03 | validation loss: 1.0733e-02\n",
      "Epoch: 13760 | training loss: 4.1598e-03 | validation loss: 1.0713e-02\n",
      "Epoch: 13770 | training loss: 4.1570e-03 | validation loss: 1.0693e-02\n",
      "Epoch: 13780 | training loss: 4.1542e-03 | validation loss: 1.0673e-02\n",
      "Epoch: 13790 | training loss: 4.1514e-03 | validation loss: 1.0654e-02\n",
      "Epoch: 13800 | training loss: 4.1486e-03 | validation loss: 1.0634e-02\n",
      "Epoch: 13810 | training loss: 4.1458e-03 | validation loss: 1.0614e-02\n",
      "Epoch: 13820 | training loss: 4.1430e-03 | validation loss: 1.0594e-02\n",
      "Epoch: 13830 | training loss: 4.1401e-03 | validation loss: 1.0574e-02\n",
      "Epoch: 13840 | training loss: 4.1374e-03 | validation loss: 1.0554e-02\n",
      "Epoch: 13850 | training loss: 4.1494e-03 | validation loss: 1.0534e-02\n",
      "Epoch: 13860 | training loss: 6.0458e-03 | validation loss: 1.1330e-02\n",
      "Epoch: 13870 | training loss: 4.7348e-03 | validation loss: 1.1010e-02\n",
      "Epoch: 13880 | training loss: 4.2091e-03 | validation loss: 1.0562e-02\n",
      "Epoch: 13890 | training loss: 4.1573e-03 | validation loss: 1.0465e-02\n",
      "Epoch: 13900 | training loss: 4.1400e-03 | validation loss: 1.0452e-02\n",
      "Epoch: 13910 | training loss: 4.1210e-03 | validation loss: 1.0437e-02\n",
      "Epoch: 13920 | training loss: 4.1200e-03 | validation loss: 1.0415e-02\n",
      "Epoch: 13930 | training loss: 4.1148e-03 | validation loss: 1.0394e-02\n",
      "Epoch: 13940 | training loss: 4.1121e-03 | validation loss: 1.0377e-02\n",
      "Epoch: 13950 | training loss: 4.1097e-03 | validation loss: 1.0356e-02\n",
      "Epoch: 13960 | training loss: 4.1071e-03 | validation loss: 1.0340e-02\n",
      "Epoch: 13970 | training loss: 4.1046e-03 | validation loss: 1.0321e-02\n",
      "Epoch: 13980 | training loss: 4.1021e-03 | validation loss: 1.0304e-02\n",
      "Epoch: 13990 | training loss: 4.0996e-03 | validation loss: 1.0286e-02\n",
      "Epoch: 14000 | training loss: 4.0971e-03 | validation loss: 1.0268e-02\n",
      "Epoch: 14010 | training loss: 4.0946e-03 | validation loss: 1.0250e-02\n",
      "Epoch: 14020 | training loss: 4.0921e-03 | validation loss: 1.0232e-02\n",
      "Epoch: 14030 | training loss: 4.0896e-03 | validation loss: 1.0213e-02\n",
      "Epoch: 14040 | training loss: 4.0875e-03 | validation loss: 1.0193e-02\n",
      "Epoch: 14050 | training loss: 4.1171e-03 | validation loss: 1.0159e-02\n",
      "Epoch: 14060 | training loss: 5.9971e-03 | validation loss: 1.0746e-02\n",
      "Epoch: 14070 | training loss: 4.0955e-03 | validation loss: 1.0178e-02\n",
      "Epoch: 14080 | training loss: 4.2068e-03 | validation loss: 1.0257e-02\n",
      "Epoch: 14090 | training loss: 4.1491e-03 | validation loss: 1.0196e-02\n",
      "Epoch: 14100 | training loss: 4.0890e-03 | validation loss: 1.0123e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14110 | training loss: 4.0706e-03 | validation loss: 1.0075e-02\n",
      "Epoch: 14120 | training loss: 4.0706e-03 | validation loss: 1.0049e-02\n",
      "Epoch: 14130 | training loss: 4.0667e-03 | validation loss: 1.0034e-02\n",
      "Epoch: 14140 | training loss: 4.0636e-03 | validation loss: 1.0024e-02\n",
      "Epoch: 14150 | training loss: 4.0613e-03 | validation loss: 1.0007e-02\n",
      "Epoch: 14160 | training loss: 4.0589e-03 | validation loss: 9.9859e-03\n",
      "Epoch: 14170 | training loss: 4.0565e-03 | validation loss: 9.9694e-03\n",
      "Epoch: 14180 | training loss: 4.0541e-03 | validation loss: 9.9528e-03\n",
      "Epoch: 14190 | training loss: 4.0518e-03 | validation loss: 9.9346e-03\n",
      "Epoch: 14200 | training loss: 4.0494e-03 | validation loss: 9.9177e-03\n",
      "Epoch: 14210 | training loss: 4.0470e-03 | validation loss: 9.9000e-03\n",
      "Epoch: 14220 | training loss: 4.0447e-03 | validation loss: 9.8828e-03\n",
      "Epoch: 14230 | training loss: 4.0423e-03 | validation loss: 9.8653e-03\n",
      "Epoch: 14240 | training loss: 4.0399e-03 | validation loss: 9.8479e-03\n",
      "Epoch: 14250 | training loss: 4.0375e-03 | validation loss: 9.8306e-03\n",
      "Epoch: 14260 | training loss: 4.0351e-03 | validation loss: 9.8132e-03\n",
      "Epoch: 14270 | training loss: 4.0327e-03 | validation loss: 9.7958e-03\n",
      "Epoch: 14280 | training loss: 4.0304e-03 | validation loss: 9.7786e-03\n",
      "Epoch: 14290 | training loss: 4.0281e-03 | validation loss: 9.7629e-03\n",
      "Epoch: 14300 | training loss: 4.0394e-03 | validation loss: 9.7716e-03\n",
      "Epoch: 14310 | training loss: 5.7834e-03 | validation loss: 1.0772e-02\n",
      "Epoch: 14320 | training loss: 4.3631e-03 | validation loss: 9.7548e-03\n",
      "Epoch: 14330 | training loss: 4.2530e-03 | validation loss: 9.7121e-03\n",
      "Epoch: 14340 | training loss: 4.1124e-03 | validation loss: 9.6638e-03\n",
      "Epoch: 14350 | training loss: 4.0477e-03 | validation loss: 9.6403e-03\n",
      "Epoch: 14360 | training loss: 4.0201e-03 | validation loss: 9.6293e-03\n",
      "Epoch: 14370 | training loss: 4.0104e-03 | validation loss: 9.6223e-03\n",
      "Epoch: 14380 | training loss: 4.0081e-03 | validation loss: 9.6124e-03\n",
      "Epoch: 14390 | training loss: 4.0062e-03 | validation loss: 9.5967e-03\n",
      "Epoch: 14400 | training loss: 4.0034e-03 | validation loss: 9.5767e-03\n",
      "Epoch: 14410 | training loss: 4.0012e-03 | validation loss: 9.5574e-03\n",
      "Epoch: 14420 | training loss: 3.9989e-03 | validation loss: 9.5410e-03\n",
      "Epoch: 14430 | training loss: 3.9967e-03 | validation loss: 9.5254e-03\n",
      "Epoch: 14440 | training loss: 3.9944e-03 | validation loss: 9.5082e-03\n",
      "Epoch: 14450 | training loss: 3.9922e-03 | validation loss: 9.4911e-03\n",
      "Epoch: 14460 | training loss: 3.9899e-03 | validation loss: 9.4747e-03\n",
      "Epoch: 14470 | training loss: 3.9877e-03 | validation loss: 9.4576e-03\n",
      "Epoch: 14480 | training loss: 3.9854e-03 | validation loss: 9.4409e-03\n",
      "Epoch: 14490 | training loss: 3.9832e-03 | validation loss: 9.4240e-03\n",
      "Epoch: 14500 | training loss: 3.9809e-03 | validation loss: 9.4073e-03\n",
      "Epoch: 14510 | training loss: 3.9787e-03 | validation loss: 9.3904e-03\n",
      "Epoch: 14520 | training loss: 3.9764e-03 | validation loss: 9.3735e-03\n",
      "Epoch: 14530 | training loss: 3.9741e-03 | validation loss: 9.3567e-03\n",
      "Epoch: 14540 | training loss: 3.9719e-03 | validation loss: 9.3398e-03\n",
      "Epoch: 14550 | training loss: 3.9696e-03 | validation loss: 9.3229e-03\n",
      "Epoch: 14560 | training loss: 3.9673e-03 | validation loss: 9.3057e-03\n",
      "Epoch: 14570 | training loss: 3.9653e-03 | validation loss: 9.2860e-03\n",
      "Epoch: 14580 | training loss: 4.0238e-03 | validation loss: 9.2579e-03\n",
      "Epoch: 14590 | training loss: 5.5420e-03 | validation loss: 9.7344e-03\n",
      "Epoch: 14600 | training loss: 4.0199e-03 | validation loss: 9.2312e-03\n",
      "Epoch: 14610 | training loss: 4.0367e-03 | validation loss: 9.3202e-03\n",
      "Epoch: 14620 | training loss: 3.9970e-03 | validation loss: 9.2722e-03\n",
      "Epoch: 14630 | training loss: 3.9657e-03 | validation loss: 9.2192e-03\n",
      "Epoch: 14640 | training loss: 3.9598e-03 | validation loss: 9.1933e-03\n",
      "Epoch: 14650 | training loss: 3.9492e-03 | validation loss: 9.1686e-03\n",
      "Epoch: 14660 | training loss: 3.9473e-03 | validation loss: 9.1510e-03\n",
      "Epoch: 14670 | training loss: 3.9452e-03 | validation loss: 9.1368e-03\n",
      "Epoch: 14680 | training loss: 3.9429e-03 | validation loss: 9.1234e-03\n",
      "Epoch: 14690 | training loss: 3.9408e-03 | validation loss: 9.1074e-03\n",
      "Epoch: 14700 | training loss: 3.9388e-03 | validation loss: 9.0908e-03\n",
      "Epoch: 14710 | training loss: 3.9368e-03 | validation loss: 9.0761e-03\n",
      "Epoch: 14720 | training loss: 3.9348e-03 | validation loss: 9.0608e-03\n",
      "Epoch: 14730 | training loss: 3.9328e-03 | validation loss: 9.0454e-03\n",
      "Epoch: 14740 | training loss: 3.9308e-03 | validation loss: 9.0304e-03\n",
      "Epoch: 14750 | training loss: 3.9288e-03 | validation loss: 9.0150e-03\n",
      "Epoch: 14760 | training loss: 3.9268e-03 | validation loss: 8.9998e-03\n",
      "Epoch: 14770 | training loss: 3.9248e-03 | validation loss: 8.9846e-03\n",
      "Epoch: 14780 | training loss: 3.9227e-03 | validation loss: 8.9693e-03\n",
      "Epoch: 14790 | training loss: 3.9207e-03 | validation loss: 8.9541e-03\n",
      "Epoch: 14800 | training loss: 3.9187e-03 | validation loss: 8.9389e-03\n",
      "Epoch: 14810 | training loss: 3.9166e-03 | validation loss: 8.9236e-03\n",
      "Epoch: 14820 | training loss: 3.9146e-03 | validation loss: 8.9083e-03\n",
      "Epoch: 14830 | training loss: 3.9126e-03 | validation loss: 8.8921e-03\n",
      "Epoch: 14840 | training loss: 3.9160e-03 | validation loss: 8.8680e-03\n",
      "Epoch: 14850 | training loss: 5.0785e-03 | validation loss: 9.1967e-03\n",
      "Epoch: 14860 | training loss: 4.8220e-03 | validation loss: 9.4322e-03\n",
      "Epoch: 14870 | training loss: 4.2167e-03 | validation loss: 9.0820e-03\n",
      "Epoch: 14880 | training loss: 4.0093e-03 | validation loss: 8.9315e-03\n",
      "Epoch: 14890 | training loss: 3.9402e-03 | validation loss: 8.8604e-03\n",
      "Epoch: 14900 | training loss: 3.9130e-03 | validation loss: 8.8182e-03\n",
      "Epoch: 14910 | training loss: 3.9012e-03 | validation loss: 8.7890e-03\n",
      "Epoch: 14920 | training loss: 3.8961e-03 | validation loss: 8.7659e-03\n",
      "Epoch: 14930 | training loss: 3.8938e-03 | validation loss: 8.7467e-03\n",
      "Epoch: 14940 | training loss: 3.8919e-03 | validation loss: 8.7306e-03\n",
      "Epoch: 14950 | training loss: 3.8899e-03 | validation loss: 8.7169e-03\n",
      "Epoch: 14960 | training loss: 3.8880e-03 | validation loss: 8.7039e-03\n",
      "Epoch: 14970 | training loss: 3.8861e-03 | validation loss: 8.6899e-03\n",
      "Epoch: 14980 | training loss: 3.8842e-03 | validation loss: 8.6749e-03\n",
      "Epoch: 14990 | training loss: 3.8824e-03 | validation loss: 8.6604e-03\n",
      "Epoch: 15000 | training loss: 3.8805e-03 | validation loss: 8.6463e-03\n",
      "Epoch: 15010 | training loss: 3.8786e-03 | validation loss: 8.6318e-03\n",
      "Epoch: 15020 | training loss: 3.8767e-03 | validation loss: 8.6173e-03\n",
      "Epoch: 15030 | training loss: 3.8748e-03 | validation loss: 8.6029e-03\n",
      "Epoch: 15040 | training loss: 3.8729e-03 | validation loss: 8.5885e-03\n",
      "Epoch: 15050 | training loss: 3.8710e-03 | validation loss: 8.5741e-03\n",
      "Epoch: 15060 | training loss: 3.8690e-03 | validation loss: 8.5596e-03\n",
      "Epoch: 15070 | training loss: 3.8671e-03 | validation loss: 8.5452e-03\n",
      "Epoch: 15080 | training loss: 3.8652e-03 | validation loss: 8.5308e-03\n",
      "Epoch: 15090 | training loss: 3.8633e-03 | validation loss: 8.5163e-03\n",
      "Epoch: 15100 | training loss: 3.8614e-03 | validation loss: 8.5019e-03\n",
      "Epoch: 15110 | training loss: 3.8594e-03 | validation loss: 8.4874e-03\n",
      "Epoch: 15120 | training loss: 3.8575e-03 | validation loss: 8.4730e-03\n",
      "Epoch: 15130 | training loss: 3.8556e-03 | validation loss: 8.4597e-03\n",
      "Epoch: 15140 | training loss: 3.8656e-03 | validation loss: 8.4687e-03\n",
      "Epoch: 15150 | training loss: 6.4126e-03 | validation loss: 9.9049e-03\n",
      "Epoch: 15160 | training loss: 4.0303e-03 | validation loss: 8.5750e-03\n",
      "Epoch: 15170 | training loss: 4.0618e-03 | validation loss: 8.5997e-03\n",
      "Epoch: 15180 | training loss: 3.9033e-03 | validation loss: 8.4473e-03\n",
      "Epoch: 15190 | training loss: 3.8531e-03 | validation loss: 8.3669e-03\n",
      "Epoch: 15200 | training loss: 3.8564e-03 | validation loss: 8.3553e-03\n",
      "Epoch: 15210 | training loss: 3.8438e-03 | validation loss: 8.3461e-03\n",
      "Epoch: 15220 | training loss: 3.8408e-03 | validation loss: 8.3392e-03\n",
      "Epoch: 15230 | training loss: 3.8388e-03 | validation loss: 8.3279e-03\n",
      "Epoch: 15240 | training loss: 3.8368e-03 | validation loss: 8.3144e-03\n",
      "Epoch: 15250 | training loss: 3.8350e-03 | validation loss: 8.3006e-03\n",
      "Epoch: 15260 | training loss: 3.8332e-03 | validation loss: 8.2866e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15270 | training loss: 3.8315e-03 | validation loss: 8.2728e-03\n",
      "Epoch: 15280 | training loss: 3.8298e-03 | validation loss: 8.2596e-03\n",
      "Epoch: 15290 | training loss: 3.8281e-03 | validation loss: 8.2466e-03\n",
      "Epoch: 15300 | training loss: 3.8264e-03 | validation loss: 8.2333e-03\n",
      "Epoch: 15310 | training loss: 3.8247e-03 | validation loss: 8.2201e-03\n",
      "Epoch: 15320 | training loss: 3.8229e-03 | validation loss: 8.2070e-03\n",
      "Epoch: 15330 | training loss: 3.8212e-03 | validation loss: 8.1937e-03\n",
      "Epoch: 15340 | training loss: 3.8195e-03 | validation loss: 8.1805e-03\n",
      "Epoch: 15350 | training loss: 3.8178e-03 | validation loss: 8.1673e-03\n",
      "Epoch: 15360 | training loss: 3.8160e-03 | validation loss: 8.1541e-03\n",
      "Epoch: 15370 | training loss: 3.8143e-03 | validation loss: 8.1409e-03\n",
      "Epoch: 15380 | training loss: 3.8125e-03 | validation loss: 8.1277e-03\n",
      "Epoch: 15390 | training loss: 3.8108e-03 | validation loss: 8.1145e-03\n",
      "Epoch: 15400 | training loss: 3.8090e-03 | validation loss: 8.1012e-03\n",
      "Epoch: 15410 | training loss: 3.8073e-03 | validation loss: 8.0880e-03\n",
      "Epoch: 15420 | training loss: 3.8055e-03 | validation loss: 8.0747e-03\n",
      "Epoch: 15430 | training loss: 3.8038e-03 | validation loss: 8.0608e-03\n",
      "Epoch: 15440 | training loss: 3.8086e-03 | validation loss: 8.0406e-03\n",
      "Epoch: 15450 | training loss: 5.5221e-03 | validation loss: 8.6544e-03\n",
      "Epoch: 15460 | training loss: 4.0702e-03 | validation loss: 8.2576e-03\n",
      "Epoch: 15470 | training loss: 3.9500e-03 | validation loss: 8.1513e-03\n",
      "Epoch: 15480 | training loss: 3.8894e-03 | validation loss: 8.0736e-03\n",
      "Epoch: 15490 | training loss: 3.7976e-03 | validation loss: 7.9902e-03\n",
      "Epoch: 15500 | training loss: 3.7977e-03 | validation loss: 7.9769e-03\n",
      "Epoch: 15510 | training loss: 3.7955e-03 | validation loss: 7.9648e-03\n",
      "Epoch: 15520 | training loss: 3.7901e-03 | validation loss: 7.9527e-03\n",
      "Epoch: 15530 | training loss: 3.7879e-03 | validation loss: 7.9423e-03\n",
      "Epoch: 15540 | training loss: 3.7863e-03 | validation loss: 7.9297e-03\n",
      "Epoch: 15550 | training loss: 3.7847e-03 | validation loss: 7.9167e-03\n",
      "Epoch: 15560 | training loss: 3.7831e-03 | validation loss: 7.9052e-03\n",
      "Epoch: 15570 | training loss: 3.7816e-03 | validation loss: 7.8936e-03\n",
      "Epoch: 15580 | training loss: 3.7800e-03 | validation loss: 7.8815e-03\n",
      "Epoch: 15590 | training loss: 3.7784e-03 | validation loss: 7.8699e-03\n",
      "Epoch: 15600 | training loss: 3.7768e-03 | validation loss: 7.8579e-03\n",
      "Epoch: 15610 | training loss: 3.7752e-03 | validation loss: 7.8462e-03\n",
      "Epoch: 15620 | training loss: 3.7736e-03 | validation loss: 7.8343e-03\n",
      "Epoch: 15630 | training loss: 3.7720e-03 | validation loss: 7.8225e-03\n",
      "Epoch: 15640 | training loss: 3.7704e-03 | validation loss: 7.8106e-03\n",
      "Epoch: 15650 | training loss: 3.7688e-03 | validation loss: 7.7987e-03\n",
      "Epoch: 15660 | training loss: 3.7672e-03 | validation loss: 7.7869e-03\n",
      "Epoch: 15670 | training loss: 3.7656e-03 | validation loss: 7.7750e-03\n",
      "Epoch: 15680 | training loss: 3.7640e-03 | validation loss: 7.7630e-03\n",
      "Epoch: 15690 | training loss: 3.7624e-03 | validation loss: 7.7503e-03\n",
      "Epoch: 15700 | training loss: 3.7664e-03 | validation loss: 7.7302e-03\n",
      "Epoch: 15710 | training loss: 5.0393e-03 | validation loss: 8.1604e-03\n",
      "Epoch: 15720 | training loss: 4.6132e-03 | validation loss: 8.2421e-03\n",
      "Epoch: 15730 | training loss: 4.0955e-03 | validation loss: 7.9505e-03\n",
      "Epoch: 15740 | training loss: 3.8753e-03 | validation loss: 7.8021e-03\n",
      "Epoch: 15750 | training loss: 3.7934e-03 | validation loss: 7.7320e-03\n",
      "Epoch: 15760 | training loss: 3.7653e-03 | validation loss: 7.6963e-03\n",
      "Epoch: 15770 | training loss: 3.7546e-03 | validation loss: 7.6733e-03\n",
      "Epoch: 15780 | training loss: 3.7498e-03 | validation loss: 7.6546e-03\n",
      "Epoch: 15790 | training loss: 3.7476e-03 | validation loss: 7.6388e-03\n",
      "Epoch: 15800 | training loss: 3.7462e-03 | validation loss: 7.6255e-03\n",
      "Epoch: 15810 | training loss: 3.7447e-03 | validation loss: 7.6145e-03\n",
      "Epoch: 15820 | training loss: 3.7431e-03 | validation loss: 7.6045e-03\n",
      "Epoch: 15830 | training loss: 3.7417e-03 | validation loss: 7.5940e-03\n",
      "Epoch: 15840 | training loss: 3.7402e-03 | validation loss: 7.5824e-03\n",
      "Epoch: 15850 | training loss: 3.7387e-03 | validation loss: 7.5709e-03\n",
      "Epoch: 15860 | training loss: 3.7372e-03 | validation loss: 7.5599e-03\n",
      "Epoch: 15870 | training loss: 3.7357e-03 | validation loss: 7.5487e-03\n",
      "Epoch: 15880 | training loss: 3.7342e-03 | validation loss: 7.5375e-03\n",
      "Epoch: 15890 | training loss: 3.7327e-03 | validation loss: 7.5263e-03\n",
      "Epoch: 15900 | training loss: 3.7312e-03 | validation loss: 7.5150e-03\n",
      "Epoch: 15910 | training loss: 3.7297e-03 | validation loss: 7.5038e-03\n",
      "Epoch: 15920 | training loss: 3.7282e-03 | validation loss: 7.4926e-03\n",
      "Epoch: 15930 | training loss: 3.7267e-03 | validation loss: 7.4813e-03\n",
      "Epoch: 15940 | training loss: 3.7252e-03 | validation loss: 7.4701e-03\n",
      "Epoch: 15950 | training loss: 3.7236e-03 | validation loss: 7.4588e-03\n",
      "Epoch: 15960 | training loss: 3.7221e-03 | validation loss: 7.4475e-03\n",
      "Epoch: 15970 | training loss: 3.7206e-03 | validation loss: 7.4363e-03\n",
      "Epoch: 15980 | training loss: 3.7191e-03 | validation loss: 7.4251e-03\n",
      "Epoch: 15990 | training loss: 3.7177e-03 | validation loss: 7.4158e-03\n",
      "Epoch: 16000 | training loss: 3.7538e-03 | validation loss: 7.4511e-03\n",
      "Epoch: 16010 | training loss: 6.1039e-03 | validation loss: 8.7273e-03\n",
      "Epoch: 16020 | training loss: 4.1528e-03 | validation loss: 7.7267e-03\n",
      "Epoch: 16030 | training loss: 3.8012e-03 | validation loss: 7.4693e-03\n",
      "Epoch: 16040 | training loss: 3.7124e-03 | validation loss: 7.3707e-03\n",
      "Epoch: 16050 | training loss: 3.7179e-03 | validation loss: 7.3503e-03\n",
      "Epoch: 16060 | training loss: 3.7147e-03 | validation loss: 7.3360e-03\n",
      "Epoch: 16070 | training loss: 3.7090e-03 | validation loss: 7.3252e-03\n",
      "Epoch: 16080 | training loss: 3.7059e-03 | validation loss: 7.3165e-03\n",
      "Epoch: 16090 | training loss: 3.7039e-03 | validation loss: 7.3084e-03\n",
      "Epoch: 16100 | training loss: 3.7024e-03 | validation loss: 7.2999e-03\n",
      "Epoch: 16110 | training loss: 3.7011e-03 | validation loss: 7.2900e-03\n",
      "Epoch: 16120 | training loss: 3.6997e-03 | validation loss: 7.2789e-03\n",
      "Epoch: 16130 | training loss: 3.6983e-03 | validation loss: 7.2681e-03\n",
      "Epoch: 16140 | training loss: 3.6969e-03 | validation loss: 7.2580e-03\n",
      "Epoch: 16150 | training loss: 3.6956e-03 | validation loss: 7.2477e-03\n",
      "Epoch: 16160 | training loss: 3.6942e-03 | validation loss: 7.2373e-03\n",
      "Epoch: 16170 | training loss: 3.6928e-03 | validation loss: 7.2270e-03\n",
      "Epoch: 16180 | training loss: 3.6914e-03 | validation loss: 7.2167e-03\n",
      "Epoch: 16190 | training loss: 3.6900e-03 | validation loss: 7.2063e-03\n",
      "Epoch: 16200 | training loss: 3.6886e-03 | validation loss: 7.1960e-03\n",
      "Epoch: 16210 | training loss: 3.6872e-03 | validation loss: 7.1856e-03\n",
      "Epoch: 16220 | training loss: 3.6858e-03 | validation loss: 7.1752e-03\n",
      "Epoch: 16230 | training loss: 3.6844e-03 | validation loss: 7.1648e-03\n",
      "Epoch: 16240 | training loss: 3.6830e-03 | validation loss: 7.1544e-03\n",
      "Epoch: 16250 | training loss: 3.6816e-03 | validation loss: 7.1440e-03\n",
      "Epoch: 16260 | training loss: 3.6802e-03 | validation loss: 7.1336e-03\n",
      "Epoch: 16270 | training loss: 3.6788e-03 | validation loss: 7.1231e-03\n",
      "Epoch: 16280 | training loss: 3.6774e-03 | validation loss: 7.1120e-03\n",
      "Epoch: 16290 | training loss: 3.6821e-03 | validation loss: 7.0940e-03\n",
      "Epoch: 16300 | training loss: 5.2248e-03 | validation loss: 7.6096e-03\n",
      "Epoch: 16310 | training loss: 4.0646e-03 | validation loss: 7.3668e-03\n",
      "Epoch: 16320 | training loss: 3.7338e-03 | validation loss: 7.1419e-03\n",
      "Epoch: 16330 | training loss: 3.7541e-03 | validation loss: 7.1206e-03\n",
      "Epoch: 16340 | training loss: 3.6991e-03 | validation loss: 7.0685e-03\n",
      "Epoch: 16350 | training loss: 3.6691e-03 | validation loss: 7.0426e-03\n",
      "Epoch: 16360 | training loss: 3.6689e-03 | validation loss: 7.0362e-03\n",
      "Epoch: 16370 | training loss: 3.6674e-03 | validation loss: 7.0281e-03\n",
      "Epoch: 16380 | training loss: 3.6651e-03 | validation loss: 7.0193e-03\n",
      "Epoch: 16390 | training loss: 3.6634e-03 | validation loss: 7.0098e-03\n",
      "Epoch: 16400 | training loss: 3.6620e-03 | validation loss: 6.9995e-03\n",
      "Epoch: 16410 | training loss: 3.6607e-03 | validation loss: 6.9897e-03\n",
      "Epoch: 16420 | training loss: 3.6594e-03 | validation loss: 6.9807e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16430 | training loss: 3.6582e-03 | validation loss: 6.9714e-03\n",
      "Epoch: 16440 | training loss: 3.6569e-03 | validation loss: 6.9620e-03\n",
      "Epoch: 16450 | training loss: 3.6556e-03 | validation loss: 6.9527e-03\n",
      "Epoch: 16460 | training loss: 3.6543e-03 | validation loss: 6.9434e-03\n",
      "Epoch: 16470 | training loss: 3.6531e-03 | validation loss: 6.9341e-03\n",
      "Epoch: 16480 | training loss: 3.6518e-03 | validation loss: 6.9247e-03\n",
      "Epoch: 16490 | training loss: 3.6505e-03 | validation loss: 6.9154e-03\n",
      "Epoch: 16500 | training loss: 3.6492e-03 | validation loss: 6.9061e-03\n",
      "Epoch: 16510 | training loss: 3.6479e-03 | validation loss: 6.8967e-03\n",
      "Epoch: 16520 | training loss: 3.6466e-03 | validation loss: 6.8874e-03\n",
      "Epoch: 16530 | training loss: 3.6453e-03 | validation loss: 6.8780e-03\n",
      "Epoch: 16540 | training loss: 3.6440e-03 | validation loss: 6.8686e-03\n",
      "Epoch: 16550 | training loss: 3.6427e-03 | validation loss: 6.8596e-03\n",
      "Epoch: 16560 | training loss: 3.6432e-03 | validation loss: 6.8567e-03\n",
      "Epoch: 16570 | training loss: 4.1704e-03 | validation loss: 7.2108e-03\n",
      "Epoch: 16580 | training loss: 4.4515e-03 | validation loss: 7.0833e-03\n",
      "Epoch: 16590 | training loss: 3.7310e-03 | validation loss: 6.8264e-03\n",
      "Epoch: 16600 | training loss: 3.6742e-03 | validation loss: 6.8108e-03\n",
      "Epoch: 16610 | training loss: 3.6563e-03 | validation loss: 6.8046e-03\n",
      "Epoch: 16620 | training loss: 3.6396e-03 | validation loss: 6.7946e-03\n",
      "Epoch: 16630 | training loss: 3.6336e-03 | validation loss: 6.7889e-03\n",
      "Epoch: 16640 | training loss: 3.6323e-03 | validation loss: 6.7830e-03\n",
      "Epoch: 16650 | training loss: 3.6312e-03 | validation loss: 6.7746e-03\n",
      "Epoch: 16660 | training loss: 3.6297e-03 | validation loss: 6.7642e-03\n",
      "Epoch: 16670 | training loss: 3.6283e-03 | validation loss: 6.7536e-03\n",
      "Epoch: 16680 | training loss: 3.6271e-03 | validation loss: 6.7439e-03\n",
      "Epoch: 16690 | training loss: 3.6259e-03 | validation loss: 6.7354e-03\n",
      "Epoch: 16700 | training loss: 3.6247e-03 | validation loss: 6.7272e-03\n",
      "Epoch: 16710 | training loss: 3.6235e-03 | validation loss: 6.7183e-03\n",
      "Epoch: 16720 | training loss: 3.6223e-03 | validation loss: 6.7094e-03\n",
      "Epoch: 16730 | training loss: 3.6211e-03 | validation loss: 6.7007e-03\n",
      "Epoch: 16740 | training loss: 3.6199e-03 | validation loss: 6.6919e-03\n",
      "Epoch: 16750 | training loss: 3.6187e-03 | validation loss: 6.6831e-03\n",
      "Epoch: 16760 | training loss: 3.6175e-03 | validation loss: 6.6743e-03\n",
      "Epoch: 16770 | training loss: 3.6162e-03 | validation loss: 6.6655e-03\n",
      "Epoch: 16780 | training loss: 3.6150e-03 | validation loss: 6.6566e-03\n",
      "Epoch: 16790 | training loss: 3.6138e-03 | validation loss: 6.6478e-03\n",
      "Epoch: 16800 | training loss: 3.6126e-03 | validation loss: 6.6389e-03\n",
      "Epoch: 16810 | training loss: 3.6113e-03 | validation loss: 6.6301e-03\n",
      "Epoch: 16820 | training loss: 3.6101e-03 | validation loss: 6.6212e-03\n",
      "Epoch: 16830 | training loss: 3.6088e-03 | validation loss: 6.6123e-03\n",
      "Epoch: 16840 | training loss: 3.6076e-03 | validation loss: 6.6033e-03\n",
      "Epoch: 16850 | training loss: 3.6064e-03 | validation loss: 6.5936e-03\n",
      "Epoch: 16860 | training loss: 3.6113e-03 | validation loss: 6.5769e-03\n",
      "Epoch: 16870 | training loss: 5.2212e-03 | validation loss: 7.1493e-03\n",
      "Epoch: 16880 | training loss: 4.1659e-03 | validation loss: 6.9339e-03\n",
      "Epoch: 16890 | training loss: 3.7593e-03 | validation loss: 6.6935e-03\n",
      "Epoch: 16900 | training loss: 3.6593e-03 | validation loss: 6.6076e-03\n",
      "Epoch: 16910 | training loss: 3.6384e-03 | validation loss: 6.5733e-03\n",
      "Epoch: 16920 | training loss: 3.6139e-03 | validation loss: 6.5474e-03\n",
      "Epoch: 16930 | training loss: 3.6013e-03 | validation loss: 6.5327e-03\n",
      "Epoch: 16940 | training loss: 3.5971e-03 | validation loss: 6.5223e-03\n",
      "Epoch: 16950 | training loss: 3.5952e-03 | validation loss: 6.5124e-03\n",
      "Epoch: 16960 | training loss: 3.5940e-03 | validation loss: 6.5026e-03\n",
      "Epoch: 16970 | training loss: 3.5929e-03 | validation loss: 6.4938e-03\n",
      "Epoch: 16980 | training loss: 3.5918e-03 | validation loss: 6.4856e-03\n",
      "Epoch: 16990 | training loss: 3.5906e-03 | validation loss: 6.4777e-03\n",
      "Epoch: 17000 | training loss: 3.5895e-03 | validation loss: 6.4696e-03\n",
      "Epoch: 17010 | training loss: 3.5884e-03 | validation loss: 6.4611e-03\n",
      "Epoch: 17020 | training loss: 3.5872e-03 | validation loss: 6.4529e-03\n",
      "Epoch: 17030 | training loss: 3.5861e-03 | validation loss: 6.4448e-03\n",
      "Epoch: 17040 | training loss: 3.5850e-03 | validation loss: 6.4364e-03\n",
      "Epoch: 17050 | training loss: 3.5838e-03 | validation loss: 6.4281e-03\n",
      "Epoch: 17060 | training loss: 3.5827e-03 | validation loss: 6.4198e-03\n",
      "Epoch: 17070 | training loss: 3.5815e-03 | validation loss: 6.4115e-03\n",
      "Epoch: 17080 | training loss: 3.5804e-03 | validation loss: 6.4032e-03\n",
      "Epoch: 17090 | training loss: 3.5792e-03 | validation loss: 6.3949e-03\n",
      "Epoch: 17100 | training loss: 3.5781e-03 | validation loss: 6.3865e-03\n",
      "Epoch: 17110 | training loss: 3.5769e-03 | validation loss: 6.3781e-03\n",
      "Epoch: 17120 | training loss: 3.5757e-03 | validation loss: 6.3697e-03\n",
      "Epoch: 17130 | training loss: 3.5746e-03 | validation loss: 6.3613e-03\n",
      "Epoch: 17140 | training loss: 3.5734e-03 | validation loss: 6.3529e-03\n",
      "Epoch: 17150 | training loss: 3.5722e-03 | validation loss: 6.3443e-03\n",
      "Epoch: 17160 | training loss: 3.5713e-03 | validation loss: 6.3341e-03\n",
      "Epoch: 17170 | training loss: 3.6388e-03 | validation loss: 6.3282e-03\n",
      "Epoch: 17180 | training loss: 4.3166e-03 | validation loss: 6.5298e-03\n",
      "Epoch: 17190 | training loss: 3.6898e-03 | validation loss: 6.3595e-03\n",
      "Epoch: 17200 | training loss: 3.5863e-03 | validation loss: 6.3322e-03\n",
      "Epoch: 17210 | training loss: 3.6208e-03 | validation loss: 6.3509e-03\n",
      "Epoch: 17220 | training loss: 3.5774e-03 | validation loss: 6.3033e-03\n",
      "Epoch: 17230 | training loss: 3.5639e-03 | validation loss: 6.2814e-03\n",
      "Epoch: 17240 | training loss: 3.5640e-03 | validation loss: 6.2770e-03\n",
      "Epoch: 17250 | training loss: 3.5625e-03 | validation loss: 6.2691e-03\n",
      "Epoch: 17260 | training loss: 3.5608e-03 | validation loss: 6.2606e-03\n",
      "Epoch: 17270 | training loss: 3.5595e-03 | validation loss: 6.2545e-03\n",
      "Epoch: 17280 | training loss: 3.5584e-03 | validation loss: 6.2470e-03\n",
      "Epoch: 17290 | training loss: 3.5573e-03 | validation loss: 6.2398e-03\n",
      "Epoch: 17300 | training loss: 3.5563e-03 | validation loss: 6.2324e-03\n",
      "Epoch: 17310 | training loss: 3.5552e-03 | validation loss: 6.2252e-03\n",
      "Epoch: 17320 | training loss: 3.5542e-03 | validation loss: 6.2178e-03\n",
      "Epoch: 17330 | training loss: 3.5531e-03 | validation loss: 6.2104e-03\n",
      "Epoch: 17340 | training loss: 3.5520e-03 | validation loss: 6.2031e-03\n",
      "Epoch: 17350 | training loss: 3.5510e-03 | validation loss: 6.1957e-03\n",
      "Epoch: 17360 | training loss: 3.5499e-03 | validation loss: 6.1884e-03\n",
      "Epoch: 17370 | training loss: 3.5488e-03 | validation loss: 6.1813e-03\n",
      "Epoch: 17380 | training loss: 3.5486e-03 | validation loss: 6.1776e-03\n",
      "Epoch: 17390 | training loss: 3.6657e-03 | validation loss: 6.2690e-03\n",
      "Epoch: 17400 | training loss: 4.0403e-03 | validation loss: 6.4827e-03\n",
      "Epoch: 17410 | training loss: 4.0042e-03 | validation loss: 6.4591e-03\n",
      "Epoch: 17420 | training loss: 3.6363e-03 | validation loss: 6.2324e-03\n",
      "Epoch: 17430 | training loss: 3.5478e-03 | validation loss: 6.1509e-03\n",
      "Epoch: 17440 | training loss: 3.5448e-03 | validation loss: 6.1240e-03\n",
      "Epoch: 17450 | training loss: 3.5471e-03 | validation loss: 6.1147e-03\n",
      "Epoch: 17460 | training loss: 3.5415e-03 | validation loss: 6.1113e-03\n",
      "Epoch: 17470 | training loss: 3.5388e-03 | validation loss: 6.1102e-03\n",
      "Epoch: 17480 | training loss: 3.5381e-03 | validation loss: 6.1046e-03\n",
      "Epoch: 17490 | training loss: 3.5368e-03 | validation loss: 6.0947e-03\n",
      "Epoch: 17500 | training loss: 3.5358e-03 | validation loss: 6.0867e-03\n",
      "Epoch: 17510 | training loss: 3.5347e-03 | validation loss: 6.0809e-03\n",
      "Epoch: 17520 | training loss: 3.5337e-03 | validation loss: 6.0734e-03\n",
      "Epoch: 17530 | training loss: 3.5327e-03 | validation loss: 6.0661e-03\n",
      "Epoch: 17540 | training loss: 3.5317e-03 | validation loss: 6.0592e-03\n",
      "Epoch: 17550 | training loss: 3.5307e-03 | validation loss: 6.0518e-03\n",
      "Epoch: 17560 | training loss: 3.5296e-03 | validation loss: 6.0447e-03\n",
      "Epoch: 17570 | training loss: 3.5286e-03 | validation loss: 6.0375e-03\n",
      "Epoch: 17580 | training loss: 3.5276e-03 | validation loss: 6.0303e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17590 | training loss: 3.5265e-03 | validation loss: 6.0230e-03\n",
      "Epoch: 17600 | training loss: 3.5255e-03 | validation loss: 6.0158e-03\n",
      "Epoch: 17610 | training loss: 3.5245e-03 | validation loss: 6.0085e-03\n",
      "Epoch: 17620 | training loss: 3.5234e-03 | validation loss: 6.0013e-03\n",
      "Epoch: 17630 | training loss: 3.5224e-03 | validation loss: 5.9946e-03\n",
      "Epoch: 17640 | training loss: 3.5235e-03 | validation loss: 5.9940e-03\n",
      "Epoch: 17650 | training loss: 3.8844e-03 | validation loss: 6.2333e-03\n",
      "Epoch: 17660 | training loss: 3.7988e-03 | validation loss: 6.0342e-03\n",
      "Epoch: 17670 | training loss: 3.5461e-03 | validation loss: 6.0036e-03\n",
      "Epoch: 17680 | training loss: 3.5516e-03 | validation loss: 6.0020e-03\n",
      "Epoch: 17690 | training loss: 3.5390e-03 | validation loss: 5.9830e-03\n",
      "Epoch: 17700 | training loss: 3.5274e-03 | validation loss: 5.9652e-03\n",
      "Epoch: 17710 | training loss: 3.5193e-03 | validation loss: 5.9494e-03\n",
      "Epoch: 17720 | training loss: 3.5149e-03 | validation loss: 5.9355e-03\n",
      "Epoch: 17730 | training loss: 3.5131e-03 | validation loss: 5.9238e-03\n",
      "Epoch: 17740 | training loss: 3.5118e-03 | validation loss: 5.9151e-03\n",
      "Epoch: 17750 | training loss: 3.5107e-03 | validation loss: 5.9089e-03\n",
      "Epoch: 17760 | training loss: 3.5097e-03 | validation loss: 5.9027e-03\n",
      "Epoch: 17770 | training loss: 3.5087e-03 | validation loss: 5.8956e-03\n",
      "Epoch: 17780 | training loss: 3.5078e-03 | validation loss: 5.8885e-03\n",
      "Epoch: 17790 | training loss: 3.5068e-03 | validation loss: 5.8816e-03\n",
      "Epoch: 17800 | training loss: 3.5058e-03 | validation loss: 5.8746e-03\n",
      "Epoch: 17810 | training loss: 3.5048e-03 | validation loss: 5.8676e-03\n",
      "Epoch: 17820 | training loss: 3.5038e-03 | validation loss: 5.8606e-03\n",
      "Epoch: 17830 | training loss: 3.5028e-03 | validation loss: 5.8536e-03\n",
      "Epoch: 17840 | training loss: 3.5018e-03 | validation loss: 5.8465e-03\n",
      "Epoch: 17850 | training loss: 3.5008e-03 | validation loss: 5.8395e-03\n",
      "Epoch: 17860 | training loss: 3.4998e-03 | validation loss: 5.8324e-03\n",
      "Epoch: 17870 | training loss: 3.4988e-03 | validation loss: 5.8253e-03\n",
      "Epoch: 17880 | training loss: 3.4978e-03 | validation loss: 5.8182e-03\n",
      "Epoch: 17890 | training loss: 3.4968e-03 | validation loss: 5.8110e-03\n",
      "Epoch: 17900 | training loss: 3.4957e-03 | validation loss: 5.8038e-03\n",
      "Epoch: 17910 | training loss: 3.4948e-03 | validation loss: 5.7960e-03\n",
      "Epoch: 17920 | training loss: 3.5006e-03 | validation loss: 5.7836e-03\n",
      "Epoch: 17930 | training loss: 4.6483e-03 | validation loss: 6.2684e-03\n",
      "Epoch: 17940 | training loss: 3.5769e-03 | validation loss: 5.8530e-03\n",
      "Epoch: 17950 | training loss: 3.5009e-03 | validation loss: 5.7598e-03\n",
      "Epoch: 17960 | training loss: 3.5479e-03 | validation loss: 5.8023e-03\n",
      "Epoch: 17970 | training loss: 3.5151e-03 | validation loss: 5.7688e-03\n",
      "Epoch: 17980 | training loss: 3.4903e-03 | validation loss: 5.7504e-03\n",
      "Epoch: 17990 | training loss: 3.4877e-03 | validation loss: 5.7499e-03\n",
      "Epoch: 18000 | training loss: 3.4872e-03 | validation loss: 5.7420e-03\n",
      "Epoch: 18010 | training loss: 3.4859e-03 | validation loss: 5.7370e-03\n",
      "Epoch: 18020 | training loss: 3.4847e-03 | validation loss: 5.7289e-03\n",
      "Epoch: 18030 | training loss: 3.4837e-03 | validation loss: 5.7228e-03\n",
      "Epoch: 18040 | training loss: 3.4828e-03 | validation loss: 5.7166e-03\n",
      "Epoch: 18050 | training loss: 3.4818e-03 | validation loss: 5.7101e-03\n",
      "Epoch: 18060 | training loss: 3.4809e-03 | validation loss: 5.7037e-03\n",
      "Epoch: 18070 | training loss: 3.4800e-03 | validation loss: 5.6972e-03\n",
      "Epoch: 18080 | training loss: 3.4792e-03 | validation loss: 5.6899e-03\n",
      "Epoch: 18090 | training loss: 3.4836e-03 | validation loss: 5.6777e-03\n",
      "Epoch: 18100 | training loss: 4.0123e-03 | validation loss: 5.8286e-03\n",
      "Epoch: 18110 | training loss: 3.8392e-03 | validation loss: 5.9244e-03\n",
      "Epoch: 18120 | training loss: 3.5394e-03 | validation loss: 5.6634e-03\n",
      "Epoch: 18130 | training loss: 3.5767e-03 | validation loss: 5.6651e-03\n",
      "Epoch: 18140 | training loss: 3.4929e-03 | validation loss: 5.6446e-03\n",
      "Epoch: 18150 | training loss: 3.4741e-03 | validation loss: 5.6535e-03\n",
      "Epoch: 18160 | training loss: 3.4770e-03 | validation loss: 5.6543e-03\n",
      "Epoch: 18170 | training loss: 3.4712e-03 | validation loss: 5.6372e-03\n",
      "Epoch: 18180 | training loss: 3.4709e-03 | validation loss: 5.6270e-03\n",
      "Epoch: 18190 | training loss: 3.4694e-03 | validation loss: 5.6246e-03\n",
      "Epoch: 18200 | training loss: 3.4686e-03 | validation loss: 5.6190e-03\n",
      "Epoch: 18210 | training loss: 3.4677e-03 | validation loss: 5.6112e-03\n",
      "Epoch: 18220 | training loss: 3.4668e-03 | validation loss: 5.6064e-03\n",
      "Epoch: 18230 | training loss: 3.4659e-03 | validation loss: 5.5996e-03\n",
      "Epoch: 18240 | training loss: 3.4650e-03 | validation loss: 5.5939e-03\n",
      "Epoch: 18250 | training loss: 3.4641e-03 | validation loss: 5.5876e-03\n",
      "Epoch: 18260 | training loss: 3.4632e-03 | validation loss: 5.5816e-03\n",
      "Epoch: 18270 | training loss: 3.4623e-03 | validation loss: 5.5754e-03\n",
      "Epoch: 18280 | training loss: 3.4614e-03 | validation loss: 5.5692e-03\n",
      "Epoch: 18290 | training loss: 3.4605e-03 | validation loss: 5.5631e-03\n",
      "Epoch: 18300 | training loss: 3.4596e-03 | validation loss: 5.5568e-03\n",
      "Epoch: 18310 | training loss: 3.4587e-03 | validation loss: 5.5502e-03\n",
      "Epoch: 18320 | training loss: 3.4584e-03 | validation loss: 5.5412e-03\n",
      "Epoch: 18330 | training loss: 3.5357e-03 | validation loss: 5.5359e-03\n",
      "Epoch: 18340 | training loss: 4.7795e-03 | validation loss: 5.9705e-03\n",
      "Epoch: 18350 | training loss: 3.7857e-03 | validation loss: 5.5990e-03\n",
      "Epoch: 18360 | training loss: 3.4725e-03 | validation loss: 5.5123e-03\n",
      "Epoch: 18370 | training loss: 3.4563e-03 | validation loss: 5.5242e-03\n",
      "Epoch: 18380 | training loss: 3.4651e-03 | validation loss: 5.5296e-03\n",
      "Epoch: 18390 | training loss: 3.4603e-03 | validation loss: 5.5190e-03\n",
      "Epoch: 18400 | training loss: 3.4521e-03 | validation loss: 5.5016e-03\n",
      "Epoch: 18410 | training loss: 3.4503e-03 | validation loss: 5.4882e-03\n",
      "Epoch: 18420 | training loss: 3.4496e-03 | validation loss: 5.4813e-03\n",
      "Epoch: 18430 | training loss: 3.4483e-03 | validation loss: 5.4781e-03\n",
      "Epoch: 18440 | training loss: 3.4475e-03 | validation loss: 5.4730e-03\n",
      "Epoch: 18450 | training loss: 3.4466e-03 | validation loss: 5.4657e-03\n",
      "Epoch: 18460 | training loss: 3.4457e-03 | validation loss: 5.4599e-03\n",
      "Epoch: 18470 | training loss: 3.4449e-03 | validation loss: 5.4542e-03\n",
      "Epoch: 18480 | training loss: 3.4440e-03 | validation loss: 5.4479e-03\n",
      "Epoch: 18490 | training loss: 3.4431e-03 | validation loss: 5.4421e-03\n",
      "Epoch: 18500 | training loss: 3.4422e-03 | validation loss: 5.4359e-03\n",
      "Epoch: 18510 | training loss: 3.4414e-03 | validation loss: 5.4299e-03\n",
      "Epoch: 18520 | training loss: 3.4405e-03 | validation loss: 5.4238e-03\n",
      "Epoch: 18530 | training loss: 3.4396e-03 | validation loss: 5.4177e-03\n",
      "Epoch: 18540 | training loss: 3.4387e-03 | validation loss: 5.4116e-03\n",
      "Epoch: 18550 | training loss: 3.4378e-03 | validation loss: 5.4055e-03\n",
      "Epoch: 18560 | training loss: 3.4369e-03 | validation loss: 5.3994e-03\n",
      "Epoch: 18570 | training loss: 3.4360e-03 | validation loss: 5.3933e-03\n",
      "Epoch: 18580 | training loss: 3.4351e-03 | validation loss: 5.3874e-03\n",
      "Epoch: 18590 | training loss: 3.4350e-03 | validation loss: 5.3851e-03\n",
      "Epoch: 18600 | training loss: 3.6468e-03 | validation loss: 5.5394e-03\n",
      "Epoch: 18610 | training loss: 3.5564e-03 | validation loss: 5.4476e-03\n",
      "Epoch: 18620 | training loss: 3.5511e-03 | validation loss: 5.4752e-03\n",
      "Epoch: 18630 | training loss: 3.5726e-03 | validation loss: 5.4712e-03\n",
      "Epoch: 18640 | training loss: 3.4901e-03 | validation loss: 5.4016e-03\n",
      "Epoch: 18650 | training loss: 3.4339e-03 | validation loss: 5.3570e-03\n",
      "Epoch: 18660 | training loss: 3.4324e-03 | validation loss: 5.3495e-03\n",
      "Epoch: 18670 | training loss: 3.4303e-03 | validation loss: 5.3398e-03\n",
      "Epoch: 18680 | training loss: 3.4277e-03 | validation loss: 5.3301e-03\n",
      "Epoch: 18690 | training loss: 3.4263e-03 | validation loss: 5.3236e-03\n",
      "Epoch: 18700 | training loss: 3.4254e-03 | validation loss: 5.3188e-03\n",
      "Epoch: 18710 | training loss: 3.4246e-03 | validation loss: 5.3134e-03\n",
      "Epoch: 18720 | training loss: 3.4237e-03 | validation loss: 5.3073e-03\n",
      "Epoch: 18730 | training loss: 3.4229e-03 | validation loss: 5.3017e-03\n",
      "Epoch: 18740 | training loss: 3.4221e-03 | validation loss: 5.2964e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18750 | training loss: 3.4213e-03 | validation loss: 5.2907e-03\n",
      "Epoch: 18760 | training loss: 3.4205e-03 | validation loss: 5.2851e-03\n",
      "Epoch: 18770 | training loss: 3.4197e-03 | validation loss: 5.2795e-03\n",
      "Epoch: 18780 | training loss: 3.4189e-03 | validation loss: 5.2738e-03\n",
      "Epoch: 18790 | training loss: 3.4180e-03 | validation loss: 5.2682e-03\n",
      "Epoch: 18800 | training loss: 3.4172e-03 | validation loss: 5.2625e-03\n",
      "Epoch: 18810 | training loss: 3.4164e-03 | validation loss: 5.2568e-03\n",
      "Epoch: 18820 | training loss: 3.4155e-03 | validation loss: 5.2511e-03\n",
      "Epoch: 18830 | training loss: 3.4147e-03 | validation loss: 5.2454e-03\n",
      "Epoch: 18840 | training loss: 3.4139e-03 | validation loss: 5.2397e-03\n",
      "Epoch: 18850 | training loss: 3.4130e-03 | validation loss: 5.2339e-03\n",
      "Epoch: 18860 | training loss: 3.4122e-03 | validation loss: 5.2278e-03\n",
      "Epoch: 18870 | training loss: 3.4127e-03 | validation loss: 5.2178e-03\n",
      "Epoch: 18880 | training loss: 3.7275e-03 | validation loss: 5.2902e-03\n",
      "Epoch: 18890 | training loss: 3.8794e-03 | validation loss: 5.5720e-03\n",
      "Epoch: 18900 | training loss: 3.6649e-03 | validation loss: 5.3934e-03\n",
      "Epoch: 18910 | training loss: 3.5213e-03 | validation loss: 5.2888e-03\n",
      "Epoch: 18920 | training loss: 3.4339e-03 | validation loss: 5.2268e-03\n",
      "Epoch: 18930 | training loss: 3.4121e-03 | validation loss: 5.2042e-03\n",
      "Epoch: 18940 | training loss: 3.4060e-03 | validation loss: 5.1877e-03\n",
      "Epoch: 18950 | training loss: 3.4055e-03 | validation loss: 5.1778e-03\n",
      "Epoch: 18960 | training loss: 3.4044e-03 | validation loss: 5.1749e-03\n",
      "Epoch: 18970 | training loss: 3.4037e-03 | validation loss: 5.1710e-03\n",
      "Epoch: 18980 | training loss: 3.4028e-03 | validation loss: 5.1647e-03\n",
      "Epoch: 18990 | training loss: 3.4020e-03 | validation loss: 5.1604e-03\n",
      "Epoch: 19000 | training loss: 3.4013e-03 | validation loss: 5.1552e-03\n",
      "Epoch: 19010 | training loss: 3.4005e-03 | validation loss: 5.1501e-03\n",
      "Epoch: 19020 | training loss: 3.3997e-03 | validation loss: 5.1449e-03\n",
      "Epoch: 19030 | training loss: 3.3989e-03 | validation loss: 5.1397e-03\n",
      "Epoch: 19040 | training loss: 3.3981e-03 | validation loss: 5.1344e-03\n",
      "Epoch: 19050 | training loss: 3.3973e-03 | validation loss: 5.1293e-03\n",
      "Epoch: 19060 | training loss: 3.3965e-03 | validation loss: 5.1241e-03\n",
      "Epoch: 19070 | training loss: 3.3957e-03 | validation loss: 5.1189e-03\n",
      "Epoch: 19080 | training loss: 3.3949e-03 | validation loss: 5.1136e-03\n",
      "Epoch: 19090 | training loss: 3.3941e-03 | validation loss: 5.1080e-03\n",
      "Epoch: 19100 | training loss: 3.3940e-03 | validation loss: 5.1002e-03\n",
      "Epoch: 19110 | training loss: 3.5169e-03 | validation loss: 5.1120e-03\n",
      "Epoch: 19120 | training loss: 3.7687e-03 | validation loss: 5.1654e-03\n",
      "Epoch: 19130 | training loss: 3.8500e-03 | validation loss: 5.1926e-03\n",
      "Epoch: 19140 | training loss: 3.5667e-03 | validation loss: 5.0993e-03\n",
      "Epoch: 19150 | training loss: 3.4541e-03 | validation loss: 5.0683e-03\n",
      "Epoch: 19160 | training loss: 3.4128e-03 | validation loss: 5.0594e-03\n",
      "Epoch: 19170 | training loss: 3.3967e-03 | validation loss: 5.0572e-03\n",
      "Epoch: 19180 | training loss: 3.3898e-03 | validation loss: 5.0562e-03\n",
      "Epoch: 19190 | training loss: 3.3870e-03 | validation loss: 5.0550e-03\n",
      "Epoch: 19200 | training loss: 3.3860e-03 | validation loss: 5.0527e-03\n",
      "Epoch: 19210 | training loss: 3.3853e-03 | validation loss: 5.0487e-03\n",
      "Epoch: 19220 | training loss: 3.3845e-03 | validation loss: 5.0430e-03\n",
      "Epoch: 19230 | training loss: 3.3837e-03 | validation loss: 5.0372e-03\n",
      "Epoch: 19240 | training loss: 3.3830e-03 | validation loss: 5.0323e-03\n",
      "Epoch: 19250 | training loss: 3.3822e-03 | validation loss: 5.0277e-03\n",
      "Epoch: 19260 | training loss: 3.3814e-03 | validation loss: 5.0226e-03\n",
      "Epoch: 19270 | training loss: 3.3807e-03 | validation loss: 5.0175e-03\n",
      "Epoch: 19280 | training loss: 3.3799e-03 | validation loss: 5.0126e-03\n",
      "Epoch: 19290 | training loss: 3.3792e-03 | validation loss: 5.0075e-03\n",
      "Epoch: 19300 | training loss: 3.3784e-03 | validation loss: 5.0025e-03\n",
      "Epoch: 19310 | training loss: 3.3776e-03 | validation loss: 4.9974e-03\n",
      "Epoch: 19320 | training loss: 3.3769e-03 | validation loss: 4.9923e-03\n",
      "Epoch: 19330 | training loss: 3.3761e-03 | validation loss: 4.9872e-03\n",
      "Epoch: 19340 | training loss: 3.3753e-03 | validation loss: 4.9821e-03\n",
      "Epoch: 19350 | training loss: 3.3745e-03 | validation loss: 4.9770e-03\n",
      "Epoch: 19360 | training loss: 3.3737e-03 | validation loss: 4.9718e-03\n",
      "Epoch: 19370 | training loss: 3.3730e-03 | validation loss: 4.9667e-03\n",
      "Epoch: 19380 | training loss: 3.3722e-03 | validation loss: 4.9616e-03\n",
      "Epoch: 19390 | training loss: 3.3714e-03 | validation loss: 4.9567e-03\n",
      "Epoch: 19400 | training loss: 3.3720e-03 | validation loss: 4.9566e-03\n",
      "Epoch: 19410 | training loss: 3.7603e-03 | validation loss: 5.2173e-03\n",
      "Epoch: 19420 | training loss: 4.0702e-03 | validation loss: 5.2378e-03\n",
      "Epoch: 19430 | training loss: 3.6460e-03 | validation loss: 5.1244e-03\n",
      "Epoch: 19440 | training loss: 3.4557e-03 | validation loss: 5.0020e-03\n",
      "Epoch: 19450 | training loss: 3.3709e-03 | validation loss: 4.9386e-03\n",
      "Epoch: 19460 | training loss: 3.3822e-03 | validation loss: 4.9416e-03\n",
      "Epoch: 19470 | training loss: 3.3718e-03 | validation loss: 4.9290e-03\n",
      "Epoch: 19480 | training loss: 3.3658e-03 | validation loss: 4.9182e-03\n",
      "Epoch: 19490 | training loss: 3.3646e-03 | validation loss: 4.9100e-03\n",
      "Epoch: 19500 | training loss: 3.3638e-03 | validation loss: 4.9031e-03\n",
      "Epoch: 19510 | training loss: 3.3630e-03 | validation loss: 4.8981e-03\n",
      "Epoch: 19520 | training loss: 3.3622e-03 | validation loss: 4.8943e-03\n",
      "Epoch: 19530 | training loss: 3.3615e-03 | validation loss: 4.8902e-03\n",
      "Epoch: 19540 | training loss: 3.3608e-03 | validation loss: 4.8852e-03\n",
      "Epoch: 19550 | training loss: 3.3601e-03 | validation loss: 4.8805e-03\n",
      "Epoch: 19560 | training loss: 3.3593e-03 | validation loss: 4.8760e-03\n",
      "Epoch: 19570 | training loss: 3.3586e-03 | validation loss: 4.8713e-03\n",
      "Epoch: 19580 | training loss: 3.3579e-03 | validation loss: 4.8666e-03\n",
      "Epoch: 19590 | training loss: 3.3572e-03 | validation loss: 4.8619e-03\n",
      "Epoch: 19600 | training loss: 3.3565e-03 | validation loss: 4.8572e-03\n",
      "Epoch: 19610 | training loss: 3.3557e-03 | validation loss: 4.8525e-03\n",
      "Epoch: 19620 | training loss: 3.3550e-03 | validation loss: 4.8478e-03\n",
      "Epoch: 19630 | training loss: 3.3543e-03 | validation loss: 4.8431e-03\n",
      "Epoch: 19640 | training loss: 3.3535e-03 | validation loss: 4.8383e-03\n",
      "Epoch: 19650 | training loss: 3.3528e-03 | validation loss: 4.8335e-03\n",
      "Epoch: 19660 | training loss: 3.3521e-03 | validation loss: 4.8287e-03\n",
      "Epoch: 19670 | training loss: 3.3513e-03 | validation loss: 4.8239e-03\n",
      "Epoch: 19680 | training loss: 3.3506e-03 | validation loss: 4.8185e-03\n",
      "Epoch: 19690 | training loss: 3.3523e-03 | validation loss: 4.8076e-03\n",
      "Epoch: 19700 | training loss: 3.9734e-03 | validation loss: 4.9754e-03\n",
      "Epoch: 19710 | training loss: 4.1674e-03 | validation loss: 5.3424e-03\n",
      "Epoch: 19720 | training loss: 3.5818e-03 | validation loss: 5.0049e-03\n",
      "Epoch: 19730 | training loss: 3.3951e-03 | validation loss: 4.8323e-03\n",
      "Epoch: 19740 | training loss: 3.3534e-03 | validation loss: 4.7829e-03\n",
      "Epoch: 19750 | training loss: 3.3526e-03 | validation loss: 4.7822e-03\n",
      "Epoch: 19760 | training loss: 3.3498e-03 | validation loss: 4.7831e-03\n",
      "Epoch: 19770 | training loss: 3.3462e-03 | validation loss: 4.7788e-03\n",
      "Epoch: 19780 | training loss: 3.3442e-03 | validation loss: 4.7728e-03\n",
      "Epoch: 19790 | training loss: 3.3432e-03 | validation loss: 4.7690e-03\n",
      "Epoch: 19800 | training loss: 3.3424e-03 | validation loss: 4.7662e-03\n",
      "Epoch: 19810 | training loss: 3.3417e-03 | validation loss: 4.7617e-03\n",
      "Epoch: 19820 | training loss: 3.3410e-03 | validation loss: 4.7574e-03\n",
      "Epoch: 19830 | training loss: 3.3403e-03 | validation loss: 4.7534e-03\n",
      "Epoch: 19840 | training loss: 3.3396e-03 | validation loss: 4.7490e-03\n",
      "Epoch: 19850 | training loss: 3.3389e-03 | validation loss: 4.7449e-03\n",
      "Epoch: 19860 | training loss: 3.3383e-03 | validation loss: 4.7405e-03\n",
      "Epoch: 19870 | training loss: 3.3376e-03 | validation loss: 4.7362e-03\n",
      "Epoch: 19880 | training loss: 3.3369e-03 | validation loss: 4.7318e-03\n",
      "Epoch: 19890 | training loss: 3.3362e-03 | validation loss: 4.7274e-03\n",
      "Epoch: 19900 | training loss: 3.3355e-03 | validation loss: 4.7230e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19910 | training loss: 3.3348e-03 | validation loss: 4.7186e-03\n",
      "Epoch: 19920 | training loss: 3.3340e-03 | validation loss: 4.7143e-03\n",
      "Epoch: 19930 | training loss: 3.3334e-03 | validation loss: 4.7102e-03\n",
      "Epoch: 19940 | training loss: 3.3344e-03 | validation loss: 4.7114e-03\n",
      "Epoch: 19950 | training loss: 3.7739e-03 | validation loss: 4.9989e-03\n",
      "Epoch: 19960 | training loss: 4.0323e-03 | validation loss: 4.9309e-03\n",
      "Epoch: 19970 | training loss: 3.4212e-03 | validation loss: 4.7129e-03\n",
      "Epoch: 19980 | training loss: 3.3605e-03 | validation loss: 4.6798e-03\n",
      "Epoch: 19990 | training loss: 3.3446e-03 | validation loss: 4.6712e-03\n",
      "Epoch: 20000 | training loss: 3.3351e-03 | validation loss: 4.6696e-03\n",
      "Epoch: 20010 | training loss: 3.3300e-03 | validation loss: 4.6698e-03\n",
      "Epoch: 20020 | training loss: 3.3279e-03 | validation loss: 4.6697e-03\n",
      "Epoch: 20030 | training loss: 3.3271e-03 | validation loss: 4.6680e-03\n",
      "Epoch: 20040 | training loss: 3.3264e-03 | validation loss: 4.6645e-03\n",
      "Epoch: 20050 | training loss: 3.3256e-03 | validation loss: 4.6599e-03\n",
      "Epoch: 20060 | training loss: 3.3248e-03 | validation loss: 4.6548e-03\n",
      "Epoch: 20070 | training loss: 3.3242e-03 | validation loss: 4.6503e-03\n",
      "Epoch: 20080 | training loss: 3.3235e-03 | validation loss: 4.6464e-03\n",
      "Epoch: 20090 | training loss: 3.3229e-03 | validation loss: 4.6426e-03\n",
      "Epoch: 20100 | training loss: 3.3222e-03 | validation loss: 4.6383e-03\n",
      "Epoch: 20110 | training loss: 3.3215e-03 | validation loss: 4.6340e-03\n",
      "Epoch: 20120 | training loss: 3.3209e-03 | validation loss: 4.6299e-03\n",
      "Epoch: 20130 | training loss: 3.3202e-03 | validation loss: 4.6257e-03\n",
      "Epoch: 20140 | training loss: 3.3195e-03 | validation loss: 4.6215e-03\n",
      "Epoch: 20150 | training loss: 3.3188e-03 | validation loss: 4.6173e-03\n",
      "Epoch: 20160 | training loss: 3.3181e-03 | validation loss: 4.6131e-03\n",
      "Epoch: 20170 | training loss: 3.3175e-03 | validation loss: 4.6088e-03\n",
      "Epoch: 20180 | training loss: 3.3168e-03 | validation loss: 4.6046e-03\n",
      "Epoch: 20190 | training loss: 3.3161e-03 | validation loss: 4.6003e-03\n",
      "Epoch: 20200 | training loss: 3.3154e-03 | validation loss: 4.5961e-03\n",
      "Epoch: 20210 | training loss: 3.3147e-03 | validation loss: 4.5918e-03\n",
      "Epoch: 20220 | training loss: 3.3140e-03 | validation loss: 4.5875e-03\n",
      "Epoch: 20230 | training loss: 3.3133e-03 | validation loss: 4.5832e-03\n",
      "Epoch: 20240 | training loss: 3.3126e-03 | validation loss: 4.5790e-03\n",
      "Epoch: 20250 | training loss: 3.3122e-03 | validation loss: 4.5767e-03\n",
      "Epoch: 20260 | training loss: 3.4108e-03 | validation loss: 4.6591e-03\n",
      "Epoch: 20270 | training loss: 3.7852e-03 | validation loss: 4.9004e-03\n",
      "Epoch: 20280 | training loss: 3.5234e-03 | validation loss: 4.7575e-03\n",
      "Epoch: 20290 | training loss: 3.5135e-03 | validation loss: 4.7358e-03\n",
      "Epoch: 20300 | training loss: 3.3204e-03 | validation loss: 4.5747e-03\n",
      "Epoch: 20310 | training loss: 3.3302e-03 | validation loss: 4.5698e-03\n",
      "Epoch: 20320 | training loss: 3.3092e-03 | validation loss: 4.5503e-03\n",
      "Epoch: 20330 | training loss: 3.3092e-03 | validation loss: 4.5469e-03\n",
      "Epoch: 20340 | training loss: 3.3072e-03 | validation loss: 4.5418e-03\n",
      "Epoch: 20350 | training loss: 3.3059e-03 | validation loss: 4.5371e-03\n",
      "Epoch: 20360 | training loss: 3.3054e-03 | validation loss: 4.5326e-03\n",
      "Epoch: 20370 | training loss: 3.3046e-03 | validation loss: 4.5280e-03\n",
      "Epoch: 20380 | training loss: 3.3040e-03 | validation loss: 4.5237e-03\n",
      "Epoch: 20390 | training loss: 3.3033e-03 | validation loss: 4.5197e-03\n",
      "Epoch: 20400 | training loss: 3.3027e-03 | validation loss: 4.5161e-03\n",
      "Epoch: 20410 | training loss: 3.3021e-03 | validation loss: 4.5125e-03\n",
      "Epoch: 20420 | training loss: 3.3015e-03 | validation loss: 4.5086e-03\n",
      "Epoch: 20430 | training loss: 3.3008e-03 | validation loss: 4.5047e-03\n",
      "Epoch: 20440 | training loss: 3.3002e-03 | validation loss: 4.5010e-03\n",
      "Epoch: 20450 | training loss: 3.2996e-03 | validation loss: 4.4971e-03\n",
      "Epoch: 20460 | training loss: 3.2990e-03 | validation loss: 4.4933e-03\n",
      "Epoch: 20470 | training loss: 3.2983e-03 | validation loss: 4.4894e-03\n",
      "Epoch: 20480 | training loss: 3.2977e-03 | validation loss: 4.4855e-03\n",
      "Epoch: 20490 | training loss: 3.2970e-03 | validation loss: 4.4817e-03\n",
      "Epoch: 20500 | training loss: 3.2964e-03 | validation loss: 4.4778e-03\n",
      "Epoch: 20510 | training loss: 3.2958e-03 | validation loss: 4.4738e-03\n",
      "Epoch: 20520 | training loss: 3.2951e-03 | validation loss: 4.4699e-03\n",
      "Epoch: 20530 | training loss: 3.2945e-03 | validation loss: 4.4660e-03\n",
      "Epoch: 20540 | training loss: 3.2938e-03 | validation loss: 4.4620e-03\n",
      "Epoch: 20550 | training loss: 3.2932e-03 | validation loss: 4.4581e-03\n",
      "Epoch: 20560 | training loss: 3.2925e-03 | validation loss: 4.4540e-03\n",
      "Epoch: 20570 | training loss: 3.2920e-03 | validation loss: 4.4483e-03\n",
      "Epoch: 20580 | training loss: 3.3261e-03 | validation loss: 4.4316e-03\n",
      "Epoch: 20590 | training loss: 5.1546e-03 | validation loss: 5.0634e-03\n",
      "Epoch: 20600 | training loss: 3.6844e-03 | validation loss: 4.5609e-03\n",
      "Epoch: 20610 | training loss: 3.3078e-03 | validation loss: 4.4354e-03\n",
      "Epoch: 20620 | training loss: 3.3225e-03 | validation loss: 4.4785e-03\n",
      "Epoch: 20630 | training loss: 3.3140e-03 | validation loss: 4.4671e-03\n",
      "Epoch: 20640 | training loss: 3.2898e-03 | validation loss: 4.4331e-03\n",
      "Epoch: 20650 | training loss: 3.2878e-03 | validation loss: 4.4191e-03\n",
      "Epoch: 20660 | training loss: 3.2876e-03 | validation loss: 4.4137e-03\n",
      "Epoch: 20670 | training loss: 3.2862e-03 | validation loss: 4.4119e-03\n",
      "Epoch: 20680 | training loss: 3.2854e-03 | validation loss: 4.4104e-03\n",
      "Epoch: 20690 | training loss: 3.2847e-03 | validation loss: 4.4073e-03\n",
      "Epoch: 20700 | training loss: 3.2841e-03 | validation loss: 4.4035e-03\n",
      "Epoch: 20710 | training loss: 3.2835e-03 | validation loss: 4.4003e-03\n",
      "Epoch: 20720 | training loss: 3.2829e-03 | validation loss: 4.3968e-03\n",
      "Epoch: 20730 | training loss: 3.2823e-03 | validation loss: 4.3932e-03\n",
      "Epoch: 20740 | training loss: 3.2817e-03 | validation loss: 4.3898e-03\n",
      "Epoch: 20750 | training loss: 3.2811e-03 | validation loss: 4.3862e-03\n",
      "Epoch: 20760 | training loss: 3.2805e-03 | validation loss: 4.3827e-03\n",
      "Epoch: 20770 | training loss: 3.2799e-03 | validation loss: 4.3792e-03\n",
      "Epoch: 20780 | training loss: 3.2792e-03 | validation loss: 4.3756e-03\n",
      "Epoch: 20790 | training loss: 3.2786e-03 | validation loss: 4.3721e-03\n",
      "Epoch: 20800 | training loss: 3.2780e-03 | validation loss: 4.3685e-03\n",
      "Epoch: 20810 | training loss: 3.2774e-03 | validation loss: 4.3649e-03\n",
      "Epoch: 20820 | training loss: 3.2768e-03 | validation loss: 4.3612e-03\n",
      "Epoch: 20830 | training loss: 3.2762e-03 | validation loss: 4.3568e-03\n",
      "Epoch: 20840 | training loss: 3.2827e-03 | validation loss: 4.3470e-03\n",
      "Epoch: 20850 | training loss: 4.7331e-03 | validation loss: 4.8971e-03\n",
      "Epoch: 20860 | training loss: 4.0055e-03 | validation loss: 4.8117e-03\n",
      "Epoch: 20870 | training loss: 3.5884e-03 | validation loss: 4.5769e-03\n",
      "Epoch: 20880 | training loss: 3.3863e-03 | validation loss: 4.4454e-03\n",
      "Epoch: 20890 | training loss: 3.3123e-03 | validation loss: 4.3854e-03\n",
      "Epoch: 20900 | training loss: 3.2862e-03 | validation loss: 4.3569e-03\n",
      "Epoch: 20910 | training loss: 3.2761e-03 | validation loss: 4.3413e-03\n",
      "Epoch: 20920 | training loss: 3.2719e-03 | validation loss: 4.3309e-03\n",
      "Epoch: 20930 | training loss: 3.2704e-03 | validation loss: 4.3235e-03\n",
      "Epoch: 20940 | training loss: 3.2699e-03 | validation loss: 4.3183e-03\n",
      "Epoch: 20950 | training loss: 3.2693e-03 | validation loss: 4.3149e-03\n",
      "Epoch: 20960 | training loss: 3.2686e-03 | validation loss: 4.3124e-03\n",
      "Epoch: 20970 | training loss: 3.2680e-03 | validation loss: 4.3095e-03\n",
      "Epoch: 20980 | training loss: 3.2674e-03 | validation loss: 4.3058e-03\n",
      "Epoch: 20990 | training loss: 3.2669e-03 | validation loss: 4.3021e-03\n",
      "Epoch: 21000 | training loss: 3.2663e-03 | validation loss: 4.2989e-03\n",
      "Epoch: 21010 | training loss: 3.2657e-03 | validation loss: 4.2955e-03\n",
      "Epoch: 21020 | training loss: 3.2651e-03 | validation loss: 4.2920e-03\n",
      "Epoch: 21030 | training loss: 3.2645e-03 | validation loss: 4.2886e-03\n",
      "Epoch: 21040 | training loss: 3.2639e-03 | validation loss: 4.2851e-03\n",
      "Epoch: 21050 | training loss: 3.2633e-03 | validation loss: 4.2817e-03\n",
      "Epoch: 21060 | training loss: 3.2627e-03 | validation loss: 4.2782e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21070 | training loss: 3.2621e-03 | validation loss: 4.2747e-03\n",
      "Epoch: 21080 | training loss: 3.2615e-03 | validation loss: 4.2712e-03\n",
      "Epoch: 21090 | training loss: 3.2608e-03 | validation loss: 4.2677e-03\n",
      "Epoch: 21100 | training loss: 3.2602e-03 | validation loss: 4.2642e-03\n",
      "Epoch: 21110 | training loss: 3.2596e-03 | validation loss: 4.2607e-03\n",
      "Epoch: 21120 | training loss: 3.2590e-03 | validation loss: 4.2573e-03\n",
      "Epoch: 21130 | training loss: 3.2585e-03 | validation loss: 4.2548e-03\n",
      "Epoch: 21140 | training loss: 3.2728e-03 | validation loss: 4.2730e-03\n",
      "Epoch: 21150 | training loss: 5.7500e-03 | validation loss: 5.6469e-03\n",
      "Epoch: 21160 | training loss: 3.3873e-03 | validation loss: 4.2970e-03\n",
      "Epoch: 21170 | training loss: 3.3967e-03 | validation loss: 4.3038e-03\n",
      "Epoch: 21180 | training loss: 3.3513e-03 | validation loss: 4.2787e-03\n",
      "Epoch: 21190 | training loss: 3.2819e-03 | validation loss: 4.2331e-03\n",
      "Epoch: 21200 | training loss: 3.2589e-03 | validation loss: 4.2208e-03\n",
      "Epoch: 21210 | training loss: 3.2558e-03 | validation loss: 4.2226e-03\n",
      "Epoch: 21220 | training loss: 3.2543e-03 | validation loss: 4.2238e-03\n",
      "Epoch: 21230 | training loss: 3.2533e-03 | validation loss: 4.2227e-03\n",
      "Epoch: 21240 | training loss: 3.2525e-03 | validation loss: 4.2191e-03\n",
      "Epoch: 21250 | training loss: 3.2517e-03 | validation loss: 4.2144e-03\n",
      "Epoch: 21260 | training loss: 3.2512e-03 | validation loss: 4.2105e-03\n",
      "Epoch: 21270 | training loss: 3.2506e-03 | validation loss: 4.2078e-03\n",
      "Epoch: 21280 | training loss: 3.2500e-03 | validation loss: 4.2048e-03\n",
      "Epoch: 21290 | training loss: 3.2494e-03 | validation loss: 4.2014e-03\n",
      "Epoch: 21300 | training loss: 3.2489e-03 | validation loss: 4.1983e-03\n",
      "Epoch: 21310 | training loss: 3.2483e-03 | validation loss: 4.1951e-03\n",
      "Epoch: 21320 | training loss: 3.2477e-03 | validation loss: 4.1918e-03\n",
      "Epoch: 21330 | training loss: 3.2472e-03 | validation loss: 4.1886e-03\n",
      "Epoch: 21340 | training loss: 3.2466e-03 | validation loss: 4.1854e-03\n",
      "Epoch: 21350 | training loss: 3.2460e-03 | validation loss: 4.1821e-03\n",
      "Epoch: 21360 | training loss: 3.2454e-03 | validation loss: 4.1789e-03\n",
      "Epoch: 21370 | training loss: 3.2448e-03 | validation loss: 4.1757e-03\n",
      "Epoch: 21380 | training loss: 3.2443e-03 | validation loss: 4.1724e-03\n",
      "Epoch: 21390 | training loss: 3.2437e-03 | validation loss: 4.1691e-03\n",
      "Epoch: 21400 | training loss: 3.2431e-03 | validation loss: 4.1658e-03\n",
      "Epoch: 21410 | training loss: 3.2425e-03 | validation loss: 4.1626e-03\n",
      "Epoch: 21420 | training loss: 3.2419e-03 | validation loss: 4.1602e-03\n",
      "Epoch: 21430 | training loss: 3.2480e-03 | validation loss: 4.1733e-03\n",
      "Epoch: 21440 | training loss: 4.9710e-03 | validation loss: 5.1889e-03\n",
      "Epoch: 21450 | training loss: 3.4839e-03 | validation loss: 4.2610e-03\n",
      "Epoch: 21460 | training loss: 3.3787e-03 | validation loss: 4.2755e-03\n",
      "Epoch: 21470 | training loss: 3.3288e-03 | validation loss: 4.2419e-03\n",
      "Epoch: 21480 | training loss: 3.2420e-03 | validation loss: 4.1529e-03\n",
      "Epoch: 21490 | training loss: 3.2456e-03 | validation loss: 4.1373e-03\n",
      "Epoch: 21500 | training loss: 3.2416e-03 | validation loss: 4.1306e-03\n",
      "Epoch: 21510 | training loss: 3.2376e-03 | validation loss: 4.1289e-03\n",
      "Epoch: 21520 | training loss: 3.2365e-03 | validation loss: 4.1295e-03\n",
      "Epoch: 21530 | training loss: 3.2361e-03 | validation loss: 4.1283e-03\n",
      "Epoch: 21540 | training loss: 3.2355e-03 | validation loss: 4.1248e-03\n",
      "Epoch: 21550 | training loss: 3.2349e-03 | validation loss: 4.1214e-03\n",
      "Epoch: 21560 | training loss: 3.2344e-03 | validation loss: 4.1186e-03\n",
      "Epoch: 21570 | training loss: 3.2338e-03 | validation loss: 4.1153e-03\n",
      "Epoch: 21580 | training loss: 3.2333e-03 | validation loss: 4.1124e-03\n",
      "Epoch: 21590 | training loss: 3.2327e-03 | validation loss: 4.1094e-03\n",
      "Epoch: 21600 | training loss: 3.2322e-03 | validation loss: 4.1065e-03\n",
      "Epoch: 21610 | training loss: 3.2317e-03 | validation loss: 4.1035e-03\n",
      "Epoch: 21620 | training loss: 3.2311e-03 | validation loss: 4.1006e-03\n",
      "Epoch: 21630 | training loss: 3.2306e-03 | validation loss: 4.0977e-03\n",
      "Epoch: 21640 | training loss: 3.2300e-03 | validation loss: 4.0948e-03\n",
      "Epoch: 21650 | training loss: 3.2294e-03 | validation loss: 4.0918e-03\n",
      "Epoch: 21660 | training loss: 3.2289e-03 | validation loss: 4.0889e-03\n",
      "Epoch: 21670 | training loss: 3.2284e-03 | validation loss: 4.0868e-03\n",
      "Epoch: 21680 | training loss: 3.2357e-03 | validation loss: 4.0972e-03\n",
      "Epoch: 21690 | training loss: 4.6989e-03 | validation loss: 4.9489e-03\n",
      "Epoch: 21700 | training loss: 3.9185e-03 | validation loss: 4.2502e-03\n",
      "Epoch: 21710 | training loss: 3.5396e-03 | validation loss: 4.1221e-03\n",
      "Epoch: 21720 | training loss: 3.3340e-03 | validation loss: 4.0671e-03\n",
      "Epoch: 21730 | training loss: 3.2587e-03 | validation loss: 4.0564e-03\n",
      "Epoch: 21740 | training loss: 3.2331e-03 | validation loss: 4.0577e-03\n",
      "Epoch: 21750 | training loss: 3.2252e-03 | validation loss: 4.0607e-03\n",
      "Epoch: 21760 | training loss: 3.2239e-03 | validation loss: 4.0624e-03\n",
      "Epoch: 21770 | training loss: 3.2237e-03 | validation loss: 4.0610e-03\n",
      "Epoch: 21780 | training loss: 3.2228e-03 | validation loss: 4.0566e-03\n",
      "Epoch: 21790 | training loss: 3.2221e-03 | validation loss: 4.0516e-03\n",
      "Epoch: 21800 | training loss: 3.2216e-03 | validation loss: 4.0481e-03\n",
      "Epoch: 21810 | training loss: 3.2211e-03 | validation loss: 4.0459e-03\n",
      "Epoch: 21820 | training loss: 3.2205e-03 | validation loss: 4.0434e-03\n",
      "Epoch: 21830 | training loss: 3.2200e-03 | validation loss: 4.0402e-03\n",
      "Epoch: 21840 | training loss: 3.2195e-03 | validation loss: 4.0375e-03\n",
      "Epoch: 21850 | training loss: 3.2189e-03 | validation loss: 4.0348e-03\n",
      "Epoch: 21860 | training loss: 3.2184e-03 | validation loss: 4.0319e-03\n",
      "Epoch: 21870 | training loss: 3.2179e-03 | validation loss: 4.0291e-03\n",
      "Epoch: 21880 | training loss: 3.2173e-03 | validation loss: 4.0263e-03\n",
      "Epoch: 21890 | training loss: 3.2168e-03 | validation loss: 4.0234e-03\n",
      "Epoch: 21900 | training loss: 3.2162e-03 | validation loss: 4.0206e-03\n",
      "Epoch: 21910 | training loss: 3.2157e-03 | validation loss: 4.0177e-03\n",
      "Epoch: 21920 | training loss: 3.2151e-03 | validation loss: 4.0149e-03\n",
      "Epoch: 21930 | training loss: 3.2146e-03 | validation loss: 4.0120e-03\n",
      "Epoch: 21940 | training loss: 3.2140e-03 | validation loss: 4.0091e-03\n",
      "Epoch: 21950 | training loss: 3.2135e-03 | validation loss: 4.0060e-03\n",
      "Epoch: 21960 | training loss: 3.2132e-03 | validation loss: 4.0014e-03\n",
      "Epoch: 21970 | training loss: 3.2564e-03 | validation loss: 3.9935e-03\n",
      "Epoch: 21980 | training loss: 5.5265e-03 | validation loss: 4.8411e-03\n",
      "Epoch: 21990 | training loss: 3.7524e-03 | validation loss: 4.1744e-03\n",
      "Epoch: 22000 | training loss: 3.3890e-03 | validation loss: 4.0484e-03\n",
      "Epoch: 22010 | training loss: 3.2669e-03 | validation loss: 4.0034e-03\n",
      "Epoch: 22020 | training loss: 3.2231e-03 | validation loss: 3.9885e-03\n",
      "Epoch: 22030 | training loss: 3.2121e-03 | validation loss: 3.9865e-03\n",
      "Epoch: 22040 | training loss: 3.2100e-03 | validation loss: 3.9859e-03\n",
      "Epoch: 22050 | training loss: 3.2091e-03 | validation loss: 3.9827e-03\n",
      "Epoch: 22060 | training loss: 3.2081e-03 | validation loss: 3.9778e-03\n",
      "Epoch: 22070 | training loss: 3.2075e-03 | validation loss: 3.9728e-03\n",
      "Epoch: 22080 | training loss: 3.2069e-03 | validation loss: 3.9694e-03\n",
      "Epoch: 22090 | training loss: 3.2064e-03 | validation loss: 3.9674e-03\n",
      "Epoch: 22100 | training loss: 3.2058e-03 | validation loss: 3.9653e-03\n",
      "Epoch: 22110 | training loss: 3.2053e-03 | validation loss: 3.9624e-03\n",
      "Epoch: 22120 | training loss: 3.2048e-03 | validation loss: 3.9596e-03\n",
      "Epoch: 22130 | training loss: 3.2043e-03 | validation loss: 3.9571e-03\n",
      "Epoch: 22140 | training loss: 3.2038e-03 | validation loss: 3.9543e-03\n",
      "Epoch: 22150 | training loss: 3.2032e-03 | validation loss: 3.9517e-03\n",
      "Epoch: 22160 | training loss: 3.2027e-03 | validation loss: 3.9489e-03\n",
      "Epoch: 22170 | training loss: 3.2022e-03 | validation loss: 3.9463e-03\n",
      "Epoch: 22180 | training loss: 3.2017e-03 | validation loss: 3.9435e-03\n",
      "Epoch: 22190 | training loss: 3.2011e-03 | validation loss: 3.9408e-03\n",
      "Epoch: 22200 | training loss: 3.2006e-03 | validation loss: 3.9381e-03\n",
      "Epoch: 22210 | training loss: 3.2001e-03 | validation loss: 3.9354e-03\n",
      "Epoch: 22220 | training loss: 3.1995e-03 | validation loss: 3.9326e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22230 | training loss: 3.1990e-03 | validation loss: 3.9299e-03\n",
      "Epoch: 22240 | training loss: 3.1984e-03 | validation loss: 3.9272e-03\n",
      "Epoch: 22250 | training loss: 3.1979e-03 | validation loss: 3.9251e-03\n",
      "Epoch: 22260 | training loss: 3.2006e-03 | validation loss: 3.9336e-03\n",
      "Epoch: 22270 | training loss: 4.1001e-03 | validation loss: 4.5471e-03\n",
      "Epoch: 22280 | training loss: 3.6527e-03 | validation loss: 4.0501e-03\n",
      "Epoch: 22290 | training loss: 3.2277e-03 | validation loss: 3.9312e-03\n",
      "Epoch: 22300 | training loss: 3.2479e-03 | validation loss: 3.9813e-03\n",
      "Epoch: 22310 | training loss: 3.2283e-03 | validation loss: 3.9534e-03\n",
      "Epoch: 22320 | training loss: 3.1953e-03 | validation loss: 3.9087e-03\n",
      "Epoch: 22330 | training loss: 3.1961e-03 | validation loss: 3.9021e-03\n",
      "Epoch: 22340 | training loss: 3.1951e-03 | validation loss: 3.8982e-03\n",
      "Epoch: 22350 | training loss: 3.1934e-03 | validation loss: 3.8971e-03\n",
      "Epoch: 22360 | training loss: 3.1926e-03 | validation loss: 3.8973e-03\n",
      "Epoch: 22370 | training loss: 3.1920e-03 | validation loss: 3.8950e-03\n",
      "Epoch: 22380 | training loss: 3.1915e-03 | validation loss: 3.8932e-03\n",
      "Epoch: 22390 | training loss: 3.1910e-03 | validation loss: 3.8905e-03\n",
      "Epoch: 22400 | training loss: 3.1905e-03 | validation loss: 3.8882e-03\n",
      "Epoch: 22410 | training loss: 3.1901e-03 | validation loss: 3.8857e-03\n",
      "Epoch: 22420 | training loss: 3.1896e-03 | validation loss: 3.8832e-03\n",
      "Epoch: 22430 | training loss: 3.1891e-03 | validation loss: 3.8808e-03\n",
      "Epoch: 22440 | training loss: 3.1886e-03 | validation loss: 3.8784e-03\n",
      "Epoch: 22450 | training loss: 3.1881e-03 | validation loss: 3.8759e-03\n",
      "Epoch: 22460 | training loss: 3.1876e-03 | validation loss: 3.8731e-03\n",
      "Epoch: 22470 | training loss: 3.1885e-03 | validation loss: 3.8672e-03\n",
      "Epoch: 22480 | training loss: 3.4278e-03 | validation loss: 3.9261e-03\n",
      "Epoch: 22490 | training loss: 3.2054e-03 | validation loss: 3.8787e-03\n",
      "Epoch: 22500 | training loss: 3.3953e-03 | validation loss: 3.8864e-03\n",
      "Epoch: 22510 | training loss: 3.3129e-03 | validation loss: 3.8719e-03\n",
      "Epoch: 22520 | training loss: 3.2387e-03 | validation loss: 3.8561e-03\n",
      "Epoch: 22530 | training loss: 3.1995e-03 | validation loss: 3.8493e-03\n",
      "Epoch: 22540 | training loss: 3.1851e-03 | validation loss: 3.8510e-03\n",
      "Epoch: 22550 | training loss: 3.1836e-03 | validation loss: 3.8550e-03\n",
      "Epoch: 22560 | training loss: 3.1837e-03 | validation loss: 3.8543e-03\n",
      "Epoch: 22570 | training loss: 3.1825e-03 | validation loss: 3.8489e-03\n",
      "Epoch: 22580 | training loss: 3.1820e-03 | validation loss: 3.8441e-03\n",
      "Epoch: 22590 | training loss: 3.1815e-03 | validation loss: 3.8422e-03\n",
      "Epoch: 22600 | training loss: 3.1810e-03 | validation loss: 3.8411e-03\n",
      "Epoch: 22610 | training loss: 3.1805e-03 | validation loss: 3.8384e-03\n",
      "Epoch: 22620 | training loss: 3.1800e-03 | validation loss: 3.8358e-03\n",
      "Epoch: 22630 | training loss: 3.1795e-03 | validation loss: 3.8338e-03\n",
      "Epoch: 22640 | training loss: 3.1790e-03 | validation loss: 3.8313e-03\n",
      "Epoch: 22650 | training loss: 3.1785e-03 | validation loss: 3.8291e-03\n",
      "Epoch: 22660 | training loss: 3.1781e-03 | validation loss: 3.8267e-03\n",
      "Epoch: 22670 | training loss: 3.1776e-03 | validation loss: 3.8244e-03\n",
      "Epoch: 22680 | training loss: 3.1771e-03 | validation loss: 3.8220e-03\n",
      "Epoch: 22690 | training loss: 3.1766e-03 | validation loss: 3.8197e-03\n",
      "Epoch: 22700 | training loss: 3.1761e-03 | validation loss: 3.8173e-03\n",
      "Epoch: 22710 | training loss: 3.1756e-03 | validation loss: 3.8150e-03\n",
      "Epoch: 22720 | training loss: 3.1751e-03 | validation loss: 3.8126e-03\n",
      "Epoch: 22730 | training loss: 3.1746e-03 | validation loss: 3.8101e-03\n",
      "Epoch: 22740 | training loss: 3.1742e-03 | validation loss: 3.8065e-03\n",
      "Epoch: 22750 | training loss: 3.1897e-03 | validation loss: 3.7951e-03\n",
      "Epoch: 22760 | training loss: 5.6445e-03 | validation loss: 4.7298e-03\n",
      "Epoch: 22770 | training loss: 3.1826e-03 | validation loss: 3.8274e-03\n",
      "Epoch: 22780 | training loss: 3.1826e-03 | validation loss: 3.8241e-03\n",
      "Epoch: 22790 | training loss: 3.1783e-03 | validation loss: 3.8073e-03\n",
      "Epoch: 22800 | training loss: 3.1763e-03 | validation loss: 3.7991e-03\n",
      "Epoch: 22810 | training loss: 3.1733e-03 | validation loss: 3.7953e-03\n",
      "Epoch: 22820 | training loss: 3.1714e-03 | validation loss: 3.7930e-03\n",
      "Epoch: 22830 | training loss: 3.1703e-03 | validation loss: 3.7906e-03\n",
      "Epoch: 22840 | training loss: 3.1697e-03 | validation loss: 3.7875e-03\n",
      "Epoch: 22850 | training loss: 3.1691e-03 | validation loss: 3.7839e-03\n",
      "Epoch: 22860 | training loss: 3.1686e-03 | validation loss: 3.7805e-03\n",
      "Epoch: 22870 | training loss: 3.1681e-03 | validation loss: 3.7778e-03\n",
      "Epoch: 22880 | training loss: 3.1676e-03 | validation loss: 3.7757e-03\n",
      "Epoch: 22890 | training loss: 3.1671e-03 | validation loss: 3.7737e-03\n",
      "Epoch: 22900 | training loss: 3.1666e-03 | validation loss: 3.7714e-03\n",
      "Epoch: 22910 | training loss: 3.1662e-03 | validation loss: 3.7692e-03\n",
      "Epoch: 22920 | training loss: 3.1657e-03 | validation loss: 3.7670e-03\n",
      "Epoch: 22930 | training loss: 3.1652e-03 | validation loss: 3.7647e-03\n",
      "Epoch: 22940 | training loss: 3.1647e-03 | validation loss: 3.7625e-03\n",
      "Epoch: 22950 | training loss: 3.1642e-03 | validation loss: 3.7602e-03\n",
      "Epoch: 22960 | training loss: 3.1638e-03 | validation loss: 3.7580e-03\n",
      "Epoch: 22970 | training loss: 3.1633e-03 | validation loss: 3.7557e-03\n",
      "Epoch: 22980 | training loss: 3.1628e-03 | validation loss: 3.7534e-03\n",
      "Epoch: 22990 | training loss: 3.1623e-03 | validation loss: 3.7512e-03\n",
      "Epoch: 23000 | training loss: 3.1618e-03 | validation loss: 3.7489e-03\n",
      "Epoch: 23010 | training loss: 3.1613e-03 | validation loss: 3.7466e-03\n",
      "Epoch: 23020 | training loss: 3.1608e-03 | validation loss: 3.7443e-03\n",
      "Epoch: 23030 | training loss: 3.1603e-03 | validation loss: 3.7420e-03\n",
      "Epoch: 23040 | training loss: 3.1598e-03 | validation loss: 3.7395e-03\n",
      "Epoch: 23050 | training loss: 3.1596e-03 | validation loss: 3.7345e-03\n",
      "Epoch: 23060 | training loss: 3.2479e-03 | validation loss: 3.7389e-03\n",
      "Epoch: 23070 | training loss: 3.6399e-03 | validation loss: 3.9007e-03\n",
      "Epoch: 23080 | training loss: 3.2647e-03 | validation loss: 3.8514e-03\n",
      "Epoch: 23090 | training loss: 3.2854e-03 | validation loss: 3.8494e-03\n",
      "Epoch: 23100 | training loss: 3.1639e-03 | validation loss: 3.7397e-03\n",
      "Epoch: 23110 | training loss: 3.1662e-03 | validation loss: 3.7277e-03\n",
      "Epoch: 23120 | training loss: 3.1616e-03 | validation loss: 3.7151e-03\n",
      "Epoch: 23130 | training loss: 3.1561e-03 | validation loss: 3.7237e-03\n",
      "Epoch: 23140 | training loss: 3.1556e-03 | validation loss: 3.7207e-03\n",
      "Epoch: 23150 | training loss: 3.1551e-03 | validation loss: 3.7195e-03\n",
      "Epoch: 23160 | training loss: 3.1545e-03 | validation loss: 3.7174e-03\n",
      "Epoch: 23170 | training loss: 3.1539e-03 | validation loss: 3.7138e-03\n",
      "Epoch: 23180 | training loss: 3.1535e-03 | validation loss: 3.7112e-03\n",
      "Epoch: 23190 | training loss: 3.1531e-03 | validation loss: 3.7086e-03\n",
      "Epoch: 23200 | training loss: 3.1533e-03 | validation loss: 3.7037e-03\n",
      "Epoch: 23210 | training loss: 3.1843e-03 | validation loss: 3.6922e-03\n",
      "Epoch: 23220 | training loss: 4.4528e-03 | validation loss: 4.1141e-03\n",
      "Epoch: 23230 | training loss: 3.5690e-03 | validation loss: 3.9907e-03\n",
      "Epoch: 23240 | training loss: 3.1789e-03 | validation loss: 3.7383e-03\n",
      "Epoch: 23250 | training loss: 3.1966e-03 | validation loss: 3.6865e-03\n",
      "Epoch: 23260 | training loss: 3.1550e-03 | validation loss: 3.6874e-03\n",
      "Epoch: 23270 | training loss: 3.1569e-03 | validation loss: 3.7100e-03\n",
      "Epoch: 23280 | training loss: 3.1491e-03 | validation loss: 3.6910e-03\n",
      "Epoch: 23290 | training loss: 3.1493e-03 | validation loss: 3.6864e-03\n",
      "Epoch: 23300 | training loss: 3.1486e-03 | validation loss: 3.6911e-03\n",
      "Epoch: 23310 | training loss: 3.1479e-03 | validation loss: 3.6842e-03\n",
      "Epoch: 23320 | training loss: 3.1474e-03 | validation loss: 3.6850e-03\n",
      "Epoch: 23330 | training loss: 3.1469e-03 | validation loss: 3.6814e-03\n",
      "Epoch: 23340 | training loss: 3.1465e-03 | validation loss: 3.6806e-03\n",
      "Epoch: 23350 | training loss: 3.1460e-03 | validation loss: 3.6780e-03\n",
      "Epoch: 23360 | training loss: 3.1456e-03 | validation loss: 3.6760e-03\n",
      "Epoch: 23370 | training loss: 3.1451e-03 | validation loss: 3.6742e-03\n",
      "Epoch: 23380 | training loss: 3.1446e-03 | validation loss: 3.6723e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23390 | training loss: 3.1442e-03 | validation loss: 3.6705e-03\n",
      "Epoch: 23400 | training loss: 3.1438e-03 | validation loss: 3.6697e-03\n",
      "Epoch: 23410 | training loss: 3.1485e-03 | validation loss: 3.6796e-03\n",
      "Epoch: 23420 | training loss: 3.7765e-03 | validation loss: 4.0746e-03\n",
      "Epoch: 23430 | training loss: 3.7705e-03 | validation loss: 3.8309e-03\n",
      "Epoch: 23440 | training loss: 3.1423e-03 | validation loss: 3.6589e-03\n",
      "Epoch: 23450 | training loss: 3.1789e-03 | validation loss: 3.7051e-03\n",
      "Epoch: 23460 | training loss: 3.1786e-03 | validation loss: 3.7030e-03\n",
      "Epoch: 23470 | training loss: 3.1528e-03 | validation loss: 3.6767e-03\n",
      "Epoch: 23480 | training loss: 3.1405e-03 | validation loss: 3.6548e-03\n",
      "Epoch: 23490 | training loss: 3.1411e-03 | validation loss: 3.6461e-03\n",
      "Epoch: 23500 | training loss: 3.1399e-03 | validation loss: 3.6460e-03\n",
      "Epoch: 23510 | training loss: 3.1391e-03 | validation loss: 3.6484e-03\n",
      "Epoch: 23520 | training loss: 3.1386e-03 | validation loss: 3.6462e-03\n",
      "Epoch: 23530 | training loss: 3.1381e-03 | validation loss: 3.6424e-03\n",
      "Epoch: 23540 | training loss: 3.1377e-03 | validation loss: 3.6414e-03\n",
      "Epoch: 23550 | training loss: 3.1372e-03 | validation loss: 3.6396e-03\n",
      "Epoch: 23560 | training loss: 3.1368e-03 | validation loss: 3.6374e-03\n",
      "Epoch: 23570 | training loss: 3.1364e-03 | validation loss: 3.6358e-03\n",
      "Epoch: 23580 | training loss: 3.1359e-03 | validation loss: 3.6337e-03\n",
      "Epoch: 23590 | training loss: 3.1355e-03 | validation loss: 3.6319e-03\n",
      "Epoch: 23600 | training loss: 3.1350e-03 | validation loss: 3.6300e-03\n",
      "Epoch: 23610 | training loss: 3.1346e-03 | validation loss: 3.6280e-03\n",
      "Epoch: 23620 | training loss: 3.1341e-03 | validation loss: 3.6261e-03\n",
      "Epoch: 23630 | training loss: 3.1337e-03 | validation loss: 3.6242e-03\n",
      "Epoch: 23640 | training loss: 3.1332e-03 | validation loss: 3.6223e-03\n",
      "Epoch: 23650 | training loss: 3.1327e-03 | validation loss: 3.6203e-03\n",
      "Epoch: 23660 | training loss: 3.1323e-03 | validation loss: 3.6178e-03\n",
      "Epoch: 23670 | training loss: 3.1345e-03 | validation loss: 3.6101e-03\n",
      "Epoch: 23680 | training loss: 3.7520e-03 | validation loss: 3.7869e-03\n",
      "Epoch: 23690 | training loss: 4.2295e-03 | validation loss: 4.2871e-03\n",
      "Epoch: 23700 | training loss: 3.4053e-03 | validation loss: 3.8179e-03\n",
      "Epoch: 23710 | training loss: 3.2447e-03 | validation loss: 3.7103e-03\n",
      "Epoch: 23720 | training loss: 3.1769e-03 | validation loss: 3.6584e-03\n",
      "Epoch: 23730 | training loss: 3.1469e-03 | validation loss: 3.6304e-03\n",
      "Epoch: 23740 | training loss: 3.1348e-03 | validation loss: 3.6158e-03\n",
      "Epoch: 23750 | training loss: 3.1301e-03 | validation loss: 3.6077e-03\n",
      "Epoch: 23760 | training loss: 3.1283e-03 | validation loss: 3.6021e-03\n",
      "Epoch: 23770 | training loss: 3.1276e-03 | validation loss: 3.5978e-03\n",
      "Epoch: 23780 | training loss: 3.1272e-03 | validation loss: 3.5949e-03\n",
      "Epoch: 23790 | training loss: 3.1268e-03 | validation loss: 3.5932e-03\n",
      "Epoch: 23800 | training loss: 3.1263e-03 | validation loss: 3.5922e-03\n",
      "Epoch: 23810 | training loss: 3.1259e-03 | validation loss: 3.5908e-03\n",
      "Epoch: 23820 | training loss: 3.1255e-03 | validation loss: 3.5889e-03\n",
      "Epoch: 23830 | training loss: 3.1250e-03 | validation loss: 3.5869e-03\n",
      "Epoch: 23840 | training loss: 3.1246e-03 | validation loss: 3.5851e-03\n",
      "Epoch: 23850 | training loss: 3.1241e-03 | validation loss: 3.5834e-03\n",
      "Epoch: 23860 | training loss: 3.1237e-03 | validation loss: 3.5815e-03\n",
      "Epoch: 23870 | training loss: 3.1233e-03 | validation loss: 3.5797e-03\n",
      "Epoch: 23880 | training loss: 3.1228e-03 | validation loss: 3.5779e-03\n",
      "Epoch: 23890 | training loss: 3.1224e-03 | validation loss: 3.5761e-03\n",
      "Epoch: 23900 | training loss: 3.1219e-03 | validation loss: 3.5742e-03\n",
      "Epoch: 23910 | training loss: 3.1215e-03 | validation loss: 3.5724e-03\n",
      "Epoch: 23920 | training loss: 3.1210e-03 | validation loss: 3.5706e-03\n",
      "Epoch: 23930 | training loss: 3.1206e-03 | validation loss: 3.5690e-03\n",
      "Epoch: 23940 | training loss: 3.1208e-03 | validation loss: 3.5712e-03\n",
      "Epoch: 23950 | training loss: 3.3325e-03 | validation loss: 3.7862e-03\n",
      "Epoch: 23960 | training loss: 3.6416e-03 | validation loss: 3.9093e-03\n",
      "Epoch: 23970 | training loss: 3.2826e-03 | validation loss: 3.5995e-03\n",
      "Epoch: 23980 | training loss: 3.1834e-03 | validation loss: 3.5994e-03\n",
      "Epoch: 23990 | training loss: 3.1387e-03 | validation loss: 3.5537e-03\n",
      "Epoch: 24000 | training loss: 3.1250e-03 | validation loss: 3.5489e-03\n",
      "Epoch: 24010 | training loss: 3.1202e-03 | validation loss: 3.5580e-03\n",
      "Epoch: 24020 | training loss: 3.1185e-03 | validation loss: 3.5572e-03\n",
      "Epoch: 24030 | training loss: 3.1181e-03 | validation loss: 3.5578e-03\n",
      "Epoch: 24040 | training loss: 3.1292e-03 | validation loss: 3.5754e-03\n",
      "Epoch: 24050 | training loss: 3.4187e-03 | validation loss: 3.7793e-03\n",
      "Epoch: 24060 | training loss: 3.1413e-03 | validation loss: 3.5836e-03\n",
      "Epoch: 24070 | training loss: 3.1204e-03 | validation loss: 3.5593e-03\n",
      "Epoch: 24080 | training loss: 3.1384e-03 | validation loss: 3.5280e-03\n",
      "Epoch: 24090 | training loss: 3.1306e-03 | validation loss: 3.5703e-03\n",
      "Epoch: 24100 | training loss: 3.1206e-03 | validation loss: 3.5294e-03\n",
      "Epoch: 24110 | training loss: 3.1155e-03 | validation loss: 3.5479e-03\n",
      "Epoch: 24120 | training loss: 3.1130e-03 | validation loss: 3.5342e-03\n",
      "Epoch: 24130 | training loss: 3.1123e-03 | validation loss: 3.5344e-03\n",
      "Epoch: 24140 | training loss: 3.1121e-03 | validation loss: 3.5367e-03\n",
      "Epoch: 24150 | training loss: 3.1115e-03 | validation loss: 3.5335e-03\n",
      "Epoch: 24160 | training loss: 3.1110e-03 | validation loss: 3.5311e-03\n",
      "Epoch: 24170 | training loss: 3.1106e-03 | validation loss: 3.5297e-03\n",
      "Epoch: 24180 | training loss: 3.1105e-03 | validation loss: 3.5306e-03\n",
      "Epoch: 24190 | training loss: 3.1307e-03 | validation loss: 3.5590e-03\n",
      "Epoch: 24200 | training loss: 4.5314e-03 | validation loss: 4.3620e-03\n",
      "Epoch: 24210 | training loss: 3.6323e-03 | validation loss: 3.6650e-03\n",
      "Epoch: 24220 | training loss: 3.2230e-03 | validation loss: 3.5242e-03\n",
      "Epoch: 24230 | training loss: 3.1116e-03 | validation loss: 3.5295e-03\n",
      "Epoch: 24240 | training loss: 3.1335e-03 | validation loss: 3.5532e-03\n",
      "Epoch: 24250 | training loss: 3.1101e-03 | validation loss: 3.5237e-03\n",
      "Epoch: 24260 | training loss: 3.1093e-03 | validation loss: 3.5069e-03\n",
      "Epoch: 24270 | training loss: 3.1069e-03 | validation loss: 3.5096e-03\n",
      "Epoch: 24280 | training loss: 3.1066e-03 | validation loss: 3.5149e-03\n",
      "Epoch: 24290 | training loss: 3.1058e-03 | validation loss: 3.5094e-03\n",
      "Epoch: 24300 | training loss: 3.1054e-03 | validation loss: 3.5076e-03\n",
      "Epoch: 24310 | training loss: 3.1050e-03 | validation loss: 3.5071e-03\n",
      "Epoch: 24320 | training loss: 3.1045e-03 | validation loss: 3.5045e-03\n",
      "Epoch: 24330 | training loss: 3.1041e-03 | validation loss: 3.5037e-03\n",
      "Epoch: 24340 | training loss: 3.1037e-03 | validation loss: 3.5016e-03\n",
      "Epoch: 24350 | training loss: 3.1033e-03 | validation loss: 3.5002e-03\n",
      "Epoch: 24360 | training loss: 3.1029e-03 | validation loss: 3.4987e-03\n",
      "Epoch: 24370 | training loss: 3.1025e-03 | validation loss: 3.4970e-03\n",
      "Epoch: 24380 | training loss: 3.1020e-03 | validation loss: 3.4954e-03\n",
      "Epoch: 24390 | training loss: 3.1016e-03 | validation loss: 3.4937e-03\n",
      "Epoch: 24400 | training loss: 3.1012e-03 | validation loss: 3.4918e-03\n",
      "Epoch: 24410 | training loss: 3.1012e-03 | validation loss: 3.4876e-03\n",
      "Epoch: 24420 | training loss: 3.1501e-03 | validation loss: 3.4779e-03\n",
      "Epoch: 24430 | training loss: 5.0768e-03 | validation loss: 4.1892e-03\n",
      "Epoch: 24440 | training loss: 3.1913e-03 | validation loss: 3.4796e-03\n",
      "Epoch: 24450 | training loss: 3.1072e-03 | validation loss: 3.5005e-03\n",
      "Epoch: 24460 | training loss: 3.1286e-03 | validation loss: 3.5234e-03\n",
      "Epoch: 24470 | training loss: 3.1225e-03 | validation loss: 3.5176e-03\n",
      "Epoch: 24480 | training loss: 3.1063e-03 | validation loss: 3.4984e-03\n",
      "Epoch: 24490 | training loss: 3.0979e-03 | validation loss: 3.4810e-03\n",
      "Epoch: 24500 | training loss: 3.0977e-03 | validation loss: 3.4728e-03\n",
      "Epoch: 24510 | training loss: 3.0972e-03 | validation loss: 3.4721e-03\n",
      "Epoch: 24520 | training loss: 3.0964e-03 | validation loss: 3.4739e-03\n",
      "Epoch: 24530 | training loss: 3.0960e-03 | validation loss: 3.4731e-03\n",
      "Epoch: 24540 | training loss: 3.0956e-03 | validation loss: 3.4699e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24550 | training loss: 3.0952e-03 | validation loss: 3.4687e-03\n",
      "Epoch: 24560 | training loss: 3.0948e-03 | validation loss: 3.4676e-03\n",
      "Epoch: 24570 | training loss: 3.0943e-03 | validation loss: 3.4656e-03\n",
      "Epoch: 24580 | training loss: 3.0939e-03 | validation loss: 3.4644e-03\n",
      "Epoch: 24590 | training loss: 3.0935e-03 | validation loss: 3.4627e-03\n",
      "Epoch: 24600 | training loss: 3.0931e-03 | validation loss: 3.4613e-03\n",
      "Epoch: 24610 | training loss: 3.0927e-03 | validation loss: 3.4597e-03\n",
      "Epoch: 24620 | training loss: 3.0923e-03 | validation loss: 3.4581e-03\n",
      "Epoch: 24630 | training loss: 3.0919e-03 | validation loss: 3.4562e-03\n",
      "Epoch: 24640 | training loss: 3.0936e-03 | validation loss: 3.4509e-03\n",
      "Epoch: 24650 | training loss: 3.5355e-03 | validation loss: 3.6973e-03\n",
      "Epoch: 24660 | training loss: 3.3881e-03 | validation loss: 3.6732e-03\n",
      "Epoch: 24670 | training loss: 3.2057e-03 | validation loss: 3.5791e-03\n",
      "Epoch: 24680 | training loss: 3.1313e-03 | validation loss: 3.5079e-03\n",
      "Epoch: 24690 | training loss: 3.1039e-03 | validation loss: 3.4649e-03\n",
      "Epoch: 24700 | training loss: 3.0963e-03 | validation loss: 3.4477e-03\n",
      "Epoch: 24710 | training loss: 3.1060e-03 | validation loss: 3.4329e-03\n",
      "Epoch: 24720 | training loss: 3.3093e-03 | validation loss: 3.4572e-03\n",
      "Epoch: 24730 | training loss: 3.2097e-03 | validation loss: 3.4372e-03\n",
      "Epoch: 24740 | training loss: 3.1438e-03 | validation loss: 3.5079e-03\n",
      "Epoch: 24750 | training loss: 3.1122e-03 | validation loss: 3.4233e-03\n",
      "Epoch: 24760 | training loss: 3.1012e-03 | validation loss: 3.4651e-03\n",
      "Epoch: 24770 | training loss: 3.0933e-03 | validation loss: 3.4252e-03\n",
      "Epoch: 24780 | training loss: 3.0865e-03 | validation loss: 3.4393e-03\n",
      "Epoch: 24790 | training loss: 3.0868e-03 | validation loss: 3.4406e-03\n",
      "Epoch: 24800 | training loss: 3.0852e-03 | validation loss: 3.4332e-03\n",
      "Epoch: 24810 | training loss: 3.0850e-03 | validation loss: 3.4292e-03\n",
      "Epoch: 24820 | training loss: 3.0863e-03 | validation loss: 3.4233e-03\n",
      "Epoch: 24830 | training loss: 3.1373e-03 | validation loss: 3.4126e-03\n",
      "Epoch: 24840 | training loss: 4.1426e-03 | validation loss: 3.7332e-03\n",
      "Epoch: 24850 | training loss: 3.3996e-03 | validation loss: 3.6579e-03\n",
      "Epoch: 24860 | training loss: 3.0874e-03 | validation loss: 3.4160e-03\n",
      "Epoch: 24870 | training loss: 3.1081e-03 | validation loss: 3.4113e-03\n",
      "Epoch: 24880 | training loss: 3.0982e-03 | validation loss: 3.4502e-03\n",
      "Epoch: 24890 | training loss: 3.0844e-03 | validation loss: 3.4136e-03\n",
      "Epoch: 24900 | training loss: 3.0816e-03 | validation loss: 3.4206e-03\n",
      "Epoch: 24910 | training loss: 3.0810e-03 | validation loss: 3.4166e-03\n",
      "Epoch: 24920 | training loss: 3.0806e-03 | validation loss: 3.4173e-03\n",
      "Epoch: 24930 | training loss: 3.0803e-03 | validation loss: 3.4134e-03\n",
      "Epoch: 24940 | training loss: 3.0799e-03 | validation loss: 3.4145e-03\n",
      "Epoch: 24950 | training loss: 3.0794e-03 | validation loss: 3.4122e-03\n",
      "Epoch: 24960 | training loss: 3.0790e-03 | validation loss: 3.4100e-03\n",
      "Epoch: 24970 | training loss: 3.0787e-03 | validation loss: 3.4083e-03\n",
      "Epoch: 24980 | training loss: 3.0786e-03 | validation loss: 3.4052e-03\n",
      "Epoch: 24990 | training loss: 3.0865e-03 | validation loss: 3.3959e-03\n",
      "Epoch: 25000 | training loss: 3.5504e-03 | validation loss: 3.5159e-03\n",
      "Epoch: 25010 | training loss: 3.1653e-03 | validation loss: 3.4951e-03\n",
      "Epoch: 25020 | training loss: 3.3112e-03 | validation loss: 3.4380e-03\n",
      "Epoch: 25030 | training loss: 3.0789e-03 | validation loss: 3.3920e-03\n",
      "Epoch: 25040 | training loss: 3.1066e-03 | validation loss: 3.4404e-03\n",
      "Epoch: 25050 | training loss: 3.0760e-03 | validation loss: 3.3980e-03\n",
      "Epoch: 25060 | training loss: 3.0780e-03 | validation loss: 3.3893e-03\n",
      "Epoch: 25070 | training loss: 3.0762e-03 | validation loss: 3.4017e-03\n",
      "Epoch: 25080 | training loss: 3.0746e-03 | validation loss: 3.3929e-03\n",
      "Epoch: 25090 | training loss: 3.0741e-03 | validation loss: 3.3930e-03\n",
      "Epoch: 25100 | training loss: 3.0737e-03 | validation loss: 3.3917e-03\n",
      "Epoch: 25110 | training loss: 3.0733e-03 | validation loss: 3.3903e-03\n",
      "Epoch: 25120 | training loss: 3.0729e-03 | validation loss: 3.3885e-03\n",
      "Epoch: 25130 | training loss: 3.0725e-03 | validation loss: 3.3880e-03\n",
      "Epoch: 25140 | training loss: 3.0721e-03 | validation loss: 3.3862e-03\n",
      "Epoch: 25150 | training loss: 3.0717e-03 | validation loss: 3.3847e-03\n",
      "Epoch: 25160 | training loss: 3.0713e-03 | validation loss: 3.3831e-03\n",
      "Epoch: 25170 | training loss: 3.0710e-03 | validation loss: 3.3809e-03\n",
      "Epoch: 25180 | training loss: 3.0720e-03 | validation loss: 3.3745e-03\n",
      "Epoch: 25190 | training loss: 3.1850e-03 | validation loss: 3.3960e-03\n",
      "Epoch: 25200 | training loss: 3.2972e-03 | validation loss: 3.4038e-03\n",
      "Epoch: 25210 | training loss: 3.2398e-03 | validation loss: 3.4128e-03\n",
      "Epoch: 25220 | training loss: 3.1622e-03 | validation loss: 3.4357e-03\n",
      "Epoch: 25230 | training loss: 3.1014e-03 | validation loss: 3.3545e-03\n",
      "Epoch: 25240 | training loss: 3.0712e-03 | validation loss: 3.3735e-03\n",
      "Epoch: 25250 | training loss: 3.0712e-03 | validation loss: 3.3837e-03\n",
      "Epoch: 25260 | training loss: 3.0682e-03 | validation loss: 3.3766e-03\n",
      "Epoch: 25270 | training loss: 3.0675e-03 | validation loss: 3.3711e-03\n",
      "Epoch: 25280 | training loss: 3.0677e-03 | validation loss: 3.3639e-03\n",
      "Epoch: 25290 | training loss: 3.0948e-03 | validation loss: 3.3508e-03\n",
      "Epoch: 25300 | training loss: 4.0616e-03 | validation loss: 3.6501e-03\n",
      "Epoch: 25310 | training loss: 3.5090e-03 | validation loss: 3.6733e-03\n",
      "Epoch: 25320 | training loss: 3.1032e-03 | validation loss: 3.3503e-03\n",
      "Epoch: 25330 | training loss: 3.0929e-03 | validation loss: 3.3491e-03\n",
      "Epoch: 25340 | training loss: 3.0829e-03 | validation loss: 3.3911e-03\n",
      "Epoch: 25350 | training loss: 3.0647e-03 | validation loss: 3.3574e-03\n",
      "Epoch: 25360 | training loss: 3.0645e-03 | validation loss: 3.3552e-03\n",
      "Epoch: 25370 | training loss: 3.0643e-03 | validation loss: 3.3620e-03\n",
      "Epoch: 25380 | training loss: 3.0635e-03 | validation loss: 3.3533e-03\n",
      "Epoch: 25390 | training loss: 3.0629e-03 | validation loss: 3.3566e-03\n",
      "Epoch: 25400 | training loss: 3.0624e-03 | validation loss: 3.3531e-03\n",
      "Epoch: 25410 | training loss: 3.0620e-03 | validation loss: 3.3523e-03\n",
      "Epoch: 25420 | training loss: 3.0617e-03 | validation loss: 3.3518e-03\n",
      "Epoch: 25430 | training loss: 3.0613e-03 | validation loss: 3.3503e-03\n",
      "Epoch: 25440 | training loss: 3.0609e-03 | validation loss: 3.3489e-03\n",
      "Epoch: 25450 | training loss: 3.0606e-03 | validation loss: 3.3475e-03\n",
      "Epoch: 25460 | training loss: 3.0602e-03 | validation loss: 3.3461e-03\n",
      "Epoch: 25470 | training loss: 3.0601e-03 | validation loss: 3.3426e-03\n",
      "Epoch: 25480 | training loss: 3.0964e-03 | validation loss: 3.3310e-03\n",
      "Epoch: 25490 | training loss: 5.3220e-03 | validation loss: 4.1564e-03\n",
      "Epoch: 25500 | training loss: 3.1191e-03 | validation loss: 3.3285e-03\n",
      "Epoch: 25510 | training loss: 3.0631e-03 | validation loss: 3.3491e-03\n",
      "Epoch: 25520 | training loss: 3.0731e-03 | validation loss: 3.3622e-03\n",
      "Epoch: 25530 | training loss: 3.0730e-03 | validation loss: 3.3630e-03\n",
      "Epoch: 25540 | training loss: 3.0671e-03 | validation loss: 3.3569e-03\n",
      "Epoch: 25550 | training loss: 3.0603e-03 | validation loss: 3.3471e-03\n",
      "Epoch: 25560 | training loss: 3.0568e-03 | validation loss: 3.3375e-03\n",
      "Epoch: 25570 | training loss: 3.0564e-03 | validation loss: 3.3314e-03\n",
      "Epoch: 25580 | training loss: 3.0561e-03 | validation loss: 3.3298e-03\n",
      "Epoch: 25590 | training loss: 3.0555e-03 | validation loss: 3.3309e-03\n",
      "Epoch: 25600 | training loss: 3.0552e-03 | validation loss: 3.3307e-03\n",
      "Epoch: 25610 | training loss: 3.0548e-03 | validation loss: 3.3284e-03\n",
      "Epoch: 25620 | training loss: 3.0545e-03 | validation loss: 3.3272e-03\n",
      "Epoch: 25630 | training loss: 3.0541e-03 | validation loss: 3.3265e-03\n",
      "Epoch: 25640 | training loss: 3.0537e-03 | validation loss: 3.3250e-03\n",
      "Epoch: 25650 | training loss: 3.0534e-03 | validation loss: 3.3240e-03\n",
      "Epoch: 25660 | training loss: 3.0530e-03 | validation loss: 3.3228e-03\n",
      "Epoch: 25670 | training loss: 3.0526e-03 | validation loss: 3.3216e-03\n",
      "Epoch: 25680 | training loss: 3.0523e-03 | validation loss: 3.3204e-03\n",
      "Epoch: 25690 | training loss: 3.0519e-03 | validation loss: 3.3193e-03\n",
      "Epoch: 25700 | training loss: 3.0515e-03 | validation loss: 3.3181e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25710 | training loss: 3.0511e-03 | validation loss: 3.3169e-03\n",
      "Epoch: 25720 | training loss: 3.0507e-03 | validation loss: 3.3157e-03\n",
      "Epoch: 25730 | training loss: 3.0504e-03 | validation loss: 3.3145e-03\n",
      "Epoch: 25740 | training loss: 3.0501e-03 | validation loss: 3.3124e-03\n",
      "Epoch: 25750 | training loss: 3.0730e-03 | validation loss: 3.3127e-03\n",
      "Epoch: 25760 | training loss: 3.4621e-03 | validation loss: 3.5237e-03\n",
      "Epoch: 25770 | training loss: 3.3788e-03 | validation loss: 3.4063e-03\n",
      "Epoch: 25780 | training loss: 3.4406e-03 | validation loss: 3.3701e-03\n",
      "Epoch: 25790 | training loss: 3.1826e-03 | validation loss: 3.4189e-03\n",
      "Epoch: 25800 | training loss: 3.0863e-03 | validation loss: 3.2839e-03\n",
      "Epoch: 25810 | training loss: 3.0537e-03 | validation loss: 3.3220e-03\n",
      "Epoch: 25820 | training loss: 3.0475e-03 | validation loss: 3.3085e-03\n",
      "Epoch: 25830 | training loss: 3.0500e-03 | validation loss: 3.2952e-03\n",
      "Epoch: 25840 | training loss: 3.0466e-03 | validation loss: 3.3010e-03\n",
      "Epoch: 25850 | training loss: 3.0464e-03 | validation loss: 3.3046e-03\n",
      "Epoch: 25860 | training loss: 3.0481e-03 | validation loss: 3.3099e-03\n",
      "Epoch: 25870 | training loss: 3.0886e-03 | validation loss: 3.3563e-03\n",
      "Epoch: 25880 | training loss: 3.8566e-03 | validation loss: 3.8232e-03\n",
      "Epoch: 25890 | training loss: 3.3340e-03 | validation loss: 3.3382e-03\n",
      "Epoch: 25900 | training loss: 3.1375e-03 | validation loss: 3.3911e-03\n",
      "Epoch: 25910 | training loss: 3.0531e-03 | validation loss: 3.2835e-03\n",
      "Epoch: 25920 | training loss: 3.0437e-03 | validation loss: 3.2942e-03\n",
      "Epoch: 25930 | training loss: 3.0437e-03 | validation loss: 3.2961e-03\n",
      "Epoch: 25940 | training loss: 3.0431e-03 | validation loss: 3.2903e-03\n",
      "Epoch: 25950 | training loss: 3.0427e-03 | validation loss: 3.2898e-03\n",
      "Epoch: 25960 | training loss: 3.0426e-03 | validation loss: 3.2925e-03\n",
      "Epoch: 25970 | training loss: 3.0421e-03 | validation loss: 3.2870e-03\n",
      "Epoch: 25980 | training loss: 3.0417e-03 | validation loss: 3.2865e-03\n",
      "Epoch: 25990 | training loss: 3.0413e-03 | validation loss: 3.2863e-03\n",
      "Epoch: 26000 | training loss: 3.0409e-03 | validation loss: 3.2853e-03\n",
      "Epoch: 26010 | training loss: 3.0406e-03 | validation loss: 3.2834e-03\n",
      "Epoch: 26020 | training loss: 3.0421e-03 | validation loss: 3.2770e-03\n",
      "Epoch: 26030 | training loss: 3.2237e-03 | validation loss: 3.2935e-03\n",
      "Epoch: 26040 | training loss: 3.2274e-03 | validation loss: 3.3140e-03\n",
      "Epoch: 26050 | training loss: 3.4333e-03 | validation loss: 3.3707e-03\n",
      "Epoch: 26060 | training loss: 3.0823e-03 | validation loss: 3.2664e-03\n",
      "Epoch: 26070 | training loss: 3.0421e-03 | validation loss: 3.2871e-03\n",
      "Epoch: 26080 | training loss: 3.0543e-03 | validation loss: 3.3025e-03\n",
      "Epoch: 26090 | training loss: 3.0412e-03 | validation loss: 3.2846e-03\n",
      "Epoch: 26100 | training loss: 3.0382e-03 | validation loss: 3.2703e-03\n",
      "Epoch: 26110 | training loss: 3.0379e-03 | validation loss: 3.2701e-03\n",
      "Epoch: 26120 | training loss: 3.0370e-03 | validation loss: 3.2752e-03\n",
      "Epoch: 26130 | training loss: 3.0366e-03 | validation loss: 3.2732e-03\n",
      "Epoch: 26140 | training loss: 3.0362e-03 | validation loss: 3.2699e-03\n",
      "Epoch: 26150 | training loss: 3.0359e-03 | validation loss: 3.2704e-03\n",
      "Epoch: 26160 | training loss: 3.0355e-03 | validation loss: 3.2688e-03\n",
      "Epoch: 26170 | training loss: 3.0352e-03 | validation loss: 3.2680e-03\n",
      "Epoch: 26180 | training loss: 3.0348e-03 | validation loss: 3.2668e-03\n",
      "Epoch: 26190 | training loss: 3.0345e-03 | validation loss: 3.2660e-03\n",
      "Epoch: 26200 | training loss: 3.0341e-03 | validation loss: 3.2648e-03\n",
      "Epoch: 26210 | training loss: 3.0338e-03 | validation loss: 3.2639e-03\n",
      "Epoch: 26220 | training loss: 3.0334e-03 | validation loss: 3.2629e-03\n",
      "Epoch: 26230 | training loss: 3.0331e-03 | validation loss: 3.2619e-03\n",
      "Epoch: 26240 | training loss: 3.0327e-03 | validation loss: 3.2609e-03\n",
      "Epoch: 26250 | training loss: 3.0324e-03 | validation loss: 3.2604e-03\n",
      "Epoch: 26260 | training loss: 3.0332e-03 | validation loss: 3.2648e-03\n",
      "Epoch: 26270 | training loss: 3.1975e-03 | validation loss: 3.3983e-03\n",
      "Epoch: 26280 | training loss: 3.2394e-03 | validation loss: 3.4260e-03\n",
      "Epoch: 26290 | training loss: 3.4858e-03 | validation loss: 3.5761e-03\n",
      "Epoch: 26300 | training loss: 3.1925e-03 | validation loss: 3.3965e-03\n",
      "Epoch: 26310 | training loss: 3.0699e-03 | validation loss: 3.3054e-03\n",
      "Epoch: 26320 | training loss: 3.0338e-03 | validation loss: 3.2645e-03\n",
      "Epoch: 26330 | training loss: 3.0300e-03 | validation loss: 3.2482e-03\n",
      "Epoch: 26340 | training loss: 3.0314e-03 | validation loss: 3.2442e-03\n",
      "Epoch: 26350 | training loss: 3.0298e-03 | validation loss: 3.2455e-03\n",
      "Epoch: 26360 | training loss: 3.0286e-03 | validation loss: 3.2489e-03\n",
      "Epoch: 26370 | training loss: 3.0284e-03 | validation loss: 3.2497e-03\n",
      "Epoch: 26380 | training loss: 3.0280e-03 | validation loss: 3.2467e-03\n",
      "Epoch: 26390 | training loss: 3.0276e-03 | validation loss: 3.2450e-03\n",
      "Epoch: 26400 | training loss: 3.0273e-03 | validation loss: 3.2449e-03\n",
      "Epoch: 26410 | training loss: 3.0269e-03 | validation loss: 3.2438e-03\n",
      "Epoch: 26420 | training loss: 3.0266e-03 | validation loss: 3.2426e-03\n",
      "Epoch: 26430 | training loss: 3.0262e-03 | validation loss: 3.2419e-03\n",
      "Epoch: 26440 | training loss: 3.0259e-03 | validation loss: 3.2407e-03\n",
      "Epoch: 26450 | training loss: 3.0255e-03 | validation loss: 3.2398e-03\n",
      "Epoch: 26460 | training loss: 3.0252e-03 | validation loss: 3.2388e-03\n",
      "Epoch: 26470 | training loss: 3.0248e-03 | validation loss: 3.2376e-03\n",
      "Epoch: 26480 | training loss: 3.0248e-03 | validation loss: 3.2345e-03\n",
      "Epoch: 26490 | training loss: 3.1038e-03 | validation loss: 3.2541e-03\n",
      "Epoch: 26500 | training loss: 3.1047e-03 | validation loss: 3.2548e-03\n",
      "Epoch: 26510 | training loss: 3.0418e-03 | validation loss: 3.2525e-03\n",
      "Epoch: 26520 | training loss: 3.0282e-03 | validation loss: 3.2291e-03\n",
      "Epoch: 26530 | training loss: 3.0271e-03 | validation loss: 3.2208e-03\n",
      "Epoch: 26540 | training loss: 3.0238e-03 | validation loss: 3.2295e-03\n",
      "Epoch: 26550 | training loss: 3.0240e-03 | validation loss: 3.2335e-03\n",
      "Epoch: 26560 | training loss: 3.0260e-03 | validation loss: 3.2399e-03\n",
      "Epoch: 26570 | training loss: 3.0699e-03 | validation loss: 3.2901e-03\n",
      "Epoch: 26580 | training loss: 3.6893e-03 | validation loss: 3.6779e-03\n",
      "Epoch: 26590 | training loss: 3.2215e-03 | validation loss: 3.2380e-03\n",
      "Epoch: 26600 | training loss: 3.1113e-03 | validation loss: 3.3206e-03\n",
      "Epoch: 26610 | training loss: 3.0536e-03 | validation loss: 3.2079e-03\n",
      "Epoch: 26620 | training loss: 3.0322e-03 | validation loss: 3.2492e-03\n",
      "Epoch: 26630 | training loss: 3.0246e-03 | validation loss: 3.2129e-03\n",
      "Epoch: 26640 | training loss: 3.0209e-03 | validation loss: 3.2300e-03\n",
      "Epoch: 26650 | training loss: 3.0189e-03 | validation loss: 3.2208e-03\n",
      "Epoch: 26660 | training loss: 3.0190e-03 | validation loss: 3.2172e-03\n",
      "Epoch: 26670 | training loss: 3.0183e-03 | validation loss: 3.2189e-03\n",
      "Epoch: 26680 | training loss: 3.0179e-03 | validation loss: 3.2193e-03\n",
      "Epoch: 26690 | training loss: 3.0177e-03 | validation loss: 3.2201e-03\n",
      "Epoch: 26700 | training loss: 3.0214e-03 | validation loss: 3.2301e-03\n",
      "Epoch: 26710 | training loss: 3.2612e-03 | validation loss: 3.4128e-03\n",
      "Epoch: 26720 | training loss: 3.0972e-03 | validation loss: 3.2901e-03\n",
      "Epoch: 26730 | training loss: 3.2340e-03 | validation loss: 3.3891e-03\n",
      "Epoch: 26740 | training loss: 3.0362e-03 | validation loss: 3.2057e-03\n",
      "Epoch: 26750 | training loss: 3.0469e-03 | validation loss: 3.2040e-03\n",
      "Epoch: 26760 | training loss: 3.0192e-03 | validation loss: 3.2254e-03\n",
      "Epoch: 26770 | training loss: 3.0171e-03 | validation loss: 3.2190e-03\n",
      "Epoch: 26780 | training loss: 3.0165e-03 | validation loss: 3.2031e-03\n",
      "Epoch: 26790 | training loss: 3.0146e-03 | validation loss: 3.2113e-03\n",
      "Epoch: 26800 | training loss: 3.0141e-03 | validation loss: 3.2083e-03\n",
      "Epoch: 26810 | training loss: 3.0137e-03 | validation loss: 3.2072e-03\n",
      "Epoch: 26820 | training loss: 3.0134e-03 | validation loss: 3.2064e-03\n",
      "Epoch: 26830 | training loss: 3.0131e-03 | validation loss: 3.2058e-03\n",
      "Epoch: 26840 | training loss: 3.0128e-03 | validation loss: 3.2043e-03\n",
      "Epoch: 26850 | training loss: 3.0124e-03 | validation loss: 3.2040e-03\n",
      "Epoch: 26860 | training loss: 3.0121e-03 | validation loss: 3.2032e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26870 | training loss: 3.0118e-03 | validation loss: 3.2022e-03\n",
      "Epoch: 26880 | training loss: 3.0114e-03 | validation loss: 3.2014e-03\n",
      "Epoch: 26890 | training loss: 3.0111e-03 | validation loss: 3.2011e-03\n",
      "Epoch: 26900 | training loss: 3.0120e-03 | validation loss: 3.2056e-03\n",
      "Epoch: 26910 | training loss: 3.1393e-03 | validation loss: 3.3150e-03\n",
      "Epoch: 26920 | training loss: 3.6218e-03 | validation loss: 3.6073e-03\n",
      "Epoch: 26930 | training loss: 3.4315e-03 | validation loss: 3.4996e-03\n",
      "Epoch: 26940 | training loss: 3.0530e-03 | validation loss: 3.2546e-03\n",
      "Epoch: 26950 | training loss: 3.0104e-03 | validation loss: 3.1925e-03\n",
      "Epoch: 26960 | training loss: 3.0216e-03 | validation loss: 3.1801e-03\n",
      "Epoch: 26970 | training loss: 3.0156e-03 | validation loss: 3.1813e-03\n",
      "Epoch: 26980 | training loss: 3.0084e-03 | validation loss: 3.1910e-03\n",
      "Epoch: 26990 | training loss: 3.0087e-03 | validation loss: 3.1970e-03\n",
      "Epoch: 27000 | training loss: 3.0077e-03 | validation loss: 3.1927e-03\n",
      "Epoch: 27010 | training loss: 3.0074e-03 | validation loss: 3.1884e-03\n",
      "Epoch: 27020 | training loss: 3.0069e-03 | validation loss: 3.1890e-03\n",
      "Epoch: 27030 | training loss: 3.0066e-03 | validation loss: 3.1891e-03\n",
      "Epoch: 27040 | training loss: 3.0063e-03 | validation loss: 3.1871e-03\n",
      "Epoch: 27050 | training loss: 3.0060e-03 | validation loss: 3.1869e-03\n",
      "Epoch: 27060 | training loss: 3.0056e-03 | validation loss: 3.1858e-03\n",
      "Epoch: 27070 | training loss: 3.0053e-03 | validation loss: 3.1851e-03\n",
      "Epoch: 27080 | training loss: 3.0050e-03 | validation loss: 3.1841e-03\n",
      "Epoch: 27090 | training loss: 3.0046e-03 | validation loss: 3.1834e-03\n",
      "Epoch: 27100 | training loss: 3.0043e-03 | validation loss: 3.1825e-03\n",
      "Epoch: 27110 | training loss: 3.0040e-03 | validation loss: 3.1811e-03\n",
      "Epoch: 27120 | training loss: 3.0054e-03 | validation loss: 3.1761e-03\n",
      "Epoch: 27130 | training loss: 3.3431e-03 | validation loss: 3.3477e-03\n",
      "Epoch: 27140 | training loss: 3.3656e-03 | validation loss: 3.4624e-03\n",
      "Epoch: 27150 | training loss: 3.0977e-03 | validation loss: 3.2835e-03\n",
      "Epoch: 27160 | training loss: 3.0327e-03 | validation loss: 3.2315e-03\n",
      "Epoch: 27170 | training loss: 3.0110e-03 | validation loss: 3.2027e-03\n",
      "Epoch: 27180 | training loss: 3.0094e-03 | validation loss: 3.1987e-03\n",
      "Epoch: 27190 | training loss: 3.0720e-03 | validation loss: 3.2568e-03\n",
      "Epoch: 27200 | training loss: 3.5298e-03 | validation loss: 3.5456e-03\n",
      "Epoch: 27210 | training loss: 3.1395e-03 | validation loss: 3.1673e-03\n",
      "Epoch: 27220 | training loss: 3.0473e-03 | validation loss: 3.2325e-03\n",
      "Epoch: 27230 | training loss: 3.0128e-03 | validation loss: 3.1568e-03\n",
      "Epoch: 27240 | training loss: 3.0008e-03 | validation loss: 3.1766e-03\n",
      "Epoch: 27250 | training loss: 3.0008e-03 | validation loss: 3.1774e-03\n",
      "Epoch: 27260 | training loss: 3.0011e-03 | validation loss: 3.1630e-03\n",
      "Epoch: 27270 | training loss: 2.9994e-03 | validation loss: 3.1659e-03\n",
      "Epoch: 27280 | training loss: 2.9987e-03 | validation loss: 3.1682e-03\n",
      "Epoch: 27290 | training loss: 2.9984e-03 | validation loss: 3.1688e-03\n",
      "Epoch: 27300 | training loss: 2.9991e-03 | validation loss: 3.1731e-03\n",
      "Epoch: 27310 | training loss: 3.0511e-03 | validation loss: 3.2315e-03\n",
      "Epoch: 27320 | training loss: 4.3951e-03 | validation loss: 4.0040e-03\n",
      "Epoch: 27330 | training loss: 3.1285e-03 | validation loss: 3.1875e-03\n",
      "Epoch: 27340 | training loss: 3.1126e-03 | validation loss: 3.1710e-03\n",
      "Epoch: 27350 | training loss: 3.0047e-03 | validation loss: 3.1823e-03\n",
      "Epoch: 27360 | training loss: 3.0126e-03 | validation loss: 3.1891e-03\n",
      "Epoch: 27370 | training loss: 2.9984e-03 | validation loss: 3.1528e-03\n",
      "Epoch: 27380 | training loss: 2.9964e-03 | validation loss: 3.1557e-03\n",
      "Epoch: 27390 | training loss: 2.9963e-03 | validation loss: 3.1656e-03\n",
      "Epoch: 27400 | training loss: 2.9953e-03 | validation loss: 3.1573e-03\n",
      "Epoch: 27410 | training loss: 2.9947e-03 | validation loss: 3.1596e-03\n",
      "Epoch: 27420 | training loss: 2.9944e-03 | validation loss: 3.1571e-03\n",
      "Epoch: 27430 | training loss: 2.9941e-03 | validation loss: 3.1575e-03\n",
      "Epoch: 27440 | training loss: 2.9938e-03 | validation loss: 3.1558e-03\n",
      "Epoch: 27450 | training loss: 2.9935e-03 | validation loss: 3.1557e-03\n",
      "Epoch: 27460 | training loss: 2.9931e-03 | validation loss: 3.1549e-03\n",
      "Epoch: 27470 | training loss: 2.9928e-03 | validation loss: 3.1539e-03\n",
      "Epoch: 27480 | training loss: 2.9925e-03 | validation loss: 3.1531e-03\n",
      "Epoch: 27490 | training loss: 2.9922e-03 | validation loss: 3.1519e-03\n",
      "Epoch: 27500 | training loss: 2.9923e-03 | validation loss: 3.1487e-03\n",
      "Epoch: 27510 | training loss: 3.0202e-03 | validation loss: 3.1372e-03\n",
      "Epoch: 27520 | training loss: 4.7238e-03 | validation loss: 3.7442e-03\n",
      "Epoch: 27530 | training loss: 3.2413e-03 | validation loss: 3.3445e-03\n",
      "Epoch: 27540 | training loss: 3.1990e-03 | validation loss: 3.3208e-03\n",
      "Epoch: 27550 | training loss: 3.0123e-03 | validation loss: 3.1864e-03\n",
      "Epoch: 27560 | training loss: 2.9943e-03 | validation loss: 3.1401e-03\n",
      "Epoch: 27570 | training loss: 2.9995e-03 | validation loss: 3.1324e-03\n",
      "Epoch: 27580 | training loss: 2.9899e-03 | validation loss: 3.1419e-03\n",
      "Epoch: 27590 | training loss: 2.9903e-03 | validation loss: 3.1521e-03\n",
      "Epoch: 27600 | training loss: 2.9889e-03 | validation loss: 3.1453e-03\n",
      "Epoch: 27610 | training loss: 2.9887e-03 | validation loss: 3.1414e-03\n",
      "Epoch: 27620 | training loss: 2.9883e-03 | validation loss: 3.1437e-03\n",
      "Epoch: 27630 | training loss: 2.9879e-03 | validation loss: 3.1422e-03\n",
      "Epoch: 27640 | training loss: 2.9876e-03 | validation loss: 3.1411e-03\n",
      "Epoch: 27650 | training loss: 2.9873e-03 | validation loss: 3.1410e-03\n",
      "Epoch: 27660 | training loss: 2.9870e-03 | validation loss: 3.1398e-03\n",
      "Epoch: 27670 | training loss: 2.9867e-03 | validation loss: 3.1393e-03\n",
      "Epoch: 27680 | training loss: 2.9864e-03 | validation loss: 3.1386e-03\n",
      "Epoch: 27690 | training loss: 2.9861e-03 | validation loss: 3.1377e-03\n",
      "Epoch: 27700 | training loss: 2.9857e-03 | validation loss: 3.1369e-03\n",
      "Epoch: 27710 | training loss: 2.9855e-03 | validation loss: 3.1353e-03\n",
      "Epoch: 27720 | training loss: 2.9924e-03 | validation loss: 3.1291e-03\n",
      "Epoch: 27730 | training loss: 3.6476e-03 | validation loss: 3.5019e-03\n",
      "Epoch: 27740 | training loss: 3.0463e-03 | validation loss: 3.1499e-03\n",
      "Epoch: 27750 | training loss: 3.0675e-03 | validation loss: 3.1797e-03\n",
      "Epoch: 27760 | training loss: 3.0211e-03 | validation loss: 3.1593e-03\n",
      "Epoch: 27770 | training loss: 2.9999e-03 | validation loss: 3.1376e-03\n",
      "Epoch: 27780 | training loss: 3.0259e-03 | validation loss: 3.1160e-03\n",
      "Epoch: 27790 | training loss: 3.4012e-03 | validation loss: 3.1944e-03\n",
      "Epoch: 27800 | training loss: 3.0001e-03 | validation loss: 3.1610e-03\n",
      "Epoch: 27810 | training loss: 2.9828e-03 | validation loss: 3.1253e-03\n",
      "Epoch: 27820 | training loss: 2.9845e-03 | validation loss: 3.1199e-03\n",
      "Epoch: 27830 | training loss: 2.9893e-03 | validation loss: 3.1466e-03\n",
      "Epoch: 27840 | training loss: 2.9863e-03 | validation loss: 3.1171e-03\n",
      "Epoch: 27850 | training loss: 2.9813e-03 | validation loss: 3.1253e-03\n",
      "Epoch: 27860 | training loss: 2.9820e-03 | validation loss: 3.1325e-03\n",
      "Epoch: 27870 | training loss: 2.9836e-03 | validation loss: 3.1365e-03\n",
      "Epoch: 27880 | training loss: 3.0038e-03 | validation loss: 3.1628e-03\n",
      "Epoch: 27890 | training loss: 3.3653e-03 | validation loss: 3.4089e-03\n",
      "Epoch: 27900 | training loss: 2.9807e-03 | validation loss: 3.1230e-03\n",
      "Epoch: 27910 | training loss: 2.9810e-03 | validation loss: 3.1288e-03\n",
      "Epoch: 27920 | training loss: 2.9820e-03 | validation loss: 3.1153e-03\n",
      "Epoch: 27930 | training loss: 2.9796e-03 | validation loss: 3.1268e-03\n",
      "Epoch: 27940 | training loss: 2.9787e-03 | validation loss: 3.1223e-03\n",
      "Epoch: 27950 | training loss: 2.9798e-03 | validation loss: 3.1141e-03\n",
      "Epoch: 27960 | training loss: 2.9791e-03 | validation loss: 3.1255e-03\n",
      "Epoch: 27970 | training loss: 2.9778e-03 | validation loss: 3.1208e-03\n",
      "Epoch: 27980 | training loss: 2.9774e-03 | validation loss: 3.1169e-03\n",
      "Epoch: 27990 | training loss: 2.9776e-03 | validation loss: 3.1141e-03\n",
      "Epoch: 28000 | training loss: 2.9836e-03 | validation loss: 3.1063e-03\n",
      "Epoch: 28010 | training loss: 3.2059e-03 | validation loss: 3.1430e-03\n",
      "Epoch: 28020 | training loss: 3.1574e-03 | validation loss: 3.1375e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28030 | training loss: 2.9965e-03 | validation loss: 3.0975e-03\n",
      "Epoch: 28040 | training loss: 3.0550e-03 | validation loss: 3.1943e-03\n",
      "Epoch: 28050 | training loss: 2.9921e-03 | validation loss: 3.1034e-03\n",
      "Epoch: 28060 | training loss: 2.9751e-03 | validation loss: 3.1132e-03\n",
      "Epoch: 28070 | training loss: 2.9757e-03 | validation loss: 3.1170e-03\n",
      "Epoch: 28080 | training loss: 2.9752e-03 | validation loss: 3.1081e-03\n",
      "Epoch: 28090 | training loss: 2.9744e-03 | validation loss: 3.1135e-03\n",
      "Epoch: 28100 | training loss: 2.9739e-03 | validation loss: 3.1095e-03\n",
      "Epoch: 28110 | training loss: 2.9736e-03 | validation loss: 3.1090e-03\n",
      "Epoch: 28120 | training loss: 2.9733e-03 | validation loss: 3.1101e-03\n",
      "Epoch: 28130 | training loss: 2.9730e-03 | validation loss: 3.1084e-03\n",
      "Epoch: 28140 | training loss: 2.9727e-03 | validation loss: 3.1073e-03\n",
      "Epoch: 28150 | training loss: 2.9724e-03 | validation loss: 3.1061e-03\n",
      "Epoch: 28160 | training loss: 2.9727e-03 | validation loss: 3.1027e-03\n",
      "Epoch: 28170 | training loss: 3.0034e-03 | validation loss: 3.0935e-03\n",
      "Epoch: 28180 | training loss: 4.3422e-03 | validation loss: 3.5655e-03\n",
      "Epoch: 28190 | training loss: 3.4082e-03 | validation loss: 3.4141e-03\n",
      "Epoch: 28200 | training loss: 3.0167e-03 | validation loss: 3.1514e-03\n",
      "Epoch: 28210 | training loss: 3.0074e-03 | validation loss: 3.0937e-03\n",
      "Epoch: 28220 | training loss: 2.9824e-03 | validation loss: 3.0963e-03\n",
      "Epoch: 28230 | training loss: 2.9749e-03 | validation loss: 3.1120e-03\n",
      "Epoch: 28240 | training loss: 2.9703e-03 | validation loss: 3.1054e-03\n",
      "Epoch: 28250 | training loss: 2.9706e-03 | validation loss: 3.0965e-03\n",
      "Epoch: 28260 | training loss: 2.9694e-03 | validation loss: 3.1024e-03\n",
      "Epoch: 28270 | training loss: 2.9688e-03 | validation loss: 3.0989e-03\n",
      "Epoch: 28280 | training loss: 2.9685e-03 | validation loss: 3.0986e-03\n",
      "Epoch: 28290 | training loss: 2.9682e-03 | validation loss: 3.0981e-03\n",
      "Epoch: 28300 | training loss: 2.9679e-03 | validation loss: 3.0977e-03\n",
      "Epoch: 28310 | training loss: 2.9676e-03 | validation loss: 3.0968e-03\n",
      "Epoch: 28320 | training loss: 2.9674e-03 | validation loss: 3.0975e-03\n",
      "Epoch: 28330 | training loss: 2.9688e-03 | validation loss: 3.1051e-03\n",
      "Epoch: 28340 | training loss: 3.1464e-03 | validation loss: 3.3048e-03\n",
      "Epoch: 28350 | training loss: 3.1653e-03 | validation loss: 3.2269e-03\n",
      "Epoch: 28360 | training loss: 3.0241e-03 | validation loss: 3.1157e-03\n",
      "Epoch: 28370 | training loss: 3.0058e-03 | validation loss: 3.1591e-03\n",
      "Epoch: 28380 | training loss: 2.9742e-03 | validation loss: 3.1161e-03\n",
      "Epoch: 28390 | training loss: 2.9676e-03 | validation loss: 3.0829e-03\n",
      "Epoch: 28400 | training loss: 2.9679e-03 | validation loss: 3.0811e-03\n",
      "Epoch: 28410 | training loss: 2.9662e-03 | validation loss: 3.0833e-03\n",
      "Epoch: 28420 | training loss: 2.9750e-03 | validation loss: 3.0778e-03\n",
      "Epoch: 28430 | training loss: 3.2027e-03 | validation loss: 3.1058e-03\n",
      "Epoch: 28440 | training loss: 3.0973e-03 | validation loss: 3.0924e-03\n",
      "Epoch: 28450 | training loss: 2.9670e-03 | validation loss: 3.0969e-03\n",
      "Epoch: 28460 | training loss: 2.9728e-03 | validation loss: 3.1058e-03\n",
      "Epoch: 28470 | training loss: 2.9747e-03 | validation loss: 3.0737e-03\n",
      "Epoch: 28480 | training loss: 2.9687e-03 | validation loss: 3.1030e-03\n",
      "Epoch: 28490 | training loss: 2.9643e-03 | validation loss: 3.0802e-03\n",
      "Epoch: 28500 | training loss: 2.9624e-03 | validation loss: 3.0874e-03\n",
      "Epoch: 28510 | training loss: 2.9621e-03 | validation loss: 3.0872e-03\n",
      "Epoch: 28520 | training loss: 2.9619e-03 | validation loss: 3.0820e-03\n",
      "Epoch: 28530 | training loss: 2.9614e-03 | validation loss: 3.0824e-03\n",
      "Epoch: 28540 | training loss: 2.9611e-03 | validation loss: 3.0828e-03\n",
      "Epoch: 28550 | training loss: 2.9608e-03 | validation loss: 3.0818e-03\n",
      "Epoch: 28560 | training loss: 2.9611e-03 | validation loss: 3.0783e-03\n",
      "Epoch: 28570 | training loss: 2.9962e-03 | validation loss: 3.0665e-03\n",
      "Epoch: 28580 | training loss: 4.5485e-03 | validation loss: 3.6077e-03\n",
      "Epoch: 28590 | training loss: 3.2108e-03 | validation loss: 3.2735e-03\n",
      "Epoch: 28600 | training loss: 3.1089e-03 | validation loss: 3.2149e-03\n",
      "Epoch: 28610 | training loss: 2.9618e-03 | validation loss: 3.0876e-03\n",
      "Epoch: 28620 | training loss: 2.9800e-03 | validation loss: 3.0663e-03\n",
      "Epoch: 28630 | training loss: 2.9604e-03 | validation loss: 3.0691e-03\n",
      "Epoch: 28640 | training loss: 2.9608e-03 | validation loss: 3.0863e-03\n",
      "Epoch: 28650 | training loss: 2.9582e-03 | validation loss: 3.0799e-03\n",
      "Epoch: 28660 | training loss: 2.9580e-03 | validation loss: 3.0729e-03\n",
      "Epoch: 28670 | training loss: 2.9575e-03 | validation loss: 3.0775e-03\n",
      "Epoch: 28680 | training loss: 2.9572e-03 | validation loss: 3.0752e-03\n",
      "Epoch: 28690 | training loss: 2.9569e-03 | validation loss: 3.0746e-03\n",
      "Epoch: 28700 | training loss: 2.9566e-03 | validation loss: 3.0744e-03\n",
      "Epoch: 28710 | training loss: 2.9563e-03 | validation loss: 3.0737e-03\n",
      "Epoch: 28720 | training loss: 2.9560e-03 | validation loss: 3.0729e-03\n",
      "Epoch: 28730 | training loss: 2.9557e-03 | validation loss: 3.0727e-03\n",
      "Epoch: 28740 | training loss: 2.9554e-03 | validation loss: 3.0721e-03\n",
      "Epoch: 28750 | training loss: 2.9551e-03 | validation loss: 3.0714e-03\n",
      "Epoch: 28760 | training loss: 2.9548e-03 | validation loss: 3.0709e-03\n",
      "Epoch: 28770 | training loss: 2.9546e-03 | validation loss: 3.0703e-03\n",
      "Epoch: 28780 | training loss: 2.9560e-03 | validation loss: 3.0717e-03\n",
      "Epoch: 28790 | training loss: 3.1839e-03 | validation loss: 3.2359e-03\n",
      "Epoch: 28800 | training loss: 4.0698e-03 | validation loss: 3.4476e-03\n",
      "Epoch: 28810 | training loss: 3.2912e-03 | validation loss: 3.2919e-03\n",
      "Epoch: 28820 | training loss: 3.0422e-03 | validation loss: 3.0675e-03\n",
      "Epoch: 28830 | training loss: 2.9777e-03 | validation loss: 3.0506e-03\n",
      "Epoch: 28840 | training loss: 2.9623e-03 | validation loss: 3.0833e-03\n",
      "Epoch: 28850 | training loss: 2.9555e-03 | validation loss: 3.0575e-03\n",
      "Epoch: 28860 | training loss: 2.9533e-03 | validation loss: 3.0742e-03\n",
      "Epoch: 28870 | training loss: 2.9524e-03 | validation loss: 3.0634e-03\n",
      "Epoch: 28880 | training loss: 2.9518e-03 | validation loss: 3.0684e-03\n",
      "Epoch: 28890 | training loss: 2.9514e-03 | validation loss: 3.0633e-03\n",
      "Epoch: 28900 | training loss: 2.9510e-03 | validation loss: 3.0635e-03\n",
      "Epoch: 28910 | training loss: 2.9508e-03 | validation loss: 3.0637e-03\n",
      "Epoch: 28920 | training loss: 2.9505e-03 | validation loss: 3.0632e-03\n",
      "Epoch: 28930 | training loss: 2.9503e-03 | validation loss: 3.0634e-03\n",
      "Epoch: 28940 | training loss: 2.9510e-03 | validation loss: 3.0680e-03\n",
      "Epoch: 28950 | training loss: 2.9953e-03 | validation loss: 3.1219e-03\n",
      "Epoch: 28960 | training loss: 4.2391e-03 | validation loss: 3.8504e-03\n",
      "Epoch: 28970 | training loss: 3.2031e-03 | validation loss: 3.1033e-03\n",
      "Epoch: 28980 | training loss: 2.9930e-03 | validation loss: 3.0501e-03\n",
      "Epoch: 28990 | training loss: 2.9889e-03 | validation loss: 3.1133e-03\n",
      "Epoch: 29000 | training loss: 2.9502e-03 | validation loss: 3.0676e-03\n",
      "Epoch: 29010 | training loss: 2.9550e-03 | validation loss: 3.0475e-03\n",
      "Epoch: 29020 | training loss: 2.9489e-03 | validation loss: 3.0632e-03\n",
      "Epoch: 29030 | training loss: 2.9476e-03 | validation loss: 3.0568e-03\n",
      "Epoch: 29040 | training loss: 2.9474e-03 | validation loss: 3.0547e-03\n",
      "Epoch: 29050 | training loss: 2.9471e-03 | validation loss: 3.0571e-03\n",
      "Epoch: 29060 | training loss: 2.9468e-03 | validation loss: 3.0549e-03\n",
      "Epoch: 29070 | training loss: 2.9465e-03 | validation loss: 3.0550e-03\n",
      "Epoch: 29080 | training loss: 2.9462e-03 | validation loss: 3.0548e-03\n",
      "Epoch: 29090 | training loss: 2.9459e-03 | validation loss: 3.0537e-03\n",
      "Epoch: 29100 | training loss: 2.9457e-03 | validation loss: 3.0532e-03\n",
      "Epoch: 29110 | training loss: 2.9454e-03 | validation loss: 3.0527e-03\n",
      "Epoch: 29120 | training loss: 2.9451e-03 | validation loss: 3.0518e-03\n",
      "Epoch: 29130 | training loss: 2.9452e-03 | validation loss: 3.0490e-03\n",
      "Epoch: 29140 | training loss: 2.9663e-03 | validation loss: 3.0368e-03\n",
      "Epoch: 29150 | training loss: 4.4156e-03 | validation loss: 3.5287e-03\n",
      "Epoch: 29160 | training loss: 3.4528e-03 | validation loss: 3.4010e-03\n",
      "Epoch: 29170 | training loss: 3.0818e-03 | validation loss: 3.1775e-03\n",
      "Epoch: 29180 | training loss: 2.9457e-03 | validation loss: 3.0546e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29190 | training loss: 2.9662e-03 | validation loss: 3.0391e-03\n",
      "Epoch: 29200 | training loss: 2.9478e-03 | validation loss: 3.0378e-03\n",
      "Epoch: 29210 | training loss: 2.9442e-03 | validation loss: 3.0519e-03\n",
      "Epoch: 29220 | training loss: 2.9432e-03 | validation loss: 3.0528e-03\n",
      "Epoch: 29230 | training loss: 2.9425e-03 | validation loss: 3.0455e-03\n",
      "Epoch: 29240 | training loss: 2.9419e-03 | validation loss: 3.0456e-03\n",
      "Epoch: 29250 | training loss: 2.9417e-03 | validation loss: 3.0471e-03\n",
      "Epoch: 29260 | training loss: 2.9414e-03 | validation loss: 3.0450e-03\n",
      "Epoch: 29270 | training loss: 2.9411e-03 | validation loss: 3.0452e-03\n",
      "Epoch: 29280 | training loss: 2.9408e-03 | validation loss: 3.0444e-03\n",
      "Epoch: 29290 | training loss: 2.9406e-03 | validation loss: 3.0441e-03\n",
      "Epoch: 29300 | training loss: 2.9403e-03 | validation loss: 3.0435e-03\n",
      "Epoch: 29310 | training loss: 2.9400e-03 | validation loss: 3.0431e-03\n",
      "Epoch: 29320 | training loss: 2.9397e-03 | validation loss: 3.0426e-03\n",
      "Epoch: 29330 | training loss: 2.9395e-03 | validation loss: 3.0421e-03\n",
      "Epoch: 29340 | training loss: 2.9392e-03 | validation loss: 3.0417e-03\n",
      "Epoch: 29350 | training loss: 2.9389e-03 | validation loss: 3.0413e-03\n",
      "Epoch: 29360 | training loss: 2.9390e-03 | validation loss: 3.0419e-03\n",
      "Epoch: 29370 | training loss: 2.9717e-03 | validation loss: 3.0701e-03\n",
      "Epoch: 29380 | training loss: 4.4826e-03 | validation loss: 3.9573e-03\n",
      "Epoch: 29390 | training loss: 3.5356e-03 | validation loss: 3.2047e-03\n",
      "Epoch: 29400 | training loss: 3.1085e-03 | validation loss: 3.0538e-03\n",
      "Epoch: 29410 | training loss: 2.9785e-03 | validation loss: 3.0498e-03\n",
      "Epoch: 29420 | training loss: 2.9531e-03 | validation loss: 3.0561e-03\n",
      "Epoch: 29430 | training loss: 2.9369e-03 | validation loss: 3.0374e-03\n",
      "Epoch: 29440 | training loss: 2.9381e-03 | validation loss: 3.0321e-03\n",
      "Epoch: 29450 | training loss: 2.9364e-03 | validation loss: 3.0390e-03\n",
      "Epoch: 29460 | training loss: 2.9362e-03 | validation loss: 3.0391e-03\n",
      "Epoch: 29470 | training loss: 2.9359e-03 | validation loss: 3.0344e-03\n",
      "Epoch: 29480 | training loss: 2.9356e-03 | validation loss: 3.0357e-03\n",
      "Epoch: 29490 | training loss: 2.9353e-03 | validation loss: 3.0341e-03\n",
      "Epoch: 29500 | training loss: 2.9350e-03 | validation loss: 3.0340e-03\n",
      "Epoch: 29510 | training loss: 2.9348e-03 | validation loss: 3.0334e-03\n",
      "Epoch: 29520 | training loss: 2.9345e-03 | validation loss: 3.0334e-03\n",
      "Epoch: 29530 | training loss: 2.9342e-03 | validation loss: 3.0328e-03\n",
      "Epoch: 29540 | training loss: 2.9340e-03 | validation loss: 3.0324e-03\n",
      "Epoch: 29550 | training loss: 2.9337e-03 | validation loss: 3.0319e-03\n",
      "Epoch: 29560 | training loss: 2.9334e-03 | validation loss: 3.0315e-03\n",
      "Epoch: 29570 | training loss: 2.9332e-03 | validation loss: 3.0311e-03\n",
      "Epoch: 29580 | training loss: 2.9329e-03 | validation loss: 3.0306e-03\n",
      "Epoch: 29590 | training loss: 2.9327e-03 | validation loss: 3.0295e-03\n",
      "Epoch: 29600 | training loss: 2.9346e-03 | validation loss: 3.0221e-03\n",
      "Epoch: 29610 | training loss: 3.6049e-03 | validation loss: 3.1854e-03\n",
      "Epoch: 29620 | training loss: 4.2249e-03 | validation loss: 3.8249e-03\n",
      "Epoch: 29630 | training loss: 3.3184e-03 | validation loss: 3.3178e-03\n",
      "Epoch: 29640 | training loss: 3.0618e-03 | validation loss: 3.1435e-03\n",
      "Epoch: 29650 | training loss: 2.9727e-03 | validation loss: 3.0670e-03\n",
      "Epoch: 29660 | training loss: 2.9458e-03 | validation loss: 3.0383e-03\n",
      "Epoch: 29670 | training loss: 2.9365e-03 | validation loss: 3.0287e-03\n",
      "Epoch: 29680 | training loss: 2.9328e-03 | validation loss: 3.0262e-03\n",
      "Epoch: 29690 | training loss: 2.9311e-03 | validation loss: 3.0255e-03\n",
      "Epoch: 29700 | training loss: 2.9302e-03 | validation loss: 3.0250e-03\n",
      "Epoch: 29710 | training loss: 2.9297e-03 | validation loss: 3.0245e-03\n",
      "Epoch: 29720 | training loss: 2.9294e-03 | validation loss: 3.0241e-03\n",
      "Epoch: 29730 | training loss: 2.9292e-03 | validation loss: 3.0239e-03\n",
      "Epoch: 29740 | training loss: 2.9289e-03 | validation loss: 3.0237e-03\n",
      "Epoch: 29750 | training loss: 2.9287e-03 | validation loss: 3.0234e-03\n",
      "Epoch: 29760 | training loss: 2.9284e-03 | validation loss: 3.0229e-03\n",
      "Epoch: 29770 | training loss: 2.9282e-03 | validation loss: 3.0224e-03\n",
      "Epoch: 29780 | training loss: 2.9279e-03 | validation loss: 3.0220e-03\n",
      "Epoch: 29790 | training loss: 2.9277e-03 | validation loss: 3.0216e-03\n",
      "Epoch: 29800 | training loss: 2.9274e-03 | validation loss: 3.0212e-03\n",
      "Epoch: 29810 | training loss: 2.9271e-03 | validation loss: 3.0208e-03\n",
      "Epoch: 29820 | training loss: 2.9269e-03 | validation loss: 3.0204e-03\n",
      "Epoch: 29830 | training loss: 2.9266e-03 | validation loss: 3.0199e-03\n",
      "Epoch: 29840 | training loss: 2.9264e-03 | validation loss: 3.0195e-03\n",
      "Epoch: 29850 | training loss: 2.9261e-03 | validation loss: 3.0191e-03\n",
      "Epoch: 29860 | training loss: 2.9258e-03 | validation loss: 3.0187e-03\n",
      "Epoch: 29870 | training loss: 2.9256e-03 | validation loss: 3.0183e-03\n",
      "Epoch: 29880 | training loss: 2.9253e-03 | validation loss: 3.0178e-03\n",
      "Epoch: 29890 | training loss: 2.9250e-03 | validation loss: 3.0174e-03\n",
      "Epoch: 29900 | training loss: 2.9248e-03 | validation loss: 3.0170e-03\n",
      "Epoch: 29910 | training loss: 2.9245e-03 | validation loss: 3.0165e-03\n",
      "Epoch: 29920 | training loss: 2.9243e-03 | validation loss: 3.0153e-03\n",
      "Epoch: 29930 | training loss: 2.9480e-03 | validation loss: 3.0149e-03\n",
      "Epoch: 29940 | training loss: 5.5859e-03 | validation loss: 4.0482e-03\n",
      "Epoch: 29950 | training loss: 3.2457e-03 | validation loss: 3.3066e-03\n",
      "Epoch: 29960 | training loss: 2.9311e-03 | validation loss: 3.0211e-03\n",
      "Epoch: 29970 | training loss: 2.9847e-03 | validation loss: 3.0300e-03\n",
      "Epoch: 29980 | training loss: 2.9253e-03 | validation loss: 3.0270e-03\n",
      "Epoch: 29990 | training loss: 2.9320e-03 | validation loss: 3.0403e-03\n",
      "Epoch: 30000 | training loss: 2.9238e-03 | validation loss: 3.0211e-03\n",
      "Epoch: 30010 | training loss: 2.9237e-03 | validation loss: 3.0145e-03\n",
      "Epoch: 30020 | training loss: 2.9221e-03 | validation loss: 3.0129e-03\n",
      "Epoch: 30030 | training loss: 2.9216e-03 | validation loss: 3.0119e-03\n",
      "Epoch: 30040 | training loss: 2.9214e-03 | validation loss: 3.0110e-03\n",
      "Epoch: 30050 | training loss: 2.9211e-03 | validation loss: 3.0109e-03\n",
      "Epoch: 30060 | training loss: 2.9209e-03 | validation loss: 3.0109e-03\n",
      "Epoch: 30070 | training loss: 2.9206e-03 | validation loss: 3.0101e-03\n",
      "Epoch: 30080 | training loss: 2.9204e-03 | validation loss: 3.0094e-03\n",
      "Epoch: 30090 | training loss: 2.9202e-03 | validation loss: 3.0092e-03\n",
      "Epoch: 30100 | training loss: 2.9199e-03 | validation loss: 3.0089e-03\n",
      "Epoch: 30110 | training loss: 2.9197e-03 | validation loss: 3.0085e-03\n",
      "Epoch: 30120 | training loss: 2.9194e-03 | validation loss: 3.0081e-03\n",
      "Epoch: 30130 | training loss: 2.9192e-03 | validation loss: 3.0078e-03\n",
      "Epoch: 30140 | training loss: 2.9189e-03 | validation loss: 3.0074e-03\n",
      "Epoch: 30150 | training loss: 2.9187e-03 | validation loss: 3.0070e-03\n",
      "Epoch: 30160 | training loss: 2.9184e-03 | validation loss: 3.0066e-03\n",
      "Epoch: 30170 | training loss: 2.9182e-03 | validation loss: 3.0063e-03\n",
      "Epoch: 30180 | training loss: 2.9179e-03 | validation loss: 3.0059e-03\n",
      "Epoch: 30190 | training loss: 2.9177e-03 | validation loss: 3.0055e-03\n",
      "Epoch: 30200 | training loss: 2.9174e-03 | validation loss: 3.0051e-03\n",
      "Epoch: 30210 | training loss: 2.9172e-03 | validation loss: 3.0045e-03\n",
      "Epoch: 30220 | training loss: 2.9172e-03 | validation loss: 3.0011e-03\n",
      "Epoch: 30230 | training loss: 2.9919e-03 | validation loss: 2.9845e-03\n",
      "Epoch: 30240 | training loss: 4.0017e-03 | validation loss: 3.3509e-03\n",
      "Epoch: 30250 | training loss: 3.4707e-03 | validation loss: 3.1674e-03\n",
      "Epoch: 30260 | training loss: 3.0982e-03 | validation loss: 3.0693e-03\n",
      "Epoch: 30270 | training loss: 2.9857e-03 | validation loss: 3.0400e-03\n",
      "Epoch: 30280 | training loss: 2.9375e-03 | validation loss: 3.0127e-03\n",
      "Epoch: 30290 | training loss: 2.9213e-03 | validation loss: 2.9989e-03\n",
      "Epoch: 30300 | training loss: 2.9172e-03 | validation loss: 2.9952e-03\n",
      "Epoch: 30310 | training loss: 2.9157e-03 | validation loss: 2.9955e-03\n",
      "Epoch: 30320 | training loss: 2.9148e-03 | validation loss: 2.9971e-03\n",
      "Epoch: 30330 | training loss: 2.9143e-03 | validation loss: 2.9987e-03\n",
      "Epoch: 30340 | training loss: 2.9141e-03 | validation loss: 2.9998e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30350 | training loss: 2.9138e-03 | validation loss: 2.9999e-03\n",
      "Epoch: 30360 | training loss: 2.9136e-03 | validation loss: 2.9993e-03\n",
      "Epoch: 30370 | training loss: 2.9133e-03 | validation loss: 2.9986e-03\n",
      "Epoch: 30380 | training loss: 2.9131e-03 | validation loss: 2.9983e-03\n",
      "Epoch: 30390 | training loss: 2.9129e-03 | validation loss: 2.9981e-03\n",
      "Epoch: 30400 | training loss: 2.9126e-03 | validation loss: 2.9976e-03\n",
      "Epoch: 30410 | training loss: 2.9124e-03 | validation loss: 2.9972e-03\n",
      "Epoch: 30420 | training loss: 2.9121e-03 | validation loss: 2.9969e-03\n",
      "Epoch: 30430 | training loss: 2.9119e-03 | validation loss: 2.9965e-03\n",
      "Epoch: 30440 | training loss: 2.9116e-03 | validation loss: 2.9962e-03\n",
      "Epoch: 30450 | training loss: 2.9114e-03 | validation loss: 2.9958e-03\n",
      "Epoch: 30460 | training loss: 2.9111e-03 | validation loss: 2.9954e-03\n",
      "Epoch: 30470 | training loss: 2.9109e-03 | validation loss: 2.9951e-03\n",
      "Epoch: 30480 | training loss: 2.9106e-03 | validation loss: 2.9947e-03\n",
      "Epoch: 30490 | training loss: 2.9104e-03 | validation loss: 2.9943e-03\n",
      "Epoch: 30500 | training loss: 2.9101e-03 | validation loss: 2.9940e-03\n",
      "Epoch: 30510 | training loss: 2.9099e-03 | validation loss: 2.9936e-03\n",
      "Epoch: 30520 | training loss: 2.9096e-03 | validation loss: 2.9932e-03\n",
      "Epoch: 30530 | training loss: 2.9094e-03 | validation loss: 2.9922e-03\n",
      "Epoch: 30540 | training loss: 2.9123e-03 | validation loss: 2.9852e-03\n",
      "Epoch: 30550 | training loss: 3.8679e-03 | validation loss: 3.2893e-03\n",
      "Epoch: 30560 | training loss: 4.2581e-03 | validation loss: 3.8143e-03\n",
      "Epoch: 30570 | training loss: 3.3334e-03 | validation loss: 3.2917e-03\n",
      "Epoch: 30580 | training loss: 2.9966e-03 | validation loss: 3.0764e-03\n",
      "Epoch: 30590 | training loss: 2.9147e-03 | validation loss: 3.0060e-03\n",
      "Epoch: 30600 | training loss: 2.9078e-03 | validation loss: 2.9897e-03\n",
      "Epoch: 30610 | training loss: 2.9083e-03 | validation loss: 2.9861e-03\n",
      "Epoch: 30620 | training loss: 2.9078e-03 | validation loss: 2.9861e-03\n",
      "Epoch: 30630 | training loss: 2.9072e-03 | validation loss: 2.9873e-03\n",
      "Epoch: 30640 | training loss: 2.9068e-03 | validation loss: 2.9884e-03\n",
      "Epoch: 30650 | training loss: 2.9065e-03 | validation loss: 2.9886e-03\n",
      "Epoch: 30660 | training loss: 2.9063e-03 | validation loss: 2.9883e-03\n",
      "Epoch: 30670 | training loss: 2.9060e-03 | validation loss: 2.9880e-03\n",
      "Epoch: 30680 | training loss: 2.9058e-03 | validation loss: 2.9875e-03\n",
      "Epoch: 30690 | training loss: 2.9055e-03 | validation loss: 2.9869e-03\n",
      "Epoch: 30700 | training loss: 2.9053e-03 | validation loss: 2.9865e-03\n",
      "Epoch: 30710 | training loss: 2.9051e-03 | validation loss: 2.9862e-03\n",
      "Epoch: 30720 | training loss: 2.9048e-03 | validation loss: 2.9859e-03\n",
      "Epoch: 30730 | training loss: 2.9046e-03 | validation loss: 2.9856e-03\n",
      "Epoch: 30740 | training loss: 2.9043e-03 | validation loss: 2.9852e-03\n",
      "Epoch: 30750 | training loss: 2.9041e-03 | validation loss: 2.9849e-03\n",
      "Epoch: 30760 | training loss: 2.9038e-03 | validation loss: 2.9845e-03\n",
      "Epoch: 30770 | training loss: 2.9036e-03 | validation loss: 2.9842e-03\n",
      "Epoch: 30780 | training loss: 2.9033e-03 | validation loss: 2.9838e-03\n",
      "Epoch: 30790 | training loss: 2.9031e-03 | validation loss: 2.9835e-03\n",
      "Epoch: 30800 | training loss: 2.9028e-03 | validation loss: 2.9832e-03\n",
      "Epoch: 30810 | training loss: 2.9026e-03 | validation loss: 2.9833e-03\n",
      "Epoch: 30820 | training loss: 2.9031e-03 | validation loss: 2.9892e-03\n",
      "Epoch: 30830 | training loss: 3.1501e-03 | validation loss: 3.2610e-03\n",
      "Epoch: 30840 | training loss: 3.3758e-03 | validation loss: 3.2580e-03\n",
      "Epoch: 30850 | training loss: 3.0485e-03 | validation loss: 3.0180e-03\n",
      "Epoch: 30860 | training loss: 2.9570e-03 | validation loss: 2.9872e-03\n",
      "Epoch: 30870 | training loss: 2.9185e-03 | validation loss: 2.9741e-03\n",
      "Epoch: 30880 | training loss: 2.9056e-03 | validation loss: 2.9757e-03\n",
      "Epoch: 30890 | training loss: 2.9024e-03 | validation loss: 2.9719e-03\n",
      "Epoch: 30900 | training loss: 2.9013e-03 | validation loss: 2.9810e-03\n",
      "Epoch: 30910 | training loss: 2.9004e-03 | validation loss: 2.9774e-03\n",
      "Epoch: 30920 | training loss: 2.9002e-03 | validation loss: 2.9757e-03\n",
      "Epoch: 30930 | training loss: 2.9000e-03 | validation loss: 2.9748e-03\n",
      "Epoch: 30940 | training loss: 2.9017e-03 | validation loss: 2.9699e-03\n",
      "Epoch: 30950 | training loss: 2.9578e-03 | validation loss: 2.9577e-03\n",
      "Epoch: 30960 | training loss: 3.8827e-03 | validation loss: 3.2364e-03\n",
      "Epoch: 30970 | training loss: 3.1843e-03 | validation loss: 3.1992e-03\n",
      "Epoch: 30980 | training loss: 2.9068e-03 | validation loss: 2.9627e-03\n",
      "Epoch: 30990 | training loss: 2.9142e-03 | validation loss: 2.9600e-03\n",
      "Epoch: 31000 | training loss: 2.9132e-03 | validation loss: 3.0039e-03\n",
      "Epoch: 31010 | training loss: 2.9026e-03 | validation loss: 2.9659e-03\n",
      "Epoch: 31020 | training loss: 2.8989e-03 | validation loss: 2.9826e-03\n",
      "Epoch: 31030 | training loss: 2.8979e-03 | validation loss: 2.9719e-03\n",
      "Epoch: 31040 | training loss: 2.8975e-03 | validation loss: 2.9784e-03\n",
      "Epoch: 31050 | training loss: 2.8970e-03 | validation loss: 2.9730e-03\n",
      "Epoch: 31060 | training loss: 2.8967e-03 | validation loss: 2.9744e-03\n",
      "Epoch: 31070 | training loss: 2.8965e-03 | validation loss: 2.9752e-03\n",
      "Epoch: 31080 | training loss: 2.8962e-03 | validation loss: 2.9748e-03\n",
      "Epoch: 31090 | training loss: 2.8960e-03 | validation loss: 2.9752e-03\n",
      "Epoch: 31100 | training loss: 2.8971e-03 | validation loss: 2.9802e-03\n",
      "Epoch: 31110 | training loss: 2.9529e-03 | validation loss: 3.0408e-03\n",
      "Epoch: 31120 | training loss: 4.2240e-03 | validation loss: 3.7809e-03\n",
      "Epoch: 31130 | training loss: 3.0670e-03 | validation loss: 2.9957e-03\n",
      "Epoch: 31140 | training loss: 2.9712e-03 | validation loss: 2.9638e-03\n",
      "Epoch: 31150 | training loss: 2.9209e-03 | validation loss: 3.0076e-03\n",
      "Epoch: 31160 | training loss: 2.9010e-03 | validation loss: 2.9854e-03\n",
      "Epoch: 31170 | training loss: 2.9005e-03 | validation loss: 2.9603e-03\n",
      "Epoch: 31180 | training loss: 2.8941e-03 | validation loss: 2.9739e-03\n",
      "Epoch: 31190 | training loss: 2.8938e-03 | validation loss: 2.9731e-03\n",
      "Epoch: 31200 | training loss: 2.8936e-03 | validation loss: 2.9673e-03\n",
      "Epoch: 31210 | training loss: 2.8933e-03 | validation loss: 2.9716e-03\n",
      "Epoch: 31220 | training loss: 2.8930e-03 | validation loss: 2.9684e-03\n",
      "Epoch: 31230 | training loss: 2.8927e-03 | validation loss: 2.9695e-03\n",
      "Epoch: 31240 | training loss: 2.8924e-03 | validation loss: 2.9689e-03\n",
      "Epoch: 31250 | training loss: 2.8922e-03 | validation loss: 2.9681e-03\n",
      "Epoch: 31260 | training loss: 2.8920e-03 | validation loss: 2.9681e-03\n",
      "Epoch: 31270 | training loss: 2.8917e-03 | validation loss: 2.9679e-03\n",
      "Epoch: 31280 | training loss: 2.8915e-03 | validation loss: 2.9678e-03\n",
      "Epoch: 31290 | training loss: 2.8913e-03 | validation loss: 2.9685e-03\n",
      "Epoch: 31300 | training loss: 2.8943e-03 | validation loss: 2.9778e-03\n",
      "Epoch: 31310 | training loss: 3.2405e-03 | validation loss: 3.2255e-03\n",
      "Epoch: 31320 | training loss: 2.9506e-03 | validation loss: 2.9530e-03\n",
      "Epoch: 31330 | training loss: 3.1104e-03 | validation loss: 3.1449e-03\n",
      "Epoch: 31340 | training loss: 3.0162e-03 | validation loss: 3.0830e-03\n",
      "Epoch: 31350 | training loss: 2.9072e-03 | validation loss: 2.9952e-03\n",
      "Epoch: 31360 | training loss: 2.8906e-03 | validation loss: 2.9599e-03\n",
      "Epoch: 31370 | training loss: 2.8950e-03 | validation loss: 2.9546e-03\n",
      "Epoch: 31380 | training loss: 2.8897e-03 | validation loss: 2.9607e-03\n",
      "Epoch: 31390 | training loss: 2.8894e-03 | validation loss: 2.9679e-03\n",
      "Epoch: 31400 | training loss: 2.8888e-03 | validation loss: 2.9653e-03\n",
      "Epoch: 31410 | training loss: 2.8885e-03 | validation loss: 2.9617e-03\n",
      "Epoch: 31420 | training loss: 2.8882e-03 | validation loss: 2.9634e-03\n",
      "Epoch: 31430 | training loss: 2.8880e-03 | validation loss: 2.9630e-03\n",
      "Epoch: 31440 | training loss: 2.8877e-03 | validation loss: 2.9621e-03\n",
      "Epoch: 31450 | training loss: 2.8875e-03 | validation loss: 2.9623e-03\n",
      "Epoch: 31460 | training loss: 2.8872e-03 | validation loss: 2.9617e-03\n",
      "Epoch: 31470 | training loss: 2.8870e-03 | validation loss: 2.9616e-03\n",
      "Epoch: 31480 | training loss: 2.8868e-03 | validation loss: 2.9612e-03\n",
      "Epoch: 31490 | training loss: 2.8865e-03 | validation loss: 2.9608e-03\n",
      "Epoch: 31500 | training loss: 2.8863e-03 | validation loss: 2.9599e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31510 | training loss: 2.8881e-03 | validation loss: 2.9537e-03\n",
      "Epoch: 31520 | training loss: 3.3232e-03 | validation loss: 3.1604e-03\n",
      "Epoch: 31530 | training loss: 3.2100e-03 | validation loss: 3.2106e-03\n",
      "Epoch: 31540 | training loss: 3.0114e-03 | validation loss: 3.1144e-03\n",
      "Epoch: 31550 | training loss: 2.9256e-03 | validation loss: 3.0179e-03\n",
      "Epoch: 31560 | training loss: 2.9028e-03 | validation loss: 2.9747e-03\n",
      "Epoch: 31570 | training loss: 2.8911e-03 | validation loss: 2.9647e-03\n",
      "Epoch: 31580 | training loss: 2.8873e-03 | validation loss: 2.9574e-03\n",
      "Epoch: 31590 | training loss: 2.8961e-03 | validation loss: 2.9434e-03\n",
      "Epoch: 31600 | training loss: 3.1534e-03 | validation loss: 2.9744e-03\n",
      "Epoch: 31610 | training loss: 2.9426e-03 | validation loss: 2.9418e-03\n",
      "Epoch: 31620 | training loss: 2.8840e-03 | validation loss: 2.9542e-03\n",
      "Epoch: 31630 | training loss: 2.8999e-03 | validation loss: 2.9882e-03\n",
      "Epoch: 31640 | training loss: 2.8971e-03 | validation loss: 2.9407e-03\n",
      "Epoch: 31650 | training loss: 2.8893e-03 | validation loss: 2.9734e-03\n",
      "Epoch: 31660 | training loss: 2.8847e-03 | validation loss: 2.9481e-03\n",
      "Epoch: 31670 | training loss: 2.8826e-03 | validation loss: 2.9580e-03\n",
      "Epoch: 31680 | training loss: 2.8823e-03 | validation loss: 2.9573e-03\n",
      "Epoch: 31690 | training loss: 2.8822e-03 | validation loss: 2.9523e-03\n",
      "Epoch: 31700 | training loss: 2.8818e-03 | validation loss: 2.9531e-03\n",
      "Epoch: 31710 | training loss: 2.8815e-03 | validation loss: 2.9534e-03\n",
      "Epoch: 31720 | training loss: 2.8814e-03 | validation loss: 2.9521e-03\n",
      "Epoch: 31730 | training loss: 2.8839e-03 | validation loss: 2.9453e-03\n",
      "Epoch: 31740 | training loss: 3.0444e-03 | validation loss: 2.9537e-03\n",
      "Epoch: 31750 | training loss: 3.2900e-03 | validation loss: 3.0440e-03\n",
      "Epoch: 31760 | training loss: 2.9862e-03 | validation loss: 2.9427e-03\n",
      "Epoch: 31770 | training loss: 2.9510e-03 | validation loss: 3.0239e-03\n",
      "Epoch: 31780 | training loss: 2.8989e-03 | validation loss: 2.9790e-03\n",
      "Epoch: 31790 | training loss: 2.8894e-03 | validation loss: 2.9382e-03\n",
      "Epoch: 31800 | training loss: 2.8802e-03 | validation loss: 2.9485e-03\n",
      "Epoch: 31810 | training loss: 2.8813e-03 | validation loss: 2.9611e-03\n",
      "Epoch: 31820 | training loss: 2.8795e-03 | validation loss: 2.9482e-03\n",
      "Epoch: 31830 | training loss: 2.8789e-03 | validation loss: 2.9516e-03\n",
      "Epoch: 31840 | training loss: 2.8786e-03 | validation loss: 2.9507e-03\n",
      "Epoch: 31850 | training loss: 2.8784e-03 | validation loss: 2.9506e-03\n",
      "Epoch: 31860 | training loss: 2.8781e-03 | validation loss: 2.9499e-03\n",
      "Epoch: 31870 | training loss: 2.8779e-03 | validation loss: 2.9503e-03\n",
      "Epoch: 31880 | training loss: 2.8777e-03 | validation loss: 2.9494e-03\n",
      "Epoch: 31890 | training loss: 2.8775e-03 | validation loss: 2.9492e-03\n",
      "Epoch: 31900 | training loss: 2.8772e-03 | validation loss: 2.9491e-03\n",
      "Epoch: 31910 | training loss: 2.8770e-03 | validation loss: 2.9488e-03\n",
      "Epoch: 31920 | training loss: 2.8768e-03 | validation loss: 2.9486e-03\n",
      "Epoch: 31930 | training loss: 2.8765e-03 | validation loss: 2.9486e-03\n",
      "Epoch: 31940 | training loss: 2.8768e-03 | validation loss: 2.9518e-03\n",
      "Epoch: 31950 | training loss: 2.9638e-03 | validation loss: 3.0375e-03\n",
      "Epoch: 31960 | training loss: 3.9399e-03 | validation loss: 3.6161e-03\n",
      "Epoch: 31970 | training loss: 3.5074e-03 | validation loss: 3.3763e-03\n",
      "Epoch: 31980 | training loss: 3.0944e-03 | validation loss: 3.1313e-03\n",
      "Epoch: 31990 | training loss: 2.9513e-03 | validation loss: 3.0316e-03\n",
      "Epoch: 32000 | training loss: 2.9022e-03 | validation loss: 2.9900e-03\n",
      "Epoch: 32010 | training loss: 2.8848e-03 | validation loss: 2.9691e-03\n",
      "Epoch: 32020 | training loss: 2.8780e-03 | validation loss: 2.9576e-03\n",
      "Epoch: 32030 | training loss: 2.8751e-03 | validation loss: 2.9508e-03\n",
      "Epoch: 32040 | training loss: 2.8742e-03 | validation loss: 2.9466e-03\n",
      "Epoch: 32050 | training loss: 2.8739e-03 | validation loss: 2.9438e-03\n",
      "Epoch: 32060 | training loss: 2.8737e-03 | validation loss: 2.9431e-03\n",
      "Epoch: 32070 | training loss: 2.8734e-03 | validation loss: 2.9438e-03\n",
      "Epoch: 32080 | training loss: 2.8732e-03 | validation loss: 2.9443e-03\n",
      "Epoch: 32090 | training loss: 2.8730e-03 | validation loss: 2.9440e-03\n",
      "Epoch: 32100 | training loss: 2.8728e-03 | validation loss: 2.9433e-03\n",
      "Epoch: 32110 | training loss: 2.8725e-03 | validation loss: 2.9432e-03\n",
      "Epoch: 32120 | training loss: 2.8723e-03 | validation loss: 2.9430e-03\n",
      "Epoch: 32130 | training loss: 2.8721e-03 | validation loss: 2.9426e-03\n",
      "Epoch: 32140 | training loss: 2.8718e-03 | validation loss: 2.9424e-03\n",
      "Epoch: 32150 | training loss: 2.8716e-03 | validation loss: 2.9421e-03\n",
      "Epoch: 32160 | training loss: 2.8714e-03 | validation loss: 2.9419e-03\n",
      "Epoch: 32170 | training loss: 2.8712e-03 | validation loss: 2.9416e-03\n",
      "Epoch: 32180 | training loss: 2.8709e-03 | validation loss: 2.9417e-03\n",
      "Epoch: 32190 | training loss: 2.8712e-03 | validation loss: 2.9451e-03\n",
      "Epoch: 32200 | training loss: 2.9956e-03 | validation loss: 3.0929e-03\n",
      "Epoch: 32210 | training loss: 3.0725e-03 | validation loss: 3.0544e-03\n",
      "Epoch: 32220 | training loss: 2.8866e-03 | validation loss: 2.9221e-03\n",
      "Epoch: 32230 | training loss: 2.8724e-03 | validation loss: 2.9375e-03\n",
      "Epoch: 32240 | training loss: 2.8713e-03 | validation loss: 2.9439e-03\n",
      "Epoch: 32250 | training loss: 2.8694e-03 | validation loss: 2.9381e-03\n",
      "Epoch: 32260 | training loss: 2.8696e-03 | validation loss: 2.9367e-03\n",
      "Epoch: 32270 | training loss: 2.8716e-03 | validation loss: 2.9323e-03\n",
      "Epoch: 32280 | training loss: 2.9124e-03 | validation loss: 2.9176e-03\n",
      "Epoch: 32290 | training loss: 3.6407e-03 | validation loss: 3.1102e-03\n",
      "Epoch: 32300 | training loss: 3.1385e-03 | validation loss: 3.1576e-03\n",
      "Epoch: 32310 | training loss: 2.9623e-03 | validation loss: 2.9218e-03\n",
      "Epoch: 32320 | training loss: 2.8822e-03 | validation loss: 2.9656e-03\n",
      "Epoch: 32330 | training loss: 2.8685e-03 | validation loss: 2.9320e-03\n",
      "Epoch: 32340 | training loss: 2.8675e-03 | validation loss: 2.9384e-03\n",
      "Epoch: 32350 | training loss: 2.8673e-03 | validation loss: 2.9349e-03\n",
      "Epoch: 32360 | training loss: 2.8674e-03 | validation loss: 2.9404e-03\n",
      "Epoch: 32370 | training loss: 2.8671e-03 | validation loss: 2.9329e-03\n",
      "Epoch: 32380 | training loss: 2.8666e-03 | validation loss: 2.9363e-03\n",
      "Epoch: 32390 | training loss: 2.8665e-03 | validation loss: 2.9377e-03\n",
      "Epoch: 32400 | training loss: 2.8663e-03 | validation loss: 2.9381e-03\n",
      "Epoch: 32410 | training loss: 2.8671e-03 | validation loss: 2.9422e-03\n",
      "Epoch: 32420 | training loss: 2.8933e-03 | validation loss: 2.9784e-03\n",
      "Epoch: 32430 | training loss: 3.7287e-03 | validation loss: 3.4973e-03\n",
      "Epoch: 32440 | training loss: 3.2212e-03 | validation loss: 2.9964e-03\n",
      "Epoch: 32450 | training loss: 2.9702e-03 | validation loss: 3.0316e-03\n",
      "Epoch: 32460 | training loss: 2.8656e-03 | validation loss: 2.9347e-03\n",
      "Epoch: 32470 | training loss: 2.8792e-03 | validation loss: 2.9213e-03\n",
      "Epoch: 32480 | training loss: 2.8717e-03 | validation loss: 2.9527e-03\n",
      "Epoch: 32490 | training loss: 2.8659e-03 | validation loss: 2.9270e-03\n",
      "Epoch: 32500 | training loss: 2.8644e-03 | validation loss: 2.9363e-03\n",
      "Epoch: 32510 | training loss: 2.8640e-03 | validation loss: 2.9307e-03\n",
      "Epoch: 32520 | training loss: 2.8637e-03 | validation loss: 2.9348e-03\n",
      "Epoch: 32530 | training loss: 2.8634e-03 | validation loss: 2.9311e-03\n",
      "Epoch: 32540 | training loss: 2.8631e-03 | validation loss: 2.9319e-03\n",
      "Epoch: 32550 | training loss: 2.8629e-03 | validation loss: 2.9324e-03\n",
      "Epoch: 32560 | training loss: 2.8627e-03 | validation loss: 2.9325e-03\n",
      "Epoch: 32570 | training loss: 2.8626e-03 | validation loss: 2.9336e-03\n",
      "Epoch: 32580 | training loss: 2.8662e-03 | validation loss: 2.9435e-03\n",
      "Epoch: 32590 | training loss: 3.0671e-03 | validation loss: 3.1014e-03\n",
      "Epoch: 32600 | training loss: 3.0937e-03 | validation loss: 3.1177e-03\n",
      "Epoch: 32610 | training loss: 2.9946e-03 | validation loss: 3.0556e-03\n",
      "Epoch: 32620 | training loss: 2.9226e-03 | validation loss: 2.9198e-03\n",
      "Epoch: 32630 | training loss: 2.8760e-03 | validation loss: 2.9127e-03\n",
      "Epoch: 32640 | training loss: 2.8720e-03 | validation loss: 2.9522e-03\n",
      "Epoch: 32650 | training loss: 2.8608e-03 | validation loss: 2.9318e-03\n",
      "Epoch: 32660 | training loss: 2.8619e-03 | validation loss: 2.9222e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32670 | training loss: 2.8610e-03 | validation loss: 2.9343e-03\n",
      "Epoch: 32680 | training loss: 2.8602e-03 | validation loss: 2.9260e-03\n",
      "Epoch: 32690 | training loss: 2.8599e-03 | validation loss: 2.9298e-03\n",
      "Epoch: 32700 | training loss: 2.8596e-03 | validation loss: 2.9270e-03\n",
      "Epoch: 32710 | training loss: 2.8594e-03 | validation loss: 2.9286e-03\n",
      "Epoch: 32720 | training loss: 2.8592e-03 | validation loss: 2.9273e-03\n",
      "Epoch: 32730 | training loss: 2.8589e-03 | validation loss: 2.9271e-03\n",
      "Epoch: 32740 | training loss: 2.8587e-03 | validation loss: 2.9272e-03\n",
      "Epoch: 32750 | training loss: 2.8585e-03 | validation loss: 2.9274e-03\n",
      "Epoch: 32760 | training loss: 2.8584e-03 | validation loss: 2.9294e-03\n",
      "Epoch: 32770 | training loss: 2.8725e-03 | validation loss: 2.9600e-03\n",
      "Epoch: 32780 | training loss: 3.5058e-03 | validation loss: 3.5371e-03\n",
      "Epoch: 32790 | training loss: 3.0157e-03 | validation loss: 3.0801e-03\n",
      "Epoch: 32800 | training loss: 2.8898e-03 | validation loss: 2.9435e-03\n",
      "Epoch: 32810 | training loss: 2.9069e-03 | validation loss: 2.8974e-03\n",
      "Epoch: 32820 | training loss: 2.8681e-03 | validation loss: 2.9110e-03\n",
      "Epoch: 32830 | training loss: 2.8633e-03 | validation loss: 2.9328e-03\n",
      "Epoch: 32840 | training loss: 2.8733e-03 | validation loss: 2.9558e-03\n",
      "Epoch: 32850 | training loss: 2.9914e-03 | validation loss: 3.0581e-03\n",
      "Epoch: 32860 | training loss: 3.1640e-03 | validation loss: 3.1691e-03\n",
      "Epoch: 32870 | training loss: 2.9665e-03 | validation loss: 2.9144e-03\n",
      "Epoch: 32880 | training loss: 2.8772e-03 | validation loss: 2.9608e-03\n",
      "Epoch: 32890 | training loss: 2.8555e-03 | validation loss: 2.9241e-03\n",
      "Epoch: 32900 | training loss: 2.8627e-03 | validation loss: 2.9101e-03\n",
      "Epoch: 32910 | training loss: 2.8553e-03 | validation loss: 2.9255e-03\n",
      "Epoch: 32920 | training loss: 2.8578e-03 | validation loss: 2.9340e-03\n",
      "Epoch: 32930 | training loss: 2.8658e-03 | validation loss: 2.9468e-03\n",
      "Epoch: 32940 | training loss: 2.9758e-03 | validation loss: 3.0408e-03\n",
      "Epoch: 32950 | training loss: 3.3163e-03 | validation loss: 3.2533e-03\n",
      "Epoch: 32960 | training loss: 3.0177e-03 | validation loss: 2.9296e-03\n",
      "Epoch: 32970 | training loss: 2.9137e-03 | validation loss: 2.9911e-03\n",
      "Epoch: 32980 | training loss: 2.8734e-03 | validation loss: 2.9052e-03\n",
      "Epoch: 32990 | training loss: 2.8563e-03 | validation loss: 2.9315e-03\n",
      "Epoch: 33000 | training loss: 2.8537e-03 | validation loss: 2.9251e-03\n",
      "Epoch: 33010 | training loss: 2.8548e-03 | validation loss: 2.9140e-03\n",
      "Epoch: 33020 | training loss: 2.8533e-03 | validation loss: 2.9167e-03\n",
      "Epoch: 33030 | training loss: 2.8527e-03 | validation loss: 2.9181e-03\n",
      "Epoch: 33040 | training loss: 2.8531e-03 | validation loss: 2.9155e-03\n",
      "Epoch: 33050 | training loss: 2.8714e-03 | validation loss: 2.9049e-03\n",
      "Epoch: 33060 | training loss: 3.6243e-03 | validation loss: 3.1277e-03\n",
      "Epoch: 33070 | training loss: 3.1890e-03 | validation loss: 3.1757e-03\n",
      "Epoch: 33080 | training loss: 2.9826e-03 | validation loss: 2.9249e-03\n",
      "Epoch: 33090 | training loss: 2.8560e-03 | validation loss: 2.9072e-03\n",
      "Epoch: 33100 | training loss: 2.8745e-03 | validation loss: 2.9528e-03\n",
      "Epoch: 33110 | training loss: 2.8545e-03 | validation loss: 2.9111e-03\n",
      "Epoch: 33120 | training loss: 2.8507e-03 | validation loss: 2.9176e-03\n",
      "Epoch: 33130 | training loss: 2.8508e-03 | validation loss: 2.9207e-03\n",
      "Epoch: 33140 | training loss: 2.8505e-03 | validation loss: 2.9155e-03\n",
      "Epoch: 33150 | training loss: 2.8501e-03 | validation loss: 2.9187e-03\n",
      "Epoch: 33160 | training loss: 2.8498e-03 | validation loss: 2.9168e-03\n",
      "Epoch: 33170 | training loss: 2.8496e-03 | validation loss: 2.9164e-03\n",
      "Epoch: 33180 | training loss: 2.8494e-03 | validation loss: 2.9173e-03\n",
      "Epoch: 33190 | training loss: 2.8492e-03 | validation loss: 2.9167e-03\n",
      "Epoch: 33200 | training loss: 2.8489e-03 | validation loss: 2.9162e-03\n",
      "Epoch: 33210 | training loss: 2.8487e-03 | validation loss: 2.9159e-03\n",
      "Epoch: 33220 | training loss: 2.8485e-03 | validation loss: 2.9154e-03\n",
      "Epoch: 33230 | training loss: 2.8485e-03 | validation loss: 2.9134e-03\n",
      "Epoch: 33240 | training loss: 2.8697e-03 | validation loss: 2.9023e-03\n",
      "Epoch: 33250 | training loss: 4.9946e-03 | validation loss: 3.6870e-03\n",
      "Epoch: 33260 | training loss: 2.9246e-03 | validation loss: 3.0064e-03\n",
      "Epoch: 33270 | training loss: 2.9754e-03 | validation loss: 3.0309e-03\n",
      "Epoch: 33280 | training loss: 2.9177e-03 | validation loss: 2.9803e-03\n",
      "Epoch: 33290 | training loss: 2.8793e-03 | validation loss: 2.9498e-03\n",
      "Epoch: 33300 | training loss: 2.8579e-03 | validation loss: 2.9321e-03\n",
      "Epoch: 33310 | training loss: 2.8484e-03 | validation loss: 2.9201e-03\n",
      "Epoch: 33320 | training loss: 2.8465e-03 | validation loss: 2.9125e-03\n",
      "Epoch: 33330 | training loss: 2.8466e-03 | validation loss: 2.9110e-03\n",
      "Epoch: 33340 | training loss: 2.8461e-03 | validation loss: 2.9116e-03\n",
      "Epoch: 33350 | training loss: 2.8458e-03 | validation loss: 2.9129e-03\n",
      "Epoch: 33360 | training loss: 2.8456e-03 | validation loss: 2.9130e-03\n",
      "Epoch: 33370 | training loss: 2.8454e-03 | validation loss: 2.9119e-03\n",
      "Epoch: 33380 | training loss: 2.8452e-03 | validation loss: 2.9117e-03\n",
      "Epoch: 33390 | training loss: 2.8450e-03 | validation loss: 2.9118e-03\n",
      "Epoch: 33400 | training loss: 2.8447e-03 | validation loss: 2.9114e-03\n",
      "Epoch: 33410 | training loss: 2.8445e-03 | validation loss: 2.9112e-03\n",
      "Epoch: 33420 | training loss: 2.8443e-03 | validation loss: 2.9110e-03\n",
      "Epoch: 33430 | training loss: 2.8441e-03 | validation loss: 2.9108e-03\n",
      "Epoch: 33440 | training loss: 2.8439e-03 | validation loss: 2.9104e-03\n",
      "Epoch: 33450 | training loss: 2.8437e-03 | validation loss: 2.9088e-03\n",
      "Epoch: 33460 | training loss: 2.8541e-03 | validation loss: 2.8949e-03\n",
      "Epoch: 33470 | training loss: 3.8441e-03 | validation loss: 3.3266e-03\n",
      "Epoch: 33480 | training loss: 3.0209e-03 | validation loss: 3.0223e-03\n",
      "Epoch: 33490 | training loss: 2.9090e-03 | validation loss: 2.8873e-03\n",
      "Epoch: 33500 | training loss: 2.8719e-03 | validation loss: 2.9275e-03\n",
      "Epoch: 33510 | training loss: 2.8543e-03 | validation loss: 2.8907e-03\n",
      "Epoch: 33520 | training loss: 2.8464e-03 | validation loss: 2.9109e-03\n",
      "Epoch: 33530 | training loss: 2.8430e-03 | validation loss: 2.9028e-03\n",
      "Epoch: 33540 | training loss: 2.8421e-03 | validation loss: 2.9044e-03\n",
      "Epoch: 33550 | training loss: 2.8417e-03 | validation loss: 2.9096e-03\n",
      "Epoch: 33560 | training loss: 2.8416e-03 | validation loss: 2.9111e-03\n",
      "Epoch: 33570 | training loss: 2.8418e-03 | validation loss: 2.9133e-03\n",
      "Epoch: 33580 | training loss: 2.8494e-03 | validation loss: 2.9297e-03\n",
      "Epoch: 33590 | training loss: 3.1291e-03 | validation loss: 3.1427e-03\n",
      "Epoch: 33600 | training loss: 2.8780e-03 | validation loss: 2.9523e-03\n",
      "Epoch: 33610 | training loss: 2.8962e-03 | validation loss: 2.9719e-03\n",
      "Epoch: 33620 | training loss: 2.9124e-03 | validation loss: 2.8901e-03\n",
      "Epoch: 33630 | training loss: 2.8526e-03 | validation loss: 2.9319e-03\n",
      "Epoch: 33640 | training loss: 2.8399e-03 | validation loss: 2.9053e-03\n",
      "Epoch: 33650 | training loss: 2.8401e-03 | validation loss: 2.9033e-03\n",
      "Epoch: 33660 | training loss: 2.8398e-03 | validation loss: 2.9093e-03\n",
      "Epoch: 33670 | training loss: 2.8393e-03 | validation loss: 2.9044e-03\n",
      "Epoch: 33680 | training loss: 2.8391e-03 | validation loss: 2.9048e-03\n",
      "Epoch: 33690 | training loss: 2.8389e-03 | validation loss: 2.9070e-03\n",
      "Epoch: 33700 | training loss: 2.8387e-03 | validation loss: 2.9046e-03\n",
      "Epoch: 33710 | training loss: 2.8385e-03 | validation loss: 2.9039e-03\n",
      "Epoch: 33720 | training loss: 2.8383e-03 | validation loss: 2.9035e-03\n",
      "Epoch: 33730 | training loss: 2.8384e-03 | validation loss: 2.9013e-03\n",
      "Epoch: 33740 | training loss: 2.8478e-03 | validation loss: 2.8912e-03\n",
      "Epoch: 33750 | training loss: 3.3511e-03 | validation loss: 3.0137e-03\n",
      "Epoch: 33760 | training loss: 2.9587e-03 | validation loss: 3.0269e-03\n",
      "Epoch: 33770 | training loss: 3.0561e-03 | validation loss: 2.9339e-03\n",
      "Epoch: 33780 | training loss: 2.8380e-03 | validation loss: 2.8961e-03\n",
      "Epoch: 33790 | training loss: 2.8661e-03 | validation loss: 2.9418e-03\n",
      "Epoch: 33800 | training loss: 2.8383e-03 | validation loss: 2.8973e-03\n",
      "Epoch: 33810 | training loss: 2.8378e-03 | validation loss: 2.8989e-03\n",
      "Epoch: 33820 | training loss: 2.8377e-03 | validation loss: 2.9095e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33830 | training loss: 2.8365e-03 | validation loss: 2.8988e-03\n",
      "Epoch: 33840 | training loss: 2.8360e-03 | validation loss: 2.9046e-03\n",
      "Epoch: 33850 | training loss: 2.8357e-03 | validation loss: 2.9007e-03\n",
      "Epoch: 33860 | training loss: 2.8355e-03 | validation loss: 2.9029e-03\n",
      "Epoch: 33870 | training loss: 2.8352e-03 | validation loss: 2.9012e-03\n",
      "Epoch: 33880 | training loss: 2.8350e-03 | validation loss: 2.9014e-03\n",
      "Epoch: 33890 | training loss: 2.8348e-03 | validation loss: 2.9015e-03\n",
      "Epoch: 33900 | training loss: 2.8346e-03 | validation loss: 2.9014e-03\n",
      "Epoch: 33910 | training loss: 2.8344e-03 | validation loss: 2.9016e-03\n",
      "Epoch: 33920 | training loss: 2.8346e-03 | validation loss: 2.9040e-03\n",
      "Epoch: 33930 | training loss: 2.8519e-03 | validation loss: 2.9307e-03\n",
      "Epoch: 33940 | training loss: 3.9963e-03 | validation loss: 3.6206e-03\n",
      "Epoch: 33950 | training loss: 3.4772e-03 | validation loss: 3.0696e-03\n",
      "Epoch: 33960 | training loss: 2.8495e-03 | validation loss: 2.8877e-03\n",
      "Epoch: 33970 | training loss: 2.8822e-03 | validation loss: 2.9574e-03\n",
      "Epoch: 33980 | training loss: 2.8524e-03 | validation loss: 2.9302e-03\n",
      "Epoch: 33990 | training loss: 2.8346e-03 | validation loss: 2.8933e-03\n",
      "Epoch: 34000 | training loss: 2.8355e-03 | validation loss: 2.8916e-03\n",
      "Epoch: 34010 | training loss: 2.8330e-03 | validation loss: 2.9030e-03\n",
      "Epoch: 34020 | training loss: 2.8323e-03 | validation loss: 2.9000e-03\n",
      "Epoch: 34030 | training loss: 2.8321e-03 | validation loss: 2.8964e-03\n",
      "Epoch: 34040 | training loss: 2.8318e-03 | validation loss: 2.8994e-03\n",
      "Epoch: 34050 | training loss: 2.8316e-03 | validation loss: 2.8972e-03\n",
      "Epoch: 34060 | training loss: 2.8314e-03 | validation loss: 2.8981e-03\n",
      "Epoch: 34070 | training loss: 2.8312e-03 | validation loss: 2.8972e-03\n",
      "Epoch: 34080 | training loss: 2.8310e-03 | validation loss: 2.8975e-03\n",
      "Epoch: 34090 | training loss: 2.8307e-03 | validation loss: 2.8973e-03\n",
      "Epoch: 34100 | training loss: 2.8306e-03 | validation loss: 2.8984e-03\n",
      "Epoch: 34110 | training loss: 2.8384e-03 | validation loss: 2.9199e-03\n",
      "Epoch: 34120 | training loss: 3.5499e-03 | validation loss: 3.5760e-03\n",
      "Epoch: 34130 | training loss: 2.8440e-03 | validation loss: 2.9120e-03\n",
      "Epoch: 34140 | training loss: 2.8795e-03 | validation loss: 2.8763e-03\n",
      "Epoch: 34150 | training loss: 2.8553e-03 | validation loss: 2.9025e-03\n",
      "Epoch: 34160 | training loss: 2.8416e-03 | validation loss: 2.8986e-03\n",
      "Epoch: 34170 | training loss: 2.8316e-03 | validation loss: 2.8896e-03\n",
      "Epoch: 34180 | training loss: 2.8297e-03 | validation loss: 2.8890e-03\n",
      "Epoch: 34190 | training loss: 2.8371e-03 | validation loss: 2.8825e-03\n",
      "Epoch: 34200 | training loss: 3.0372e-03 | validation loss: 2.8979e-03\n",
      "Epoch: 34210 | training loss: 3.0212e-03 | validation loss: 2.9058e-03\n",
      "Epoch: 34220 | training loss: 2.8311e-03 | validation loss: 2.9055e-03\n",
      "Epoch: 34230 | training loss: 2.8455e-03 | validation loss: 2.9253e-03\n",
      "Epoch: 34240 | training loss: 2.8474e-03 | validation loss: 2.8754e-03\n",
      "Epoch: 34250 | training loss: 2.8369e-03 | validation loss: 2.9146e-03\n",
      "Epoch: 34260 | training loss: 2.8310e-03 | validation loss: 2.8841e-03\n",
      "Epoch: 34270 | training loss: 2.8285e-03 | validation loss: 2.9007e-03\n",
      "Epoch: 34280 | training loss: 2.8272e-03 | validation loss: 2.8913e-03\n",
      "Epoch: 34290 | training loss: 2.8269e-03 | validation loss: 2.8917e-03\n",
      "Epoch: 34300 | training loss: 2.8267e-03 | validation loss: 2.8948e-03\n",
      "Epoch: 34310 | training loss: 2.8265e-03 | validation loss: 2.8947e-03\n",
      "Epoch: 34320 | training loss: 2.8264e-03 | validation loss: 2.8952e-03\n",
      "Epoch: 34330 | training loss: 2.8282e-03 | validation loss: 2.9019e-03\n",
      "Epoch: 34340 | training loss: 2.8971e-03 | validation loss: 2.9733e-03\n",
      "Epoch: 34350 | training loss: 3.9320e-03 | validation loss: 3.5864e-03\n",
      "Epoch: 34360 | training loss: 3.0122e-03 | validation loss: 2.9189e-03\n",
      "Epoch: 34370 | training loss: 2.8427e-03 | validation loss: 2.8754e-03\n",
      "Epoch: 34380 | training loss: 2.8699e-03 | validation loss: 2.9429e-03\n",
      "Epoch: 34390 | training loss: 2.8288e-03 | validation loss: 2.8821e-03\n",
      "Epoch: 34400 | training loss: 2.8254e-03 | validation loss: 2.8891e-03\n",
      "Epoch: 34410 | training loss: 2.8259e-03 | validation loss: 2.8979e-03\n",
      "Epoch: 34420 | training loss: 2.8251e-03 | validation loss: 2.8860e-03\n",
      "Epoch: 34430 | training loss: 2.8244e-03 | validation loss: 2.8941e-03\n",
      "Epoch: 34440 | training loss: 2.8240e-03 | validation loss: 2.8888e-03\n",
      "Epoch: 34450 | training loss: 2.8237e-03 | validation loss: 2.8906e-03\n",
      "Epoch: 34460 | training loss: 2.8235e-03 | validation loss: 2.8907e-03\n",
      "Epoch: 34470 | training loss: 2.8233e-03 | validation loss: 2.8896e-03\n",
      "Epoch: 34480 | training loss: 2.8231e-03 | validation loss: 2.8891e-03\n",
      "Epoch: 34490 | training loss: 2.8230e-03 | validation loss: 2.8885e-03\n",
      "Epoch: 34500 | training loss: 2.8232e-03 | validation loss: 2.8858e-03\n",
      "Epoch: 34510 | training loss: 2.8435e-03 | validation loss: 2.8748e-03\n",
      "Epoch: 34520 | training loss: 3.9089e-03 | validation loss: 3.2222e-03\n",
      "Epoch: 34530 | training loss: 3.3933e-03 | validation loss: 3.2831e-03\n",
      "Epoch: 34540 | training loss: 2.8230e-03 | validation loss: 2.8858e-03\n",
      "Epoch: 34550 | training loss: 2.8922e-03 | validation loss: 2.8747e-03\n",
      "Epoch: 34560 | training loss: 2.8219e-03 | validation loss: 2.8846e-03\n",
      "Epoch: 34570 | training loss: 2.8308e-03 | validation loss: 2.9094e-03\n",
      "Epoch: 34580 | training loss: 2.8216e-03 | validation loss: 2.8844e-03\n",
      "Epoch: 34590 | training loss: 2.8214e-03 | validation loss: 2.8842e-03\n",
      "Epoch: 34600 | training loss: 2.8212e-03 | validation loss: 2.8914e-03\n",
      "Epoch: 34610 | training loss: 2.8207e-03 | validation loss: 2.8852e-03\n",
      "Epoch: 34620 | training loss: 2.8204e-03 | validation loss: 2.8883e-03\n",
      "Epoch: 34630 | training loss: 2.8202e-03 | validation loss: 2.8861e-03\n",
      "Epoch: 34640 | training loss: 2.8200e-03 | validation loss: 2.8872e-03\n",
      "Epoch: 34650 | training loss: 2.8198e-03 | validation loss: 2.8862e-03\n",
      "Epoch: 34660 | training loss: 2.8196e-03 | validation loss: 2.8862e-03\n",
      "Epoch: 34670 | training loss: 2.8194e-03 | validation loss: 2.8862e-03\n",
      "Epoch: 34680 | training loss: 2.8192e-03 | validation loss: 2.8860e-03\n",
      "Epoch: 34690 | training loss: 2.8190e-03 | validation loss: 2.8850e-03\n",
      "Epoch: 34700 | training loss: 2.8340e-03 | validation loss: 2.8768e-03\n",
      "Epoch: 34710 | training loss: 3.5356e-03 | validation loss: 3.2123e-03\n",
      "Epoch: 34720 | training loss: 3.1508e-03 | validation loss: 2.9823e-03\n",
      "Epoch: 34730 | training loss: 2.9229e-03 | validation loss: 2.9441e-03\n",
      "Epoch: 34740 | training loss: 2.8346e-03 | validation loss: 2.8837e-03\n",
      "Epoch: 34750 | training loss: 2.8250e-03 | validation loss: 2.8673e-03\n",
      "Epoch: 34760 | training loss: 2.8211e-03 | validation loss: 2.8735e-03\n",
      "Epoch: 34770 | training loss: 2.8223e-03 | validation loss: 2.8728e-03\n",
      "Epoch: 34780 | training loss: 2.8586e-03 | validation loss: 2.8612e-03\n",
      "Epoch: 34790 | training loss: 3.3310e-03 | validation loss: 2.9690e-03\n",
      "Epoch: 34800 | training loss: 2.8913e-03 | validation loss: 2.9707e-03\n",
      "Epoch: 34810 | training loss: 2.8481e-03 | validation loss: 2.8625e-03\n",
      "Epoch: 34820 | training loss: 2.8280e-03 | validation loss: 2.9088e-03\n",
      "Epoch: 34830 | training loss: 2.8181e-03 | validation loss: 2.8755e-03\n",
      "Epoch: 34840 | training loss: 2.8164e-03 | validation loss: 2.8803e-03\n",
      "Epoch: 34850 | training loss: 2.8179e-03 | validation loss: 2.8920e-03\n",
      "Epoch: 34860 | training loss: 2.8161e-03 | validation loss: 2.8795e-03\n",
      "Epoch: 34870 | training loss: 2.8164e-03 | validation loss: 2.8775e-03\n",
      "Epoch: 34880 | training loss: 2.8166e-03 | validation loss: 2.8761e-03\n",
      "Epoch: 34890 | training loss: 2.8247e-03 | validation loss: 2.8676e-03\n",
      "Epoch: 34900 | training loss: 3.0254e-03 | validation loss: 2.8885e-03\n",
      "Epoch: 34910 | training loss: 3.0278e-03 | validation loss: 2.8991e-03\n",
      "Epoch: 34920 | training loss: 2.8293e-03 | validation loss: 2.9102e-03\n",
      "Epoch: 34930 | training loss: 2.8186e-03 | validation loss: 2.8943e-03\n",
      "Epoch: 34940 | training loss: 2.8237e-03 | validation loss: 2.8672e-03\n",
      "Epoch: 34950 | training loss: 2.8195e-03 | validation loss: 2.8961e-03\n",
      "Epoch: 34960 | training loss: 2.8156e-03 | validation loss: 2.8744e-03\n",
      "Epoch: 34970 | training loss: 2.8139e-03 | validation loss: 2.8832e-03\n",
      "Epoch: 34980 | training loss: 2.8137e-03 | validation loss: 2.8833e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34990 | training loss: 2.8137e-03 | validation loss: 2.8779e-03\n",
      "Epoch: 35000 | training loss: 2.8133e-03 | validation loss: 2.8793e-03\n",
      "Epoch: 35010 | training loss: 2.8130e-03 | validation loss: 2.8804e-03\n",
      "Epoch: 35020 | training loss: 2.8128e-03 | validation loss: 2.8807e-03\n",
      "Epoch: 35030 | training loss: 2.8128e-03 | validation loss: 2.8822e-03\n",
      "Epoch: 35040 | training loss: 2.8195e-03 | validation loss: 2.8981e-03\n",
      "Epoch: 35050 | training loss: 3.4656e-03 | validation loss: 3.3233e-03\n",
      "Epoch: 35060 | training loss: 3.3108e-03 | validation loss: 2.9830e-03\n",
      "Epoch: 35070 | training loss: 2.8434e-03 | validation loss: 2.9124e-03\n",
      "Epoch: 35080 | training loss: 2.9049e-03 | validation loss: 2.9677e-03\n",
      "Epoch: 35090 | training loss: 2.8330e-03 | validation loss: 2.9121e-03\n",
      "Epoch: 35100 | training loss: 2.8122e-03 | validation loss: 2.8764e-03\n",
      "Epoch: 35110 | training loss: 2.8160e-03 | validation loss: 2.8715e-03\n",
      "Epoch: 35120 | training loss: 2.8110e-03 | validation loss: 2.8785e-03\n",
      "Epoch: 35130 | training loss: 2.8114e-03 | validation loss: 2.8825e-03\n",
      "Epoch: 35140 | training loss: 2.8107e-03 | validation loss: 2.8771e-03\n",
      "Epoch: 35150 | training loss: 2.8105e-03 | validation loss: 2.8775e-03\n",
      "Epoch: 35160 | training loss: 2.8103e-03 | validation loss: 2.8789e-03\n",
      "Epoch: 35170 | training loss: 2.8101e-03 | validation loss: 2.8772e-03\n",
      "Epoch: 35180 | training loss: 2.8099e-03 | validation loss: 2.8780e-03\n",
      "Epoch: 35190 | training loss: 2.8097e-03 | validation loss: 2.8772e-03\n",
      "Epoch: 35200 | training loss: 2.8095e-03 | validation loss: 2.8774e-03\n",
      "Epoch: 35210 | training loss: 2.8093e-03 | validation loss: 2.8771e-03\n",
      "Epoch: 35220 | training loss: 2.8091e-03 | validation loss: 2.8769e-03\n",
      "Epoch: 35230 | training loss: 2.8089e-03 | validation loss: 2.8768e-03\n",
      "Epoch: 35240 | training loss: 2.8087e-03 | validation loss: 2.8767e-03\n",
      "Epoch: 35250 | training loss: 2.8085e-03 | validation loss: 2.8766e-03\n",
      "Epoch: 35260 | training loss: 2.8083e-03 | validation loss: 2.8769e-03\n",
      "Epoch: 35270 | training loss: 2.8088e-03 | validation loss: 2.8807e-03\n",
      "Epoch: 35280 | training loss: 2.8936e-03 | validation loss: 2.9634e-03\n",
      "Epoch: 35290 | training loss: 4.0634e-03 | validation loss: 3.6536e-03\n",
      "Epoch: 35300 | training loss: 3.2424e-03 | validation loss: 3.1827e-03\n",
      "Epoch: 35310 | training loss: 2.8712e-03 | validation loss: 2.9368e-03\n",
      "Epoch: 35320 | training loss: 2.8103e-03 | validation loss: 2.8803e-03\n",
      "Epoch: 35330 | training loss: 2.8089e-03 | validation loss: 2.8716e-03\n",
      "Epoch: 35340 | training loss: 2.8122e-03 | validation loss: 2.8685e-03\n",
      "Epoch: 35350 | training loss: 2.8098e-03 | validation loss: 2.8684e-03\n",
      "Epoch: 35360 | training loss: 2.8068e-03 | validation loss: 2.8729e-03\n",
      "Epoch: 35370 | training loss: 2.8065e-03 | validation loss: 2.8761e-03\n",
      "Epoch: 35380 | training loss: 2.8063e-03 | validation loss: 2.8757e-03\n",
      "Epoch: 35390 | training loss: 2.8059e-03 | validation loss: 2.8737e-03\n",
      "Epoch: 35400 | training loss: 2.8058e-03 | validation loss: 2.8734e-03\n",
      "Epoch: 35410 | training loss: 2.8056e-03 | validation loss: 2.8739e-03\n",
      "Epoch: 35420 | training loss: 2.8054e-03 | validation loss: 2.8735e-03\n",
      "Epoch: 35430 | training loss: 2.8052e-03 | validation loss: 2.8733e-03\n",
      "Epoch: 35440 | training loss: 2.8050e-03 | validation loss: 2.8732e-03\n",
      "Epoch: 35450 | training loss: 2.8048e-03 | validation loss: 2.8730e-03\n",
      "Epoch: 35460 | training loss: 2.8046e-03 | validation loss: 2.8728e-03\n",
      "Epoch: 35470 | training loss: 2.8044e-03 | validation loss: 2.8723e-03\n",
      "Epoch: 35480 | training loss: 2.8045e-03 | validation loss: 2.8688e-03\n",
      "Epoch: 35490 | training loss: 2.8601e-03 | validation loss: 2.8606e-03\n",
      "Epoch: 35500 | training loss: 3.0113e-03 | validation loss: 2.8621e-03\n",
      "Epoch: 35510 | training loss: 2.9757e-03 | validation loss: 2.9769e-03\n",
      "Epoch: 35520 | training loss: 2.8748e-03 | validation loss: 2.8578e-03\n",
      "Epoch: 35530 | training loss: 2.8291e-03 | validation loss: 2.8841e-03\n",
      "Epoch: 35540 | training loss: 2.8100e-03 | validation loss: 2.8569e-03\n",
      "Epoch: 35550 | training loss: 2.8036e-03 | validation loss: 2.8681e-03\n",
      "Epoch: 35560 | training loss: 2.8033e-03 | validation loss: 2.8768e-03\n",
      "Epoch: 35570 | training loss: 2.8029e-03 | validation loss: 2.8747e-03\n",
      "Epoch: 35580 | training loss: 2.8025e-03 | validation loss: 2.8719e-03\n",
      "Epoch: 35590 | training loss: 2.8023e-03 | validation loss: 2.8686e-03\n",
      "Epoch: 35600 | training loss: 2.8046e-03 | validation loss: 2.8609e-03\n",
      "Epoch: 35610 | training loss: 2.9532e-03 | validation loss: 2.8598e-03\n",
      "Epoch: 35620 | training loss: 3.2322e-03 | validation loss: 2.9581e-03\n",
      "Epoch: 35630 | training loss: 2.8888e-03 | validation loss: 2.8532e-03\n",
      "Epoch: 35640 | training loss: 2.8795e-03 | validation loss: 2.9499e-03\n",
      "Epoch: 35650 | training loss: 2.8151e-03 | validation loss: 2.8912e-03\n",
      "Epoch: 35660 | training loss: 2.8128e-03 | validation loss: 2.8526e-03\n",
      "Epoch: 35670 | training loss: 2.8010e-03 | validation loss: 2.8663e-03\n",
      "Epoch: 35680 | training loss: 2.8023e-03 | validation loss: 2.8778e-03\n",
      "Epoch: 35690 | training loss: 2.8011e-03 | validation loss: 2.8661e-03\n",
      "Epoch: 35700 | training loss: 2.8004e-03 | validation loss: 2.8714e-03\n",
      "Epoch: 35710 | training loss: 2.8001e-03 | validation loss: 2.8683e-03\n",
      "Epoch: 35720 | training loss: 2.7999e-03 | validation loss: 2.8694e-03\n",
      "Epoch: 35730 | training loss: 2.7997e-03 | validation loss: 2.8685e-03\n",
      "Epoch: 35740 | training loss: 2.7996e-03 | validation loss: 2.8692e-03\n",
      "Epoch: 35750 | training loss: 2.7994e-03 | validation loss: 2.8686e-03\n",
      "Epoch: 35760 | training loss: 2.7992e-03 | validation loss: 2.8683e-03\n",
      "Epoch: 35770 | training loss: 2.7990e-03 | validation loss: 2.8681e-03\n",
      "Epoch: 35780 | training loss: 2.7988e-03 | validation loss: 2.8678e-03\n",
      "Epoch: 35790 | training loss: 2.7987e-03 | validation loss: 2.8662e-03\n",
      "Epoch: 35800 | training loss: 2.8045e-03 | validation loss: 2.8572e-03\n",
      "Epoch: 35810 | training loss: 3.3487e-03 | validation loss: 2.9929e-03\n",
      "Epoch: 35820 | training loss: 3.1396e-03 | validation loss: 3.1349e-03\n",
      "Epoch: 35830 | training loss: 2.8859e-03 | validation loss: 2.8703e-03\n",
      "Epoch: 35840 | training loss: 2.9005e-03 | validation loss: 2.8569e-03\n",
      "Epoch: 35850 | training loss: 2.8111e-03 | validation loss: 2.8457e-03\n",
      "Epoch: 35860 | training loss: 2.8009e-03 | validation loss: 2.8759e-03\n",
      "Epoch: 35870 | training loss: 2.8018e-03 | validation loss: 2.8844e-03\n",
      "Epoch: 35880 | training loss: 2.7971e-03 | validation loss: 2.8668e-03\n",
      "Epoch: 35890 | training loss: 2.7975e-03 | validation loss: 2.8611e-03\n",
      "Epoch: 35900 | training loss: 2.7968e-03 | validation loss: 2.8688e-03\n",
      "Epoch: 35910 | training loss: 2.7965e-03 | validation loss: 2.8671e-03\n",
      "Epoch: 35920 | training loss: 2.7963e-03 | validation loss: 2.8652e-03\n",
      "Epoch: 35930 | training loss: 2.7961e-03 | validation loss: 2.8669e-03\n",
      "Epoch: 35940 | training loss: 2.7960e-03 | validation loss: 2.8656e-03\n",
      "Epoch: 35950 | training loss: 2.7958e-03 | validation loss: 2.8661e-03\n",
      "Epoch: 35960 | training loss: 2.7956e-03 | validation loss: 2.8656e-03\n",
      "Epoch: 35970 | training loss: 2.7954e-03 | validation loss: 2.8656e-03\n",
      "Epoch: 35980 | training loss: 2.7952e-03 | validation loss: 2.8655e-03\n",
      "Epoch: 35990 | training loss: 2.7950e-03 | validation loss: 2.8654e-03\n",
      "Epoch: 36000 | training loss: 2.7948e-03 | validation loss: 2.8655e-03\n",
      "Epoch: 36010 | training loss: 2.7950e-03 | validation loss: 2.8678e-03\n",
      "Epoch: 36020 | training loss: 2.8387e-03 | validation loss: 2.9259e-03\n",
      "Epoch: 36030 | training loss: 2.9243e-03 | validation loss: 2.9966e-03\n",
      "Epoch: 36040 | training loss: 2.9476e-03 | validation loss: 2.9915e-03\n",
      "Epoch: 36050 | training loss: 2.9800e-03 | validation loss: 2.8832e-03\n",
      "Epoch: 36060 | training loss: 2.9661e-03 | validation loss: 2.8615e-03\n",
      "Epoch: 36070 | training loss: 2.8881e-03 | validation loss: 2.9665e-03\n",
      "Epoch: 36080 | training loss: 2.8257e-03 | validation loss: 2.8380e-03\n",
      "Epoch: 36090 | training loss: 2.7962e-03 | validation loss: 2.8695e-03\n",
      "Epoch: 36100 | training loss: 2.7972e-03 | validation loss: 2.8745e-03\n",
      "Epoch: 36110 | training loss: 2.7941e-03 | validation loss: 2.8562e-03\n",
      "Epoch: 36120 | training loss: 2.7960e-03 | validation loss: 2.8538e-03\n",
      "Epoch: 36130 | training loss: 2.8039e-03 | validation loss: 2.8476e-03\n",
      "Epoch: 36140 | training loss: 2.9178e-03 | validation loss: 2.8488e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36150 | training loss: 3.2062e-03 | validation loss: 2.9284e-03\n",
      "Epoch: 36160 | training loss: 2.9455e-03 | validation loss: 3.0036e-03\n",
      "Epoch: 36170 | training loss: 2.8484e-03 | validation loss: 2.8435e-03\n",
      "Epoch: 36180 | training loss: 2.8102e-03 | validation loss: 2.8962e-03\n",
      "Epoch: 36190 | training loss: 2.7938e-03 | validation loss: 2.8549e-03\n",
      "Epoch: 36200 | training loss: 2.7921e-03 | validation loss: 2.8573e-03\n",
      "Epoch: 36210 | training loss: 2.7928e-03 | validation loss: 2.8702e-03\n",
      "Epoch: 36220 | training loss: 2.7919e-03 | validation loss: 2.8682e-03\n",
      "Epoch: 36230 | training loss: 2.7917e-03 | validation loss: 2.8679e-03\n",
      "Epoch: 36240 | training loss: 2.7978e-03 | validation loss: 2.8806e-03\n",
      "Epoch: 36250 | training loss: 2.9682e-03 | validation loss: 3.0202e-03\n",
      "Epoch: 36260 | training loss: 3.1625e-03 | validation loss: 3.1378e-03\n",
      "Epoch: 36270 | training loss: 2.8151e-03 | validation loss: 2.8542e-03\n",
      "Epoch: 36280 | training loss: 2.8008e-03 | validation loss: 2.8491e-03\n",
      "Epoch: 36290 | training loss: 2.8106e-03 | validation loss: 2.8923e-03\n",
      "Epoch: 36300 | training loss: 2.8001e-03 | validation loss: 2.8487e-03\n",
      "Epoch: 36310 | training loss: 2.7935e-03 | validation loss: 2.8745e-03\n",
      "Epoch: 36320 | training loss: 2.7908e-03 | validation loss: 2.8542e-03\n",
      "Epoch: 36330 | training loss: 2.7894e-03 | validation loss: 2.8647e-03\n",
      "Epoch: 36340 | training loss: 2.7888e-03 | validation loss: 2.8604e-03\n",
      "Epoch: 36350 | training loss: 2.7888e-03 | validation loss: 2.8587e-03\n",
      "Epoch: 36360 | training loss: 2.7885e-03 | validation loss: 2.8599e-03\n",
      "Epoch: 36370 | training loss: 2.7883e-03 | validation loss: 2.8607e-03\n",
      "Epoch: 36380 | training loss: 2.7882e-03 | validation loss: 2.8615e-03\n",
      "Epoch: 36390 | training loss: 2.7898e-03 | validation loss: 2.8680e-03\n",
      "Epoch: 36400 | training loss: 2.8953e-03 | validation loss: 2.9650e-03\n",
      "Epoch: 36410 | training loss: 3.7371e-03 | validation loss: 3.4680e-03\n",
      "Epoch: 36420 | training loss: 2.8186e-03 | validation loss: 2.9064e-03\n",
      "Epoch: 36430 | training loss: 2.8929e-03 | validation loss: 2.8535e-03\n",
      "Epoch: 36440 | training loss: 2.8094e-03 | validation loss: 2.8414e-03\n",
      "Epoch: 36450 | training loss: 2.7948e-03 | validation loss: 2.8781e-03\n",
      "Epoch: 36460 | training loss: 2.7899e-03 | validation loss: 2.8706e-03\n",
      "Epoch: 36470 | training loss: 2.7884e-03 | validation loss: 2.8520e-03\n",
      "Epoch: 36480 | training loss: 2.7864e-03 | validation loss: 2.8590e-03\n",
      "Epoch: 36490 | training loss: 2.7864e-03 | validation loss: 2.8610e-03\n",
      "Epoch: 36500 | training loss: 2.7861e-03 | validation loss: 2.8566e-03\n",
      "Epoch: 36510 | training loss: 2.7859e-03 | validation loss: 2.8595e-03\n",
      "Epoch: 36520 | training loss: 2.7857e-03 | validation loss: 2.8574e-03\n",
      "Epoch: 36530 | training loss: 2.7855e-03 | validation loss: 2.8585e-03\n",
      "Epoch: 36540 | training loss: 2.7853e-03 | validation loss: 2.8578e-03\n",
      "Epoch: 36550 | training loss: 2.7851e-03 | validation loss: 2.8577e-03\n",
      "Epoch: 36560 | training loss: 2.7849e-03 | validation loss: 2.8580e-03\n",
      "Epoch: 36570 | training loss: 2.7848e-03 | validation loss: 2.8590e-03\n",
      "Epoch: 36580 | training loss: 2.7903e-03 | validation loss: 2.8758e-03\n",
      "Epoch: 36590 | training loss: 3.4156e-03 | validation loss: 3.4628e-03\n",
      "Epoch: 36600 | training loss: 2.9163e-03 | validation loss: 2.9489e-03\n",
      "Epoch: 36610 | training loss: 2.8546e-03 | validation loss: 2.8642e-03\n",
      "Epoch: 36620 | training loss: 2.8253e-03 | validation loss: 2.8370e-03\n",
      "Epoch: 36630 | training loss: 2.7971e-03 | validation loss: 2.8382e-03\n",
      "Epoch: 36640 | training loss: 2.7886e-03 | validation loss: 2.8413e-03\n",
      "Epoch: 36650 | training loss: 2.8122e-03 | validation loss: 2.8347e-03\n",
      "Epoch: 36660 | training loss: 3.2350e-03 | validation loss: 2.9256e-03\n",
      "Epoch: 36670 | training loss: 2.8048e-03 | validation loss: 2.8981e-03\n",
      "Epoch: 36680 | training loss: 2.8000e-03 | validation loss: 2.8413e-03\n",
      "Epoch: 36690 | training loss: 2.7929e-03 | validation loss: 2.8806e-03\n",
      "Epoch: 36700 | training loss: 2.7853e-03 | validation loss: 2.8473e-03\n",
      "Epoch: 36710 | training loss: 2.7824e-03 | validation loss: 2.8561e-03\n",
      "Epoch: 36720 | training loss: 2.7833e-03 | validation loss: 2.8616e-03\n",
      "Epoch: 36730 | training loss: 2.7830e-03 | validation loss: 2.8496e-03\n",
      "Epoch: 36740 | training loss: 2.7820e-03 | validation loss: 2.8531e-03\n",
      "Epoch: 36750 | training loss: 2.7817e-03 | validation loss: 2.8561e-03\n",
      "Epoch: 36760 | training loss: 2.7818e-03 | validation loss: 2.8583e-03\n",
      "Epoch: 36770 | training loss: 2.7862e-03 | validation loss: 2.8703e-03\n",
      "Epoch: 36780 | training loss: 2.9736e-03 | validation loss: 3.0250e-03\n",
      "Epoch: 36790 | training loss: 3.0745e-03 | validation loss: 3.0814e-03\n",
      "Epoch: 36800 | training loss: 2.8041e-03 | validation loss: 2.8955e-03\n",
      "Epoch: 36810 | training loss: 2.8739e-03 | validation loss: 2.8518e-03\n",
      "Epoch: 36820 | training loss: 2.7878e-03 | validation loss: 2.8736e-03\n",
      "Epoch: 36830 | training loss: 2.7833e-03 | validation loss: 2.8638e-03\n",
      "Epoch: 36840 | training loss: 2.7844e-03 | validation loss: 2.8436e-03\n",
      "Epoch: 36850 | training loss: 2.7817e-03 | validation loss: 2.8620e-03\n",
      "Epoch: 36860 | training loss: 2.7804e-03 | validation loss: 2.8507e-03\n",
      "Epoch: 36870 | training loss: 2.7799e-03 | validation loss: 2.8564e-03\n",
      "Epoch: 36880 | training loss: 2.7796e-03 | validation loss: 2.8520e-03\n",
      "Epoch: 36890 | training loss: 2.7794e-03 | validation loss: 2.8544e-03\n",
      "Epoch: 36900 | training loss: 2.7792e-03 | validation loss: 2.8539e-03\n",
      "Epoch: 36910 | training loss: 2.7790e-03 | validation loss: 2.8531e-03\n",
      "Epoch: 36920 | training loss: 2.7788e-03 | validation loss: 2.8526e-03\n",
      "Epoch: 36930 | training loss: 2.7787e-03 | validation loss: 2.8517e-03\n",
      "Epoch: 36940 | training loss: 2.7800e-03 | validation loss: 2.8471e-03\n",
      "Epoch: 36950 | training loss: 2.8546e-03 | validation loss: 2.8413e-03\n",
      "Epoch: 36960 | training loss: 4.0445e-03 | validation loss: 3.2543e-03\n",
      "Epoch: 36970 | training loss: 2.7996e-03 | validation loss: 2.8832e-03\n",
      "Epoch: 36980 | training loss: 2.9182e-03 | validation loss: 2.9826e-03\n",
      "Epoch: 36990 | training loss: 2.7780e-03 | validation loss: 2.8545e-03\n",
      "Epoch: 37000 | training loss: 2.7960e-03 | validation loss: 2.8356e-03\n",
      "Epoch: 37010 | training loss: 2.7776e-03 | validation loss: 2.8541e-03\n",
      "Epoch: 37020 | training loss: 2.7788e-03 | validation loss: 2.8614e-03\n",
      "Epoch: 37030 | training loss: 2.7778e-03 | validation loss: 2.8467e-03\n",
      "Epoch: 37040 | training loss: 2.7769e-03 | validation loss: 2.8539e-03\n",
      "Epoch: 37050 | training loss: 2.7766e-03 | validation loss: 2.8514e-03\n",
      "Epoch: 37060 | training loss: 2.7764e-03 | validation loss: 2.8517e-03\n",
      "Epoch: 37070 | training loss: 2.7763e-03 | validation loss: 2.8513e-03\n",
      "Epoch: 37080 | training loss: 2.7761e-03 | validation loss: 2.8517e-03\n",
      "Epoch: 37090 | training loss: 2.7759e-03 | validation loss: 2.8510e-03\n",
      "Epoch: 37100 | training loss: 2.7757e-03 | validation loss: 2.8512e-03\n",
      "Epoch: 37110 | training loss: 2.7756e-03 | validation loss: 2.8514e-03\n",
      "Epoch: 37120 | training loss: 2.7754e-03 | validation loss: 2.8520e-03\n",
      "Epoch: 37130 | training loss: 2.7763e-03 | validation loss: 2.8582e-03\n",
      "Epoch: 37140 | training loss: 2.9073e-03 | validation loss: 3.0142e-03\n",
      "Epoch: 37150 | training loss: 2.8955e-03 | validation loss: 2.9380e-03\n",
      "Epoch: 37160 | training loss: 2.9411e-03 | validation loss: 3.0377e-03\n",
      "Epoch: 37170 | training loss: 2.8140e-03 | validation loss: 2.9101e-03\n",
      "Epoch: 37180 | training loss: 2.8125e-03 | validation loss: 2.8589e-03\n",
      "Epoch: 37190 | training loss: 2.7875e-03 | validation loss: 2.8429e-03\n",
      "Epoch: 37200 | training loss: 2.7820e-03 | validation loss: 2.8347e-03\n",
      "Epoch: 37210 | training loss: 2.8161e-03 | validation loss: 2.8239e-03\n",
      "Epoch: 37220 | training loss: 3.1804e-03 | validation loss: 2.9034e-03\n",
      "Epoch: 37230 | training loss: 2.7814e-03 | validation loss: 2.8699e-03\n",
      "Epoch: 37240 | training loss: 2.7742e-03 | validation loss: 2.8558e-03\n",
      "Epoch: 37250 | training loss: 2.7800e-03 | validation loss: 2.8377e-03\n",
      "Epoch: 37260 | training loss: 2.7836e-03 | validation loss: 2.8735e-03\n",
      "Epoch: 37270 | training loss: 2.7766e-03 | validation loss: 2.8389e-03\n",
      "Epoch: 37280 | training loss: 2.7736e-03 | validation loss: 2.8435e-03\n",
      "Epoch: 37290 | training loss: 2.7729e-03 | validation loss: 2.8522e-03\n",
      "Epoch: 37300 | training loss: 2.7743e-03 | validation loss: 2.8578e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37310 | training loss: 2.7921e-03 | validation loss: 2.8839e-03\n",
      "Epoch: 37320 | training loss: 3.1794e-03 | validation loss: 3.1531e-03\n",
      "Epoch: 37330 | training loss: 2.7731e-03 | validation loss: 2.8405e-03\n",
      "Epoch: 37340 | training loss: 2.7925e-03 | validation loss: 2.8818e-03\n",
      "Epoch: 37350 | training loss: 2.7986e-03 | validation loss: 2.8337e-03\n",
      "Epoch: 37360 | training loss: 2.7862e-03 | validation loss: 2.8763e-03\n",
      "Epoch: 37370 | training loss: 2.7769e-03 | validation loss: 2.8368e-03\n",
      "Epoch: 37380 | training loss: 2.7724e-03 | validation loss: 2.8546e-03\n",
      "Epoch: 37390 | training loss: 2.7709e-03 | validation loss: 2.8478e-03\n",
      "Epoch: 37400 | training loss: 2.7712e-03 | validation loss: 2.8442e-03\n",
      "Epoch: 37410 | training loss: 2.7706e-03 | validation loss: 2.8488e-03\n",
      "Epoch: 37420 | training loss: 2.7706e-03 | validation loss: 2.8502e-03\n",
      "Epoch: 37430 | training loss: 2.7710e-03 | validation loss: 2.8527e-03\n",
      "Epoch: 37440 | training loss: 2.7810e-03 | validation loss: 2.8703e-03\n",
      "Epoch: 37450 | training loss: 3.1062e-03 | validation loss: 3.1027e-03\n",
      "Epoch: 37460 | training loss: 2.7840e-03 | validation loss: 2.8714e-03\n",
      "Epoch: 37470 | training loss: 2.8333e-03 | validation loss: 2.9206e-03\n",
      "Epoch: 37480 | training loss: 2.8400e-03 | validation loss: 2.8355e-03\n",
      "Epoch: 37490 | training loss: 2.7832e-03 | validation loss: 2.8708e-03\n",
      "Epoch: 37500 | training loss: 2.7695e-03 | validation loss: 2.8445e-03\n",
      "Epoch: 37510 | training loss: 2.7690e-03 | validation loss: 2.8445e-03\n",
      "Epoch: 37520 | training loss: 2.7688e-03 | validation loss: 2.8481e-03\n",
      "Epoch: 37530 | training loss: 2.7686e-03 | validation loss: 2.8463e-03\n",
      "Epoch: 37540 | training loss: 2.7685e-03 | validation loss: 2.8449e-03\n",
      "Epoch: 37550 | training loss: 2.7683e-03 | validation loss: 2.8476e-03\n",
      "Epoch: 37560 | training loss: 2.7681e-03 | validation loss: 2.8462e-03\n",
      "Epoch: 37570 | training loss: 2.7679e-03 | validation loss: 2.8453e-03\n",
      "Epoch: 37580 | training loss: 2.7678e-03 | validation loss: 2.8446e-03\n",
      "Epoch: 37590 | training loss: 2.7684e-03 | validation loss: 2.8417e-03\n",
      "Epoch: 37600 | training loss: 2.7973e-03 | validation loss: 2.8343e-03\n",
      "Epoch: 37610 | training loss: 3.9041e-03 | validation loss: 3.2179e-03\n",
      "Epoch: 37620 | training loss: 3.2384e-03 | validation loss: 3.1722e-03\n",
      "Epoch: 37630 | training loss: 2.7795e-03 | validation loss: 2.8261e-03\n",
      "Epoch: 37640 | training loss: 2.8237e-03 | validation loss: 2.8732e-03\n",
      "Epoch: 37650 | training loss: 2.7668e-03 | validation loss: 2.8463e-03\n",
      "Epoch: 37660 | training loss: 2.7741e-03 | validation loss: 2.8500e-03\n",
      "Epoch: 37670 | training loss: 2.7667e-03 | validation loss: 2.8456e-03\n",
      "Epoch: 37680 | training loss: 2.7664e-03 | validation loss: 2.8440e-03\n",
      "Epoch: 37690 | training loss: 2.7663e-03 | validation loss: 2.8447e-03\n",
      "Epoch: 37700 | training loss: 2.7659e-03 | validation loss: 2.8448e-03\n",
      "Epoch: 37710 | training loss: 2.7656e-03 | validation loss: 2.8446e-03\n",
      "Epoch: 37720 | training loss: 2.7654e-03 | validation loss: 2.8437e-03\n",
      "Epoch: 37730 | training loss: 2.7653e-03 | validation loss: 2.8435e-03\n",
      "Epoch: 37740 | training loss: 2.7652e-03 | validation loss: 2.8415e-03\n",
      "Epoch: 37750 | training loss: 2.7684e-03 | validation loss: 2.8307e-03\n",
      "Epoch: 37760 | training loss: 3.0311e-03 | validation loss: 2.8696e-03\n",
      "Epoch: 37770 | training loss: 2.9421e-03 | validation loss: 3.0096e-03\n",
      "Epoch: 37780 | training loss: 2.8008e-03 | validation loss: 2.8452e-03\n",
      "Epoch: 37790 | training loss: 2.8069e-03 | validation loss: 2.8332e-03\n",
      "Epoch: 37800 | training loss: 2.7723e-03 | validation loss: 2.8336e-03\n",
      "Epoch: 37810 | training loss: 2.7645e-03 | validation loss: 2.8426e-03\n",
      "Epoch: 37820 | training loss: 2.7664e-03 | validation loss: 2.8570e-03\n",
      "Epoch: 37830 | training loss: 2.7640e-03 | validation loss: 2.8420e-03\n",
      "Epoch: 37840 | training loss: 2.7637e-03 | validation loss: 2.8388e-03\n",
      "Epoch: 37850 | training loss: 2.7633e-03 | validation loss: 2.8429e-03\n",
      "Epoch: 37860 | training loss: 2.7633e-03 | validation loss: 2.8457e-03\n",
      "Epoch: 37870 | training loss: 2.7647e-03 | validation loss: 2.8499e-03\n",
      "Epoch: 37880 | training loss: 2.8201e-03 | validation loss: 2.9105e-03\n",
      "Epoch: 37890 | training loss: 3.9624e-03 | validation loss: 3.5871e-03\n",
      "Epoch: 37900 | training loss: 3.0278e-03 | validation loss: 2.8829e-03\n",
      "Epoch: 37910 | training loss: 2.7739e-03 | validation loss: 2.8287e-03\n",
      "Epoch: 37920 | training loss: 2.8110e-03 | validation loss: 2.8997e-03\n",
      "Epoch: 37930 | training loss: 2.7647e-03 | validation loss: 2.8332e-03\n",
      "Epoch: 37940 | training loss: 2.7637e-03 | validation loss: 2.8356e-03\n",
      "Epoch: 37950 | training loss: 2.7640e-03 | validation loss: 2.8511e-03\n",
      "Epoch: 37960 | training loss: 2.7625e-03 | validation loss: 2.8368e-03\n",
      "Epoch: 37970 | training loss: 2.7617e-03 | validation loss: 2.8446e-03\n",
      "Epoch: 37980 | training loss: 2.7614e-03 | validation loss: 2.8396e-03\n",
      "Epoch: 37990 | training loss: 2.7611e-03 | validation loss: 2.8425e-03\n",
      "Epoch: 38000 | training loss: 2.7609e-03 | validation loss: 2.8410e-03\n",
      "Epoch: 38010 | training loss: 2.7608e-03 | validation loss: 2.8407e-03\n",
      "Epoch: 38020 | training loss: 2.7606e-03 | validation loss: 2.8411e-03\n",
      "Epoch: 38030 | training loss: 2.7604e-03 | validation loss: 2.8412e-03\n",
      "Epoch: 38040 | training loss: 2.7603e-03 | validation loss: 2.8417e-03\n",
      "Epoch: 38050 | training loss: 2.7607e-03 | validation loss: 2.8451e-03\n",
      "Epoch: 38060 | training loss: 2.7957e-03 | validation loss: 2.8890e-03\n",
      "Epoch: 38070 | training loss: 4.4295e-03 | validation loss: 3.8373e-03\n",
      "Epoch: 38080 | training loss: 2.9679e-03 | validation loss: 2.8617e-03\n",
      "Epoch: 38090 | training loss: 2.9395e-03 | validation loss: 2.8535e-03\n",
      "Epoch: 38100 | training loss: 2.7632e-03 | validation loss: 2.8311e-03\n",
      "Epoch: 38110 | training loss: 2.7750e-03 | validation loss: 2.8680e-03\n",
      "Epoch: 38120 | training loss: 2.7644e-03 | validation loss: 2.8544e-03\n",
      "Epoch: 38130 | training loss: 2.7599e-03 | validation loss: 2.8351e-03\n",
      "Epoch: 38140 | training loss: 2.7593e-03 | validation loss: 2.8361e-03\n",
      "Epoch: 38150 | training loss: 2.7589e-03 | validation loss: 2.8431e-03\n",
      "Epoch: 38160 | training loss: 2.7584e-03 | validation loss: 2.8393e-03\n",
      "Epoch: 38170 | training loss: 2.7582e-03 | validation loss: 2.8389e-03\n",
      "Epoch: 38180 | training loss: 2.7580e-03 | validation loss: 2.8401e-03\n",
      "Epoch: 38190 | training loss: 2.7579e-03 | validation loss: 2.8390e-03\n",
      "Epoch: 38200 | training loss: 2.7577e-03 | validation loss: 2.8396e-03\n",
      "Epoch: 38210 | training loss: 2.7575e-03 | validation loss: 2.8391e-03\n",
      "Epoch: 38220 | training loss: 2.7574e-03 | validation loss: 2.8391e-03\n",
      "Epoch: 38230 | training loss: 2.7572e-03 | validation loss: 2.8391e-03\n",
      "Epoch: 38240 | training loss: 2.7570e-03 | validation loss: 2.8388e-03\n",
      "Epoch: 38250 | training loss: 2.7569e-03 | validation loss: 2.8375e-03\n",
      "Epoch: 38260 | training loss: 2.7652e-03 | validation loss: 2.8270e-03\n",
      "Epoch: 38270 | training loss: 3.5648e-03 | validation loss: 3.2229e-03\n",
      "Epoch: 38280 | training loss: 2.8238e-03 | validation loss: 2.8083e-03\n",
      "Epoch: 38290 | training loss: 2.7768e-03 | validation loss: 2.8800e-03\n",
      "Epoch: 38300 | training loss: 2.7573e-03 | validation loss: 2.8472e-03\n",
      "Epoch: 38310 | training loss: 2.7618e-03 | validation loss: 2.8297e-03\n",
      "Epoch: 38320 | training loss: 2.7605e-03 | validation loss: 2.8318e-03\n",
      "Epoch: 38330 | training loss: 2.7634e-03 | validation loss: 2.8285e-03\n",
      "Epoch: 38340 | training loss: 2.8201e-03 | validation loss: 2.8181e-03\n",
      "Epoch: 38350 | training loss: 3.3052e-03 | validation loss: 2.9392e-03\n",
      "Epoch: 38360 | training loss: 2.8877e-03 | validation loss: 2.9680e-03\n",
      "Epoch: 38370 | training loss: 2.8006e-03 | validation loss: 2.8163e-03\n",
      "Epoch: 38380 | training loss: 2.7679e-03 | validation loss: 2.8644e-03\n",
      "Epoch: 38390 | training loss: 2.7558e-03 | validation loss: 2.8309e-03\n",
      "Epoch: 38400 | training loss: 2.7554e-03 | validation loss: 2.8316e-03\n",
      "Epoch: 38410 | training loss: 2.7565e-03 | validation loss: 2.8466e-03\n",
      "Epoch: 38420 | training loss: 2.7544e-03 | validation loss: 2.8390e-03\n",
      "Epoch: 38430 | training loss: 2.7543e-03 | validation loss: 2.8344e-03\n",
      "Epoch: 38440 | training loss: 2.7552e-03 | validation loss: 2.8305e-03\n",
      "Epoch: 38450 | training loss: 2.7751e-03 | validation loss: 2.8184e-03\n",
      "Epoch: 38460 | training loss: 3.3496e-03 | validation loss: 2.9616e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38470 | training loss: 2.8791e-03 | validation loss: 2.9613e-03\n",
      "Epoch: 38480 | training loss: 2.8939e-03 | validation loss: 2.8398e-03\n",
      "Epoch: 38490 | training loss: 2.7931e-03 | validation loss: 2.8904e-03\n",
      "Epoch: 38500 | training loss: 2.7555e-03 | validation loss: 2.8278e-03\n",
      "Epoch: 38510 | training loss: 2.7529e-03 | validation loss: 2.8340e-03\n",
      "Epoch: 38520 | training loss: 2.7529e-03 | validation loss: 2.8385e-03\n",
      "Epoch: 38530 | training loss: 2.7526e-03 | validation loss: 2.8358e-03\n",
      "Epoch: 38540 | training loss: 2.7525e-03 | validation loss: 2.8347e-03\n",
      "Epoch: 38550 | training loss: 2.7524e-03 | validation loss: 2.8382e-03\n",
      "Epoch: 38560 | training loss: 2.7521e-03 | validation loss: 2.8351e-03\n",
      "Epoch: 38570 | training loss: 2.7520e-03 | validation loss: 2.8345e-03\n",
      "Epoch: 38580 | training loss: 2.7518e-03 | validation loss: 2.8343e-03\n",
      "Epoch: 38590 | training loss: 2.7520e-03 | validation loss: 2.8323e-03\n",
      "Epoch: 38600 | training loss: 2.7611e-03 | validation loss: 2.8230e-03\n",
      "Epoch: 38610 | training loss: 3.2075e-03 | validation loss: 2.9299e-03\n",
      "Epoch: 38620 | training loss: 2.8032e-03 | validation loss: 2.9037e-03\n",
      "Epoch: 38630 | training loss: 2.9753e-03 | validation loss: 2.8598e-03\n",
      "Epoch: 38640 | training loss: 2.7557e-03 | validation loss: 2.8441e-03\n",
      "Epoch: 38650 | training loss: 2.7737e-03 | validation loss: 2.8709e-03\n",
      "Epoch: 38660 | training loss: 2.7572e-03 | validation loss: 2.8263e-03\n",
      "Epoch: 38670 | training loss: 2.7504e-03 | validation loss: 2.8335e-03\n",
      "Epoch: 38680 | training loss: 2.7509e-03 | validation loss: 2.8395e-03\n",
      "Epoch: 38690 | training loss: 2.7505e-03 | validation loss: 2.8316e-03\n",
      "Epoch: 38700 | training loss: 2.7501e-03 | validation loss: 2.8369e-03\n",
      "Epoch: 38710 | training loss: 2.7498e-03 | validation loss: 2.8334e-03\n",
      "Epoch: 38720 | training loss: 2.7496e-03 | validation loss: 2.8350e-03\n",
      "Epoch: 38730 | training loss: 2.7495e-03 | validation loss: 2.8346e-03\n",
      "Epoch: 38740 | training loss: 2.7493e-03 | validation loss: 2.8340e-03\n",
      "Epoch: 38750 | training loss: 2.7491e-03 | validation loss: 2.8340e-03\n",
      "Epoch: 38760 | training loss: 2.7490e-03 | validation loss: 2.8340e-03\n",
      "Epoch: 38770 | training loss: 2.7488e-03 | validation loss: 2.8336e-03\n",
      "Epoch: 38780 | training loss: 2.7490e-03 | validation loss: 2.8315e-03\n",
      "Epoch: 38790 | training loss: 2.7708e-03 | validation loss: 2.8231e-03\n",
      "Epoch: 38800 | training loss: 4.3046e-03 | validation loss: 3.3847e-03\n",
      "Epoch: 38810 | training loss: 3.1685e-03 | validation loss: 3.1422e-03\n",
      "Epoch: 38820 | training loss: 2.9384e-03 | validation loss: 2.9562e-03\n",
      "Epoch: 38830 | training loss: 2.7983e-03 | validation loss: 2.8428e-03\n",
      "Epoch: 38840 | training loss: 2.7509e-03 | validation loss: 2.8343e-03\n",
      "Epoch: 38850 | training loss: 2.7488e-03 | validation loss: 2.8411e-03\n",
      "Epoch: 38860 | training loss: 2.7501e-03 | validation loss: 2.8320e-03\n",
      "Epoch: 38870 | training loss: 2.7483e-03 | validation loss: 2.8354e-03\n",
      "Epoch: 38880 | training loss: 2.7471e-03 | validation loss: 2.8331e-03\n",
      "Epoch: 38890 | training loss: 2.7471e-03 | validation loss: 2.8322e-03\n",
      "Epoch: 38900 | training loss: 2.7468e-03 | validation loss: 2.8326e-03\n",
      "Epoch: 38910 | training loss: 2.7467e-03 | validation loss: 2.8328e-03\n",
      "Epoch: 38920 | training loss: 2.7465e-03 | validation loss: 2.8322e-03\n",
      "Epoch: 38930 | training loss: 2.7464e-03 | validation loss: 2.8324e-03\n",
      "Epoch: 38940 | training loss: 2.7462e-03 | validation loss: 2.8324e-03\n",
      "Epoch: 38950 | training loss: 2.7461e-03 | validation loss: 2.8323e-03\n",
      "Epoch: 38960 | training loss: 2.7459e-03 | validation loss: 2.8325e-03\n",
      "Epoch: 38970 | training loss: 2.7458e-03 | validation loss: 2.8338e-03\n",
      "Epoch: 38980 | training loss: 2.7485e-03 | validation loss: 2.8474e-03\n",
      "Epoch: 38990 | training loss: 3.0652e-03 | validation loss: 3.1472e-03\n",
      "Epoch: 39000 | training loss: 2.9683e-03 | validation loss: 2.8878e-03\n",
      "Epoch: 39010 | training loss: 2.7876e-03 | validation loss: 2.8803e-03\n",
      "Epoch: 39020 | training loss: 2.7721e-03 | validation loss: 2.8490e-03\n",
      "Epoch: 39030 | training loss: 2.7566e-03 | validation loss: 2.8637e-03\n",
      "Epoch: 39040 | training loss: 2.7520e-03 | validation loss: 2.8553e-03\n",
      "Epoch: 39050 | training loss: 2.7474e-03 | validation loss: 2.8391e-03\n",
      "Epoch: 39060 | training loss: 2.7448e-03 | validation loss: 2.8365e-03\n",
      "Epoch: 39070 | training loss: 2.7444e-03 | validation loss: 2.8278e-03\n",
      "Epoch: 39080 | training loss: 2.7442e-03 | validation loss: 2.8293e-03\n",
      "Epoch: 39090 | training loss: 2.7440e-03 | validation loss: 2.8310e-03\n",
      "Epoch: 39100 | training loss: 2.7438e-03 | validation loss: 2.8311e-03\n",
      "Epoch: 39110 | training loss: 2.7436e-03 | validation loss: 2.8304e-03\n",
      "Epoch: 39120 | training loss: 2.7435e-03 | validation loss: 2.8303e-03\n",
      "Epoch: 39130 | training loss: 2.7433e-03 | validation loss: 2.8306e-03\n",
      "Epoch: 39140 | training loss: 2.7432e-03 | validation loss: 2.8297e-03\n",
      "Epoch: 39150 | training loss: 2.7443e-03 | validation loss: 2.8253e-03\n",
      "Epoch: 39160 | training loss: 2.9003e-03 | validation loss: 2.8417e-03\n",
      "Epoch: 39170 | training loss: 3.0341e-03 | validation loss: 2.8649e-03\n",
      "Epoch: 39180 | training loss: 3.2039e-03 | validation loss: 2.9242e-03\n",
      "Epoch: 39190 | training loss: 2.8583e-03 | validation loss: 2.8258e-03\n",
      "Epoch: 39200 | training loss: 2.7534e-03 | validation loss: 2.8184e-03\n",
      "Epoch: 39210 | training loss: 2.7432e-03 | validation loss: 2.8374e-03\n",
      "Epoch: 39220 | training loss: 2.7475e-03 | validation loss: 2.8465e-03\n",
      "Epoch: 39230 | training loss: 2.7441e-03 | validation loss: 2.8398e-03\n",
      "Epoch: 39240 | training loss: 2.7417e-03 | validation loss: 2.8298e-03\n",
      "Epoch: 39250 | training loss: 2.7419e-03 | validation loss: 2.8264e-03\n",
      "Epoch: 39260 | training loss: 2.7415e-03 | validation loss: 2.8289e-03\n",
      "Epoch: 39270 | training loss: 2.7413e-03 | validation loss: 2.8307e-03\n",
      "Epoch: 39280 | training loss: 2.7412e-03 | validation loss: 2.8291e-03\n",
      "Epoch: 39290 | training loss: 2.7410e-03 | validation loss: 2.8291e-03\n",
      "Epoch: 39300 | training loss: 2.7409e-03 | validation loss: 2.8294e-03\n",
      "Epoch: 39310 | training loss: 2.7407e-03 | validation loss: 2.8290e-03\n",
      "Epoch: 39320 | training loss: 2.7406e-03 | validation loss: 2.8292e-03\n",
      "Epoch: 39330 | training loss: 2.7404e-03 | validation loss: 2.8290e-03\n",
      "Epoch: 39340 | training loss: 2.7402e-03 | validation loss: 2.8290e-03\n",
      "Epoch: 39350 | training loss: 2.7401e-03 | validation loss: 2.8289e-03\n",
      "Epoch: 39360 | training loss: 2.7399e-03 | validation loss: 2.8288e-03\n",
      "Epoch: 39370 | training loss: 2.7398e-03 | validation loss: 2.8287e-03\n",
      "Epoch: 39380 | training loss: 2.7396e-03 | validation loss: 2.8286e-03\n",
      "Epoch: 39390 | training loss: 2.7395e-03 | validation loss: 2.8285e-03\n",
      "Epoch: 39400 | training loss: 2.7393e-03 | validation loss: 2.8282e-03\n",
      "Epoch: 39410 | training loss: 2.7395e-03 | validation loss: 2.8257e-03\n",
      "Epoch: 39420 | training loss: 2.7929e-03 | validation loss: 2.8167e-03\n",
      "Epoch: 39430 | training loss: 4.7708e-03 | validation loss: 3.5352e-03\n",
      "Epoch: 39440 | training loss: 3.2087e-03 | validation loss: 2.9677e-03\n",
      "Epoch: 39450 | training loss: 2.8676e-03 | validation loss: 2.8617e-03\n",
      "Epoch: 39460 | training loss: 2.7821e-03 | validation loss: 2.8335e-03\n",
      "Epoch: 39470 | training loss: 2.7514e-03 | validation loss: 2.8227e-03\n",
      "Epoch: 39480 | training loss: 2.7409e-03 | validation loss: 2.8219e-03\n",
      "Epoch: 39490 | training loss: 2.7382e-03 | validation loss: 2.8263e-03\n",
      "Epoch: 39500 | training loss: 2.7380e-03 | validation loss: 2.8294e-03\n",
      "Epoch: 39510 | training loss: 2.7381e-03 | validation loss: 2.8293e-03\n",
      "Epoch: 39520 | training loss: 2.7377e-03 | validation loss: 2.8286e-03\n",
      "Epoch: 39530 | training loss: 2.7374e-03 | validation loss: 2.8275e-03\n",
      "Epoch: 39540 | training loss: 2.7373e-03 | validation loss: 2.8266e-03\n",
      "Epoch: 39550 | training loss: 2.7371e-03 | validation loss: 2.8268e-03\n",
      "Epoch: 39560 | training loss: 2.7370e-03 | validation loss: 2.8270e-03\n",
      "Epoch: 39570 | training loss: 2.7368e-03 | validation loss: 2.8269e-03\n",
      "Epoch: 39580 | training loss: 2.7367e-03 | validation loss: 2.8267e-03\n",
      "Epoch: 39590 | training loss: 2.7365e-03 | validation loss: 2.8267e-03\n",
      "Epoch: 39600 | training loss: 2.7364e-03 | validation loss: 2.8266e-03\n",
      "Epoch: 39610 | training loss: 2.7362e-03 | validation loss: 2.8265e-03\n",
      "Epoch: 39620 | training loss: 2.7361e-03 | validation loss: 2.8264e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39630 | training loss: 2.7359e-03 | validation loss: 2.8259e-03\n",
      "Epoch: 39640 | training loss: 2.7362e-03 | validation loss: 2.8217e-03\n",
      "Epoch: 39650 | training loss: 2.8161e-03 | validation loss: 2.8138e-03\n",
      "Epoch: 39660 | training loss: 2.9242e-03 | validation loss: 2.8353e-03\n",
      "Epoch: 39670 | training loss: 2.8022e-03 | validation loss: 2.8648e-03\n",
      "Epoch: 39680 | training loss: 2.7604e-03 | validation loss: 2.8158e-03\n",
      "Epoch: 39690 | training loss: 2.7464e-03 | validation loss: 2.8136e-03\n",
      "Epoch: 39700 | training loss: 2.7404e-03 | validation loss: 2.8211e-03\n",
      "Epoch: 39710 | training loss: 2.7372e-03 | validation loss: 2.8196e-03\n",
      "Epoch: 39720 | training loss: 2.7356e-03 | validation loss: 2.8189e-03\n",
      "Epoch: 39730 | training loss: 2.7348e-03 | validation loss: 2.8268e-03\n",
      "Epoch: 39740 | training loss: 2.7343e-03 | validation loss: 2.8255e-03\n",
      "Epoch: 39750 | training loss: 2.7343e-03 | validation loss: 2.8245e-03\n",
      "Epoch: 39760 | training loss: 2.7342e-03 | validation loss: 2.8227e-03\n",
      "Epoch: 39770 | training loss: 2.7358e-03 | validation loss: 2.8167e-03\n",
      "Epoch: 39780 | training loss: 2.7930e-03 | validation loss: 2.8023e-03\n",
      "Epoch: 39790 | training loss: 3.7981e-03 | validation loss: 3.1088e-03\n",
      "Epoch: 39800 | training loss: 2.9957e-03 | validation loss: 3.0309e-03\n",
      "Epoch: 39810 | training loss: 2.7336e-03 | validation loss: 2.8247e-03\n",
      "Epoch: 39820 | training loss: 2.7684e-03 | validation loss: 2.8079e-03\n",
      "Epoch: 39830 | training loss: 2.7458e-03 | validation loss: 2.8520e-03\n",
      "Epoch: 39840 | training loss: 2.7337e-03 | validation loss: 2.8208e-03\n",
      "Epoch: 39850 | training loss: 2.7328e-03 | validation loss: 2.8234e-03\n",
      "Epoch: 39860 | training loss: 2.7327e-03 | validation loss: 2.8252e-03\n",
      "Epoch: 39870 | training loss: 2.7325e-03 | validation loss: 2.8239e-03\n",
      "Epoch: 39880 | training loss: 2.7323e-03 | validation loss: 2.8236e-03\n",
      "Epoch: 39890 | training loss: 2.7322e-03 | validation loss: 2.8253e-03\n",
      "Epoch: 39900 | training loss: 2.7320e-03 | validation loss: 2.8235e-03\n",
      "Epoch: 39910 | training loss: 2.7319e-03 | validation loss: 2.8232e-03\n",
      "Epoch: 39920 | training loss: 2.7317e-03 | validation loss: 2.8231e-03\n",
      "Epoch: 39930 | training loss: 2.7317e-03 | validation loss: 2.8220e-03\n",
      "Epoch: 39940 | training loss: 2.7341e-03 | validation loss: 2.8158e-03\n",
      "Epoch: 39950 | training loss: 2.8784e-03 | validation loss: 2.8238e-03\n",
      "Epoch: 39960 | training loss: 3.2878e-03 | validation loss: 2.9617e-03\n",
      "Epoch: 39970 | training loss: 2.8118e-03 | validation loss: 2.8036e-03\n",
      "Epoch: 39980 | training loss: 2.8129e-03 | validation loss: 2.9052e-03\n",
      "Epoch: 39990 | training loss: 2.7491e-03 | validation loss: 2.8585e-03\n",
      "Epoch: 40000 | training loss: 2.7404e-03 | validation loss: 2.8143e-03\n",
      "Epoch: 40010 | training loss: 2.7317e-03 | validation loss: 2.8151e-03\n",
      "Epoch: 40020 | training loss: 2.7323e-03 | validation loss: 2.8318e-03\n",
      "Epoch: 40030 | training loss: 2.7305e-03 | validation loss: 2.8216e-03\n",
      "Epoch: 40040 | training loss: 2.7300e-03 | validation loss: 2.8219e-03\n",
      "Epoch: 40050 | training loss: 2.7299e-03 | validation loss: 2.8242e-03\n",
      "Epoch: 40060 | training loss: 2.7297e-03 | validation loss: 2.8218e-03\n",
      "Epoch: 40070 | training loss: 2.7296e-03 | validation loss: 2.8232e-03\n",
      "Epoch: 40080 | training loss: 2.7294e-03 | validation loss: 2.8226e-03\n",
      "Epoch: 40090 | training loss: 2.7293e-03 | validation loss: 2.8223e-03\n",
      "Epoch: 40100 | training loss: 2.7291e-03 | validation loss: 2.8227e-03\n",
      "Epoch: 40110 | training loss: 2.7290e-03 | validation loss: 2.8227e-03\n",
      "Epoch: 40120 | training loss: 2.7288e-03 | validation loss: 2.8227e-03\n",
      "Epoch: 40130 | training loss: 2.7287e-03 | validation loss: 2.8236e-03\n",
      "Epoch: 40140 | training loss: 2.7303e-03 | validation loss: 2.8313e-03\n",
      "Epoch: 40150 | training loss: 2.8734e-03 | validation loss: 2.9939e-03\n",
      "Epoch: 40160 | training loss: 2.7951e-03 | validation loss: 2.8651e-03\n",
      "Epoch: 40170 | training loss: 3.2165e-03 | validation loss: 3.2316e-03\n",
      "Epoch: 40180 | training loss: 2.7834e-03 | validation loss: 2.8749e-03\n",
      "Epoch: 40190 | training loss: 2.7512e-03 | validation loss: 2.8724e-03\n",
      "Epoch: 40200 | training loss: 2.7362e-03 | validation loss: 2.8210e-03\n",
      "Epoch: 40210 | training loss: 2.7288e-03 | validation loss: 2.8304e-03\n",
      "Epoch: 40220 | training loss: 2.7276e-03 | validation loss: 2.8211e-03\n",
      "Epoch: 40230 | training loss: 2.7289e-03 | validation loss: 2.8119e-03\n",
      "Epoch: 40240 | training loss: 2.7280e-03 | validation loss: 2.8255e-03\n",
      "Epoch: 40250 | training loss: 2.7272e-03 | validation loss: 2.8244e-03\n",
      "Epoch: 40260 | training loss: 2.7268e-03 | validation loss: 2.8218e-03\n",
      "Epoch: 40270 | training loss: 2.7267e-03 | validation loss: 2.8199e-03\n",
      "Epoch: 40280 | training loss: 2.7270e-03 | validation loss: 2.8166e-03\n",
      "Epoch: 40290 | training loss: 2.7459e-03 | validation loss: 2.8024e-03\n",
      "Epoch: 40300 | training loss: 3.8453e-03 | validation loss: 3.1273e-03\n",
      "Epoch: 40310 | training loss: 3.3012e-03 | validation loss: 3.2181e-03\n",
      "Epoch: 40320 | training loss: 2.7284e-03 | validation loss: 2.8340e-03\n",
      "Epoch: 40330 | training loss: 2.7921e-03 | validation loss: 2.8148e-03\n",
      "Epoch: 40340 | training loss: 2.7298e-03 | validation loss: 2.8168e-03\n",
      "Epoch: 40350 | training loss: 2.7340e-03 | validation loss: 2.8417e-03\n",
      "Epoch: 40360 | training loss: 2.7255e-03 | validation loss: 2.8201e-03\n",
      "Epoch: 40370 | training loss: 2.7263e-03 | validation loss: 2.8139e-03\n",
      "Epoch: 40380 | training loss: 2.7255e-03 | validation loss: 2.8234e-03\n",
      "Epoch: 40390 | training loss: 2.7251e-03 | validation loss: 2.8192e-03\n",
      "Epoch: 40400 | training loss: 2.7249e-03 | validation loss: 2.8203e-03\n",
      "Epoch: 40410 | training loss: 2.7247e-03 | validation loss: 2.8195e-03\n",
      "Epoch: 40420 | training loss: 2.7246e-03 | validation loss: 2.8201e-03\n",
      "Epoch: 40430 | training loss: 2.7244e-03 | validation loss: 2.8194e-03\n",
      "Epoch: 40440 | training loss: 2.7243e-03 | validation loss: 2.8198e-03\n",
      "Epoch: 40450 | training loss: 2.7242e-03 | validation loss: 2.8197e-03\n",
      "Epoch: 40460 | training loss: 2.7240e-03 | validation loss: 2.8195e-03\n",
      "Epoch: 40470 | training loss: 2.7239e-03 | validation loss: 2.8194e-03\n",
      "Epoch: 40480 | training loss: 2.7237e-03 | validation loss: 2.8193e-03\n",
      "Epoch: 40490 | training loss: 2.7236e-03 | validation loss: 2.8188e-03\n",
      "Epoch: 40500 | training loss: 2.7243e-03 | validation loss: 2.8146e-03\n",
      "Epoch: 40510 | training loss: 2.8742e-03 | validation loss: 2.8220e-03\n",
      "Epoch: 40520 | training loss: 2.9319e-03 | validation loss: 2.8499e-03\n",
      "Epoch: 40530 | training loss: 3.1214e-03 | validation loss: 2.8972e-03\n",
      "Epoch: 40540 | training loss: 2.8812e-03 | validation loss: 2.8172e-03\n",
      "Epoch: 40550 | training loss: 2.7823e-03 | validation loss: 2.7953e-03\n",
      "Epoch: 40560 | training loss: 2.7459e-03 | validation loss: 2.7970e-03\n",
      "Epoch: 40570 | training loss: 2.7310e-03 | validation loss: 2.8038e-03\n",
      "Epoch: 40580 | training loss: 2.7247e-03 | validation loss: 2.8109e-03\n",
      "Epoch: 40590 | training loss: 2.7224e-03 | validation loss: 2.8159e-03\n",
      "Epoch: 40600 | training loss: 2.7221e-03 | validation loss: 2.8195e-03\n",
      "Epoch: 40610 | training loss: 2.7221e-03 | validation loss: 2.8207e-03\n",
      "Epoch: 40620 | training loss: 2.7218e-03 | validation loss: 2.8191e-03\n",
      "Epoch: 40630 | training loss: 2.7217e-03 | validation loss: 2.8176e-03\n",
      "Epoch: 40640 | training loss: 2.7215e-03 | validation loss: 2.8177e-03\n",
      "Epoch: 40650 | training loss: 2.7214e-03 | validation loss: 2.8182e-03\n",
      "Epoch: 40660 | training loss: 2.7212e-03 | validation loss: 2.8180e-03\n",
      "Epoch: 40670 | training loss: 2.7211e-03 | validation loss: 2.8178e-03\n",
      "Epoch: 40680 | training loss: 2.7209e-03 | validation loss: 2.8179e-03\n",
      "Epoch: 40690 | training loss: 2.7208e-03 | validation loss: 2.8177e-03\n",
      "Epoch: 40700 | training loss: 2.7206e-03 | validation loss: 2.8177e-03\n",
      "Epoch: 40710 | training loss: 2.7205e-03 | validation loss: 2.8176e-03\n",
      "Epoch: 40720 | training loss: 2.7204e-03 | validation loss: 2.8175e-03\n",
      "Epoch: 40730 | training loss: 2.7202e-03 | validation loss: 2.8170e-03\n",
      "Epoch: 40740 | training loss: 2.7208e-03 | validation loss: 2.8133e-03\n",
      "Epoch: 40750 | training loss: 2.8919e-03 | validation loss: 2.8751e-03\n",
      "Epoch: 40760 | training loss: 2.9899e-03 | validation loss: 3.0589e-03\n",
      "Epoch: 40770 | training loss: 2.7465e-03 | validation loss: 2.8685e-03\n",
      "Epoch: 40780 | training loss: 2.7223e-03 | validation loss: 2.8278e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40790 | training loss: 2.7200e-03 | validation loss: 2.8123e-03\n",
      "Epoch: 40800 | training loss: 2.7200e-03 | validation loss: 2.8100e-03\n",
      "Epoch: 40810 | training loss: 2.7207e-03 | validation loss: 2.8075e-03\n",
      "Epoch: 40820 | training loss: 2.7276e-03 | validation loss: 2.7978e-03\n",
      "Epoch: 40830 | training loss: 2.9498e-03 | validation loss: 2.8172e-03\n",
      "Epoch: 40840 | training loss: 2.8360e-03 | validation loss: 2.8061e-03\n",
      "Epoch: 40850 | training loss: 2.7218e-03 | validation loss: 2.8072e-03\n",
      "Epoch: 40860 | training loss: 2.7598e-03 | validation loss: 2.8744e-03\n",
      "Epoch: 40870 | training loss: 2.7433e-03 | validation loss: 2.7966e-03\n",
      "Epoch: 40880 | training loss: 2.7260e-03 | validation loss: 2.8360e-03\n",
      "Epoch: 40890 | training loss: 2.7205e-03 | validation loss: 2.8071e-03\n",
      "Epoch: 40900 | training loss: 2.7190e-03 | validation loss: 2.8223e-03\n",
      "Epoch: 40910 | training loss: 2.7183e-03 | validation loss: 2.8113e-03\n",
      "Epoch: 40920 | training loss: 2.7177e-03 | validation loss: 2.8175e-03\n",
      "Epoch: 40930 | training loss: 2.7175e-03 | validation loss: 2.8167e-03\n",
      "Epoch: 40940 | training loss: 2.7174e-03 | validation loss: 2.8145e-03\n",
      "Epoch: 40950 | training loss: 2.7173e-03 | validation loss: 2.8137e-03\n",
      "Epoch: 40960 | training loss: 2.7176e-03 | validation loss: 2.8111e-03\n",
      "Epoch: 40970 | training loss: 2.7287e-03 | validation loss: 2.7999e-03\n",
      "Epoch: 40980 | training loss: 3.2230e-03 | validation loss: 2.9115e-03\n",
      "Epoch: 40990 | training loss: 2.8063e-03 | validation loss: 2.9192e-03\n",
      "Epoch: 41000 | training loss: 2.9162e-03 | validation loss: 2.8384e-03\n",
      "Epoch: 41010 | training loss: 2.7271e-03 | validation loss: 2.8336e-03\n",
      "Epoch: 41020 | training loss: 2.7293e-03 | validation loss: 2.8359e-03\n",
      "Epoch: 41030 | training loss: 2.7254e-03 | validation loss: 2.8020e-03\n",
      "Epoch: 41040 | training loss: 2.7174e-03 | validation loss: 2.8232e-03\n",
      "Epoch: 41050 | training loss: 2.7159e-03 | validation loss: 2.8136e-03\n",
      "Epoch: 41060 | training loss: 2.7157e-03 | validation loss: 2.8139e-03\n",
      "Epoch: 41070 | training loss: 2.7156e-03 | validation loss: 2.8146e-03\n",
      "Epoch: 41080 | training loss: 2.7154e-03 | validation loss: 2.8151e-03\n",
      "Epoch: 41090 | training loss: 2.7153e-03 | validation loss: 2.8134e-03\n",
      "Epoch: 41100 | training loss: 2.7151e-03 | validation loss: 2.8146e-03\n",
      "Epoch: 41110 | training loss: 2.7150e-03 | validation loss: 2.8147e-03\n",
      "Epoch: 41120 | training loss: 2.7149e-03 | validation loss: 2.8146e-03\n",
      "Epoch: 41130 | training loss: 2.7148e-03 | validation loss: 2.8153e-03\n",
      "Epoch: 41140 | training loss: 2.7156e-03 | validation loss: 2.8200e-03\n",
      "Epoch: 41150 | training loss: 2.7703e-03 | validation loss: 2.8803e-03\n",
      "Epoch: 41160 | training loss: 4.2192e-03 | validation loss: 3.7250e-03\n",
      "Epoch: 41170 | training loss: 2.7912e-03 | validation loss: 2.8087e-03\n",
      "Epoch: 41180 | training loss: 2.8688e-03 | validation loss: 2.8146e-03\n",
      "Epoch: 41190 | training loss: 2.7145e-03 | validation loss: 2.8066e-03\n",
      "Epoch: 41200 | training loss: 2.7337e-03 | validation loss: 2.8490e-03\n",
      "Epoch: 41210 | training loss: 2.7140e-03 | validation loss: 2.8174e-03\n",
      "Epoch: 41220 | training loss: 2.7163e-03 | validation loss: 2.8043e-03\n",
      "Epoch: 41230 | training loss: 2.7136e-03 | validation loss: 2.8163e-03\n",
      "Epoch: 41240 | training loss: 2.7133e-03 | validation loss: 2.8146e-03\n",
      "Epoch: 41250 | training loss: 2.7132e-03 | validation loss: 2.8113e-03\n",
      "Epoch: 41260 | training loss: 2.7130e-03 | validation loss: 2.8144e-03\n",
      "Epoch: 41270 | training loss: 2.7128e-03 | validation loss: 2.8120e-03\n",
      "Epoch: 41280 | training loss: 2.7127e-03 | validation loss: 2.8134e-03\n",
      "Epoch: 41290 | training loss: 2.7125e-03 | validation loss: 2.8126e-03\n",
      "Epoch: 41300 | training loss: 2.7124e-03 | validation loss: 2.8127e-03\n",
      "Epoch: 41310 | training loss: 2.7122e-03 | validation loss: 2.8131e-03\n",
      "Epoch: 41320 | training loss: 2.7122e-03 | validation loss: 2.8148e-03\n",
      "Epoch: 41330 | training loss: 2.7244e-03 | validation loss: 2.8416e-03\n",
      "Epoch: 41340 | training loss: 3.3123e-03 | validation loss: 3.3852e-03\n",
      "Epoch: 41350 | training loss: 2.7495e-03 | validation loss: 2.8629e-03\n",
      "Epoch: 41360 | training loss: 2.7602e-03 | validation loss: 2.8257e-03\n",
      "Epoch: 41370 | training loss: 2.7348e-03 | validation loss: 2.7990e-03\n",
      "Epoch: 41380 | training loss: 2.7224e-03 | validation loss: 2.7909e-03\n",
      "Epoch: 41390 | training loss: 2.8037e-03 | validation loss: 2.7941e-03\n",
      "Epoch: 41400 | training loss: 3.1712e-03 | validation loss: 2.8897e-03\n",
      "Epoch: 41410 | training loss: 2.8450e-03 | validation loss: 2.9455e-03\n",
      "Epoch: 41420 | training loss: 2.7487e-03 | validation loss: 2.7901e-03\n",
      "Epoch: 41430 | training loss: 2.7163e-03 | validation loss: 2.8272e-03\n",
      "Epoch: 41440 | training loss: 2.7109e-03 | validation loss: 2.8145e-03\n",
      "Epoch: 41450 | training loss: 2.7143e-03 | validation loss: 2.8009e-03\n",
      "Epoch: 41460 | training loss: 2.7103e-03 | validation loss: 2.8127e-03\n",
      "Epoch: 41470 | training loss: 2.7114e-03 | validation loss: 2.8185e-03\n",
      "Epoch: 41480 | training loss: 2.7149e-03 | validation loss: 2.8267e-03\n",
      "Epoch: 41490 | training loss: 2.7683e-03 | validation loss: 2.8840e-03\n",
      "Epoch: 41500 | training loss: 3.3685e-03 | validation loss: 3.2642e-03\n",
      "Epoch: 41510 | training loss: 2.8899e-03 | validation loss: 2.8188e-03\n",
      "Epoch: 41520 | training loss: 2.7886e-03 | validation loss: 2.8938e-03\n",
      "Epoch: 41530 | training loss: 2.7410e-03 | validation loss: 2.7934e-03\n",
      "Epoch: 41540 | training loss: 2.7207e-03 | validation loss: 2.8355e-03\n",
      "Epoch: 41550 | training loss: 2.7117e-03 | validation loss: 2.8021e-03\n",
      "Epoch: 41560 | training loss: 2.7089e-03 | validation loss: 2.8104e-03\n",
      "Epoch: 41570 | training loss: 2.7096e-03 | validation loss: 2.8160e-03\n",
      "Epoch: 41580 | training loss: 2.7086e-03 | validation loss: 2.8096e-03\n",
      "Epoch: 41590 | training loss: 2.7087e-03 | validation loss: 2.8074e-03\n",
      "Epoch: 41600 | training loss: 2.7100e-03 | validation loss: 2.8036e-03\n",
      "Epoch: 41610 | training loss: 2.7385e-03 | validation loss: 2.7938e-03\n",
      "Epoch: 41620 | training loss: 3.4651e-03 | validation loss: 3.0111e-03\n",
      "Epoch: 41630 | training loss: 2.9610e-03 | validation loss: 3.0158e-03\n",
      "Epoch: 41640 | training loss: 2.8441e-03 | validation loss: 2.8128e-03\n",
      "Epoch: 41650 | training loss: 2.7250e-03 | validation loss: 2.8356e-03\n",
      "Epoch: 41660 | training loss: 2.7077e-03 | validation loss: 2.8128e-03\n",
      "Epoch: 41670 | training loss: 2.7089e-03 | validation loss: 2.8028e-03\n",
      "Epoch: 41680 | training loss: 2.7082e-03 | validation loss: 2.8154e-03\n",
      "Epoch: 41690 | training loss: 2.7073e-03 | validation loss: 2.8067e-03\n",
      "Epoch: 41700 | training loss: 2.7070e-03 | validation loss: 2.8097e-03\n",
      "Epoch: 41710 | training loss: 2.7070e-03 | validation loss: 2.8106e-03\n",
      "Epoch: 41720 | training loss: 2.7067e-03 | validation loss: 2.8081e-03\n",
      "Epoch: 41730 | training loss: 2.7066e-03 | validation loss: 2.8082e-03\n",
      "Epoch: 41740 | training loss: 2.7065e-03 | validation loss: 2.8082e-03\n",
      "Epoch: 41750 | training loss: 2.7065e-03 | validation loss: 2.8069e-03\n",
      "Epoch: 41760 | training loss: 2.7119e-03 | validation loss: 2.8010e-03\n",
      "Epoch: 41770 | training loss: 3.0100e-03 | validation loss: 2.8773e-03\n",
      "Epoch: 41780 | training loss: 2.7671e-03 | validation loss: 2.7835e-03\n",
      "Epoch: 41790 | training loss: 2.9279e-03 | validation loss: 2.9028e-03\n",
      "Epoch: 41800 | training loss: 2.7196e-03 | validation loss: 2.8467e-03\n",
      "Epoch: 41810 | training loss: 2.7344e-03 | validation loss: 2.8133e-03\n",
      "Epoch: 41820 | training loss: 2.7086e-03 | validation loss: 2.8113e-03\n",
      "Epoch: 41830 | training loss: 2.7079e-03 | validation loss: 2.8115e-03\n",
      "Epoch: 41840 | training loss: 2.7054e-03 | validation loss: 2.8084e-03\n",
      "Epoch: 41850 | training loss: 2.7055e-03 | validation loss: 2.8074e-03\n",
      "Epoch: 41860 | training loss: 2.7049e-03 | validation loss: 2.8090e-03\n",
      "Epoch: 41870 | training loss: 2.7047e-03 | validation loss: 2.8069e-03\n",
      "Epoch: 41880 | training loss: 2.7046e-03 | validation loss: 2.8074e-03\n",
      "Epoch: 41890 | training loss: 2.7044e-03 | validation loss: 2.8080e-03\n",
      "Epoch: 41900 | training loss: 2.7043e-03 | validation loss: 2.8081e-03\n",
      "Epoch: 41910 | training loss: 2.7042e-03 | validation loss: 2.8099e-03\n",
      "Epoch: 41920 | training loss: 2.7072e-03 | validation loss: 2.8234e-03\n",
      "Epoch: 41930 | training loss: 2.9244e-03 | validation loss: 3.0447e-03\n",
      "Epoch: 41940 | training loss: 2.8160e-03 | validation loss: 2.8733e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41950 | training loss: 2.8154e-03 | validation loss: 2.9532e-03\n",
      "Epoch: 41960 | training loss: 2.7507e-03 | validation loss: 2.8763e-03\n",
      "Epoch: 41970 | training loss: 2.7073e-03 | validation loss: 2.8229e-03\n",
      "Epoch: 41980 | training loss: 2.7081e-03 | validation loss: 2.7916e-03\n",
      "Epoch: 41990 | training loss: 2.7058e-03 | validation loss: 2.8020e-03\n",
      "Epoch: 42000 | training loss: 2.7030e-03 | validation loss: 2.8063e-03\n",
      "Epoch: 42010 | training loss: 2.7032e-03 | validation loss: 2.8094e-03\n",
      "Epoch: 42020 | training loss: 2.7028e-03 | validation loss: 2.8069e-03\n",
      "Epoch: 42030 | training loss: 2.7026e-03 | validation loss: 2.8059e-03\n",
      "Epoch: 42040 | training loss: 2.7024e-03 | validation loss: 2.8073e-03\n",
      "Epoch: 42050 | training loss: 2.7023e-03 | validation loss: 2.8063e-03\n",
      "Epoch: 42060 | training loss: 2.7025e-03 | validation loss: 2.8089e-03\n",
      "Epoch: 42070 | training loss: 2.7198e-03 | validation loss: 2.8346e-03\n",
      "Epoch: 42080 | training loss: 3.9935e-03 | validation loss: 3.5944e-03\n",
      "Epoch: 42090 | training loss: 3.3312e-03 | validation loss: 2.9600e-03\n",
      "Epoch: 42100 | training loss: 2.7756e-03 | validation loss: 2.8010e-03\n",
      "Epoch: 42110 | training loss: 2.7168e-03 | validation loss: 2.8390e-03\n",
      "Epoch: 42120 | training loss: 2.7304e-03 | validation loss: 2.8514e-03\n",
      "Epoch: 42130 | training loss: 2.7025e-03 | validation loss: 2.8126e-03\n",
      "Epoch: 42140 | training loss: 2.7041e-03 | validation loss: 2.7963e-03\n",
      "Epoch: 42150 | training loss: 2.7013e-03 | validation loss: 2.8019e-03\n",
      "Epoch: 42160 | training loss: 2.7014e-03 | validation loss: 2.8094e-03\n",
      "Epoch: 42170 | training loss: 2.7008e-03 | validation loss: 2.8047e-03\n",
      "Epoch: 42180 | training loss: 2.7006e-03 | validation loss: 2.8043e-03\n",
      "Epoch: 42190 | training loss: 2.7005e-03 | validation loss: 2.8056e-03\n",
      "Epoch: 42200 | training loss: 2.7004e-03 | validation loss: 2.8045e-03\n",
      "Epoch: 42210 | training loss: 2.7002e-03 | validation loss: 2.8050e-03\n",
      "Epoch: 42220 | training loss: 2.7001e-03 | validation loss: 2.8046e-03\n",
      "Epoch: 42230 | training loss: 2.7000e-03 | validation loss: 2.8047e-03\n",
      "Epoch: 42240 | training loss: 2.6998e-03 | validation loss: 2.8047e-03\n",
      "Epoch: 42250 | training loss: 2.6997e-03 | validation loss: 2.8045e-03\n",
      "Epoch: 42260 | training loss: 2.6996e-03 | validation loss: 2.8044e-03\n",
      "Epoch: 42270 | training loss: 2.6994e-03 | validation loss: 2.8043e-03\n",
      "Epoch: 42280 | training loss: 2.6993e-03 | validation loss: 2.8038e-03\n",
      "Epoch: 42290 | training loss: 2.6996e-03 | validation loss: 2.8011e-03\n",
      "Epoch: 42300 | training loss: 2.7423e-03 | validation loss: 2.7913e-03\n",
      "Epoch: 42310 | training loss: 4.7366e-03 | validation loss: 3.5225e-03\n",
      "Epoch: 42320 | training loss: 2.7192e-03 | validation loss: 2.8123e-03\n",
      "Epoch: 42330 | training loss: 2.7465e-03 | validation loss: 2.8643e-03\n",
      "Epoch: 42340 | training loss: 2.7527e-03 | validation loss: 2.8567e-03\n",
      "Epoch: 42350 | training loss: 2.7255e-03 | validation loss: 2.8332e-03\n",
      "Epoch: 42360 | training loss: 2.7032e-03 | validation loss: 2.8147e-03\n",
      "Epoch: 42370 | training loss: 2.6982e-03 | validation loss: 2.8034e-03\n",
      "Epoch: 42380 | training loss: 2.6991e-03 | validation loss: 2.7996e-03\n",
      "Epoch: 42390 | training loss: 2.6981e-03 | validation loss: 2.8023e-03\n",
      "Epoch: 42400 | training loss: 2.6978e-03 | validation loss: 2.8041e-03\n",
      "Epoch: 42410 | training loss: 2.6976e-03 | validation loss: 2.8037e-03\n",
      "Epoch: 42420 | training loss: 2.6975e-03 | validation loss: 2.8024e-03\n",
      "Epoch: 42430 | training loss: 2.6974e-03 | validation loss: 2.8030e-03\n",
      "Epoch: 42440 | training loss: 2.6972e-03 | validation loss: 2.8029e-03\n",
      "Epoch: 42450 | training loss: 2.6971e-03 | validation loss: 2.8026e-03\n",
      "Epoch: 42460 | training loss: 2.6970e-03 | validation loss: 2.8027e-03\n",
      "Epoch: 42470 | training loss: 2.6968e-03 | validation loss: 2.8025e-03\n",
      "Epoch: 42480 | training loss: 2.6967e-03 | validation loss: 2.8023e-03\n",
      "Epoch: 42490 | training loss: 2.6966e-03 | validation loss: 2.8012e-03\n",
      "Epoch: 42500 | training loss: 2.6991e-03 | validation loss: 2.7922e-03\n",
      "Epoch: 42510 | training loss: 3.0870e-03 | validation loss: 2.9227e-03\n",
      "Epoch: 42520 | training loss: 2.9885e-03 | validation loss: 3.0837e-03\n",
      "Epoch: 42530 | training loss: 2.7710e-03 | validation loss: 2.9021e-03\n",
      "Epoch: 42540 | training loss: 2.7094e-03 | validation loss: 2.8245e-03\n",
      "Epoch: 42550 | training loss: 2.6975e-03 | validation loss: 2.8124e-03\n",
      "Epoch: 42560 | training loss: 2.6976e-03 | validation loss: 2.7914e-03\n",
      "Epoch: 42570 | training loss: 2.6974e-03 | validation loss: 2.8029e-03\n",
      "Epoch: 42580 | training loss: 2.6961e-03 | validation loss: 2.7996e-03\n",
      "Epoch: 42590 | training loss: 2.6954e-03 | validation loss: 2.7984e-03\n",
      "Epoch: 42600 | training loss: 2.6957e-03 | validation loss: 2.7980e-03\n",
      "Epoch: 42610 | training loss: 2.6996e-03 | validation loss: 2.7901e-03\n",
      "Epoch: 42620 | training loss: 2.8167e-03 | validation loss: 2.7859e-03\n",
      "Epoch: 42630 | training loss: 3.3158e-03 | validation loss: 2.9383e-03\n",
      "Epoch: 42640 | training loss: 2.7743e-03 | validation loss: 2.8822e-03\n",
      "Epoch: 42650 | training loss: 2.6987e-03 | validation loss: 2.8105e-03\n",
      "Epoch: 42660 | training loss: 2.7158e-03 | validation loss: 2.7833e-03\n",
      "Epoch: 42670 | training loss: 2.7063e-03 | validation loss: 2.8265e-03\n",
      "Epoch: 42680 | training loss: 2.6985e-03 | validation loss: 2.7915e-03\n",
      "Epoch: 42690 | training loss: 2.6957e-03 | validation loss: 2.8082e-03\n",
      "Epoch: 42700 | training loss: 2.6946e-03 | validation loss: 2.7957e-03\n",
      "Epoch: 42710 | training loss: 2.6939e-03 | validation loss: 2.8026e-03\n",
      "Epoch: 42720 | training loss: 2.6936e-03 | validation loss: 2.8009e-03\n",
      "Epoch: 42730 | training loss: 2.6936e-03 | validation loss: 2.7987e-03\n",
      "Epoch: 42740 | training loss: 2.6934e-03 | validation loss: 2.7987e-03\n",
      "Epoch: 42750 | training loss: 2.6934e-03 | validation loss: 2.7977e-03\n",
      "Epoch: 42760 | training loss: 2.6954e-03 | validation loss: 2.7924e-03\n",
      "Epoch: 42770 | training loss: 2.7814e-03 | validation loss: 2.7879e-03\n",
      "Epoch: 42780 | training loss: 3.7354e-03 | validation loss: 3.1107e-03\n",
      "Epoch: 42790 | training loss: 2.7588e-03 | validation loss: 2.8660e-03\n",
      "Epoch: 42800 | training loss: 2.7656e-03 | validation loss: 2.8812e-03\n",
      "Epoch: 42810 | training loss: 2.7210e-03 | validation loss: 2.7893e-03\n",
      "Epoch: 42820 | training loss: 2.6935e-03 | validation loss: 2.7918e-03\n",
      "Epoch: 42830 | training loss: 2.6974e-03 | validation loss: 2.8128e-03\n",
      "Epoch: 42840 | training loss: 2.6940e-03 | validation loss: 2.7946e-03\n",
      "Epoch: 42850 | training loss: 2.6924e-03 | validation loss: 2.8014e-03\n",
      "Epoch: 42860 | training loss: 2.6919e-03 | validation loss: 2.7983e-03\n",
      "Epoch: 42870 | training loss: 2.6918e-03 | validation loss: 2.7997e-03\n",
      "Epoch: 42880 | training loss: 2.6916e-03 | validation loss: 2.7981e-03\n",
      "Epoch: 42890 | training loss: 2.6915e-03 | validation loss: 2.7993e-03\n",
      "Epoch: 42900 | training loss: 2.6913e-03 | validation loss: 2.7988e-03\n",
      "Epoch: 42910 | training loss: 2.6912e-03 | validation loss: 2.7984e-03\n",
      "Epoch: 42920 | training loss: 2.6911e-03 | validation loss: 2.7981e-03\n",
      "Epoch: 42930 | training loss: 2.6910e-03 | validation loss: 2.7974e-03\n",
      "Epoch: 42940 | training loss: 2.6922e-03 | validation loss: 2.7937e-03\n",
      "Epoch: 42950 | training loss: 2.7680e-03 | validation loss: 2.7956e-03\n",
      "Epoch: 42960 | training loss: 3.9698e-03 | validation loss: 3.2033e-03\n",
      "Epoch: 42970 | training loss: 2.7288e-03 | validation loss: 2.8715e-03\n",
      "Epoch: 42980 | training loss: 2.7994e-03 | validation loss: 2.8816e-03\n",
      "Epoch: 42990 | training loss: 2.7275e-03 | validation loss: 2.8043e-03\n",
      "Epoch: 43000 | training loss: 2.6906e-03 | validation loss: 2.8025e-03\n",
      "Epoch: 43010 | training loss: 2.6958e-03 | validation loss: 2.8038e-03\n",
      "Epoch: 43020 | training loss: 2.6903e-03 | validation loss: 2.7958e-03\n",
      "Epoch: 43030 | training loss: 2.6904e-03 | validation loss: 2.7992e-03\n",
      "Epoch: 43040 | training loss: 2.6896e-03 | validation loss: 2.7964e-03\n",
      "Epoch: 43050 | training loss: 2.6895e-03 | validation loss: 2.7976e-03\n",
      "Epoch: 43060 | training loss: 2.6893e-03 | validation loss: 2.7974e-03\n",
      "Epoch: 43070 | training loss: 2.6892e-03 | validation loss: 2.7967e-03\n",
      "Epoch: 43080 | training loss: 2.6890e-03 | validation loss: 2.7967e-03\n",
      "Epoch: 43090 | training loss: 2.6889e-03 | validation loss: 2.7966e-03\n",
      "Epoch: 43100 | training loss: 2.6888e-03 | validation loss: 2.7963e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43110 | training loss: 2.6887e-03 | validation loss: 2.7944e-03\n",
      "Epoch: 43120 | training loss: 2.6935e-03 | validation loss: 2.7813e-03\n",
      "Epoch: 43130 | training loss: 3.1782e-03 | validation loss: 2.8844e-03\n",
      "Epoch: 43140 | training loss: 3.0427e-03 | validation loss: 3.1673e-03\n",
      "Epoch: 43150 | training loss: 2.7550e-03 | validation loss: 2.8552e-03\n",
      "Epoch: 43160 | training loss: 2.7015e-03 | validation loss: 2.8234e-03\n",
      "Epoch: 43170 | training loss: 2.6929e-03 | validation loss: 2.7804e-03\n",
      "Epoch: 43180 | training loss: 2.6922e-03 | validation loss: 2.7889e-03\n",
      "Epoch: 43190 | training loss: 2.6901e-03 | validation loss: 2.7866e-03\n",
      "Epoch: 43200 | training loss: 2.6879e-03 | validation loss: 2.7932e-03\n",
      "Epoch: 43210 | training loss: 2.6875e-03 | validation loss: 2.7953e-03\n",
      "Epoch: 43220 | training loss: 2.6874e-03 | validation loss: 2.7986e-03\n",
      "Epoch: 43230 | training loss: 2.6872e-03 | validation loss: 2.7957e-03\n",
      "Epoch: 43240 | training loss: 2.6870e-03 | validation loss: 2.7940e-03\n",
      "Epoch: 43250 | training loss: 2.6869e-03 | validation loss: 2.7945e-03\n",
      "Epoch: 43260 | training loss: 2.6870e-03 | validation loss: 2.7931e-03\n",
      "Epoch: 43270 | training loss: 2.6925e-03 | validation loss: 2.7855e-03\n",
      "Epoch: 43280 | training loss: 3.0205e-03 | validation loss: 2.8609e-03\n",
      "Epoch: 43290 | training loss: 2.6873e-03 | validation loss: 2.7907e-03\n",
      "Epoch: 43300 | training loss: 2.9443e-03 | validation loss: 2.8254e-03\n",
      "Epoch: 43310 | training loss: 2.6885e-03 | validation loss: 2.8032e-03\n",
      "Epoch: 43320 | training loss: 2.7198e-03 | validation loss: 2.8414e-03\n",
      "Epoch: 43330 | training loss: 2.6872e-03 | validation loss: 2.7905e-03\n",
      "Epoch: 43340 | training loss: 2.6886e-03 | validation loss: 2.7873e-03\n",
      "Epoch: 43350 | training loss: 2.6872e-03 | validation loss: 2.8012e-03\n",
      "Epoch: 43360 | training loss: 2.6857e-03 | validation loss: 2.7922e-03\n",
      "Epoch: 43370 | training loss: 2.6854e-03 | validation loss: 2.7944e-03\n",
      "Epoch: 43380 | training loss: 2.6853e-03 | validation loss: 2.7944e-03\n",
      "Epoch: 43390 | training loss: 2.6852e-03 | validation loss: 2.7939e-03\n",
      "Epoch: 43400 | training loss: 2.6851e-03 | validation loss: 2.7937e-03\n",
      "Epoch: 43410 | training loss: 2.6849e-03 | validation loss: 2.7943e-03\n",
      "Epoch: 43420 | training loss: 2.6848e-03 | validation loss: 2.7937e-03\n",
      "Epoch: 43430 | training loss: 2.6847e-03 | validation loss: 2.7935e-03\n",
      "Epoch: 43440 | training loss: 2.6846e-03 | validation loss: 2.7934e-03\n",
      "Epoch: 43450 | training loss: 2.6844e-03 | validation loss: 2.7931e-03\n",
      "Epoch: 43460 | training loss: 2.6845e-03 | validation loss: 2.7912e-03\n",
      "Epoch: 43470 | training loss: 2.6970e-03 | validation loss: 2.7811e-03\n",
      "Epoch: 43480 | training loss: 3.7498e-03 | validation loss: 3.1249e-03\n",
      "Epoch: 43490 | training loss: 3.4164e-03 | validation loss: 3.2787e-03\n",
      "Epoch: 43500 | training loss: 2.7137e-03 | validation loss: 2.8273e-03\n",
      "Epoch: 43510 | training loss: 2.7135e-03 | validation loss: 2.7749e-03\n",
      "Epoch: 43520 | training loss: 2.7151e-03 | validation loss: 2.7839e-03\n",
      "Epoch: 43530 | training loss: 2.6850e-03 | validation loss: 2.7910e-03\n",
      "Epoch: 43540 | training loss: 2.6862e-03 | validation loss: 2.8005e-03\n",
      "Epoch: 43550 | training loss: 2.6837e-03 | validation loss: 2.7958e-03\n",
      "Epoch: 43560 | training loss: 2.6835e-03 | validation loss: 2.7904e-03\n",
      "Epoch: 43570 | training loss: 2.6830e-03 | validation loss: 2.7920e-03\n",
      "Epoch: 43580 | training loss: 2.6829e-03 | validation loss: 2.7936e-03\n",
      "Epoch: 43590 | training loss: 2.6827e-03 | validation loss: 2.7914e-03\n",
      "Epoch: 43600 | training loss: 2.6826e-03 | validation loss: 2.7926e-03\n",
      "Epoch: 43610 | training loss: 2.6825e-03 | validation loss: 2.7919e-03\n",
      "Epoch: 43620 | training loss: 2.6823e-03 | validation loss: 2.7921e-03\n",
      "Epoch: 43630 | training loss: 2.6822e-03 | validation loss: 2.7918e-03\n",
      "Epoch: 43640 | training loss: 2.6821e-03 | validation loss: 2.7919e-03\n",
      "Epoch: 43650 | training loss: 2.6819e-03 | validation loss: 2.7918e-03\n",
      "Epoch: 43660 | training loss: 2.6818e-03 | validation loss: 2.7915e-03\n",
      "Epoch: 43670 | training loss: 2.6817e-03 | validation loss: 2.7899e-03\n",
      "Epoch: 43680 | training loss: 2.6905e-03 | validation loss: 2.7773e-03\n",
      "Epoch: 43690 | training loss: 3.5425e-03 | validation loss: 3.1627e-03\n",
      "Epoch: 43700 | training loss: 2.7117e-03 | validation loss: 2.7645e-03\n",
      "Epoch: 43710 | training loss: 2.7084e-03 | validation loss: 2.8371e-03\n",
      "Epoch: 43720 | training loss: 2.6946e-03 | validation loss: 2.7730e-03\n",
      "Epoch: 43730 | training loss: 2.6824e-03 | validation loss: 2.7980e-03\n",
      "Epoch: 43740 | training loss: 2.6824e-03 | validation loss: 2.7996e-03\n",
      "Epoch: 43750 | training loss: 2.6811e-03 | validation loss: 2.7897e-03\n",
      "Epoch: 43760 | training loss: 2.6821e-03 | validation loss: 2.7863e-03\n",
      "Epoch: 43770 | training loss: 2.6897e-03 | validation loss: 2.7776e-03\n",
      "Epoch: 43780 | training loss: 2.8491e-03 | validation loss: 2.7863e-03\n",
      "Epoch: 43790 | training loss: 3.0131e-03 | validation loss: 2.8353e-03\n",
      "Epoch: 43800 | training loss: 2.7541e-03 | validation loss: 2.8741e-03\n",
      "Epoch: 43810 | training loss: 2.6869e-03 | validation loss: 2.7772e-03\n",
      "Epoch: 43820 | training loss: 2.6801e-03 | validation loss: 2.7918e-03\n",
      "Epoch: 43830 | training loss: 2.6799e-03 | validation loss: 2.7872e-03\n",
      "Epoch: 43840 | training loss: 2.6803e-03 | validation loss: 2.7940e-03\n",
      "Epoch: 43850 | training loss: 2.6807e-03 | validation loss: 2.7835e-03\n",
      "Epoch: 43860 | training loss: 2.6798e-03 | validation loss: 2.7935e-03\n",
      "Epoch: 43870 | training loss: 2.6794e-03 | validation loss: 2.7918e-03\n",
      "Epoch: 43880 | training loss: 2.6792e-03 | validation loss: 2.7889e-03\n",
      "Epoch: 43890 | training loss: 2.6791e-03 | validation loss: 2.7874e-03\n",
      "Epoch: 43900 | training loss: 2.6803e-03 | validation loss: 2.7829e-03\n",
      "Epoch: 43910 | training loss: 2.7295e-03 | validation loss: 2.7710e-03\n",
      "Epoch: 43920 | training loss: 3.9317e-03 | validation loss: 3.1660e-03\n",
      "Epoch: 43930 | training loss: 2.9609e-03 | validation loss: 3.0057e-03\n",
      "Epoch: 43940 | training loss: 2.6988e-03 | validation loss: 2.8212e-03\n",
      "Epoch: 43950 | training loss: 2.7272e-03 | validation loss: 2.7748e-03\n",
      "Epoch: 43960 | training loss: 2.6785e-03 | validation loss: 2.7928e-03\n",
      "Epoch: 43970 | training loss: 2.6828e-03 | validation loss: 2.8031e-03\n",
      "Epoch: 43980 | training loss: 2.6804e-03 | validation loss: 2.7808e-03\n",
      "Epoch: 43990 | training loss: 2.6783e-03 | validation loss: 2.7918e-03\n",
      "Epoch: 44000 | training loss: 2.6778e-03 | validation loss: 2.7871e-03\n",
      "Epoch: 44010 | training loss: 2.6776e-03 | validation loss: 2.7892e-03\n",
      "Epoch: 44020 | training loss: 2.6775e-03 | validation loss: 2.7872e-03\n",
      "Epoch: 44030 | training loss: 2.6774e-03 | validation loss: 2.7889e-03\n",
      "Epoch: 44040 | training loss: 2.6772e-03 | validation loss: 2.7877e-03\n",
      "Epoch: 44050 | training loss: 2.6771e-03 | validation loss: 2.7875e-03\n",
      "Epoch: 44060 | training loss: 2.6770e-03 | validation loss: 2.7877e-03\n",
      "Epoch: 44070 | training loss: 2.6769e-03 | validation loss: 2.7876e-03\n",
      "Epoch: 44080 | training loss: 2.6767e-03 | validation loss: 2.7872e-03\n",
      "Epoch: 44090 | training loss: 2.6768e-03 | validation loss: 2.7850e-03\n",
      "Epoch: 44100 | training loss: 2.6983e-03 | validation loss: 2.7727e-03\n",
      "Epoch: 44110 | training loss: 4.5523e-03 | validation loss: 3.4322e-03\n",
      "Epoch: 44120 | training loss: 2.9123e-03 | validation loss: 2.9772e-03\n",
      "Epoch: 44130 | training loss: 2.9077e-03 | validation loss: 2.9761e-03\n",
      "Epoch: 44140 | training loss: 2.7543e-03 | validation loss: 2.8725e-03\n",
      "Epoch: 44150 | training loss: 2.6866e-03 | validation loss: 2.8118e-03\n",
      "Epoch: 44160 | training loss: 2.6760e-03 | validation loss: 2.7845e-03\n",
      "Epoch: 44170 | training loss: 2.6788e-03 | validation loss: 2.7775e-03\n",
      "Epoch: 44180 | training loss: 2.6765e-03 | validation loss: 2.7815e-03\n",
      "Epoch: 44190 | training loss: 2.6755e-03 | validation loss: 2.7882e-03\n",
      "Epoch: 44200 | training loss: 2.6755e-03 | validation loss: 2.7889e-03\n",
      "Epoch: 44210 | training loss: 2.6752e-03 | validation loss: 2.7858e-03\n",
      "Epoch: 44220 | training loss: 2.6751e-03 | validation loss: 2.7856e-03\n",
      "Epoch: 44230 | training loss: 2.6750e-03 | validation loss: 2.7868e-03\n",
      "Epoch: 44240 | training loss: 2.6748e-03 | validation loss: 2.7859e-03\n",
      "Epoch: 44250 | training loss: 2.6747e-03 | validation loss: 2.7861e-03\n",
      "Epoch: 44260 | training loss: 2.6746e-03 | validation loss: 2.7859e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44270 | training loss: 2.6745e-03 | validation loss: 2.7859e-03\n",
      "Epoch: 44280 | training loss: 2.6743e-03 | validation loss: 2.7857e-03\n",
      "Epoch: 44290 | training loss: 2.6742e-03 | validation loss: 2.7857e-03\n",
      "Epoch: 44300 | training loss: 2.6741e-03 | validation loss: 2.7855e-03\n",
      "Epoch: 44310 | training loss: 2.6740e-03 | validation loss: 2.7849e-03\n",
      "Epoch: 44320 | training loss: 2.6752e-03 | validation loss: 2.7797e-03\n",
      "Epoch: 44330 | training loss: 2.9358e-03 | validation loss: 2.8856e-03\n",
      "Epoch: 44340 | training loss: 3.0456e-03 | validation loss: 3.0706e-03\n",
      "Epoch: 44350 | training loss: 2.7314e-03 | validation loss: 2.8552e-03\n",
      "Epoch: 44360 | training loss: 2.6898e-03 | validation loss: 2.8258e-03\n",
      "Epoch: 44370 | training loss: 2.6788e-03 | validation loss: 2.8045e-03\n",
      "Epoch: 44380 | training loss: 2.6806e-03 | validation loss: 2.8032e-03\n",
      "Epoch: 44390 | training loss: 2.7302e-03 | validation loss: 2.8548e-03\n",
      "Epoch: 44400 | training loss: 3.1636e-03 | validation loss: 3.1438e-03\n",
      "Epoch: 44410 | training loss: 2.7505e-03 | validation loss: 2.7624e-03\n",
      "Epoch: 44420 | training loss: 2.6897e-03 | validation loss: 2.8158e-03\n",
      "Epoch: 44430 | training loss: 2.6737e-03 | validation loss: 2.7767e-03\n",
      "Epoch: 44440 | training loss: 2.6739e-03 | validation loss: 2.7767e-03\n",
      "Epoch: 44450 | training loss: 2.6768e-03 | validation loss: 2.7987e-03\n",
      "Epoch: 44460 | training loss: 2.6725e-03 | validation loss: 2.7803e-03\n",
      "Epoch: 44470 | training loss: 2.6736e-03 | validation loss: 2.7767e-03\n",
      "Epoch: 44480 | training loss: 2.6742e-03 | validation loss: 2.7752e-03\n",
      "Epoch: 44490 | training loss: 2.6870e-03 | validation loss: 2.7664e-03\n",
      "Epoch: 44500 | training loss: 2.9470e-03 | validation loss: 2.8078e-03\n",
      "Epoch: 44510 | training loss: 2.7467e-03 | validation loss: 2.7736e-03\n",
      "Epoch: 44520 | training loss: 2.6797e-03 | validation loss: 2.8019e-03\n",
      "Epoch: 44530 | training loss: 2.6715e-03 | validation loss: 2.7821e-03\n",
      "Epoch: 44540 | training loss: 2.6716e-03 | validation loss: 2.7788e-03\n",
      "Epoch: 44550 | training loss: 2.6711e-03 | validation loss: 2.7825e-03\n",
      "Epoch: 44560 | training loss: 2.6719e-03 | validation loss: 2.7885e-03\n",
      "Epoch: 44570 | training loss: 2.6722e-03 | validation loss: 2.7767e-03\n",
      "Epoch: 44580 | training loss: 2.6708e-03 | validation loss: 2.7841e-03\n",
      "Epoch: 44590 | training loss: 2.6711e-03 | validation loss: 2.7863e-03\n",
      "Epoch: 44600 | training loss: 2.6710e-03 | validation loss: 2.7866e-03\n",
      "Epoch: 44610 | training loss: 2.6736e-03 | validation loss: 2.7937e-03\n",
      "Epoch: 44620 | training loss: 2.7400e-03 | validation loss: 2.8606e-03\n",
      "Epoch: 44630 | training loss: 3.6050e-03 | validation loss: 3.3825e-03\n",
      "Epoch: 44640 | training loss: 2.9516e-03 | validation loss: 2.8282e-03\n",
      "Epoch: 44650 | training loss: 2.6950e-03 | validation loss: 2.8157e-03\n",
      "Epoch: 44660 | training loss: 2.6726e-03 | validation loss: 2.7916e-03\n",
      "Epoch: 44670 | training loss: 2.6782e-03 | validation loss: 2.7717e-03\n",
      "Epoch: 44680 | training loss: 2.6745e-03 | validation loss: 2.7946e-03\n",
      "Epoch: 44690 | training loss: 2.6714e-03 | validation loss: 2.7755e-03\n",
      "Epoch: 44700 | training loss: 2.6700e-03 | validation loss: 2.7859e-03\n",
      "Epoch: 44710 | training loss: 2.6693e-03 | validation loss: 2.7802e-03\n",
      "Epoch: 44720 | training loss: 2.6691e-03 | validation loss: 2.7801e-03\n",
      "Epoch: 44730 | training loss: 2.6690e-03 | validation loss: 2.7824e-03\n",
      "Epoch: 44740 | training loss: 2.6689e-03 | validation loss: 2.7823e-03\n",
      "Epoch: 44750 | training loss: 2.6688e-03 | validation loss: 2.7828e-03\n",
      "Epoch: 44760 | training loss: 2.6698e-03 | validation loss: 2.7871e-03\n",
      "Epoch: 44770 | training loss: 2.7070e-03 | validation loss: 2.8311e-03\n",
      "Epoch: 44780 | training loss: 3.8634e-03 | validation loss: 3.5205e-03\n",
      "Epoch: 44790 | training loss: 3.0901e-03 | validation loss: 2.8714e-03\n",
      "Epoch: 44800 | training loss: 2.6682e-03 | validation loss: 2.7825e-03\n",
      "Epoch: 44810 | training loss: 2.7215e-03 | validation loss: 2.8408e-03\n",
      "Epoch: 44820 | training loss: 2.6729e-03 | validation loss: 2.7711e-03\n",
      "Epoch: 44830 | training loss: 2.6698e-03 | validation loss: 2.7742e-03\n",
      "Epoch: 44840 | training loss: 2.6704e-03 | validation loss: 2.7896e-03\n",
      "Epoch: 44850 | training loss: 2.6684e-03 | validation loss: 2.7757e-03\n",
      "Epoch: 44860 | training loss: 2.6677e-03 | validation loss: 2.7826e-03\n",
      "Epoch: 44870 | training loss: 2.6674e-03 | validation loss: 2.7783e-03\n",
      "Epoch: 44880 | training loss: 2.6672e-03 | validation loss: 2.7808e-03\n",
      "Epoch: 44890 | training loss: 2.6671e-03 | validation loss: 2.7788e-03\n",
      "Epoch: 44900 | training loss: 2.6669e-03 | validation loss: 2.7793e-03\n",
      "Epoch: 44910 | training loss: 2.6669e-03 | validation loss: 2.7786e-03\n",
      "Epoch: 44920 | training loss: 2.6695e-03 | validation loss: 2.7711e-03\n",
      "Epoch: 44930 | training loss: 2.9685e-03 | validation loss: 2.8867e-03\n",
      "Epoch: 44940 | training loss: 2.9911e-03 | validation loss: 3.0132e-03\n",
      "Epoch: 44950 | training loss: 2.6860e-03 | validation loss: 2.8255e-03\n",
      "Epoch: 44960 | training loss: 2.6765e-03 | validation loss: 2.7894e-03\n",
      "Epoch: 44970 | training loss: 2.6777e-03 | validation loss: 2.7637e-03\n",
      "Epoch: 44980 | training loss: 2.6744e-03 | validation loss: 2.7601e-03\n",
      "Epoch: 44990 | training loss: 2.6879e-03 | validation loss: 2.7584e-03\n",
      "Epoch: 45000 | training loss: 2.8849e-03 | validation loss: 2.7867e-03\n",
      "Epoch: 45010 | training loss: 2.7858e-03 | validation loss: 2.7705e-03\n",
      "Epoch: 45020 | training loss: 2.7482e-03 | validation loss: 2.8695e-03\n",
      "Epoch: 45030 | training loss: 2.7053e-03 | validation loss: 2.7564e-03\n",
      "Epoch: 45040 | training loss: 2.6755e-03 | validation loss: 2.8003e-03\n",
      "Epoch: 45050 | training loss: 2.6653e-03 | validation loss: 2.7803e-03\n",
      "Epoch: 45060 | training loss: 2.6685e-03 | validation loss: 2.7684e-03\n",
      "Epoch: 45070 | training loss: 2.6662e-03 | validation loss: 2.7712e-03\n",
      "Epoch: 45080 | training loss: 2.6659e-03 | validation loss: 2.7717e-03\n",
      "Epoch: 45090 | training loss: 2.6737e-03 | validation loss: 2.7637e-03\n",
      "Epoch: 45100 | training loss: 2.8915e-03 | validation loss: 2.7918e-03\n",
      "Epoch: 45110 | training loss: 2.8510e-03 | validation loss: 2.7960e-03\n",
      "Epoch: 45120 | training loss: 2.6664e-03 | validation loss: 2.7778e-03\n",
      "Epoch: 45130 | training loss: 2.6877e-03 | validation loss: 2.8107e-03\n",
      "Epoch: 45140 | training loss: 2.6867e-03 | validation loss: 2.7632e-03\n",
      "Epoch: 45150 | training loss: 2.6739e-03 | validation loss: 2.7991e-03\n",
      "Epoch: 45160 | training loss: 2.6677e-03 | validation loss: 2.7670e-03\n",
      "Epoch: 45170 | training loss: 2.6652e-03 | validation loss: 2.7838e-03\n",
      "Epoch: 45180 | training loss: 2.6640e-03 | validation loss: 2.7738e-03\n",
      "Epoch: 45190 | training loss: 2.6636e-03 | validation loss: 2.7757e-03\n",
      "Epoch: 45200 | training loss: 2.6636e-03 | validation loss: 2.7784e-03\n",
      "Epoch: 45210 | training loss: 2.6634e-03 | validation loss: 2.7772e-03\n",
      "Epoch: 45220 | training loss: 2.6632e-03 | validation loss: 2.7771e-03\n",
      "Epoch: 45230 | training loss: 2.6633e-03 | validation loss: 2.7788e-03\n",
      "Epoch: 45240 | training loss: 2.6705e-03 | validation loss: 2.7940e-03\n",
      "Epoch: 45250 | training loss: 3.1162e-03 | validation loss: 3.1001e-03\n",
      "Epoch: 45260 | training loss: 2.7439e-03 | validation loss: 2.7619e-03\n",
      "Epoch: 45270 | training loss: 2.9078e-03 | validation loss: 2.9715e-03\n",
      "Epoch: 45280 | training loss: 2.6732e-03 | validation loss: 2.8009e-03\n",
      "Epoch: 45290 | training loss: 2.6902e-03 | validation loss: 2.7585e-03\n",
      "Epoch: 45300 | training loss: 2.6635e-03 | validation loss: 2.7681e-03\n",
      "Epoch: 45310 | training loss: 2.6666e-03 | validation loss: 2.7901e-03\n",
      "Epoch: 45320 | training loss: 2.6624e-03 | validation loss: 2.7715e-03\n",
      "Epoch: 45330 | training loss: 2.6620e-03 | validation loss: 2.7737e-03\n",
      "Epoch: 45340 | training loss: 2.6619e-03 | validation loss: 2.7770e-03\n",
      "Epoch: 45350 | training loss: 2.6618e-03 | validation loss: 2.7734e-03\n",
      "Epoch: 45360 | training loss: 2.6616e-03 | validation loss: 2.7756e-03\n",
      "Epoch: 45370 | training loss: 2.6615e-03 | validation loss: 2.7741e-03\n",
      "Epoch: 45380 | training loss: 2.6613e-03 | validation loss: 2.7745e-03\n",
      "Epoch: 45390 | training loss: 2.6612e-03 | validation loss: 2.7747e-03\n",
      "Epoch: 45400 | training loss: 2.6611e-03 | validation loss: 2.7745e-03\n",
      "Epoch: 45410 | training loss: 2.6610e-03 | validation loss: 2.7750e-03\n",
      "Epoch: 45420 | training loss: 2.6625e-03 | validation loss: 2.7821e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45430 | training loss: 2.8484e-03 | validation loss: 2.9852e-03\n",
      "Epoch: 45440 | training loss: 2.8625e-03 | validation loss: 2.8979e-03\n",
      "Epoch: 45450 | training loss: 2.7570e-03 | validation loss: 2.8923e-03\n",
      "Epoch: 45460 | training loss: 2.6938e-03 | validation loss: 2.8353e-03\n",
      "Epoch: 45470 | training loss: 2.6803e-03 | validation loss: 2.7844e-03\n",
      "Epoch: 45480 | training loss: 2.6880e-03 | validation loss: 2.7582e-03\n",
      "Epoch: 45490 | training loss: 2.7856e-03 | validation loss: 2.7559e-03\n",
      "Epoch: 45500 | training loss: 2.8576e-03 | validation loss: 2.7712e-03\n",
      "Epoch: 45510 | training loss: 2.6929e-03 | validation loss: 2.8206e-03\n",
      "Epoch: 45520 | training loss: 2.6699e-03 | validation loss: 2.7968e-03\n",
      "Epoch: 45530 | training loss: 2.6707e-03 | validation loss: 2.7580e-03\n",
      "Epoch: 45540 | training loss: 2.6719e-03 | validation loss: 2.7569e-03\n",
      "Epoch: 45550 | training loss: 2.6763e-03 | validation loss: 2.7546e-03\n",
      "Epoch: 45560 | training loss: 2.7527e-03 | validation loss: 2.7559e-03\n",
      "Epoch: 45570 | training loss: 3.0211e-03 | validation loss: 2.8255e-03\n",
      "Epoch: 45580 | training loss: 2.7109e-03 | validation loss: 2.8365e-03\n",
      "Epoch: 45590 | training loss: 2.6590e-03 | validation loss: 2.7740e-03\n",
      "Epoch: 45600 | training loss: 2.6741e-03 | validation loss: 2.7565e-03\n",
      "Epoch: 45610 | training loss: 2.6635e-03 | validation loss: 2.7861e-03\n",
      "Epoch: 45620 | training loss: 2.6636e-03 | validation loss: 2.7869e-03\n",
      "Epoch: 45630 | training loss: 2.6608e-03 | validation loss: 2.7815e-03\n",
      "Epoch: 45640 | training loss: 2.6667e-03 | validation loss: 2.7918e-03\n",
      "Epoch: 45650 | training loss: 2.7913e-03 | validation loss: 2.8979e-03\n",
      "Epoch: 45660 | training loss: 3.1811e-03 | validation loss: 3.1377e-03\n",
      "Epoch: 45670 | training loss: 2.8146e-03 | validation loss: 2.7824e-03\n",
      "Epoch: 45680 | training loss: 2.6935e-03 | validation loss: 2.8167e-03\n",
      "Epoch: 45690 | training loss: 2.6659e-03 | validation loss: 2.7590e-03\n",
      "Epoch: 45700 | training loss: 2.6612e-03 | validation loss: 2.7835e-03\n",
      "Epoch: 45710 | training loss: 2.6604e-03 | validation loss: 2.7626e-03\n",
      "Epoch: 45720 | training loss: 2.6591e-03 | validation loss: 2.7788e-03\n",
      "Epoch: 45730 | training loss: 2.6574e-03 | validation loss: 2.7692e-03\n",
      "Epoch: 45740 | training loss: 2.6576e-03 | validation loss: 2.7676e-03\n",
      "Epoch: 45750 | training loss: 2.6572e-03 | validation loss: 2.7684e-03\n",
      "Epoch: 45760 | training loss: 2.6573e-03 | validation loss: 2.7676e-03\n",
      "Epoch: 45770 | training loss: 2.6614e-03 | validation loss: 2.7613e-03\n",
      "Epoch: 45780 | training loss: 2.8270e-03 | validation loss: 2.7813e-03\n",
      "Epoch: 45790 | training loss: 3.0858e-03 | validation loss: 2.8636e-03\n",
      "Epoch: 45800 | training loss: 2.6621e-03 | validation loss: 2.7578e-03\n",
      "Epoch: 45810 | training loss: 2.7506e-03 | validation loss: 2.8649e-03\n",
      "Epoch: 45820 | training loss: 2.6679e-03 | validation loss: 2.7569e-03\n",
      "Epoch: 45830 | training loss: 2.6581e-03 | validation loss: 2.7623e-03\n",
      "Epoch: 45840 | training loss: 2.6603e-03 | validation loss: 2.7829e-03\n",
      "Epoch: 45850 | training loss: 2.6577e-03 | validation loss: 2.7629e-03\n",
      "Epoch: 45860 | training loss: 2.6564e-03 | validation loss: 2.7739e-03\n",
      "Epoch: 45870 | training loss: 2.6559e-03 | validation loss: 2.7667e-03\n",
      "Epoch: 45880 | training loss: 2.6557e-03 | validation loss: 2.7709e-03\n",
      "Epoch: 45890 | training loss: 2.6555e-03 | validation loss: 2.7683e-03\n",
      "Epoch: 45900 | training loss: 2.6554e-03 | validation loss: 2.7686e-03\n",
      "Epoch: 45910 | training loss: 2.6552e-03 | validation loss: 2.7694e-03\n",
      "Epoch: 45920 | training loss: 2.6551e-03 | validation loss: 2.7698e-03\n",
      "Epoch: 45930 | training loss: 2.6553e-03 | validation loss: 2.7731e-03\n",
      "Epoch: 45940 | training loss: 2.6867e-03 | validation loss: 2.8292e-03\n",
      "Epoch: 45950 | training loss: 3.0806e-03 | validation loss: 3.1895e-03\n",
      "Epoch: 45960 | training loss: 2.8492e-03 | validation loss: 2.9941e-03\n",
      "Epoch: 45970 | training loss: 2.6863e-03 | validation loss: 2.7877e-03\n",
      "Epoch: 45980 | training loss: 2.6623e-03 | validation loss: 2.7593e-03\n",
      "Epoch: 45990 | training loss: 2.6551e-03 | validation loss: 2.7662e-03\n",
      "Epoch: 46000 | training loss: 2.6593e-03 | validation loss: 2.7748e-03\n",
      "Epoch: 46010 | training loss: 2.6846e-03 | validation loss: 2.8101e-03\n",
      "Epoch: 46020 | training loss: 3.0896e-03 | validation loss: 3.0908e-03\n",
      "Epoch: 46030 | training loss: 2.6640e-03 | validation loss: 2.7516e-03\n",
      "Epoch: 46040 | training loss: 2.6608e-03 | validation loss: 2.7849e-03\n",
      "Epoch: 46050 | training loss: 2.6573e-03 | validation loss: 2.7571e-03\n",
      "Epoch: 46060 | training loss: 2.6539e-03 | validation loss: 2.7702e-03\n",
      "Epoch: 46070 | training loss: 2.6541e-03 | validation loss: 2.7717e-03\n",
      "Epoch: 46080 | training loss: 2.6555e-03 | validation loss: 2.7591e-03\n",
      "Epoch: 46090 | training loss: 2.6537e-03 | validation loss: 2.7705e-03\n",
      "Epoch: 46100 | training loss: 2.6538e-03 | validation loss: 2.7714e-03\n",
      "Epoch: 46110 | training loss: 2.6534e-03 | validation loss: 2.7701e-03\n",
      "Epoch: 46120 | training loss: 2.6546e-03 | validation loss: 2.7745e-03\n",
      "Epoch: 46130 | training loss: 2.6857e-03 | validation loss: 2.8142e-03\n",
      "Epoch: 46140 | training loss: 3.4875e-03 | validation loss: 3.3132e-03\n",
      "Epoch: 46150 | training loss: 2.9664e-03 | validation loss: 2.8159e-03\n",
      "Epoch: 46160 | training loss: 2.7687e-03 | validation loss: 2.8740e-03\n",
      "Epoch: 46170 | training loss: 2.6563e-03 | validation loss: 2.7545e-03\n",
      "Epoch: 46180 | training loss: 2.6557e-03 | validation loss: 2.7575e-03\n",
      "Epoch: 46190 | training loss: 2.6568e-03 | validation loss: 2.7801e-03\n",
      "Epoch: 46200 | training loss: 2.6543e-03 | validation loss: 2.7585e-03\n",
      "Epoch: 46210 | training loss: 2.6527e-03 | validation loss: 2.7704e-03\n",
      "Epoch: 46220 | training loss: 2.6520e-03 | validation loss: 2.7633e-03\n",
      "Epoch: 46230 | training loss: 2.6517e-03 | validation loss: 2.7652e-03\n",
      "Epoch: 46240 | training loss: 2.6516e-03 | validation loss: 2.7666e-03\n",
      "Epoch: 46250 | training loss: 2.6514e-03 | validation loss: 2.7651e-03\n",
      "Epoch: 46260 | training loss: 2.6513e-03 | validation loss: 2.7642e-03\n",
      "Epoch: 46270 | training loss: 2.6513e-03 | validation loss: 2.7631e-03\n",
      "Epoch: 46280 | training loss: 2.6533e-03 | validation loss: 2.7579e-03\n",
      "Epoch: 46290 | training loss: 2.7534e-03 | validation loss: 2.7588e-03\n",
      "Epoch: 46300 | training loss: 3.6160e-03 | validation loss: 3.0534e-03\n",
      "Epoch: 46310 | training loss: 2.6526e-03 | validation loss: 2.7679e-03\n",
      "Epoch: 46320 | training loss: 2.7756e-03 | validation loss: 2.8812e-03\n",
      "Epoch: 46330 | training loss: 2.6517e-03 | validation loss: 2.7611e-03\n",
      "Epoch: 46340 | training loss: 2.6653e-03 | validation loss: 2.7495e-03\n",
      "Epoch: 46350 | training loss: 2.6529e-03 | validation loss: 2.7728e-03\n",
      "Epoch: 46360 | training loss: 2.6504e-03 | validation loss: 2.7672e-03\n",
      "Epoch: 46370 | training loss: 2.6507e-03 | validation loss: 2.7594e-03\n",
      "Epoch: 46380 | training loss: 2.6503e-03 | validation loss: 2.7672e-03\n",
      "Epoch: 46390 | training loss: 2.6500e-03 | validation loss: 2.7616e-03\n",
      "Epoch: 46400 | training loss: 2.6498e-03 | validation loss: 2.7649e-03\n",
      "Epoch: 46410 | training loss: 2.6497e-03 | validation loss: 2.7626e-03\n",
      "Epoch: 46420 | training loss: 2.6495e-03 | validation loss: 2.7634e-03\n",
      "Epoch: 46430 | training loss: 2.6494e-03 | validation loss: 2.7636e-03\n",
      "Epoch: 46440 | training loss: 2.6493e-03 | validation loss: 2.7632e-03\n",
      "Epoch: 46450 | training loss: 2.6492e-03 | validation loss: 2.7630e-03\n",
      "Epoch: 46460 | training loss: 2.6491e-03 | validation loss: 2.7630e-03\n",
      "Epoch: 46470 | training loss: 2.6497e-03 | validation loss: 2.7653e-03\n",
      "Epoch: 46480 | training loss: 2.7285e-03 | validation loss: 2.8472e-03\n",
      "Epoch: 46490 | training loss: 2.6883e-03 | validation loss: 2.7460e-03\n",
      "Epoch: 46500 | training loss: 3.3096e-03 | validation loss: 2.9716e-03\n",
      "Epoch: 46510 | training loss: 2.8242e-03 | validation loss: 2.9619e-03\n",
      "Epoch: 46520 | training loss: 2.7269e-03 | validation loss: 2.7747e-03\n",
      "Epoch: 46530 | training loss: 2.6798e-03 | validation loss: 2.8210e-03\n",
      "Epoch: 46540 | training loss: 2.6589e-03 | validation loss: 2.7549e-03\n",
      "Epoch: 46550 | training loss: 2.6503e-03 | validation loss: 2.7739e-03\n",
      "Epoch: 46560 | training loss: 2.6480e-03 | validation loss: 2.7616e-03\n",
      "Epoch: 46570 | training loss: 2.6486e-03 | validation loss: 2.7552e-03\n",
      "Epoch: 46580 | training loss: 2.6478e-03 | validation loss: 2.7607e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46590 | training loss: 2.6479e-03 | validation loss: 2.7641e-03\n",
      "Epoch: 46600 | training loss: 2.6488e-03 | validation loss: 2.7690e-03\n",
      "Epoch: 46610 | training loss: 2.6688e-03 | validation loss: 2.7997e-03\n",
      "Epoch: 46620 | training loss: 3.2381e-03 | validation loss: 3.1806e-03\n",
      "Epoch: 46630 | training loss: 2.7749e-03 | validation loss: 2.7487e-03\n",
      "Epoch: 46640 | training loss: 2.7812e-03 | validation loss: 2.8824e-03\n",
      "Epoch: 46650 | training loss: 2.6878e-03 | validation loss: 2.7416e-03\n",
      "Epoch: 46660 | training loss: 2.6509e-03 | validation loss: 2.7740e-03\n",
      "Epoch: 46670 | training loss: 2.6469e-03 | validation loss: 2.7602e-03\n",
      "Epoch: 46680 | training loss: 2.6467e-03 | validation loss: 2.7598e-03\n",
      "Epoch: 46690 | training loss: 2.6466e-03 | validation loss: 2.7587e-03\n",
      "Epoch: 46700 | training loss: 2.6467e-03 | validation loss: 2.7627e-03\n",
      "Epoch: 46710 | training loss: 2.6465e-03 | validation loss: 2.7577e-03\n",
      "Epoch: 46720 | training loss: 2.6462e-03 | validation loss: 2.7596e-03\n",
      "Epoch: 46730 | training loss: 2.6462e-03 | validation loss: 2.7610e-03\n",
      "Epoch: 46740 | training loss: 2.6461e-03 | validation loss: 2.7616e-03\n",
      "Epoch: 46750 | training loss: 2.6471e-03 | validation loss: 2.7661e-03\n",
      "Epoch: 46760 | training loss: 2.6790e-03 | validation loss: 2.8066e-03\n",
      "Epoch: 46770 | training loss: 3.6964e-03 | validation loss: 3.4233e-03\n",
      "Epoch: 46780 | training loss: 3.0821e-03 | validation loss: 2.8530e-03\n",
      "Epoch: 46790 | training loss: 2.6755e-03 | validation loss: 2.7925e-03\n",
      "Epoch: 46800 | training loss: 2.6738e-03 | validation loss: 2.8016e-03\n",
      "Epoch: 46810 | training loss: 2.6629e-03 | validation loss: 2.7489e-03\n",
      "Epoch: 46820 | training loss: 2.6461e-03 | validation loss: 2.7603e-03\n",
      "Epoch: 46830 | training loss: 2.6454e-03 | validation loss: 2.7631e-03\n",
      "Epoch: 46840 | training loss: 2.6454e-03 | validation loss: 2.7552e-03\n",
      "Epoch: 46850 | training loss: 2.6451e-03 | validation loss: 2.7609e-03\n",
      "Epoch: 46860 | training loss: 2.6448e-03 | validation loss: 2.7572e-03\n",
      "Epoch: 46870 | training loss: 2.6446e-03 | validation loss: 2.7585e-03\n",
      "Epoch: 46880 | training loss: 2.6445e-03 | validation loss: 2.7585e-03\n",
      "Epoch: 46890 | training loss: 2.6444e-03 | validation loss: 2.7577e-03\n",
      "Epoch: 46900 | training loss: 2.6443e-03 | validation loss: 2.7576e-03\n",
      "Epoch: 46910 | training loss: 2.6441e-03 | validation loss: 2.7574e-03\n",
      "Epoch: 46920 | training loss: 2.6441e-03 | validation loss: 2.7564e-03\n",
      "Epoch: 46930 | training loss: 2.6466e-03 | validation loss: 2.7520e-03\n",
      "Epoch: 46940 | training loss: 2.8159e-03 | validation loss: 2.7864e-03\n",
      "Epoch: 46950 | training loss: 3.0567e-03 | validation loss: 2.8121e-03\n",
      "Epoch: 46960 | training loss: 2.8637e-03 | validation loss: 2.8918e-03\n",
      "Epoch: 46970 | training loss: 2.6751e-03 | validation loss: 2.8204e-03\n",
      "Epoch: 46980 | training loss: 2.6558e-03 | validation loss: 2.7499e-03\n",
      "Epoch: 46990 | training loss: 2.6565e-03 | validation loss: 2.7536e-03\n",
      "Epoch: 47000 | training loss: 2.6463e-03 | validation loss: 2.7629e-03\n",
      "Epoch: 47010 | training loss: 2.6436e-03 | validation loss: 2.7532e-03\n",
      "Epoch: 47020 | training loss: 2.6436e-03 | validation loss: 2.7608e-03\n",
      "Epoch: 47030 | training loss: 2.6429e-03 | validation loss: 2.7546e-03\n",
      "Epoch: 47040 | training loss: 2.6428e-03 | validation loss: 2.7568e-03\n",
      "Epoch: 47050 | training loss: 2.6426e-03 | validation loss: 2.7557e-03\n",
      "Epoch: 47060 | training loss: 2.6425e-03 | validation loss: 2.7558e-03\n",
      "Epoch: 47070 | training loss: 2.6424e-03 | validation loss: 2.7560e-03\n",
      "Epoch: 47080 | training loss: 2.6423e-03 | validation loss: 2.7561e-03\n",
      "Epoch: 47090 | training loss: 2.6422e-03 | validation loss: 2.7564e-03\n",
      "Epoch: 47100 | training loss: 2.6423e-03 | validation loss: 2.7591e-03\n",
      "Epoch: 47110 | training loss: 2.6518e-03 | validation loss: 2.7844e-03\n",
      "Epoch: 47120 | training loss: 3.3279e-03 | validation loss: 3.2984e-03\n",
      "Epoch: 47130 | training loss: 3.0009e-03 | validation loss: 2.8031e-03\n",
      "Epoch: 47140 | training loss: 2.7252e-03 | validation loss: 2.7989e-03\n",
      "Epoch: 47150 | training loss: 2.6524e-03 | validation loss: 2.7624e-03\n",
      "Epoch: 47160 | training loss: 2.6543e-03 | validation loss: 2.7870e-03\n",
      "Epoch: 47170 | training loss: 2.6467e-03 | validation loss: 2.7750e-03\n",
      "Epoch: 47180 | training loss: 2.6415e-03 | validation loss: 2.7529e-03\n",
      "Epoch: 47190 | training loss: 2.6419e-03 | validation loss: 2.7510e-03\n",
      "Epoch: 47200 | training loss: 2.6411e-03 | validation loss: 2.7520e-03\n",
      "Epoch: 47210 | training loss: 2.6410e-03 | validation loss: 2.7556e-03\n",
      "Epoch: 47220 | training loss: 2.6408e-03 | validation loss: 2.7547e-03\n",
      "Epoch: 47230 | training loss: 2.6407e-03 | validation loss: 2.7533e-03\n",
      "Epoch: 47240 | training loss: 2.6406e-03 | validation loss: 2.7540e-03\n",
      "Epoch: 47250 | training loss: 2.6405e-03 | validation loss: 2.7529e-03\n",
      "Epoch: 47260 | training loss: 2.6406e-03 | validation loss: 2.7516e-03\n",
      "Epoch: 47270 | training loss: 2.6511e-03 | validation loss: 2.7442e-03\n",
      "Epoch: 47280 | training loss: 3.4390e-03 | validation loss: 3.0114e-03\n",
      "Epoch: 47290 | training loss: 3.1694e-03 | validation loss: 3.1162e-03\n",
      "Epoch: 47300 | training loss: 2.6801e-03 | validation loss: 2.7230e-03\n",
      "Epoch: 47310 | training loss: 2.7234e-03 | validation loss: 2.7333e-03\n",
      "Epoch: 47320 | training loss: 2.6413e-03 | validation loss: 2.7478e-03\n",
      "Epoch: 47330 | training loss: 2.6496e-03 | validation loss: 2.7780e-03\n",
      "Epoch: 47340 | training loss: 2.6399e-03 | validation loss: 2.7576e-03\n",
      "Epoch: 47350 | training loss: 2.6408e-03 | validation loss: 2.7459e-03\n",
      "Epoch: 47360 | training loss: 2.6394e-03 | validation loss: 2.7536e-03\n",
      "Epoch: 47370 | training loss: 2.6392e-03 | validation loss: 2.7536e-03\n",
      "Epoch: 47380 | training loss: 2.6391e-03 | validation loss: 2.7512e-03\n",
      "Epoch: 47390 | training loss: 2.6390e-03 | validation loss: 2.7527e-03\n",
      "Epoch: 47400 | training loss: 2.6389e-03 | validation loss: 2.7517e-03\n",
      "Epoch: 47410 | training loss: 2.6387e-03 | validation loss: 2.7520e-03\n",
      "Epoch: 47420 | training loss: 2.6386e-03 | validation loss: 2.7519e-03\n",
      "Epoch: 47430 | training loss: 2.6385e-03 | validation loss: 2.7515e-03\n",
      "Epoch: 47440 | training loss: 2.6384e-03 | validation loss: 2.7516e-03\n",
      "Epoch: 47450 | training loss: 2.6383e-03 | validation loss: 2.7516e-03\n",
      "Epoch: 47460 | training loss: 2.6382e-03 | validation loss: 2.7516e-03\n",
      "Epoch: 47470 | training loss: 2.6381e-03 | validation loss: 2.7520e-03\n",
      "Epoch: 47480 | training loss: 2.6390e-03 | validation loss: 2.7566e-03\n",
      "Epoch: 47490 | training loss: 2.7374e-03 | validation loss: 2.8473e-03\n",
      "Epoch: 47500 | training loss: 3.6758e-03 | validation loss: 3.4069e-03\n",
      "Epoch: 47510 | training loss: 2.9351e-03 | validation loss: 2.9686e-03\n",
      "Epoch: 47520 | training loss: 2.6427e-03 | validation loss: 2.7513e-03\n",
      "Epoch: 47530 | training loss: 2.6546e-03 | validation loss: 2.7404e-03\n",
      "Epoch: 47540 | training loss: 2.6581e-03 | validation loss: 2.7464e-03\n",
      "Epoch: 47550 | training loss: 2.6416e-03 | validation loss: 2.7441e-03\n",
      "Epoch: 47560 | training loss: 2.6374e-03 | validation loss: 2.7517e-03\n",
      "Epoch: 47570 | training loss: 2.6381e-03 | validation loss: 2.7548e-03\n",
      "Epoch: 47580 | training loss: 2.6369e-03 | validation loss: 2.7498e-03\n",
      "Epoch: 47590 | training loss: 2.6369e-03 | validation loss: 2.7488e-03\n",
      "Epoch: 47600 | training loss: 2.6367e-03 | validation loss: 2.7500e-03\n",
      "Epoch: 47610 | training loss: 2.6366e-03 | validation loss: 2.7499e-03\n",
      "Epoch: 47620 | training loss: 2.6365e-03 | validation loss: 2.7492e-03\n",
      "Epoch: 47630 | training loss: 2.6364e-03 | validation loss: 2.7495e-03\n",
      "Epoch: 47640 | training loss: 2.6363e-03 | validation loss: 2.7491e-03\n",
      "Epoch: 47650 | training loss: 2.6362e-03 | validation loss: 2.7492e-03\n",
      "Epoch: 47660 | training loss: 2.6360e-03 | validation loss: 2.7488e-03\n",
      "Epoch: 47670 | training loss: 2.6360e-03 | validation loss: 2.7476e-03\n",
      "Epoch: 47680 | training loss: 2.6391e-03 | validation loss: 2.7384e-03\n",
      "Epoch: 47690 | training loss: 3.0395e-03 | validation loss: 2.8826e-03\n",
      "Epoch: 47700 | training loss: 2.9064e-03 | validation loss: 3.0033e-03\n",
      "Epoch: 47710 | training loss: 2.7012e-03 | validation loss: 2.8440e-03\n",
      "Epoch: 47720 | training loss: 2.6452e-03 | validation loss: 2.7508e-03\n",
      "Epoch: 47730 | training loss: 2.6398e-03 | validation loss: 2.7599e-03\n",
      "Epoch: 47740 | training loss: 2.6393e-03 | validation loss: 2.7348e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47750 | training loss: 2.6367e-03 | validation loss: 2.7418e-03\n",
      "Epoch: 47760 | training loss: 2.6355e-03 | validation loss: 2.7504e-03\n",
      "Epoch: 47770 | training loss: 2.6358e-03 | validation loss: 2.7549e-03\n",
      "Epoch: 47780 | training loss: 2.6395e-03 | validation loss: 2.7638e-03\n",
      "Epoch: 47790 | training loss: 2.7233e-03 | validation loss: 2.8452e-03\n",
      "Epoch: 47800 | training loss: 3.3603e-03 | validation loss: 3.2370e-03\n",
      "Epoch: 47810 | training loss: 2.8509e-03 | validation loss: 2.7691e-03\n",
      "Epoch: 47820 | training loss: 2.6721e-03 | validation loss: 2.7992e-03\n",
      "Epoch: 47830 | training loss: 2.6366e-03 | validation loss: 2.7376e-03\n",
      "Epoch: 47840 | training loss: 2.6342e-03 | validation loss: 2.7460e-03\n",
      "Epoch: 47850 | training loss: 2.6341e-03 | validation loss: 2.7459e-03\n",
      "Epoch: 47860 | training loss: 2.6343e-03 | validation loss: 2.7500e-03\n",
      "Epoch: 47870 | training loss: 2.6345e-03 | validation loss: 2.7421e-03\n",
      "Epoch: 47880 | training loss: 2.6339e-03 | validation loss: 2.7486e-03\n",
      "Epoch: 47890 | training loss: 2.6338e-03 | validation loss: 2.7482e-03\n",
      "Epoch: 47900 | training loss: 2.6335e-03 | validation loss: 2.7465e-03\n",
      "Epoch: 47910 | training loss: 2.6334e-03 | validation loss: 2.7462e-03\n",
      "Epoch: 47920 | training loss: 2.6333e-03 | validation loss: 2.7467e-03\n",
      "Epoch: 47930 | training loss: 2.6343e-03 | validation loss: 2.7521e-03\n",
      "Epoch: 47940 | training loss: 2.7384e-03 | validation loss: 2.8507e-03\n",
      "Epoch: 47950 | training loss: 3.5726e-03 | validation loss: 3.3438e-03\n",
      "Epoch: 47960 | training loss: 2.9067e-03 | validation loss: 2.9696e-03\n",
      "Epoch: 47970 | training loss: 2.6406e-03 | validation loss: 2.7682e-03\n",
      "Epoch: 47980 | training loss: 2.6607e-03 | validation loss: 2.7377e-03\n",
      "Epoch: 47990 | training loss: 2.6509e-03 | validation loss: 2.7261e-03\n",
      "Epoch: 48000 | training loss: 2.6350e-03 | validation loss: 2.7338e-03\n",
      "Epoch: 48010 | training loss: 2.6334e-03 | validation loss: 2.7502e-03\n",
      "Epoch: 48020 | training loss: 2.6331e-03 | validation loss: 2.7521e-03\n",
      "Epoch: 48030 | training loss: 2.6323e-03 | validation loss: 2.7430e-03\n",
      "Epoch: 48040 | training loss: 2.6322e-03 | validation loss: 2.7423e-03\n",
      "Epoch: 48050 | training loss: 2.6320e-03 | validation loss: 2.7461e-03\n",
      "Epoch: 48060 | training loss: 2.6319e-03 | validation loss: 2.7438e-03\n",
      "Epoch: 48070 | training loss: 2.6318e-03 | validation loss: 2.7441e-03\n",
      "Epoch: 48080 | training loss: 2.6316e-03 | validation loss: 2.7442e-03\n",
      "Epoch: 48090 | training loss: 2.6315e-03 | validation loss: 2.7439e-03\n",
      "Epoch: 48100 | training loss: 2.6314e-03 | validation loss: 2.7439e-03\n",
      "Epoch: 48110 | training loss: 2.6313e-03 | validation loss: 2.7438e-03\n",
      "Epoch: 48120 | training loss: 2.6312e-03 | validation loss: 2.7436e-03\n",
      "Epoch: 48130 | training loss: 2.6311e-03 | validation loss: 2.7435e-03\n",
      "Epoch: 48140 | training loss: 2.6310e-03 | validation loss: 2.7435e-03\n",
      "Epoch: 48150 | training loss: 2.6309e-03 | validation loss: 2.7437e-03\n",
      "Epoch: 48160 | training loss: 2.6313e-03 | validation loss: 2.7460e-03\n",
      "Epoch: 48170 | training loss: 2.6925e-03 | validation loss: 2.8144e-03\n",
      "Epoch: 48180 | training loss: 2.7272e-03 | validation loss: 2.7532e-03\n",
      "Epoch: 48190 | training loss: 3.1282e-03 | validation loss: 2.9268e-03\n",
      "Epoch: 48200 | training loss: 2.7847e-03 | validation loss: 2.9254e-03\n",
      "Epoch: 48210 | training loss: 2.6757e-03 | validation loss: 2.7557e-03\n",
      "Epoch: 48220 | training loss: 2.6399e-03 | validation loss: 2.7689e-03\n",
      "Epoch: 48230 | training loss: 2.6344e-03 | validation loss: 2.7598e-03\n",
      "Epoch: 48240 | training loss: 2.6337e-03 | validation loss: 2.7364e-03\n",
      "Epoch: 48250 | training loss: 2.6303e-03 | validation loss: 2.7392e-03\n",
      "Epoch: 48260 | training loss: 2.6298e-03 | validation loss: 2.7426e-03\n",
      "Epoch: 48270 | training loss: 2.6304e-03 | validation loss: 2.7461e-03\n",
      "Epoch: 48280 | training loss: 2.6421e-03 | validation loss: 2.7690e-03\n",
      "Epoch: 48290 | training loss: 3.0785e-03 | validation loss: 3.0784e-03\n",
      "Epoch: 48300 | training loss: 2.6678e-03 | validation loss: 2.7165e-03\n",
      "Epoch: 48310 | training loss: 2.7760e-03 | validation loss: 2.8729e-03\n",
      "Epoch: 48320 | training loss: 2.6745e-03 | validation loss: 2.7209e-03\n",
      "Epoch: 48330 | training loss: 2.6294e-03 | validation loss: 2.7435e-03\n",
      "Epoch: 48340 | training loss: 2.6313e-03 | validation loss: 2.7508e-03\n",
      "Epoch: 48350 | training loss: 2.6311e-03 | validation loss: 2.7339e-03\n",
      "Epoch: 48360 | training loss: 2.6298e-03 | validation loss: 2.7469e-03\n",
      "Epoch: 48370 | training loss: 2.6290e-03 | validation loss: 2.7373e-03\n",
      "Epoch: 48380 | training loss: 2.6286e-03 | validation loss: 2.7412e-03\n",
      "Epoch: 48390 | training loss: 2.6285e-03 | validation loss: 2.7411e-03\n",
      "Epoch: 48400 | training loss: 2.6284e-03 | validation loss: 2.7391e-03\n",
      "Epoch: 48410 | training loss: 2.6282e-03 | validation loss: 2.7393e-03\n",
      "Epoch: 48420 | training loss: 2.6281e-03 | validation loss: 2.7394e-03\n",
      "Epoch: 48430 | training loss: 2.6281e-03 | validation loss: 2.7383e-03\n",
      "Epoch: 48440 | training loss: 2.6299e-03 | validation loss: 2.7327e-03\n",
      "Epoch: 48450 | training loss: 2.7534e-03 | validation loss: 2.7348e-03\n",
      "Epoch: 48460 | training loss: 3.3556e-03 | validation loss: 2.9428e-03\n",
      "Epoch: 48470 | training loss: 2.7122e-03 | validation loss: 2.7185e-03\n",
      "Epoch: 48480 | training loss: 2.7041e-03 | validation loss: 2.8107e-03\n",
      "Epoch: 48490 | training loss: 2.6572e-03 | validation loss: 2.7821e-03\n",
      "Epoch: 48500 | training loss: 2.6324e-03 | validation loss: 2.7374e-03\n",
      "Epoch: 48510 | training loss: 2.6311e-03 | validation loss: 2.7310e-03\n",
      "Epoch: 48520 | training loss: 2.6286e-03 | validation loss: 2.7429e-03\n",
      "Epoch: 48530 | training loss: 2.6270e-03 | validation loss: 2.7402e-03\n",
      "Epoch: 48540 | training loss: 2.6271e-03 | validation loss: 2.7371e-03\n",
      "Epoch: 48550 | training loss: 2.6269e-03 | validation loss: 2.7396e-03\n",
      "Epoch: 48560 | training loss: 2.6267e-03 | validation loss: 2.7378e-03\n",
      "Epoch: 48570 | training loss: 2.6266e-03 | validation loss: 2.7387e-03\n",
      "Epoch: 48580 | training loss: 2.6265e-03 | validation loss: 2.7380e-03\n",
      "Epoch: 48590 | training loss: 2.6264e-03 | validation loss: 2.7381e-03\n",
      "Epoch: 48600 | training loss: 2.6263e-03 | validation loss: 2.7381e-03\n",
      "Epoch: 48610 | training loss: 2.6262e-03 | validation loss: 2.7378e-03\n",
      "Epoch: 48620 | training loss: 2.6261e-03 | validation loss: 2.7376e-03\n",
      "Epoch: 48630 | training loss: 2.6259e-03 | validation loss: 2.7374e-03\n",
      "Epoch: 48640 | training loss: 2.6259e-03 | validation loss: 2.7366e-03\n",
      "Epoch: 48650 | training loss: 2.6281e-03 | validation loss: 2.7320e-03\n",
      "Epoch: 48660 | training loss: 2.8486e-03 | validation loss: 2.7825e-03\n",
      "Epoch: 48670 | training loss: 2.7677e-03 | validation loss: 2.7203e-03\n",
      "Epoch: 48680 | training loss: 2.9277e-03 | validation loss: 2.8359e-03\n",
      "Epoch: 48690 | training loss: 2.7357e-03 | validation loss: 2.8472e-03\n",
      "Epoch: 48700 | training loss: 2.6599e-03 | validation loss: 2.7882e-03\n",
      "Epoch: 48710 | training loss: 2.6312e-03 | validation loss: 2.7359e-03\n",
      "Epoch: 48720 | training loss: 2.6254e-03 | validation loss: 2.7334e-03\n",
      "Epoch: 48730 | training loss: 2.6258e-03 | validation loss: 2.7400e-03\n",
      "Epoch: 48740 | training loss: 2.6255e-03 | validation loss: 2.7331e-03\n",
      "Epoch: 48750 | training loss: 2.6249e-03 | validation loss: 2.7362e-03\n",
      "Epoch: 48760 | training loss: 2.6246e-03 | validation loss: 2.7359e-03\n",
      "Epoch: 48770 | training loss: 2.6246e-03 | validation loss: 2.7363e-03\n",
      "Epoch: 48780 | training loss: 2.6244e-03 | validation loss: 2.7353e-03\n",
      "Epoch: 48790 | training loss: 2.6243e-03 | validation loss: 2.7357e-03\n",
      "Epoch: 48800 | training loss: 2.6242e-03 | validation loss: 2.7354e-03\n",
      "Epoch: 48810 | training loss: 2.6241e-03 | validation loss: 2.7351e-03\n",
      "Epoch: 48820 | training loss: 2.6240e-03 | validation loss: 2.7350e-03\n",
      "Epoch: 48830 | training loss: 2.6239e-03 | validation loss: 2.7347e-03\n",
      "Epoch: 48840 | training loss: 2.6238e-03 | validation loss: 2.7332e-03\n",
      "Epoch: 48850 | training loss: 2.6266e-03 | validation loss: 2.7233e-03\n",
      "Epoch: 48860 | training loss: 2.9262e-03 | validation loss: 2.7560e-03\n",
      "Epoch: 48870 | training loss: 2.7973e-03 | validation loss: 2.9408e-03\n",
      "Epoch: 48880 | training loss: 2.6845e-03 | validation loss: 2.7187e-03\n",
      "Epoch: 48890 | training loss: 2.6610e-03 | validation loss: 2.7448e-03\n",
      "Epoch: 48900 | training loss: 2.6409e-03 | validation loss: 2.7215e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48910 | training loss: 2.6336e-03 | validation loss: 2.7150e-03\n",
      "Epoch: 48920 | training loss: 2.6262e-03 | validation loss: 2.7306e-03\n",
      "Epoch: 48930 | training loss: 2.6232e-03 | validation loss: 2.7298e-03\n",
      "Epoch: 48940 | training loss: 2.6230e-03 | validation loss: 2.7371e-03\n",
      "Epoch: 48950 | training loss: 2.6229e-03 | validation loss: 2.7351e-03\n",
      "Epoch: 48960 | training loss: 2.6226e-03 | validation loss: 2.7340e-03\n",
      "Epoch: 48970 | training loss: 2.6225e-03 | validation loss: 2.7320e-03\n",
      "Epoch: 48980 | training loss: 2.6224e-03 | validation loss: 2.7330e-03\n",
      "Epoch: 48990 | training loss: 2.6223e-03 | validation loss: 2.7334e-03\n",
      "Epoch: 49000 | training loss: 2.6222e-03 | validation loss: 2.7330e-03\n",
      "Epoch: 49010 | training loss: 2.6221e-03 | validation loss: 2.7336e-03\n",
      "Epoch: 49020 | training loss: 2.6227e-03 | validation loss: 2.7366e-03\n",
      "Epoch: 49030 | training loss: 2.6627e-03 | validation loss: 2.7806e-03\n",
      "Epoch: 49040 | training loss: 4.2248e-03 | validation loss: 3.6831e-03\n",
      "Epoch: 49050 | training loss: 2.7930e-03 | validation loss: 2.7275e-03\n",
      "Epoch: 49060 | training loss: 2.7803e-03 | validation loss: 2.7338e-03\n",
      "Epoch: 49070 | training loss: 2.6220e-03 | validation loss: 2.7304e-03\n",
      "Epoch: 49080 | training loss: 2.6416e-03 | validation loss: 2.7682e-03\n",
      "Epoch: 49090 | training loss: 2.6232e-03 | validation loss: 2.7422e-03\n",
      "Epoch: 49100 | training loss: 2.6237e-03 | validation loss: 2.7250e-03\n",
      "Epoch: 49110 | training loss: 2.6212e-03 | validation loss: 2.7300e-03\n",
      "Epoch: 49120 | training loss: 2.6213e-03 | validation loss: 2.7343e-03\n",
      "Epoch: 49130 | training loss: 2.6210e-03 | validation loss: 2.7303e-03\n",
      "Epoch: 49140 | training loss: 2.6208e-03 | validation loss: 2.7319e-03\n",
      "Epoch: 49150 | training loss: 2.6207e-03 | validation loss: 2.7308e-03\n",
      "Epoch: 49160 | training loss: 2.6206e-03 | validation loss: 2.7314e-03\n",
      "Epoch: 49170 | training loss: 2.6205e-03 | validation loss: 2.7307e-03\n",
      "Epoch: 49180 | training loss: 2.6204e-03 | validation loss: 2.7311e-03\n",
      "Epoch: 49190 | training loss: 2.6203e-03 | validation loss: 2.7307e-03\n",
      "Epoch: 49200 | training loss: 2.6202e-03 | validation loss: 2.7306e-03\n",
      "Epoch: 49210 | training loss: 2.6201e-03 | validation loss: 2.7305e-03\n",
      "Epoch: 49220 | training loss: 2.6200e-03 | validation loss: 2.7303e-03\n",
      "Epoch: 49230 | training loss: 2.6199e-03 | validation loss: 2.7297e-03\n",
      "Epoch: 49240 | training loss: 2.6205e-03 | validation loss: 2.7264e-03\n",
      "Epoch: 49250 | training loss: 2.6990e-03 | validation loss: 2.7249e-03\n",
      "Epoch: 49260 | training loss: 3.9988e-03 | validation loss: 3.1789e-03\n",
      "Epoch: 49270 | training loss: 2.8655e-03 | validation loss: 2.7982e-03\n",
      "Epoch: 49280 | training loss: 2.6277e-03 | validation loss: 2.7494e-03\n",
      "Epoch: 49290 | training loss: 2.6333e-03 | validation loss: 2.7523e-03\n",
      "Epoch: 49300 | training loss: 2.6379e-03 | validation loss: 2.7467e-03\n",
      "Epoch: 49310 | training loss: 2.6265e-03 | validation loss: 2.7380e-03\n",
      "Epoch: 49320 | training loss: 2.6193e-03 | validation loss: 2.7324e-03\n",
      "Epoch: 49330 | training loss: 2.6195e-03 | validation loss: 2.7272e-03\n",
      "Epoch: 49340 | training loss: 2.6190e-03 | validation loss: 2.7276e-03\n",
      "Epoch: 49350 | training loss: 2.6187e-03 | validation loss: 2.7296e-03\n",
      "Epoch: 49360 | training loss: 2.6186e-03 | validation loss: 2.7289e-03\n",
      "Epoch: 49370 | training loss: 2.6185e-03 | validation loss: 2.7284e-03\n",
      "Epoch: 49380 | training loss: 2.6184e-03 | validation loss: 2.7285e-03\n",
      "Epoch: 49390 | training loss: 2.6183e-03 | validation loss: 2.7283e-03\n",
      "Epoch: 49400 | training loss: 2.6182e-03 | validation loss: 2.7282e-03\n",
      "Epoch: 49410 | training loss: 2.6181e-03 | validation loss: 2.7282e-03\n",
      "Epoch: 49420 | training loss: 2.6180e-03 | validation loss: 2.7280e-03\n",
      "Epoch: 49430 | training loss: 2.6179e-03 | validation loss: 2.7279e-03\n",
      "Epoch: 49440 | training loss: 2.6178e-03 | validation loss: 2.7281e-03\n",
      "Epoch: 49450 | training loss: 2.6178e-03 | validation loss: 2.7302e-03\n",
      "Epoch: 49460 | training loss: 2.6320e-03 | validation loss: 2.7654e-03\n",
      "Epoch: 49470 | training loss: 3.6350e-03 | validation loss: 3.5827e-03\n",
      "Epoch: 49480 | training loss: 2.7328e-03 | validation loss: 2.7758e-03\n",
      "Epoch: 49490 | training loss: 2.6521e-03 | validation loss: 2.7850e-03\n",
      "Epoch: 49500 | training loss: 2.6303e-03 | validation loss: 2.7167e-03\n",
      "Epoch: 49510 | training loss: 2.6218e-03 | validation loss: 2.7425e-03\n",
      "Epoch: 49520 | training loss: 2.6187e-03 | validation loss: 2.7185e-03\n",
      "Epoch: 49530 | training loss: 2.6175e-03 | validation loss: 2.7275e-03\n",
      "Epoch: 49540 | training loss: 2.6171e-03 | validation loss: 2.7242e-03\n",
      "Epoch: 49550 | training loss: 2.6170e-03 | validation loss: 2.7217e-03\n",
      "Epoch: 49560 | training loss: 2.6167e-03 | validation loss: 2.7236e-03\n",
      "Epoch: 49570 | training loss: 2.6165e-03 | validation loss: 2.7245e-03\n",
      "Epoch: 49580 | training loss: 2.6170e-03 | validation loss: 2.7219e-03\n",
      "Epoch: 49590 | training loss: 2.6341e-03 | validation loss: 2.7090e-03\n",
      "Epoch: 49600 | training loss: 3.4503e-03 | validation loss: 2.9382e-03\n",
      "Epoch: 49610 | training loss: 3.0395e-03 | validation loss: 3.0326e-03\n",
      "Epoch: 49620 | training loss: 2.6966e-03 | validation loss: 2.7171e-03\n",
      "Epoch: 49630 | training loss: 2.6462e-03 | validation loss: 2.7117e-03\n",
      "Epoch: 49640 | training loss: 2.6344e-03 | validation loss: 2.7578e-03\n",
      "Epoch: 49650 | training loss: 2.6159e-03 | validation loss: 2.7277e-03\n",
      "Epoch: 49660 | training loss: 2.6181e-03 | validation loss: 2.7179e-03\n",
      "Epoch: 49670 | training loss: 2.6167e-03 | validation loss: 2.7308e-03\n",
      "Epoch: 49680 | training loss: 2.6157e-03 | validation loss: 2.7215e-03\n",
      "Epoch: 49690 | training loss: 2.6154e-03 | validation loss: 2.7260e-03\n",
      "Epoch: 49700 | training loss: 2.6153e-03 | validation loss: 2.7233e-03\n",
      "Epoch: 49710 | training loss: 2.6151e-03 | validation loss: 2.7251e-03\n",
      "Epoch: 49720 | training loss: 2.6150e-03 | validation loss: 2.7237e-03\n",
      "Epoch: 49730 | training loss: 2.6149e-03 | validation loss: 2.7238e-03\n",
      "Epoch: 49740 | training loss: 2.6148e-03 | validation loss: 2.7240e-03\n",
      "Epoch: 49750 | training loss: 2.6147e-03 | validation loss: 2.7241e-03\n",
      "Epoch: 49760 | training loss: 2.6146e-03 | validation loss: 2.7246e-03\n",
      "Epoch: 49770 | training loss: 2.6153e-03 | validation loss: 2.7288e-03\n",
      "Epoch: 49780 | training loss: 2.6661e-03 | validation loss: 2.7853e-03\n",
      "Epoch: 49790 | training loss: 4.2344e-03 | validation loss: 3.6878e-03\n",
      "Epoch: 49800 | training loss: 2.6593e-03 | validation loss: 2.7179e-03\n",
      "Epoch: 49810 | training loss: 2.7853e-03 | validation loss: 2.7360e-03\n",
      "Epoch: 49820 | training loss: 2.6291e-03 | validation loss: 2.7047e-03\n",
      "Epoch: 49830 | training loss: 2.6238e-03 | validation loss: 2.7406e-03\n",
      "Epoch: 49840 | training loss: 2.6197e-03 | validation loss: 2.7394e-03\n",
      "Epoch: 49850 | training loss: 2.6146e-03 | validation loss: 2.7200e-03\n",
      "Epoch: 49860 | training loss: 2.6142e-03 | validation loss: 2.7180e-03\n",
      "Epoch: 49870 | training loss: 2.6139e-03 | validation loss: 2.7256e-03\n",
      "Epoch: 49880 | training loss: 2.6135e-03 | validation loss: 2.7222e-03\n",
      "Epoch: 49890 | training loss: 2.6134e-03 | validation loss: 2.7214e-03\n",
      "Epoch: 49900 | training loss: 2.6133e-03 | validation loss: 2.7228e-03\n",
      "Epoch: 49910 | training loss: 2.6131e-03 | validation loss: 2.7215e-03\n",
      "Epoch: 49920 | training loss: 2.6130e-03 | validation loss: 2.7221e-03\n",
      "Epoch: 49930 | training loss: 2.6129e-03 | validation loss: 2.7217e-03\n",
      "Epoch: 49940 | training loss: 2.6128e-03 | validation loss: 2.7215e-03\n",
      "Epoch: 49950 | training loss: 2.6127e-03 | validation loss: 2.7216e-03\n",
      "Epoch: 49960 | training loss: 2.6126e-03 | validation loss: 2.7215e-03\n",
      "Epoch: 49970 | training loss: 2.6125e-03 | validation loss: 2.7215e-03\n",
      "Epoch: 49980 | training loss: 2.6124e-03 | validation loss: 2.7220e-03\n",
      "Epoch: 49990 | training loss: 2.6130e-03 | validation loss: 2.7272e-03\n",
      "Epoch: 50000 | training loss: 2.6824e-03 | validation loss: 2.8247e-03\n",
      "Epoch: 50010 | training loss: 2.7991e-03 | validation loss: 2.9116e-03\n",
      "Epoch: 50020 | training loss: 2.9673e-03 | validation loss: 3.0632e-03\n",
      "Epoch: 50030 | training loss: 2.7564e-03 | validation loss: 2.7920e-03\n",
      "Epoch: 50040 | training loss: 2.6617e-03 | validation loss: 2.8006e-03\n",
      "Epoch: 50050 | training loss: 2.6229e-03 | validation loss: 2.7289e-03\n",
      "Epoch: 50060 | training loss: 2.6147e-03 | validation loss: 2.7223e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50070 | training loss: 2.6139e-03 | validation loss: 2.7313e-03\n",
      "Epoch: 50080 | training loss: 2.6128e-03 | validation loss: 2.7263e-03\n",
      "Epoch: 50090 | training loss: 2.6130e-03 | validation loss: 2.7258e-03\n",
      "Epoch: 50100 | training loss: 2.6241e-03 | validation loss: 2.7449e-03\n",
      "Epoch: 50110 | training loss: 2.8966e-03 | validation loss: 2.9469e-03\n",
      "Epoch: 50120 | training loss: 2.6723e-03 | validation loss: 2.7876e-03\n",
      "Epoch: 50130 | training loss: 2.6115e-03 | validation loss: 2.7212e-03\n",
      "Epoch: 50140 | training loss: 2.6285e-03 | validation loss: 2.7033e-03\n",
      "Epoch: 50150 | training loss: 2.6259e-03 | validation loss: 2.7465e-03\n",
      "Epoch: 50160 | training loss: 2.6175e-03 | validation loss: 2.7078e-03\n",
      "Epoch: 50170 | training loss: 2.6128e-03 | validation loss: 2.7277e-03\n",
      "Epoch: 50180 | training loss: 2.6108e-03 | validation loss: 2.7156e-03\n",
      "Epoch: 50190 | training loss: 2.6105e-03 | validation loss: 2.7165e-03\n",
      "Epoch: 50200 | training loss: 2.6106e-03 | validation loss: 2.7210e-03\n",
      "Epoch: 50210 | training loss: 2.6103e-03 | validation loss: 2.7194e-03\n",
      "Epoch: 50220 | training loss: 2.6101e-03 | validation loss: 2.7183e-03\n",
      "Epoch: 50230 | training loss: 2.6100e-03 | validation loss: 2.7185e-03\n",
      "Epoch: 50240 | training loss: 2.6103e-03 | validation loss: 2.7211e-03\n",
      "Epoch: 50250 | training loss: 2.6312e-03 | validation loss: 2.7524e-03\n",
      "Epoch: 50260 | training loss: 4.0253e-03 | validation loss: 3.5736e-03\n",
      "Epoch: 50270 | training loss: 3.1468e-03 | validation loss: 2.8550e-03\n",
      "Epoch: 50280 | training loss: 2.7189e-03 | validation loss: 2.7113e-03\n",
      "Epoch: 50290 | training loss: 2.6148e-03 | validation loss: 2.7255e-03\n",
      "Epoch: 50300 | training loss: 2.6362e-03 | validation loss: 2.7526e-03\n",
      "Epoch: 50310 | training loss: 2.6115e-03 | validation loss: 2.7246e-03\n",
      "Epoch: 50320 | training loss: 2.6117e-03 | validation loss: 2.7106e-03\n",
      "Epoch: 50330 | training loss: 2.6094e-03 | validation loss: 2.7148e-03\n",
      "Epoch: 50340 | training loss: 2.6094e-03 | validation loss: 2.7202e-03\n",
      "Epoch: 50350 | training loss: 2.6089e-03 | validation loss: 2.7155e-03\n",
      "Epoch: 50360 | training loss: 2.6088e-03 | validation loss: 2.7159e-03\n",
      "Epoch: 50370 | training loss: 2.6087e-03 | validation loss: 2.7169e-03\n",
      "Epoch: 50380 | training loss: 2.6086e-03 | validation loss: 2.7156e-03\n",
      "Epoch: 50390 | training loss: 2.6085e-03 | validation loss: 2.7163e-03\n",
      "Epoch: 50400 | training loss: 2.6084e-03 | validation loss: 2.7157e-03\n",
      "Epoch: 50410 | training loss: 2.6083e-03 | validation loss: 2.7158e-03\n",
      "Epoch: 50420 | training loss: 2.6082e-03 | validation loss: 2.7157e-03\n",
      "Epoch: 50430 | training loss: 2.6081e-03 | validation loss: 2.7155e-03\n",
      "Epoch: 50440 | training loss: 2.6080e-03 | validation loss: 2.7154e-03\n",
      "Epoch: 50450 | training loss: 2.6079e-03 | validation loss: 2.7152e-03\n",
      "Epoch: 50460 | training loss: 2.6078e-03 | validation loss: 2.7147e-03\n",
      "Epoch: 50470 | training loss: 2.6080e-03 | validation loss: 2.7122e-03\n",
      "Epoch: 50480 | training loss: 2.6466e-03 | validation loss: 2.7019e-03\n",
      "Epoch: 50490 | training loss: 4.7511e-03 | validation loss: 3.4850e-03\n",
      "Epoch: 50500 | training loss: 2.6239e-03 | validation loss: 2.7066e-03\n",
      "Epoch: 50510 | training loss: 2.6448e-03 | validation loss: 2.7629e-03\n",
      "Epoch: 50520 | training loss: 2.6546e-03 | validation loss: 2.7673e-03\n",
      "Epoch: 50530 | training loss: 2.6349e-03 | validation loss: 2.7511e-03\n",
      "Epoch: 50540 | training loss: 2.6148e-03 | validation loss: 2.7310e-03\n",
      "Epoch: 50550 | training loss: 2.6071e-03 | validation loss: 2.7162e-03\n",
      "Epoch: 50560 | training loss: 2.6075e-03 | validation loss: 2.7103e-03\n",
      "Epoch: 50570 | training loss: 2.6071e-03 | validation loss: 2.7111e-03\n",
      "Epoch: 50580 | training loss: 2.6066e-03 | validation loss: 2.7140e-03\n",
      "Epoch: 50590 | training loss: 2.6066e-03 | validation loss: 2.7146e-03\n",
      "Epoch: 50600 | training loss: 2.6064e-03 | validation loss: 2.7130e-03\n",
      "Epoch: 50610 | training loss: 2.6063e-03 | validation loss: 2.7130e-03\n",
      "Epoch: 50620 | training loss: 2.6062e-03 | validation loss: 2.7134e-03\n",
      "Epoch: 50630 | training loss: 2.6061e-03 | validation loss: 2.7128e-03\n",
      "Epoch: 50640 | training loss: 2.6060e-03 | validation loss: 2.7129e-03\n",
      "Epoch: 50650 | training loss: 2.6059e-03 | validation loss: 2.7127e-03\n",
      "Epoch: 50660 | training loss: 2.6058e-03 | validation loss: 2.7126e-03\n",
      "Epoch: 50670 | training loss: 2.6057e-03 | validation loss: 2.7120e-03\n",
      "Epoch: 50680 | training loss: 2.6064e-03 | validation loss: 2.7074e-03\n",
      "Epoch: 50690 | training loss: 2.7674e-03 | validation loss: 2.7488e-03\n",
      "Epoch: 50700 | training loss: 2.8795e-03 | validation loss: 2.9154e-03\n",
      "Epoch: 50710 | training loss: 2.6418e-03 | validation loss: 2.7800e-03\n",
      "Epoch: 50720 | training loss: 2.6165e-03 | validation loss: 2.7029e-03\n",
      "Epoch: 50730 | training loss: 2.6088e-03 | validation loss: 2.7247e-03\n",
      "Epoch: 50740 | training loss: 2.6056e-03 | validation loss: 2.7063e-03\n",
      "Epoch: 50750 | training loss: 2.6064e-03 | validation loss: 2.7029e-03\n",
      "Epoch: 50760 | training loss: 2.6056e-03 | validation loss: 2.7078e-03\n",
      "Epoch: 50770 | training loss: 2.6052e-03 | validation loss: 2.7114e-03\n",
      "Epoch: 50780 | training loss: 2.6061e-03 | validation loss: 2.7184e-03\n",
      "Epoch: 50790 | training loss: 2.6393e-03 | validation loss: 2.7637e-03\n",
      "Epoch: 50800 | training loss: 3.5125e-03 | validation loss: 3.3088e-03\n",
      "Epoch: 50810 | training loss: 2.9603e-03 | validation loss: 2.7686e-03\n",
      "Epoch: 50820 | training loss: 2.6709e-03 | validation loss: 2.7856e-03\n",
      "Epoch: 50830 | training loss: 2.6056e-03 | validation loss: 2.7149e-03\n",
      "Epoch: 50840 | training loss: 2.6162e-03 | validation loss: 2.6941e-03\n",
      "Epoch: 50850 | training loss: 2.6106e-03 | validation loss: 2.7260e-03\n",
      "Epoch: 50860 | training loss: 2.6061e-03 | validation loss: 2.7023e-03\n",
      "Epoch: 50870 | training loss: 2.6047e-03 | validation loss: 2.7152e-03\n",
      "Epoch: 50880 | training loss: 2.6041e-03 | validation loss: 2.7063e-03\n",
      "Epoch: 50890 | training loss: 2.6038e-03 | validation loss: 2.7111e-03\n",
      "Epoch: 50900 | training loss: 2.6036e-03 | validation loss: 2.7092e-03\n",
      "Epoch: 50910 | training loss: 2.6035e-03 | validation loss: 2.7081e-03\n",
      "Epoch: 50920 | training loss: 2.6034e-03 | validation loss: 2.7082e-03\n",
      "Epoch: 50930 | training loss: 2.6033e-03 | validation loss: 2.7079e-03\n",
      "Epoch: 50940 | training loss: 2.6035e-03 | validation loss: 2.7056e-03\n",
      "Epoch: 50950 | training loss: 2.6159e-03 | validation loss: 2.6946e-03\n",
      "Epoch: 50960 | training loss: 3.3668e-03 | validation loss: 2.9079e-03\n",
      "Epoch: 50970 | training loss: 3.0394e-03 | validation loss: 3.0264e-03\n",
      "Epoch: 50980 | training loss: 2.6843e-03 | validation loss: 2.7162e-03\n",
      "Epoch: 50990 | training loss: 2.6606e-03 | validation loss: 2.6943e-03\n",
      "Epoch: 51000 | training loss: 2.6096e-03 | validation loss: 2.7160e-03\n",
      "Epoch: 51010 | training loss: 2.6094e-03 | validation loss: 2.7234e-03\n",
      "Epoch: 51020 | training loss: 2.6049e-03 | validation loss: 2.7046e-03\n",
      "Epoch: 51030 | training loss: 2.6024e-03 | validation loss: 2.7065e-03\n",
      "Epoch: 51040 | training loss: 2.6025e-03 | validation loss: 2.7096e-03\n",
      "Epoch: 51050 | training loss: 2.6023e-03 | validation loss: 2.7061e-03\n",
      "Epoch: 51060 | training loss: 2.6021e-03 | validation loss: 2.7082e-03\n",
      "Epoch: 51070 | training loss: 2.6020e-03 | validation loss: 2.7065e-03\n",
      "Epoch: 51080 | training loss: 2.6019e-03 | validation loss: 2.7074e-03\n",
      "Epoch: 51090 | training loss: 2.6017e-03 | validation loss: 2.7069e-03\n",
      "Epoch: 51100 | training loss: 2.6017e-03 | validation loss: 2.7066e-03\n",
      "Epoch: 51110 | training loss: 2.6016e-03 | validation loss: 2.7067e-03\n",
      "Epoch: 51120 | training loss: 2.6015e-03 | validation loss: 2.7067e-03\n",
      "Epoch: 51130 | training loss: 2.6014e-03 | validation loss: 2.7068e-03\n",
      "Epoch: 51140 | training loss: 2.6014e-03 | validation loss: 2.7079e-03\n",
      "Epoch: 51150 | training loss: 2.6072e-03 | validation loss: 2.7199e-03\n",
      "Epoch: 51160 | training loss: 3.1494e-03 | validation loss: 3.0734e-03\n",
      "Epoch: 51170 | training loss: 2.9514e-03 | validation loss: 2.8352e-03\n",
      "Epoch: 51180 | training loss: 2.6767e-03 | validation loss: 2.7961e-03\n",
      "Epoch: 51190 | training loss: 2.6807e-03 | validation loss: 2.7611e-03\n",
      "Epoch: 51200 | training loss: 2.6352e-03 | validation loss: 2.7162e-03\n",
      "Epoch: 51210 | training loss: 2.6034e-03 | validation loss: 2.7087e-03\n",
      "Epoch: 51220 | training loss: 2.6015e-03 | validation loss: 2.7088e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51230 | training loss: 2.6022e-03 | validation loss: 2.7035e-03\n",
      "Epoch: 51240 | training loss: 2.6004e-03 | validation loss: 2.7064e-03\n",
      "Epoch: 51250 | training loss: 2.6004e-03 | validation loss: 2.7040e-03\n",
      "Epoch: 51260 | training loss: 2.6001e-03 | validation loss: 2.7051e-03\n",
      "Epoch: 51270 | training loss: 2.6001e-03 | validation loss: 2.7041e-03\n",
      "Epoch: 51280 | training loss: 2.5999e-03 | validation loss: 2.7045e-03\n",
      "Epoch: 51290 | training loss: 2.5998e-03 | validation loss: 2.7043e-03\n",
      "Epoch: 51300 | training loss: 2.5997e-03 | validation loss: 2.7040e-03\n",
      "Epoch: 51310 | training loss: 2.5996e-03 | validation loss: 2.7038e-03\n",
      "Epoch: 51320 | training loss: 2.5995e-03 | validation loss: 2.7034e-03\n",
      "Epoch: 51330 | training loss: 2.5995e-03 | validation loss: 2.7020e-03\n",
      "Epoch: 51340 | training loss: 2.6018e-03 | validation loss: 2.6928e-03\n",
      "Epoch: 51350 | training loss: 2.8498e-03 | validation loss: 2.7224e-03\n",
      "Epoch: 51360 | training loss: 2.7689e-03 | validation loss: 2.8741e-03\n",
      "Epoch: 51370 | training loss: 2.6223e-03 | validation loss: 2.6799e-03\n",
      "Epoch: 51380 | training loss: 2.6452e-03 | validation loss: 2.7221e-03\n",
      "Epoch: 51390 | training loss: 2.6240e-03 | validation loss: 2.6796e-03\n",
      "Epoch: 51400 | training loss: 2.6071e-03 | validation loss: 2.7000e-03\n",
      "Epoch: 51410 | training loss: 2.6000e-03 | validation loss: 2.6944e-03\n",
      "Epoch: 51420 | training loss: 2.5990e-03 | validation loss: 2.7065e-03\n",
      "Epoch: 51430 | training loss: 2.5990e-03 | validation loss: 2.7035e-03\n",
      "Epoch: 51440 | training loss: 2.5985e-03 | validation loss: 2.7038e-03\n",
      "Epoch: 51450 | training loss: 2.5984e-03 | validation loss: 2.7015e-03\n",
      "Epoch: 51460 | training loss: 2.5982e-03 | validation loss: 2.7010e-03\n",
      "Epoch: 51470 | training loss: 2.5982e-03 | validation loss: 2.7013e-03\n",
      "Epoch: 51480 | training loss: 2.5982e-03 | validation loss: 2.6995e-03\n",
      "Epoch: 51490 | training loss: 2.6017e-03 | validation loss: 2.6932e-03\n",
      "Epoch: 51500 | training loss: 2.8217e-03 | validation loss: 2.7302e-03\n",
      "Epoch: 51510 | training loss: 2.7336e-03 | validation loss: 2.7042e-03\n",
      "Epoch: 51520 | training loss: 2.8175e-03 | validation loss: 2.7266e-03\n",
      "Epoch: 51530 | training loss: 2.6173e-03 | validation loss: 2.7321e-03\n",
      "Epoch: 51540 | training loss: 2.6330e-03 | validation loss: 2.7469e-03\n",
      "Epoch: 51550 | training loss: 2.5993e-03 | validation loss: 2.6945e-03\n",
      "Epoch: 51560 | training loss: 2.6009e-03 | validation loss: 2.6926e-03\n",
      "Epoch: 51570 | training loss: 2.5987e-03 | validation loss: 2.7071e-03\n",
      "Epoch: 51580 | training loss: 2.5971e-03 | validation loss: 2.6996e-03\n",
      "Epoch: 51590 | training loss: 2.5971e-03 | validation loss: 2.6990e-03\n",
      "Epoch: 51600 | training loss: 2.5970e-03 | validation loss: 2.7012e-03\n",
      "Epoch: 51610 | training loss: 2.5969e-03 | validation loss: 2.6991e-03\n",
      "Epoch: 51620 | training loss: 2.5968e-03 | validation loss: 2.7001e-03\n",
      "Epoch: 51630 | training loss: 2.5967e-03 | validation loss: 2.6996e-03\n",
      "Epoch: 51640 | training loss: 2.5966e-03 | validation loss: 2.6993e-03\n",
      "Epoch: 51650 | training loss: 2.5965e-03 | validation loss: 2.6995e-03\n",
      "Epoch: 51660 | training loss: 2.5964e-03 | validation loss: 2.6995e-03\n",
      "Epoch: 51670 | training loss: 2.5963e-03 | validation loss: 2.6994e-03\n",
      "Epoch: 51680 | training loss: 2.5962e-03 | validation loss: 2.7000e-03\n",
      "Epoch: 51690 | training loss: 2.5972e-03 | validation loss: 2.7048e-03\n",
      "Epoch: 51700 | training loss: 2.6850e-03 | validation loss: 2.7873e-03\n",
      "Epoch: 51710 | training loss: 3.8009e-03 | validation loss: 3.4385e-03\n",
      "Epoch: 51720 | training loss: 2.6911e-03 | validation loss: 2.7887e-03\n",
      "Epoch: 51730 | training loss: 2.6407e-03 | validation loss: 2.6833e-03\n",
      "Epoch: 51740 | training loss: 2.6529e-03 | validation loss: 2.6893e-03\n",
      "Epoch: 51750 | training loss: 2.6006e-03 | validation loss: 2.6913e-03\n",
      "Epoch: 51760 | training loss: 2.5988e-03 | validation loss: 2.7075e-03\n",
      "Epoch: 51770 | training loss: 2.5972e-03 | validation loss: 2.7050e-03\n",
      "Epoch: 51780 | training loss: 2.5956e-03 | validation loss: 2.6953e-03\n",
      "Epoch: 51790 | training loss: 2.5953e-03 | validation loss: 2.6960e-03\n",
      "Epoch: 51800 | training loss: 2.5952e-03 | validation loss: 2.6993e-03\n",
      "Epoch: 51810 | training loss: 2.5950e-03 | validation loss: 2.6967e-03\n",
      "Epoch: 51820 | training loss: 2.5949e-03 | validation loss: 2.6973e-03\n",
      "Epoch: 51830 | training loss: 2.5948e-03 | validation loss: 2.6972e-03\n",
      "Epoch: 51840 | training loss: 2.5947e-03 | validation loss: 2.6969e-03\n",
      "Epoch: 51850 | training loss: 2.5946e-03 | validation loss: 2.6968e-03\n",
      "Epoch: 51860 | training loss: 2.5945e-03 | validation loss: 2.6968e-03\n",
      "Epoch: 51870 | training loss: 2.5944e-03 | validation loss: 2.6964e-03\n",
      "Epoch: 51880 | training loss: 2.5943e-03 | validation loss: 2.6956e-03\n",
      "Epoch: 51890 | training loss: 2.5958e-03 | validation loss: 2.6896e-03\n",
      "Epoch: 51900 | training loss: 2.8325e-03 | validation loss: 2.7697e-03\n",
      "Epoch: 51910 | training loss: 2.9688e-03 | validation loss: 2.9518e-03\n",
      "Epoch: 51920 | training loss: 2.6355e-03 | validation loss: 2.7693e-03\n",
      "Epoch: 51930 | training loss: 2.5940e-03 | validation loss: 2.6981e-03\n",
      "Epoch: 51940 | training loss: 2.6012e-03 | validation loss: 2.6782e-03\n",
      "Epoch: 51950 | training loss: 2.5967e-03 | validation loss: 2.6885e-03\n",
      "Epoch: 51960 | training loss: 2.5964e-03 | validation loss: 2.6967e-03\n",
      "Epoch: 51970 | training loss: 2.6001e-03 | validation loss: 2.7107e-03\n",
      "Epoch: 51980 | training loss: 2.6946e-03 | validation loss: 2.8023e-03\n",
      "Epoch: 51990 | training loss: 3.1860e-03 | validation loss: 3.1084e-03\n",
      "Epoch: 52000 | training loss: 2.7845e-03 | validation loss: 2.7066e-03\n",
      "Epoch: 52010 | training loss: 2.6477e-03 | validation loss: 2.7625e-03\n",
      "Epoch: 52020 | training loss: 2.6085e-03 | validation loss: 2.6792e-03\n",
      "Epoch: 52030 | training loss: 2.5994e-03 | validation loss: 2.7108e-03\n",
      "Epoch: 52040 | training loss: 2.5966e-03 | validation loss: 2.6842e-03\n",
      "Epoch: 52050 | training loss: 2.5942e-03 | validation loss: 2.7011e-03\n",
      "Epoch: 52060 | training loss: 2.5926e-03 | validation loss: 2.6938e-03\n",
      "Epoch: 52070 | training loss: 2.5930e-03 | validation loss: 2.6902e-03\n",
      "Epoch: 52080 | training loss: 2.5927e-03 | validation loss: 2.6905e-03\n",
      "Epoch: 52090 | training loss: 2.5931e-03 | validation loss: 2.6888e-03\n",
      "Epoch: 52100 | training loss: 2.6027e-03 | validation loss: 2.6799e-03\n",
      "Epoch: 52110 | training loss: 2.9350e-03 | validation loss: 2.7470e-03\n",
      "Epoch: 52120 | training loss: 2.6010e-03 | validation loss: 2.6921e-03\n",
      "Epoch: 52130 | training loss: 2.6713e-03 | validation loss: 2.6820e-03\n",
      "Epoch: 52140 | training loss: 2.6628e-03 | validation loss: 2.7647e-03\n",
      "Epoch: 52150 | training loss: 2.6013e-03 | validation loss: 2.6797e-03\n",
      "Epoch: 52160 | training loss: 2.5918e-03 | validation loss: 2.6939e-03\n",
      "Epoch: 52170 | training loss: 2.5922e-03 | validation loss: 2.6967e-03\n",
      "Epoch: 52180 | training loss: 2.5919e-03 | validation loss: 2.6888e-03\n",
      "Epoch: 52190 | training loss: 2.5915e-03 | validation loss: 2.6936e-03\n",
      "Epoch: 52200 | training loss: 2.5914e-03 | validation loss: 2.6926e-03\n",
      "Epoch: 52210 | training loss: 2.5914e-03 | validation loss: 2.6904e-03\n",
      "Epoch: 52220 | training loss: 2.5912e-03 | validation loss: 2.6922e-03\n",
      "Epoch: 52230 | training loss: 2.5911e-03 | validation loss: 2.6925e-03\n",
      "Epoch: 52240 | training loss: 2.5911e-03 | validation loss: 2.6928e-03\n",
      "Epoch: 52250 | training loss: 2.5915e-03 | validation loss: 2.6955e-03\n",
      "Epoch: 52260 | training loss: 2.6070e-03 | validation loss: 2.7192e-03\n",
      "Epoch: 52270 | training loss: 3.3457e-03 | validation loss: 3.1830e-03\n",
      "Epoch: 52280 | training loss: 2.9491e-03 | validation loss: 2.7598e-03\n",
      "Epoch: 52290 | training loss: 2.7136e-03 | validation loss: 2.8019e-03\n",
      "Epoch: 52300 | training loss: 2.6103e-03 | validation loss: 2.7244e-03\n",
      "Epoch: 52310 | training loss: 2.6131e-03 | validation loss: 2.6755e-03\n",
      "Epoch: 52320 | training loss: 2.5903e-03 | validation loss: 2.6900e-03\n",
      "Epoch: 52330 | training loss: 2.5924e-03 | validation loss: 2.7002e-03\n",
      "Epoch: 52340 | training loss: 2.5914e-03 | validation loss: 2.6842e-03\n",
      "Epoch: 52350 | training loss: 2.5903e-03 | validation loss: 2.6937e-03\n",
      "Epoch: 52360 | training loss: 2.5900e-03 | validation loss: 2.6881e-03\n",
      "Epoch: 52370 | training loss: 2.5898e-03 | validation loss: 2.6910e-03\n",
      "Epoch: 52380 | training loss: 2.5897e-03 | validation loss: 2.6888e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52390 | training loss: 2.5896e-03 | validation loss: 2.6900e-03\n",
      "Epoch: 52400 | training loss: 2.5895e-03 | validation loss: 2.6896e-03\n",
      "Epoch: 52410 | training loss: 2.5894e-03 | validation loss: 2.6891e-03\n",
      "Epoch: 52420 | training loss: 2.5893e-03 | validation loss: 2.6888e-03\n",
      "Epoch: 52430 | training loss: 2.5893e-03 | validation loss: 2.6888e-03\n",
      "Epoch: 52440 | training loss: 2.5930e-03 | validation loss: 2.6962e-03\n",
      "Epoch: 52450 | training loss: 3.1381e-03 | validation loss: 3.1744e-03\n",
      "Epoch: 52460 | training loss: 2.7496e-03 | validation loss: 2.7819e-03\n",
      "Epoch: 52470 | training loss: 2.9157e-03 | validation loss: 2.9166e-03\n",
      "Epoch: 52480 | training loss: 2.6523e-03 | validation loss: 2.7318e-03\n",
      "Epoch: 52490 | training loss: 2.6477e-03 | validation loss: 2.6607e-03\n",
      "Epoch: 52500 | training loss: 2.6197e-03 | validation loss: 2.7262e-03\n",
      "Epoch: 52510 | training loss: 2.5926e-03 | validation loss: 2.6745e-03\n",
      "Epoch: 52520 | training loss: 2.5928e-03 | validation loss: 2.6740e-03\n",
      "Epoch: 52530 | training loss: 2.5886e-03 | validation loss: 2.6869e-03\n",
      "Epoch: 52540 | training loss: 2.5900e-03 | validation loss: 2.6950e-03\n",
      "Epoch: 52550 | training loss: 2.6027e-03 | validation loss: 2.7171e-03\n",
      "Epoch: 52560 | training loss: 2.8565e-03 | validation loss: 2.9104e-03\n",
      "Epoch: 52570 | training loss: 2.6505e-03 | validation loss: 2.7584e-03\n",
      "Epoch: 52580 | training loss: 2.5942e-03 | validation loss: 2.6734e-03\n",
      "Epoch: 52590 | training loss: 2.5878e-03 | validation loss: 2.6842e-03\n",
      "Epoch: 52600 | training loss: 2.5880e-03 | validation loss: 2.6893e-03\n",
      "Epoch: 52610 | training loss: 2.5876e-03 | validation loss: 2.6874e-03\n",
      "Epoch: 52620 | training loss: 2.5884e-03 | validation loss: 2.6811e-03\n",
      "Epoch: 52630 | training loss: 2.5887e-03 | validation loss: 2.6929e-03\n",
      "Epoch: 52640 | training loss: 2.5874e-03 | validation loss: 2.6846e-03\n",
      "Epoch: 52650 | training loss: 2.5876e-03 | validation loss: 2.6822e-03\n",
      "Epoch: 52660 | training loss: 2.5877e-03 | validation loss: 2.6813e-03\n",
      "Epoch: 52670 | training loss: 2.5912e-03 | validation loss: 2.6756e-03\n",
      "Epoch: 52680 | training loss: 2.6808e-03 | validation loss: 2.6729e-03\n",
      "Epoch: 52690 | training loss: 3.3784e-03 | validation loss: 2.8974e-03\n",
      "Epoch: 52700 | training loss: 2.7801e-03 | validation loss: 2.8420e-03\n",
      "Epoch: 52710 | training loss: 2.5950e-03 | validation loss: 2.6746e-03\n",
      "Epoch: 52720 | training loss: 2.5911e-03 | validation loss: 2.6782e-03\n",
      "Epoch: 52730 | training loss: 2.5935e-03 | validation loss: 2.7016e-03\n",
      "Epoch: 52740 | training loss: 2.5901e-03 | validation loss: 2.6753e-03\n",
      "Epoch: 52750 | training loss: 2.5876e-03 | validation loss: 2.6911e-03\n",
      "Epoch: 52760 | training loss: 2.5864e-03 | validation loss: 2.6823e-03\n",
      "Epoch: 52770 | training loss: 2.5862e-03 | validation loss: 2.6832e-03\n",
      "Epoch: 52780 | training loss: 2.5862e-03 | validation loss: 2.6862e-03\n",
      "Epoch: 52790 | training loss: 2.5860e-03 | validation loss: 2.6842e-03\n",
      "Epoch: 52800 | training loss: 2.5859e-03 | validation loss: 2.6833e-03\n",
      "Epoch: 52810 | training loss: 2.5859e-03 | validation loss: 2.6821e-03\n",
      "Epoch: 52820 | training loss: 2.5875e-03 | validation loss: 2.6774e-03\n",
      "Epoch: 52830 | training loss: 2.6760e-03 | validation loss: 2.6765e-03\n",
      "Epoch: 52840 | training loss: 3.6871e-03 | validation loss: 3.0310e-03\n",
      "Epoch: 52850 | training loss: 2.5909e-03 | validation loss: 2.6904e-03\n",
      "Epoch: 52860 | training loss: 2.7184e-03 | validation loss: 2.8044e-03\n",
      "Epoch: 52870 | training loss: 2.5858e-03 | validation loss: 2.6857e-03\n",
      "Epoch: 52880 | training loss: 2.6023e-03 | validation loss: 2.6669e-03\n",
      "Epoch: 52890 | training loss: 2.5859e-03 | validation loss: 2.6865e-03\n",
      "Epoch: 52900 | training loss: 2.5861e-03 | validation loss: 2.6899e-03\n",
      "Epoch: 52910 | training loss: 2.5858e-03 | validation loss: 2.6768e-03\n",
      "Epoch: 52920 | training loss: 2.5850e-03 | validation loss: 2.6853e-03\n",
      "Epoch: 52930 | training loss: 2.5847e-03 | validation loss: 2.6806e-03\n",
      "Epoch: 52940 | training loss: 2.5846e-03 | validation loss: 2.6827e-03\n",
      "Epoch: 52950 | training loss: 2.5845e-03 | validation loss: 2.6811e-03\n",
      "Epoch: 52960 | training loss: 2.5844e-03 | validation loss: 2.6820e-03\n",
      "Epoch: 52970 | training loss: 2.5843e-03 | validation loss: 2.6813e-03\n",
      "Epoch: 52980 | training loss: 2.5842e-03 | validation loss: 2.6811e-03\n",
      "Epoch: 52990 | training loss: 2.5841e-03 | validation loss: 2.6810e-03\n",
      "Epoch: 53000 | training loss: 2.5840e-03 | validation loss: 2.6804e-03\n",
      "Epoch: 53010 | training loss: 2.5849e-03 | validation loss: 2.6763e-03\n",
      "Epoch: 53020 | training loss: 2.7289e-03 | validation loss: 2.7244e-03\n",
      "Epoch: 53030 | training loss: 2.7539e-03 | validation loss: 2.7889e-03\n",
      "Epoch: 53040 | training loss: 2.7781e-03 | validation loss: 2.6756e-03\n",
      "Epoch: 53050 | training loss: 2.5966e-03 | validation loss: 2.6707e-03\n",
      "Epoch: 53060 | training loss: 2.6236e-03 | validation loss: 2.7203e-03\n",
      "Epoch: 53070 | training loss: 2.5886e-03 | validation loss: 2.6727e-03\n",
      "Epoch: 53080 | training loss: 2.5933e-03 | validation loss: 2.6612e-03\n",
      "Epoch: 53090 | training loss: 2.6097e-03 | validation loss: 2.6576e-03\n",
      "Epoch: 53100 | training loss: 2.7558e-03 | validation loss: 2.6780e-03\n",
      "Epoch: 53110 | training loss: 2.7458e-03 | validation loss: 2.6800e-03\n",
      "Epoch: 53120 | training loss: 2.6673e-03 | validation loss: 2.7715e-03\n",
      "Epoch: 53130 | training loss: 2.5909e-03 | validation loss: 2.6663e-03\n",
      "Epoch: 53140 | training loss: 2.5895e-03 | validation loss: 2.6664e-03\n",
      "Epoch: 53150 | training loss: 2.5865e-03 | validation loss: 2.6912e-03\n",
      "Epoch: 53160 | training loss: 2.5898e-03 | validation loss: 2.6970e-03\n",
      "Epoch: 53170 | training loss: 2.6005e-03 | validation loss: 2.7110e-03\n",
      "Epoch: 53180 | training loss: 2.7280e-03 | validation loss: 2.8138e-03\n",
      "Epoch: 53190 | training loss: 2.8851e-03 | validation loss: 2.9126e-03\n",
      "Epoch: 53200 | training loss: 2.7003e-03 | validation loss: 2.6738e-03\n",
      "Epoch: 53210 | training loss: 2.6104e-03 | validation loss: 2.7196e-03\n",
      "Epoch: 53220 | training loss: 2.5824e-03 | validation loss: 2.6740e-03\n",
      "Epoch: 53230 | training loss: 2.5879e-03 | validation loss: 2.6669e-03\n",
      "Epoch: 53240 | training loss: 2.5830e-03 | validation loss: 2.6837e-03\n",
      "Epoch: 53250 | training loss: 2.5853e-03 | validation loss: 2.6889e-03\n",
      "Epoch: 53260 | training loss: 2.5910e-03 | validation loss: 2.6979e-03\n",
      "Epoch: 53270 | training loss: 2.6707e-03 | validation loss: 2.7688e-03\n",
      "Epoch: 53280 | training loss: 3.1302e-03 | validation loss: 3.0542e-03\n",
      "Epoch: 53290 | training loss: 2.7379e-03 | validation loss: 2.6859e-03\n",
      "Epoch: 53300 | training loss: 2.6337e-03 | validation loss: 2.7363e-03\n",
      "Epoch: 53310 | training loss: 2.5951e-03 | validation loss: 2.6641e-03\n",
      "Epoch: 53320 | training loss: 2.5820e-03 | validation loss: 2.6813e-03\n",
      "Epoch: 53330 | training loss: 2.5826e-03 | validation loss: 2.6830e-03\n",
      "Epoch: 53340 | training loss: 2.5828e-03 | validation loss: 2.6704e-03\n",
      "Epoch: 53350 | training loss: 2.5816e-03 | validation loss: 2.6719e-03\n",
      "Epoch: 53360 | training loss: 2.5810e-03 | validation loss: 2.6738e-03\n",
      "Epoch: 53370 | training loss: 2.5815e-03 | validation loss: 2.6716e-03\n",
      "Epoch: 53380 | training loss: 2.5977e-03 | validation loss: 2.6626e-03\n",
      "Epoch: 53390 | training loss: 3.2393e-03 | validation loss: 2.8532e-03\n",
      "Epoch: 53400 | training loss: 2.8028e-03 | validation loss: 2.8563e-03\n",
      "Epoch: 53410 | training loss: 2.7543e-03 | validation loss: 2.6897e-03\n",
      "Epoch: 53420 | training loss: 2.5814e-03 | validation loss: 2.6776e-03\n",
      "Epoch: 53430 | training loss: 2.5969e-03 | validation loss: 2.7040e-03\n",
      "Epoch: 53440 | training loss: 2.5881e-03 | validation loss: 2.6640e-03\n",
      "Epoch: 53450 | training loss: 2.5810e-03 | validation loss: 2.6797e-03\n",
      "Epoch: 53460 | training loss: 2.5799e-03 | validation loss: 2.6741e-03\n",
      "Epoch: 53470 | training loss: 2.5798e-03 | validation loss: 2.6742e-03\n",
      "Epoch: 53480 | training loss: 2.5797e-03 | validation loss: 2.6738e-03\n",
      "Epoch: 53490 | training loss: 2.5797e-03 | validation loss: 2.6751e-03\n",
      "Epoch: 53500 | training loss: 2.5796e-03 | validation loss: 2.6729e-03\n",
      "Epoch: 53510 | training loss: 2.5795e-03 | validation loss: 2.6737e-03\n",
      "Epoch: 53520 | training loss: 2.5794e-03 | validation loss: 2.6740e-03\n",
      "Epoch: 53530 | training loss: 2.5793e-03 | validation loss: 2.6740e-03\n",
      "Epoch: 53540 | training loss: 2.5795e-03 | validation loss: 2.6740e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53550 | training loss: 2.5942e-03 | validation loss: 2.6786e-03\n",
      "Epoch: 53560 | training loss: 3.1525e-03 | validation loss: 3.0180e-03\n",
      "Epoch: 53570 | training loss: 3.0194e-03 | validation loss: 3.0359e-03\n",
      "Epoch: 53580 | training loss: 2.6730e-03 | validation loss: 2.7139e-03\n",
      "Epoch: 53590 | training loss: 2.6155e-03 | validation loss: 2.7362e-03\n",
      "Epoch: 53600 | training loss: 2.5904e-03 | validation loss: 2.6865e-03\n",
      "Epoch: 53610 | training loss: 2.5821e-03 | validation loss: 2.6768e-03\n",
      "Epoch: 53620 | training loss: 2.5817e-03 | validation loss: 2.6862e-03\n",
      "Epoch: 53630 | training loss: 2.5796e-03 | validation loss: 2.6648e-03\n",
      "Epoch: 53640 | training loss: 2.5791e-03 | validation loss: 2.6655e-03\n",
      "Epoch: 53650 | training loss: 2.5783e-03 | validation loss: 2.6695e-03\n",
      "Epoch: 53660 | training loss: 2.5781e-03 | validation loss: 2.6710e-03\n",
      "Epoch: 53670 | training loss: 2.5781e-03 | validation loss: 2.6694e-03\n",
      "Epoch: 53680 | training loss: 2.5828e-03 | validation loss: 2.6593e-03\n",
      "Epoch: 53690 | training loss: 2.9944e-03 | validation loss: 2.7341e-03\n",
      "Epoch: 53700 | training loss: 2.7120e-03 | validation loss: 2.8172e-03\n",
      "Epoch: 53710 | training loss: 2.8003e-03 | validation loss: 2.7235e-03\n",
      "Epoch: 53720 | training loss: 2.6267e-03 | validation loss: 2.6650e-03\n",
      "Epoch: 53730 | training loss: 2.5847e-03 | validation loss: 2.6875e-03\n",
      "Epoch: 53740 | training loss: 2.5883e-03 | validation loss: 2.6896e-03\n",
      "Epoch: 53750 | training loss: 2.5783e-03 | validation loss: 2.6631e-03\n",
      "Epoch: 53760 | training loss: 2.5784e-03 | validation loss: 2.6634e-03\n",
      "Epoch: 53770 | training loss: 2.5777e-03 | validation loss: 2.6737e-03\n",
      "Epoch: 53780 | training loss: 2.5771e-03 | validation loss: 2.6698e-03\n",
      "Epoch: 53790 | training loss: 2.5770e-03 | validation loss: 2.6693e-03\n",
      "Epoch: 53800 | training loss: 2.5769e-03 | validation loss: 2.6697e-03\n",
      "Epoch: 53810 | training loss: 2.5768e-03 | validation loss: 2.6689e-03\n",
      "Epoch: 53820 | training loss: 2.5767e-03 | validation loss: 2.6694e-03\n",
      "Epoch: 53830 | training loss: 2.5766e-03 | validation loss: 2.6690e-03\n",
      "Epoch: 53840 | training loss: 2.5766e-03 | validation loss: 2.6688e-03\n",
      "Epoch: 53850 | training loss: 2.5765e-03 | validation loss: 2.6687e-03\n",
      "Epoch: 53860 | training loss: 2.5764e-03 | validation loss: 2.6687e-03\n",
      "Epoch: 53870 | training loss: 2.5763e-03 | validation loss: 2.6686e-03\n",
      "Epoch: 53880 | training loss: 2.5762e-03 | validation loss: 2.6689e-03\n",
      "Epoch: 53890 | training loss: 2.5766e-03 | validation loss: 2.6722e-03\n",
      "Epoch: 53900 | training loss: 2.6233e-03 | validation loss: 2.7258e-03\n",
      "Epoch: 53910 | training loss: 4.4669e-03 | validation loss: 3.7718e-03\n",
      "Epoch: 53920 | training loss: 2.5836e-03 | validation loss: 2.6937e-03\n",
      "Epoch: 53930 | training loss: 2.6677e-03 | validation loss: 2.6650e-03\n",
      "Epoch: 53940 | training loss: 2.6436e-03 | validation loss: 2.6469e-03\n",
      "Epoch: 53950 | training loss: 2.5915e-03 | validation loss: 2.6470e-03\n",
      "Epoch: 53960 | training loss: 2.5755e-03 | validation loss: 2.6681e-03\n",
      "Epoch: 53970 | training loss: 2.5782e-03 | validation loss: 2.6795e-03\n",
      "Epoch: 53980 | training loss: 2.5758e-03 | validation loss: 2.6708e-03\n",
      "Epoch: 53990 | training loss: 2.5755e-03 | validation loss: 2.6642e-03\n",
      "Epoch: 54000 | training loss: 2.5752e-03 | validation loss: 2.6652e-03\n",
      "Epoch: 54010 | training loss: 2.5751e-03 | validation loss: 2.6679e-03\n",
      "Epoch: 54020 | training loss: 2.5750e-03 | validation loss: 2.6660e-03\n",
      "Epoch: 54030 | training loss: 2.5749e-03 | validation loss: 2.6660e-03\n",
      "Epoch: 54040 | training loss: 2.5748e-03 | validation loss: 2.6663e-03\n",
      "Epoch: 54050 | training loss: 2.5747e-03 | validation loss: 2.6657e-03\n",
      "Epoch: 54060 | training loss: 2.5746e-03 | validation loss: 2.6659e-03\n",
      "Epoch: 54070 | training loss: 2.5745e-03 | validation loss: 2.6656e-03\n",
      "Epoch: 54080 | training loss: 2.5744e-03 | validation loss: 2.6656e-03\n",
      "Epoch: 54090 | training loss: 2.5744e-03 | validation loss: 2.6659e-03\n",
      "Epoch: 54100 | training loss: 2.5752e-03 | validation loss: 2.6704e-03\n",
      "Epoch: 54110 | training loss: 2.6968e-03 | validation loss: 2.8048e-03\n",
      "Epoch: 54120 | training loss: 2.6324e-03 | validation loss: 2.6842e-03\n",
      "Epoch: 54130 | training loss: 2.6152e-03 | validation loss: 2.7214e-03\n",
      "Epoch: 54140 | training loss: 2.6119e-03 | validation loss: 2.7148e-03\n",
      "Epoch: 54150 | training loss: 2.5821e-03 | validation loss: 2.6786e-03\n",
      "Epoch: 54160 | training loss: 2.5749e-03 | validation loss: 2.6570e-03\n",
      "Epoch: 54170 | training loss: 2.6112e-03 | validation loss: 2.6391e-03\n",
      "Epoch: 54180 | training loss: 3.5888e-03 | validation loss: 2.9283e-03\n",
      "Epoch: 54190 | training loss: 2.9498e-03 | validation loss: 2.9387e-03\n",
      "Epoch: 54200 | training loss: 2.5917e-03 | validation loss: 2.6474e-03\n",
      "Epoch: 54210 | training loss: 2.5956e-03 | validation loss: 2.6483e-03\n",
      "Epoch: 54220 | training loss: 2.5905e-03 | validation loss: 2.6954e-03\n",
      "Epoch: 54230 | training loss: 2.5755e-03 | validation loss: 2.6574e-03\n",
      "Epoch: 54240 | training loss: 2.5731e-03 | validation loss: 2.6641e-03\n",
      "Epoch: 54250 | training loss: 2.5730e-03 | validation loss: 2.6632e-03\n",
      "Epoch: 54260 | training loss: 2.5729e-03 | validation loss: 2.6618e-03\n",
      "Epoch: 54270 | training loss: 2.5728e-03 | validation loss: 2.6618e-03\n",
      "Epoch: 54280 | training loss: 2.5727e-03 | validation loss: 2.6635e-03\n",
      "Epoch: 54290 | training loss: 2.5726e-03 | validation loss: 2.6613e-03\n",
      "Epoch: 54300 | training loss: 2.5725e-03 | validation loss: 2.6614e-03\n",
      "Epoch: 54310 | training loss: 2.5724e-03 | validation loss: 2.6616e-03\n",
      "Epoch: 54320 | training loss: 2.5723e-03 | validation loss: 2.6614e-03\n",
      "Epoch: 54330 | training loss: 2.5723e-03 | validation loss: 2.6605e-03\n",
      "Epoch: 54340 | training loss: 2.5738e-03 | validation loss: 2.6551e-03\n",
      "Epoch: 54350 | training loss: 2.7359e-03 | validation loss: 2.6667e-03\n",
      "Epoch: 54360 | training loss: 2.8822e-03 | validation loss: 2.7337e-03\n",
      "Epoch: 54370 | training loss: 2.9620e-03 | validation loss: 2.7348e-03\n",
      "Epoch: 54380 | training loss: 2.6145e-03 | validation loss: 2.6335e-03\n",
      "Epoch: 54390 | training loss: 2.5830e-03 | validation loss: 2.6628e-03\n",
      "Epoch: 54400 | training loss: 2.5852e-03 | validation loss: 2.6829e-03\n",
      "Epoch: 54410 | training loss: 2.5753e-03 | validation loss: 2.6768e-03\n",
      "Epoch: 54420 | training loss: 2.5725e-03 | validation loss: 2.6627e-03\n",
      "Epoch: 54430 | training loss: 2.5719e-03 | validation loss: 2.6552e-03\n",
      "Epoch: 54440 | training loss: 2.5715e-03 | validation loss: 2.6590e-03\n",
      "Epoch: 54450 | training loss: 2.5713e-03 | validation loss: 2.6621e-03\n",
      "Epoch: 54460 | training loss: 2.5712e-03 | validation loss: 2.6592e-03\n",
      "Epoch: 54470 | training loss: 2.5711e-03 | validation loss: 2.6592e-03\n",
      "Epoch: 54480 | training loss: 2.5710e-03 | validation loss: 2.6599e-03\n",
      "Epoch: 54490 | training loss: 2.5709e-03 | validation loss: 2.6590e-03\n",
      "Epoch: 54500 | training loss: 2.5708e-03 | validation loss: 2.6594e-03\n",
      "Epoch: 54510 | training loss: 2.5707e-03 | validation loss: 2.6589e-03\n",
      "Epoch: 54520 | training loss: 2.5706e-03 | validation loss: 2.6589e-03\n",
      "Epoch: 54530 | training loss: 2.5705e-03 | validation loss: 2.6588e-03\n",
      "Epoch: 54540 | training loss: 2.5704e-03 | validation loss: 2.6586e-03\n",
      "Epoch: 54550 | training loss: 2.5704e-03 | validation loss: 2.6584e-03\n",
      "Epoch: 54560 | training loss: 2.5703e-03 | validation loss: 2.6583e-03\n",
      "Epoch: 54570 | training loss: 2.5703e-03 | validation loss: 2.6583e-03\n",
      "Epoch: 54580 | training loss: 2.5791e-03 | validation loss: 2.6658e-03\n",
      "Epoch: 54590 | training loss: 3.3872e-03 | validation loss: 3.1513e-03\n",
      "Epoch: 54600 | training loss: 2.6297e-03 | validation loss: 2.6419e-03\n",
      "Epoch: 54610 | training loss: 2.6275e-03 | validation loss: 2.6603e-03\n",
      "Epoch: 54620 | training loss: 2.6439e-03 | validation loss: 2.7614e-03\n",
      "Epoch: 54630 | training loss: 2.5897e-03 | validation loss: 2.6844e-03\n",
      "Epoch: 54640 | training loss: 2.5759e-03 | validation loss: 2.6609e-03\n",
      "Epoch: 54650 | training loss: 2.5718e-03 | validation loss: 2.6684e-03\n",
      "Epoch: 54660 | training loss: 2.5702e-03 | validation loss: 2.6516e-03\n",
      "Epoch: 54670 | training loss: 2.5696e-03 | validation loss: 2.6590e-03\n",
      "Epoch: 54680 | training loss: 2.5694e-03 | validation loss: 2.6537e-03\n",
      "Epoch: 54690 | training loss: 2.5692e-03 | validation loss: 2.6569e-03\n",
      "Epoch: 54700 | training loss: 2.5691e-03 | validation loss: 2.6558e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54710 | training loss: 2.5690e-03 | validation loss: 2.6552e-03\n",
      "Epoch: 54720 | training loss: 2.5689e-03 | validation loss: 2.6554e-03\n",
      "Epoch: 54730 | training loss: 2.5688e-03 | validation loss: 2.6553e-03\n",
      "Epoch: 54740 | training loss: 2.5687e-03 | validation loss: 2.6548e-03\n",
      "Epoch: 54750 | training loss: 2.5688e-03 | validation loss: 2.6528e-03\n",
      "Epoch: 54760 | training loss: 2.5799e-03 | validation loss: 2.6403e-03\n",
      "Epoch: 54770 | training loss: 3.7102e-03 | validation loss: 2.9785e-03\n",
      "Epoch: 54780 | training loss: 3.3129e-03 | validation loss: 3.1390e-03\n",
      "Epoch: 54790 | training loss: 2.6614e-03 | validation loss: 2.7447e-03\n",
      "Epoch: 54800 | training loss: 2.5712e-03 | validation loss: 2.6432e-03\n",
      "Epoch: 54810 | training loss: 2.5924e-03 | validation loss: 2.6363e-03\n",
      "Epoch: 54820 | training loss: 2.5771e-03 | validation loss: 2.6409e-03\n",
      "Epoch: 54830 | training loss: 2.5681e-03 | validation loss: 2.6545e-03\n",
      "Epoch: 54840 | training loss: 2.5694e-03 | validation loss: 2.6608e-03\n",
      "Epoch: 54850 | training loss: 2.5678e-03 | validation loss: 2.6542e-03\n",
      "Epoch: 54860 | training loss: 2.5680e-03 | validation loss: 2.6518e-03\n",
      "Epoch: 54870 | training loss: 2.5677e-03 | validation loss: 2.6546e-03\n",
      "Epoch: 54880 | training loss: 2.5676e-03 | validation loss: 2.6537e-03\n",
      "Epoch: 54890 | training loss: 2.5675e-03 | validation loss: 2.6528e-03\n",
      "Epoch: 54900 | training loss: 2.5674e-03 | validation loss: 2.6534e-03\n",
      "Epoch: 54910 | training loss: 2.5673e-03 | validation loss: 2.6528e-03\n",
      "Epoch: 54920 | training loss: 2.5672e-03 | validation loss: 2.6530e-03\n",
      "Epoch: 54930 | training loss: 2.5672e-03 | validation loss: 2.6526e-03\n",
      "Epoch: 54940 | training loss: 2.5671e-03 | validation loss: 2.6525e-03\n",
      "Epoch: 54950 | training loss: 2.5670e-03 | validation loss: 2.6524e-03\n",
      "Epoch: 54960 | training loss: 2.5669e-03 | validation loss: 2.6523e-03\n",
      "Epoch: 54970 | training loss: 2.5668e-03 | validation loss: 2.6522e-03\n",
      "Epoch: 54980 | training loss: 2.5667e-03 | validation loss: 2.6521e-03\n",
      "Epoch: 54990 | training loss: 2.5667e-03 | validation loss: 2.6529e-03\n",
      "Epoch: 55000 | training loss: 2.5702e-03 | validation loss: 2.6633e-03\n",
      "Epoch: 55010 | training loss: 3.1053e-03 | validation loss: 3.0214e-03\n",
      "Epoch: 55020 | training loss: 3.1479e-03 | validation loss: 2.7898e-03\n",
      "Epoch: 55030 | training loss: 2.5735e-03 | validation loss: 2.6337e-03\n",
      "Epoch: 55040 | training loss: 2.5836e-03 | validation loss: 2.6702e-03\n",
      "Epoch: 55050 | training loss: 2.5872e-03 | validation loss: 2.6832e-03\n",
      "Epoch: 55060 | training loss: 2.5809e-03 | validation loss: 2.6821e-03\n",
      "Epoch: 55070 | training loss: 2.5714e-03 | validation loss: 2.6691e-03\n",
      "Epoch: 55080 | training loss: 2.5662e-03 | validation loss: 2.6541e-03\n",
      "Epoch: 55090 | training loss: 2.5661e-03 | validation loss: 2.6467e-03\n",
      "Epoch: 55100 | training loss: 2.5660e-03 | validation loss: 2.6474e-03\n",
      "Epoch: 55110 | training loss: 2.5656e-03 | validation loss: 2.6502e-03\n",
      "Epoch: 55120 | training loss: 2.5656e-03 | validation loss: 2.6509e-03\n",
      "Epoch: 55130 | training loss: 2.5655e-03 | validation loss: 2.6496e-03\n",
      "Epoch: 55140 | training loss: 2.5654e-03 | validation loss: 2.6492e-03\n",
      "Epoch: 55150 | training loss: 2.5653e-03 | validation loss: 2.6497e-03\n",
      "Epoch: 55160 | training loss: 2.5652e-03 | validation loss: 2.6491e-03\n",
      "Epoch: 55170 | training loss: 2.5651e-03 | validation loss: 2.6492e-03\n",
      "Epoch: 55180 | training loss: 2.5650e-03 | validation loss: 2.6490e-03\n",
      "Epoch: 55190 | training loss: 2.5649e-03 | validation loss: 2.6489e-03\n",
      "Epoch: 55200 | training loss: 2.5648e-03 | validation loss: 2.6487e-03\n",
      "Epoch: 55210 | training loss: 2.5648e-03 | validation loss: 2.6485e-03\n",
      "Epoch: 55220 | training loss: 2.5647e-03 | validation loss: 2.6484e-03\n",
      "Epoch: 55230 | training loss: 2.5646e-03 | validation loss: 2.6480e-03\n",
      "Epoch: 55240 | training loss: 2.5648e-03 | validation loss: 2.6457e-03\n",
      "Epoch: 55250 | training loss: 2.6467e-03 | validation loss: 2.6677e-03\n",
      "Epoch: 55260 | training loss: 2.6082e-03 | validation loss: 2.6962e-03\n",
      "Epoch: 55270 | training loss: 2.5706e-03 | validation loss: 2.6348e-03\n",
      "Epoch: 55280 | training loss: 2.5700e-03 | validation loss: 2.6326e-03\n",
      "Epoch: 55290 | training loss: 2.5842e-03 | validation loss: 2.6251e-03\n",
      "Epoch: 55300 | training loss: 2.7955e-03 | validation loss: 2.6553e-03\n",
      "Epoch: 55310 | training loss: 2.6526e-03 | validation loss: 2.6296e-03\n",
      "Epoch: 55320 | training loss: 2.6204e-03 | validation loss: 2.7168e-03\n",
      "Epoch: 55330 | training loss: 2.5947e-03 | validation loss: 2.6272e-03\n",
      "Epoch: 55340 | training loss: 2.5795e-03 | validation loss: 2.6767e-03\n",
      "Epoch: 55350 | training loss: 2.5666e-03 | validation loss: 2.6367e-03\n",
      "Epoch: 55360 | training loss: 2.5642e-03 | validation loss: 2.6407e-03\n",
      "Epoch: 55370 | training loss: 2.5643e-03 | validation loss: 2.6512e-03\n",
      "Epoch: 55380 | training loss: 2.5653e-03 | validation loss: 2.6543e-03\n",
      "Epoch: 55390 | training loss: 2.5723e-03 | validation loss: 2.6670e-03\n",
      "Epoch: 55400 | training loss: 2.6969e-03 | validation loss: 2.7740e-03\n",
      "Epoch: 55410 | training loss: 3.0063e-03 | validation loss: 2.9658e-03\n",
      "Epoch: 55420 | training loss: 2.7107e-03 | validation loss: 2.6462e-03\n",
      "Epoch: 55430 | training loss: 2.6084e-03 | validation loss: 2.7034e-03\n",
      "Epoch: 55440 | training loss: 2.5794e-03 | validation loss: 2.6299e-03\n",
      "Epoch: 55450 | training loss: 2.5710e-03 | validation loss: 2.6639e-03\n",
      "Epoch: 55460 | training loss: 2.5663e-03 | validation loss: 2.6352e-03\n",
      "Epoch: 55470 | training loss: 2.5628e-03 | validation loss: 2.6469e-03\n",
      "Epoch: 55480 | training loss: 2.5631e-03 | validation loss: 2.6486e-03\n",
      "Epoch: 55490 | training loss: 2.5624e-03 | validation loss: 2.6439e-03\n",
      "Epoch: 55500 | training loss: 2.5624e-03 | validation loss: 2.6417e-03\n",
      "Epoch: 55510 | training loss: 2.5636e-03 | validation loss: 2.6378e-03\n",
      "Epoch: 55520 | training loss: 2.5999e-03 | validation loss: 2.6272e-03\n",
      "Epoch: 55530 | training loss: 3.6051e-03 | validation loss: 2.9514e-03\n",
      "Epoch: 55540 | training loss: 2.9685e-03 | validation loss: 2.9328e-03\n",
      "Epoch: 55550 | training loss: 2.5916e-03 | validation loss: 2.6353e-03\n",
      "Epoch: 55560 | training loss: 2.5823e-03 | validation loss: 2.6321e-03\n",
      "Epoch: 55570 | training loss: 2.5803e-03 | validation loss: 2.6719e-03\n",
      "Epoch: 55580 | training loss: 2.5643e-03 | validation loss: 2.6343e-03\n",
      "Epoch: 55590 | training loss: 2.5616e-03 | validation loss: 2.6435e-03\n",
      "Epoch: 55600 | training loss: 2.5615e-03 | validation loss: 2.6437e-03\n",
      "Epoch: 55610 | training loss: 2.5614e-03 | validation loss: 2.6413e-03\n",
      "Epoch: 55620 | training loss: 2.5613e-03 | validation loss: 2.6417e-03\n",
      "Epoch: 55630 | training loss: 2.5613e-03 | validation loss: 2.6428e-03\n",
      "Epoch: 55640 | training loss: 2.5612e-03 | validation loss: 2.6409e-03\n",
      "Epoch: 55650 | training loss: 2.5611e-03 | validation loss: 2.6411e-03\n",
      "Epoch: 55660 | training loss: 2.5610e-03 | validation loss: 2.6414e-03\n",
      "Epoch: 55670 | training loss: 2.5609e-03 | validation loss: 2.6414e-03\n",
      "Epoch: 55680 | training loss: 2.5608e-03 | validation loss: 2.6419e-03\n",
      "Epoch: 55690 | training loss: 2.5620e-03 | validation loss: 2.6472e-03\n",
      "Epoch: 55700 | training loss: 2.6833e-03 | validation loss: 2.7533e-03\n",
      "Epoch: 55710 | training loss: 3.2779e-03 | validation loss: 3.1090e-03\n",
      "Epoch: 55720 | training loss: 2.8940e-03 | validation loss: 2.8895e-03\n",
      "Epoch: 55730 | training loss: 2.5662e-03 | validation loss: 2.6574e-03\n",
      "Epoch: 55740 | training loss: 2.5795e-03 | validation loss: 2.6254e-03\n",
      "Epoch: 55750 | training loss: 2.5805e-03 | validation loss: 2.6241e-03\n",
      "Epoch: 55760 | training loss: 2.5620e-03 | validation loss: 2.6334e-03\n",
      "Epoch: 55770 | training loss: 2.5612e-03 | validation loss: 2.6461e-03\n",
      "Epoch: 55780 | training loss: 2.5606e-03 | validation loss: 2.6439e-03\n",
      "Epoch: 55790 | training loss: 2.5600e-03 | validation loss: 2.6376e-03\n",
      "Epoch: 55800 | training loss: 2.5598e-03 | validation loss: 2.6380e-03\n",
      "Epoch: 55810 | training loss: 2.5598e-03 | validation loss: 2.6402e-03\n",
      "Epoch: 55820 | training loss: 2.5596e-03 | validation loss: 2.6383e-03\n",
      "Epoch: 55830 | training loss: 2.5595e-03 | validation loss: 2.6388e-03\n",
      "Epoch: 55840 | training loss: 2.5594e-03 | validation loss: 2.6386e-03\n",
      "Epoch: 55850 | training loss: 2.5594e-03 | validation loss: 2.6384e-03\n",
      "Epoch: 55860 | training loss: 2.5593e-03 | validation loss: 2.6382e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55870 | training loss: 2.5592e-03 | validation loss: 2.6381e-03\n",
      "Epoch: 55880 | training loss: 2.5591e-03 | validation loss: 2.6378e-03\n",
      "Epoch: 55890 | training loss: 2.5590e-03 | validation loss: 2.6375e-03\n",
      "Epoch: 55900 | training loss: 2.5592e-03 | validation loss: 2.6350e-03\n",
      "Epoch: 55910 | training loss: 2.6281e-03 | validation loss: 2.6435e-03\n",
      "Epoch: 55920 | training loss: 2.6429e-03 | validation loss: 2.6433e-03\n",
      "Epoch: 55930 | training loss: 2.5664e-03 | validation loss: 2.6297e-03\n",
      "Epoch: 55940 | training loss: 2.5680e-03 | validation loss: 2.6551e-03\n",
      "Epoch: 55950 | training loss: 2.5587e-03 | validation loss: 2.6356e-03\n",
      "Epoch: 55960 | training loss: 2.5611e-03 | validation loss: 2.6292e-03\n",
      "Epoch: 55970 | training loss: 2.5662e-03 | validation loss: 2.6229e-03\n",
      "Epoch: 55980 | training loss: 2.6320e-03 | validation loss: 2.6165e-03\n",
      "Epoch: 55990 | training loss: 3.0609e-03 | validation loss: 2.7292e-03\n",
      "Epoch: 56000 | training loss: 2.6859e-03 | validation loss: 2.7578e-03\n",
      "Epoch: 56010 | training loss: 2.5960e-03 | validation loss: 2.6156e-03\n",
      "Epoch: 56020 | training loss: 2.5654e-03 | validation loss: 2.6534e-03\n",
      "Epoch: 56030 | training loss: 2.5579e-03 | validation loss: 2.6344e-03\n",
      "Epoch: 56040 | training loss: 2.5607e-03 | validation loss: 2.6259e-03\n",
      "Epoch: 56050 | training loss: 2.5587e-03 | validation loss: 2.6405e-03\n",
      "Epoch: 56060 | training loss: 2.5589e-03 | validation loss: 2.6417e-03\n",
      "Epoch: 56070 | training loss: 2.5590e-03 | validation loss: 2.6421e-03\n",
      "Epoch: 56080 | training loss: 2.5675e-03 | validation loss: 2.6571e-03\n",
      "Epoch: 56090 | training loss: 2.7637e-03 | validation loss: 2.8106e-03\n",
      "Epoch: 56100 | training loss: 2.7802e-03 | validation loss: 2.8149e-03\n",
      "Epoch: 56110 | training loss: 2.5838e-03 | validation loss: 2.6183e-03\n",
      "Epoch: 56120 | training loss: 2.5573e-03 | validation loss: 2.6323e-03\n",
      "Epoch: 56130 | training loss: 2.5601e-03 | validation loss: 2.6447e-03\n",
      "Epoch: 56140 | training loss: 2.5586e-03 | validation loss: 2.6274e-03\n",
      "Epoch: 56150 | training loss: 2.5571e-03 | validation loss: 2.6353e-03\n",
      "Epoch: 56160 | training loss: 2.5570e-03 | validation loss: 2.6352e-03\n",
      "Epoch: 56170 | training loss: 2.5573e-03 | validation loss: 2.6292e-03\n",
      "Epoch: 56180 | training loss: 2.5567e-03 | validation loss: 2.6338e-03\n",
      "Epoch: 56190 | training loss: 2.5568e-03 | validation loss: 2.6351e-03\n",
      "Epoch: 56200 | training loss: 2.5569e-03 | validation loss: 2.6363e-03\n",
      "Epoch: 56210 | training loss: 2.5604e-03 | validation loss: 2.6448e-03\n",
      "Epoch: 56220 | training loss: 2.6637e-03 | validation loss: 2.7364e-03\n",
      "Epoch: 56230 | training loss: 3.3443e-03 | validation loss: 3.1414e-03\n",
      "Epoch: 56240 | training loss: 2.6711e-03 | validation loss: 2.6383e-03\n",
      "Epoch: 56250 | training loss: 2.5606e-03 | validation loss: 2.6215e-03\n",
      "Epoch: 56260 | training loss: 2.5841e-03 | validation loss: 2.6683e-03\n",
      "Epoch: 56270 | training loss: 2.5687e-03 | validation loss: 2.6202e-03\n",
      "Epoch: 56280 | training loss: 2.5591e-03 | validation loss: 2.6424e-03\n",
      "Epoch: 56290 | training loss: 2.5566e-03 | validation loss: 2.6263e-03\n",
      "Epoch: 56300 | training loss: 2.5561e-03 | validation loss: 2.6348e-03\n",
      "Epoch: 56310 | training loss: 2.5558e-03 | validation loss: 2.6283e-03\n",
      "Epoch: 56320 | training loss: 2.5555e-03 | validation loss: 2.6321e-03\n",
      "Epoch: 56330 | training loss: 2.5554e-03 | validation loss: 2.6312e-03\n",
      "Epoch: 56340 | training loss: 2.5553e-03 | validation loss: 2.6298e-03\n",
      "Epoch: 56350 | training loss: 2.5552e-03 | validation loss: 2.6291e-03\n",
      "Epoch: 56360 | training loss: 2.5555e-03 | validation loss: 2.6273e-03\n",
      "Epoch: 56370 | training loss: 2.5635e-03 | validation loss: 2.6195e-03\n",
      "Epoch: 56380 | training loss: 2.9705e-03 | validation loss: 2.7237e-03\n",
      "Epoch: 56390 | training loss: 2.5786e-03 | validation loss: 2.6666e-03\n",
      "Epoch: 56400 | training loss: 2.7873e-03 | validation loss: 2.6608e-03\n",
      "Epoch: 56410 | training loss: 2.5604e-03 | validation loss: 2.6420e-03\n",
      "Epoch: 56420 | training loss: 2.5795e-03 | validation loss: 2.6655e-03\n",
      "Epoch: 56430 | training loss: 2.5605e-03 | validation loss: 2.6193e-03\n",
      "Epoch: 56440 | training loss: 2.5546e-03 | validation loss: 2.6271e-03\n",
      "Epoch: 56450 | training loss: 2.5554e-03 | validation loss: 2.6342e-03\n",
      "Epoch: 56460 | training loss: 2.5549e-03 | validation loss: 2.6250e-03\n",
      "Epoch: 56470 | training loss: 2.5544e-03 | validation loss: 2.6308e-03\n",
      "Epoch: 56480 | training loss: 2.5542e-03 | validation loss: 2.6267e-03\n",
      "Epoch: 56490 | training loss: 2.5541e-03 | validation loss: 2.6289e-03\n",
      "Epoch: 56500 | training loss: 2.5539e-03 | validation loss: 2.6277e-03\n",
      "Epoch: 56510 | training loss: 2.5539e-03 | validation loss: 2.6274e-03\n",
      "Epoch: 56520 | training loss: 2.5538e-03 | validation loss: 2.6279e-03\n",
      "Epoch: 56530 | training loss: 2.5539e-03 | validation loss: 2.6300e-03\n",
      "Epoch: 56540 | training loss: 2.5748e-03 | validation loss: 2.6687e-03\n",
      "Epoch: 56550 | training loss: 3.0929e-03 | validation loss: 3.1362e-03\n",
      "Epoch: 56560 | training loss: 2.6808e-03 | validation loss: 2.7844e-03\n",
      "Epoch: 56570 | training loss: 2.5624e-03 | validation loss: 2.6201e-03\n",
      "Epoch: 56580 | training loss: 2.5648e-03 | validation loss: 2.6074e-03\n",
      "Epoch: 56590 | training loss: 2.5588e-03 | validation loss: 2.6146e-03\n",
      "Epoch: 56600 | training loss: 2.5563e-03 | validation loss: 2.6181e-03\n",
      "Epoch: 56610 | training loss: 2.5551e-03 | validation loss: 2.6163e-03\n",
      "Epoch: 56620 | training loss: 2.6025e-03 | validation loss: 2.6066e-03\n",
      "Epoch: 56630 | training loss: 3.6485e-03 | validation loss: 2.9344e-03\n",
      "Epoch: 56640 | training loss: 2.8552e-03 | validation loss: 2.8541e-03\n",
      "Epoch: 56650 | training loss: 2.5529e-03 | validation loss: 2.6238e-03\n",
      "Epoch: 56660 | training loss: 2.5920e-03 | validation loss: 2.6079e-03\n",
      "Epoch: 56670 | training loss: 2.5635e-03 | validation loss: 2.6470e-03\n",
      "Epoch: 56680 | training loss: 2.5526e-03 | validation loss: 2.6232e-03\n",
      "Epoch: 56690 | training loss: 2.5528e-03 | validation loss: 2.6214e-03\n",
      "Epoch: 56700 | training loss: 2.5527e-03 | validation loss: 2.6280e-03\n",
      "Epoch: 56710 | training loss: 2.5524e-03 | validation loss: 2.6221e-03\n",
      "Epoch: 56720 | training loss: 2.5522e-03 | validation loss: 2.6247e-03\n",
      "Epoch: 56730 | training loss: 2.5521e-03 | validation loss: 2.6242e-03\n",
      "Epoch: 56740 | training loss: 2.5520e-03 | validation loss: 2.6229e-03\n",
      "Epoch: 56750 | training loss: 2.5519e-03 | validation loss: 2.6235e-03\n",
      "Epoch: 56760 | training loss: 2.5518e-03 | validation loss: 2.6238e-03\n",
      "Epoch: 56770 | training loss: 2.5518e-03 | validation loss: 2.6241e-03\n",
      "Epoch: 56780 | training loss: 2.5521e-03 | validation loss: 2.6268e-03\n",
      "Epoch: 56790 | training loss: 2.5696e-03 | validation loss: 2.6537e-03\n",
      "Epoch: 56800 | training loss: 3.5876e-03 | validation loss: 3.2702e-03\n",
      "Epoch: 56810 | training loss: 3.1356e-03 | validation loss: 2.7724e-03\n",
      "Epoch: 56820 | training loss: 2.5568e-03 | validation loss: 2.6205e-03\n",
      "Epoch: 56830 | training loss: 2.6232e-03 | validation loss: 2.6926e-03\n",
      "Epoch: 56840 | training loss: 2.5528e-03 | validation loss: 2.6315e-03\n",
      "Epoch: 56850 | training loss: 2.5605e-03 | validation loss: 2.6149e-03\n",
      "Epoch: 56860 | training loss: 2.5511e-03 | validation loss: 2.6215e-03\n",
      "Epoch: 56870 | training loss: 2.5518e-03 | validation loss: 2.6259e-03\n",
      "Epoch: 56880 | training loss: 2.5513e-03 | validation loss: 2.6195e-03\n",
      "Epoch: 56890 | training loss: 2.5508e-03 | validation loss: 2.6226e-03\n",
      "Epoch: 56900 | training loss: 2.5507e-03 | validation loss: 2.6208e-03\n",
      "Epoch: 56910 | training loss: 2.5506e-03 | validation loss: 2.6215e-03\n",
      "Epoch: 56920 | training loss: 2.5505e-03 | validation loss: 2.6207e-03\n",
      "Epoch: 56930 | training loss: 2.5504e-03 | validation loss: 2.6212e-03\n",
      "Epoch: 56940 | training loss: 2.5503e-03 | validation loss: 2.6206e-03\n",
      "Epoch: 56950 | training loss: 2.5503e-03 | validation loss: 2.6204e-03\n",
      "Epoch: 56960 | training loss: 2.5502e-03 | validation loss: 2.6204e-03\n",
      "Epoch: 56970 | training loss: 2.5501e-03 | validation loss: 2.6202e-03\n",
      "Epoch: 56980 | training loss: 2.5500e-03 | validation loss: 2.6201e-03\n",
      "Epoch: 56990 | training loss: 2.5499e-03 | validation loss: 2.6203e-03\n",
      "Epoch: 57000 | training loss: 2.5503e-03 | validation loss: 2.6233e-03\n",
      "Epoch: 57010 | training loss: 2.6322e-03 | validation loss: 2.7011e-03\n",
      "Epoch: 57020 | training loss: 3.7745e-03 | validation loss: 3.3664e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57030 | training loss: 3.1742e-03 | validation loss: 3.0227e-03\n",
      "Epoch: 57040 | training loss: 2.7625e-03 | validation loss: 2.7712e-03\n",
      "Epoch: 57050 | training loss: 2.6260e-03 | validation loss: 2.6791e-03\n",
      "Epoch: 57060 | training loss: 2.5765e-03 | validation loss: 2.6462e-03\n",
      "Epoch: 57070 | training loss: 2.5586e-03 | validation loss: 2.6330e-03\n",
      "Epoch: 57080 | training loss: 2.5519e-03 | validation loss: 2.6247e-03\n",
      "Epoch: 57090 | training loss: 2.5495e-03 | validation loss: 2.6197e-03\n",
      "Epoch: 57100 | training loss: 2.5490e-03 | validation loss: 2.6182e-03\n",
      "Epoch: 57110 | training loss: 2.5490e-03 | validation loss: 2.6172e-03\n",
      "Epoch: 57120 | training loss: 2.5489e-03 | validation loss: 2.6170e-03\n",
      "Epoch: 57130 | training loss: 2.5488e-03 | validation loss: 2.6175e-03\n",
      "Epoch: 57140 | training loss: 2.5487e-03 | validation loss: 2.6176e-03\n",
      "Epoch: 57150 | training loss: 2.5486e-03 | validation loss: 2.6173e-03\n",
      "Epoch: 57160 | training loss: 2.5486e-03 | validation loss: 2.6170e-03\n",
      "Epoch: 57170 | training loss: 2.5485e-03 | validation loss: 2.6170e-03\n",
      "Epoch: 57180 | training loss: 2.5484e-03 | validation loss: 2.6168e-03\n",
      "Epoch: 57190 | training loss: 2.5483e-03 | validation loss: 2.6166e-03\n",
      "Epoch: 57200 | training loss: 2.5482e-03 | validation loss: 2.6165e-03\n",
      "Epoch: 57210 | training loss: 2.5481e-03 | validation loss: 2.6164e-03\n",
      "Epoch: 57220 | training loss: 2.5481e-03 | validation loss: 2.6166e-03\n",
      "Epoch: 57230 | training loss: 2.5484e-03 | validation loss: 2.6209e-03\n",
      "Epoch: 57240 | training loss: 2.6402e-03 | validation loss: 2.7454e-03\n",
      "Epoch: 57250 | training loss: 2.7366e-03 | validation loss: 2.7606e-03\n",
      "Epoch: 57260 | training loss: 2.5663e-03 | validation loss: 2.6297e-03\n",
      "Epoch: 57270 | training loss: 2.5574e-03 | validation loss: 2.6216e-03\n",
      "Epoch: 57280 | training loss: 2.5555e-03 | validation loss: 2.6385e-03\n",
      "Epoch: 57290 | training loss: 2.5518e-03 | validation loss: 2.6114e-03\n",
      "Epoch: 57300 | training loss: 2.5495e-03 | validation loss: 2.6261e-03\n",
      "Epoch: 57310 | training loss: 2.5482e-03 | validation loss: 2.6155e-03\n",
      "Epoch: 57320 | training loss: 2.5476e-03 | validation loss: 2.6164e-03\n",
      "Epoch: 57330 | training loss: 2.5473e-03 | validation loss: 2.6168e-03\n",
      "Epoch: 57340 | training loss: 2.5471e-03 | validation loss: 2.6135e-03\n",
      "Epoch: 57350 | training loss: 2.5471e-03 | validation loss: 2.6121e-03\n",
      "Epoch: 57360 | training loss: 2.5471e-03 | validation loss: 2.6109e-03\n",
      "Epoch: 57370 | training loss: 2.5496e-03 | validation loss: 2.6049e-03\n",
      "Epoch: 57380 | training loss: 2.6614e-03 | validation loss: 2.6027e-03\n",
      "Epoch: 57390 | training loss: 3.2783e-03 | validation loss: 2.8041e-03\n",
      "Epoch: 57400 | training loss: 2.5546e-03 | validation loss: 2.6196e-03\n",
      "Epoch: 57410 | training loss: 2.6378e-03 | validation loss: 2.6986e-03\n",
      "Epoch: 57420 | training loss: 2.5637e-03 | validation loss: 2.5959e-03\n",
      "Epoch: 57430 | training loss: 2.5483e-03 | validation loss: 2.6061e-03\n",
      "Epoch: 57440 | training loss: 2.5512e-03 | validation loss: 2.6270e-03\n",
      "Epoch: 57450 | training loss: 2.5480e-03 | validation loss: 2.6068e-03\n",
      "Epoch: 57460 | training loss: 2.5466e-03 | validation loss: 2.6152e-03\n",
      "Epoch: 57470 | training loss: 2.5462e-03 | validation loss: 2.6098e-03\n",
      "Epoch: 57480 | training loss: 2.5461e-03 | validation loss: 2.6133e-03\n",
      "Epoch: 57490 | training loss: 2.5459e-03 | validation loss: 2.6105e-03\n",
      "Epoch: 57500 | training loss: 2.5458e-03 | validation loss: 2.6117e-03\n",
      "Epoch: 57510 | training loss: 2.5457e-03 | validation loss: 2.6117e-03\n",
      "Epoch: 57520 | training loss: 2.5456e-03 | validation loss: 2.6111e-03\n",
      "Epoch: 57530 | training loss: 2.5456e-03 | validation loss: 2.6108e-03\n",
      "Epoch: 57540 | training loss: 2.5455e-03 | validation loss: 2.6102e-03\n",
      "Epoch: 57550 | training loss: 2.5458e-03 | validation loss: 2.6075e-03\n",
      "Epoch: 57560 | training loss: 2.5763e-03 | validation loss: 2.5963e-03\n",
      "Epoch: 57570 | training loss: 4.3501e-03 | validation loss: 3.2356e-03\n",
      "Epoch: 57580 | training loss: 2.7116e-03 | validation loss: 2.7437e-03\n",
      "Epoch: 57590 | training loss: 2.7506e-03 | validation loss: 2.7784e-03\n",
      "Epoch: 57600 | training loss: 2.5846e-03 | validation loss: 2.6677e-03\n",
      "Epoch: 57610 | training loss: 2.5464e-03 | validation loss: 2.6129e-03\n",
      "Epoch: 57620 | training loss: 2.5525e-03 | validation loss: 2.5961e-03\n",
      "Epoch: 57630 | training loss: 2.5466e-03 | validation loss: 2.6008e-03\n",
      "Epoch: 57640 | training loss: 2.5452e-03 | validation loss: 2.6143e-03\n",
      "Epoch: 57650 | training loss: 2.5449e-03 | validation loss: 2.6126e-03\n",
      "Epoch: 57660 | training loss: 2.5447e-03 | validation loss: 2.6067e-03\n",
      "Epoch: 57670 | training loss: 2.5445e-03 | validation loss: 2.6089e-03\n",
      "Epoch: 57680 | training loss: 2.5444e-03 | validation loss: 2.6093e-03\n",
      "Epoch: 57690 | training loss: 2.5443e-03 | validation loss: 2.6081e-03\n",
      "Epoch: 57700 | training loss: 2.5442e-03 | validation loss: 2.6087e-03\n",
      "Epoch: 57710 | training loss: 2.5441e-03 | validation loss: 2.6080e-03\n",
      "Epoch: 57720 | training loss: 2.5440e-03 | validation loss: 2.6082e-03\n",
      "Epoch: 57730 | training loss: 2.5440e-03 | validation loss: 2.6079e-03\n",
      "Epoch: 57740 | training loss: 2.5439e-03 | validation loss: 2.6078e-03\n",
      "Epoch: 57750 | training loss: 2.5438e-03 | validation loss: 2.6077e-03\n",
      "Epoch: 57760 | training loss: 2.5437e-03 | validation loss: 2.6079e-03\n",
      "Epoch: 57770 | training loss: 2.5442e-03 | validation loss: 2.6110e-03\n",
      "Epoch: 57780 | training loss: 2.6267e-03 | validation loss: 2.7058e-03\n",
      "Epoch: 57790 | training loss: 2.5469e-03 | validation loss: 2.6039e-03\n",
      "Epoch: 57800 | training loss: 2.6229e-03 | validation loss: 2.6931e-03\n",
      "Epoch: 57810 | training loss: 2.5911e-03 | validation loss: 2.6546e-03\n",
      "Epoch: 57820 | training loss: 2.6014e-03 | validation loss: 2.6051e-03\n",
      "Epoch: 57830 | training loss: 3.2960e-03 | validation loss: 2.7875e-03\n",
      "Epoch: 57840 | training loss: 2.8015e-03 | validation loss: 2.8154e-03\n",
      "Epoch: 57850 | training loss: 2.6347e-03 | validation loss: 2.5916e-03\n",
      "Epoch: 57860 | training loss: 2.5607e-03 | validation loss: 2.6359e-03\n",
      "Epoch: 57870 | training loss: 2.5454e-03 | validation loss: 2.5962e-03\n",
      "Epoch: 57880 | training loss: 2.5435e-03 | validation loss: 2.6093e-03\n",
      "Epoch: 57890 | training loss: 2.5434e-03 | validation loss: 2.6001e-03\n",
      "Epoch: 57900 | training loss: 2.5434e-03 | validation loss: 2.6100e-03\n",
      "Epoch: 57910 | training loss: 2.5428e-03 | validation loss: 2.6023e-03\n",
      "Epoch: 57920 | training loss: 2.5425e-03 | validation loss: 2.6032e-03\n",
      "Epoch: 57930 | training loss: 2.5424e-03 | validation loss: 2.6050e-03\n",
      "Epoch: 57940 | training loss: 2.5424e-03 | validation loss: 2.6061e-03\n",
      "Epoch: 57950 | training loss: 2.5435e-03 | validation loss: 2.6110e-03\n",
      "Epoch: 57960 | training loss: 2.5805e-03 | validation loss: 2.6564e-03\n",
      "Epoch: 57970 | training loss: 3.6515e-03 | validation loss: 3.2961e-03\n",
      "Epoch: 57980 | training loss: 2.9435e-03 | validation loss: 2.6942e-03\n",
      "Epoch: 57990 | training loss: 2.5504e-03 | validation loss: 2.6158e-03\n",
      "Epoch: 58000 | training loss: 2.5792e-03 | validation loss: 2.6460e-03\n",
      "Epoch: 58010 | training loss: 2.5557e-03 | validation loss: 2.5895e-03\n",
      "Epoch: 58020 | training loss: 2.5419e-03 | validation loss: 2.6066e-03\n",
      "Epoch: 58030 | training loss: 2.5422e-03 | validation loss: 2.6081e-03\n",
      "Epoch: 58040 | training loss: 2.5421e-03 | validation loss: 2.5987e-03\n",
      "Epoch: 58050 | training loss: 2.5417e-03 | validation loss: 2.6053e-03\n",
      "Epoch: 58060 | training loss: 2.5414e-03 | validation loss: 2.6015e-03\n",
      "Epoch: 58070 | training loss: 2.5413e-03 | validation loss: 2.6025e-03\n",
      "Epoch: 58080 | training loss: 2.5412e-03 | validation loss: 2.6029e-03\n",
      "Epoch: 58090 | training loss: 2.5411e-03 | validation loss: 2.6017e-03\n",
      "Epoch: 58100 | training loss: 2.5410e-03 | validation loss: 2.6015e-03\n",
      "Epoch: 58110 | training loss: 2.5409e-03 | validation loss: 2.6012e-03\n",
      "Epoch: 58120 | training loss: 2.5410e-03 | validation loss: 2.5999e-03\n",
      "Epoch: 58130 | training loss: 2.5451e-03 | validation loss: 2.5931e-03\n",
      "Epoch: 58140 | training loss: 2.8204e-03 | validation loss: 2.6481e-03\n",
      "Epoch: 58150 | training loss: 2.5610e-03 | validation loss: 2.5954e-03\n",
      "Epoch: 58160 | training loss: 2.8315e-03 | validation loss: 2.6479e-03\n",
      "Epoch: 58170 | training loss: 2.5416e-03 | validation loss: 2.5941e-03\n",
      "Epoch: 58180 | training loss: 2.5772e-03 | validation loss: 2.6497e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58190 | training loss: 2.5416e-03 | validation loss: 2.6090e-03\n",
      "Epoch: 58200 | training loss: 2.5453e-03 | validation loss: 2.5894e-03\n",
      "Epoch: 58210 | training loss: 2.5402e-03 | validation loss: 2.6020e-03\n",
      "Epoch: 58220 | training loss: 2.5404e-03 | validation loss: 2.6037e-03\n",
      "Epoch: 58230 | training loss: 2.5402e-03 | validation loss: 2.5973e-03\n",
      "Epoch: 58240 | training loss: 2.5400e-03 | validation loss: 2.6013e-03\n",
      "Epoch: 58250 | training loss: 2.5398e-03 | validation loss: 2.5990e-03\n",
      "Epoch: 58260 | training loss: 2.5398e-03 | validation loss: 2.6000e-03\n",
      "Epoch: 58270 | training loss: 2.5397e-03 | validation loss: 2.5989e-03\n",
      "Epoch: 58280 | training loss: 2.5396e-03 | validation loss: 2.5994e-03\n",
      "Epoch: 58290 | training loss: 2.5395e-03 | validation loss: 2.5991e-03\n",
      "Epoch: 58300 | training loss: 2.5394e-03 | validation loss: 2.5987e-03\n",
      "Epoch: 58310 | training loss: 2.5393e-03 | validation loss: 2.5983e-03\n",
      "Epoch: 58320 | training loss: 2.5397e-03 | validation loss: 2.5954e-03\n",
      "Epoch: 58330 | training loss: 2.6152e-03 | validation loss: 2.6097e-03\n",
      "Epoch: 58340 | training loss: 2.5813e-03 | validation loss: 2.5894e-03\n",
      "Epoch: 58350 | training loss: 2.6643e-03 | validation loss: 2.5901e-03\n",
      "Epoch: 58360 | training loss: 2.5798e-03 | validation loss: 2.5830e-03\n",
      "Epoch: 58370 | training loss: 2.5554e-03 | validation loss: 2.6025e-03\n",
      "Epoch: 58380 | training loss: 2.5610e-03 | validation loss: 2.6222e-03\n",
      "Epoch: 58390 | training loss: 2.6457e-03 | validation loss: 2.7003e-03\n",
      "Epoch: 58400 | training loss: 2.8535e-03 | validation loss: 2.8391e-03\n",
      "Epoch: 58410 | training loss: 2.6029e-03 | validation loss: 2.5778e-03\n",
      "Epoch: 58420 | training loss: 2.5389e-03 | validation loss: 2.5995e-03\n",
      "Epoch: 58430 | training loss: 2.5507e-03 | validation loss: 2.6219e-03\n",
      "Epoch: 58440 | training loss: 2.5426e-03 | validation loss: 2.5868e-03\n",
      "Epoch: 58450 | training loss: 2.5436e-03 | validation loss: 2.5857e-03\n",
      "Epoch: 58460 | training loss: 2.5427e-03 | validation loss: 2.5859e-03\n",
      "Epoch: 58470 | training loss: 2.5615e-03 | validation loss: 2.5788e-03\n",
      "Epoch: 58480 | training loss: 2.8545e-03 | validation loss: 2.6394e-03\n",
      "Epoch: 58490 | training loss: 2.5604e-03 | validation loss: 2.5828e-03\n",
      "Epoch: 58500 | training loss: 2.5505e-03 | validation loss: 2.6204e-03\n",
      "Epoch: 58510 | training loss: 2.5449e-03 | validation loss: 2.5838e-03\n",
      "Epoch: 58520 | training loss: 2.5449e-03 | validation loss: 2.6120e-03\n",
      "Epoch: 58530 | training loss: 2.5440e-03 | validation loss: 2.5843e-03\n",
      "Epoch: 58540 | training loss: 2.5390e-03 | validation loss: 2.6021e-03\n",
      "Epoch: 58550 | training loss: 2.5381e-03 | validation loss: 2.5992e-03\n",
      "Epoch: 58560 | training loss: 2.5375e-03 | validation loss: 2.5930e-03\n",
      "Epoch: 58570 | training loss: 2.5380e-03 | validation loss: 2.5902e-03\n",
      "Epoch: 58580 | training loss: 2.5448e-03 | validation loss: 2.5832e-03\n",
      "Epoch: 58590 | training loss: 2.7332e-03 | validation loss: 2.6106e-03\n",
      "Epoch: 58600 | training loss: 2.8312e-03 | validation loss: 2.6487e-03\n",
      "Epoch: 58610 | training loss: 2.5457e-03 | validation loss: 2.6072e-03\n",
      "Epoch: 58620 | training loss: 2.5577e-03 | validation loss: 2.6240e-03\n",
      "Epoch: 58630 | training loss: 2.5613e-03 | validation loss: 2.5817e-03\n",
      "Epoch: 58640 | training loss: 2.5473e-03 | validation loss: 2.6148e-03\n",
      "Epoch: 58650 | training loss: 2.5405e-03 | validation loss: 2.5847e-03\n",
      "Epoch: 58660 | training loss: 2.5382e-03 | validation loss: 2.6002e-03\n",
      "Epoch: 58670 | training loss: 2.5371e-03 | validation loss: 2.5897e-03\n",
      "Epoch: 58680 | training loss: 2.5365e-03 | validation loss: 2.5936e-03\n",
      "Epoch: 58690 | training loss: 2.5364e-03 | validation loss: 2.5943e-03\n",
      "Epoch: 58700 | training loss: 2.5363e-03 | validation loss: 2.5923e-03\n",
      "Epoch: 58710 | training loss: 2.5362e-03 | validation loss: 2.5914e-03\n",
      "Epoch: 58720 | training loss: 2.5365e-03 | validation loss: 2.5895e-03\n",
      "Epoch: 58730 | training loss: 2.5438e-03 | validation loss: 2.5821e-03\n",
      "Epoch: 58740 | training loss: 2.9052e-03 | validation loss: 2.6705e-03\n",
      "Epoch: 58750 | training loss: 2.5379e-03 | validation loss: 2.6020e-03\n",
      "Epoch: 58760 | training loss: 2.7543e-03 | validation loss: 2.6190e-03\n",
      "Epoch: 58770 | training loss: 2.5532e-03 | validation loss: 2.6175e-03\n",
      "Epoch: 58780 | training loss: 2.5546e-03 | validation loss: 2.6225e-03\n",
      "Epoch: 58790 | training loss: 2.5444e-03 | validation loss: 2.5802e-03\n",
      "Epoch: 58800 | training loss: 2.5355e-03 | validation loss: 2.5918e-03\n",
      "Epoch: 58810 | training loss: 2.5360e-03 | validation loss: 2.5952e-03\n",
      "Epoch: 58820 | training loss: 2.5358e-03 | validation loss: 2.5873e-03\n",
      "Epoch: 58830 | training loss: 2.5355e-03 | validation loss: 2.5931e-03\n",
      "Epoch: 58840 | training loss: 2.5352e-03 | validation loss: 2.5891e-03\n",
      "Epoch: 58850 | training loss: 2.5351e-03 | validation loss: 2.5908e-03\n",
      "Epoch: 58860 | training loss: 2.5350e-03 | validation loss: 2.5902e-03\n",
      "Epoch: 58870 | training loss: 2.5349e-03 | validation loss: 2.5894e-03\n",
      "Epoch: 58880 | training loss: 2.5348e-03 | validation loss: 2.5895e-03\n",
      "Epoch: 58890 | training loss: 2.5348e-03 | validation loss: 2.5887e-03\n",
      "Epoch: 58900 | training loss: 2.5369e-03 | validation loss: 2.5834e-03\n",
      "Epoch: 58910 | training loss: 2.7856e-03 | validation loss: 2.6878e-03\n",
      "Epoch: 58920 | training loss: 2.8697e-03 | validation loss: 2.8033e-03\n",
      "Epoch: 58930 | training loss: 2.6091e-03 | validation loss: 2.5857e-03\n",
      "Epoch: 58940 | training loss: 2.5503e-03 | validation loss: 2.6078e-03\n",
      "Epoch: 58950 | training loss: 2.5591e-03 | validation loss: 2.6121e-03\n",
      "Epoch: 58960 | training loss: 2.5410e-03 | validation loss: 2.5913e-03\n",
      "Epoch: 58970 | training loss: 2.5352e-03 | validation loss: 2.5890e-03\n",
      "Epoch: 58980 | training loss: 2.5369e-03 | validation loss: 2.6000e-03\n",
      "Epoch: 58990 | training loss: 2.6180e-03 | validation loss: 2.6813e-03\n",
      "Epoch: 59000 | training loss: 3.4318e-03 | validation loss: 3.1662e-03\n",
      "Epoch: 59010 | training loss: 2.6770e-03 | validation loss: 2.5962e-03\n",
      "Epoch: 59020 | training loss: 2.5405e-03 | validation loss: 2.5794e-03\n",
      "Epoch: 59030 | training loss: 2.5677e-03 | validation loss: 2.6336e-03\n",
      "Epoch: 59040 | training loss: 2.5447e-03 | validation loss: 2.5742e-03\n",
      "Epoch: 59050 | training loss: 2.5348e-03 | validation loss: 2.5926e-03\n",
      "Epoch: 59060 | training loss: 2.5335e-03 | validation loss: 2.5849e-03\n",
      "Epoch: 59070 | training loss: 2.5334e-03 | validation loss: 2.5872e-03\n",
      "Epoch: 59080 | training loss: 2.5334e-03 | validation loss: 2.5853e-03\n",
      "Epoch: 59090 | training loss: 2.5333e-03 | validation loss: 2.5880e-03\n",
      "Epoch: 59100 | training loss: 2.5332e-03 | validation loss: 2.5851e-03\n",
      "Epoch: 59110 | training loss: 2.5331e-03 | validation loss: 2.5853e-03\n",
      "Epoch: 59120 | training loss: 2.5330e-03 | validation loss: 2.5858e-03\n",
      "Epoch: 59130 | training loss: 2.5329e-03 | validation loss: 2.5860e-03\n",
      "Epoch: 59140 | training loss: 2.5329e-03 | validation loss: 2.5871e-03\n",
      "Epoch: 59150 | training loss: 2.5359e-03 | validation loss: 2.5961e-03\n",
      "Epoch: 59160 | training loss: 2.7861e-03 | validation loss: 2.7848e-03\n",
      "Epoch: 59170 | training loss: 2.5676e-03 | validation loss: 2.6205e-03\n",
      "Epoch: 59180 | training loss: 2.8692e-03 | validation loss: 2.8309e-03\n",
      "Epoch: 59190 | training loss: 2.5526e-03 | validation loss: 2.6237e-03\n",
      "Epoch: 59200 | training loss: 2.5530e-03 | validation loss: 2.5804e-03\n",
      "Epoch: 59210 | training loss: 2.5441e-03 | validation loss: 2.5716e-03\n",
      "Epoch: 59220 | training loss: 2.5334e-03 | validation loss: 2.5842e-03\n",
      "Epoch: 59230 | training loss: 2.5337e-03 | validation loss: 2.5913e-03\n",
      "Epoch: 59240 | training loss: 2.5324e-03 | validation loss: 2.5838e-03\n",
      "Epoch: 59250 | training loss: 2.5321e-03 | validation loss: 2.5822e-03\n",
      "Epoch: 59260 | training loss: 2.5320e-03 | validation loss: 2.5851e-03\n",
      "Epoch: 59270 | training loss: 2.5319e-03 | validation loss: 2.5832e-03\n",
      "Epoch: 59280 | training loss: 2.5318e-03 | validation loss: 2.5836e-03\n",
      "Epoch: 59290 | training loss: 2.5317e-03 | validation loss: 2.5833e-03\n",
      "Epoch: 59300 | training loss: 2.5316e-03 | validation loss: 2.5832e-03\n",
      "Epoch: 59310 | training loss: 2.5315e-03 | validation loss: 2.5830e-03\n",
      "Epoch: 59320 | training loss: 2.5314e-03 | validation loss: 2.5829e-03\n",
      "Epoch: 59330 | training loss: 2.5314e-03 | validation loss: 2.5827e-03\n",
      "Epoch: 59340 | training loss: 2.5313e-03 | validation loss: 2.5826e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59350 | training loss: 2.5312e-03 | validation loss: 2.5824e-03\n",
      "Epoch: 59360 | training loss: 2.5311e-03 | validation loss: 2.5823e-03\n",
      "Epoch: 59370 | training loss: 2.5321e-03 | validation loss: 2.5831e-03\n",
      "Epoch: 59380 | training loss: 2.6387e-03 | validation loss: 2.6525e-03\n",
      "Epoch: 59390 | training loss: 3.6780e-03 | validation loss: 3.3619e-03\n",
      "Epoch: 59400 | training loss: 2.6085e-03 | validation loss: 2.5587e-03\n",
      "Epoch: 59410 | training loss: 2.5950e-03 | validation loss: 2.5837e-03\n",
      "Epoch: 59420 | training loss: 2.5542e-03 | validation loss: 2.6267e-03\n",
      "Epoch: 59430 | training loss: 2.5513e-03 | validation loss: 2.6243e-03\n",
      "Epoch: 59440 | training loss: 2.5354e-03 | validation loss: 2.5878e-03\n",
      "Epoch: 59450 | training loss: 2.5318e-03 | validation loss: 2.5823e-03\n",
      "Epoch: 59460 | training loss: 2.5310e-03 | validation loss: 2.5863e-03\n",
      "Epoch: 59470 | training loss: 2.5305e-03 | validation loss: 2.5796e-03\n",
      "Epoch: 59480 | training loss: 2.5303e-03 | validation loss: 2.5814e-03\n",
      "Epoch: 59490 | training loss: 2.5302e-03 | validation loss: 2.5802e-03\n",
      "Epoch: 59500 | training loss: 2.5301e-03 | validation loss: 2.5805e-03\n",
      "Epoch: 59510 | training loss: 2.5300e-03 | validation loss: 2.5796e-03\n",
      "Epoch: 59520 | training loss: 2.5299e-03 | validation loss: 2.5797e-03\n",
      "Epoch: 59530 | training loss: 2.5299e-03 | validation loss: 2.5794e-03\n",
      "Epoch: 59540 | training loss: 2.5298e-03 | validation loss: 2.5791e-03\n",
      "Epoch: 59550 | training loss: 2.5297e-03 | validation loss: 2.5789e-03\n",
      "Epoch: 59560 | training loss: 2.5296e-03 | validation loss: 2.5785e-03\n",
      "Epoch: 59570 | training loss: 2.5297e-03 | validation loss: 2.5764e-03\n",
      "Epoch: 59580 | training loss: 2.5432e-03 | validation loss: 2.5648e-03\n",
      "Epoch: 59590 | training loss: 3.7884e-03 | validation loss: 2.9659e-03\n",
      "Epoch: 59600 | training loss: 3.2318e-03 | validation loss: 3.0337e-03\n",
      "Epoch: 59610 | training loss: 2.6481e-03 | validation loss: 2.6865e-03\n",
      "Epoch: 59620 | training loss: 2.5297e-03 | validation loss: 2.5754e-03\n",
      "Epoch: 59630 | training loss: 2.5502e-03 | validation loss: 2.5660e-03\n",
      "Epoch: 59640 | training loss: 2.5384e-03 | validation loss: 2.5688e-03\n",
      "Epoch: 59650 | training loss: 2.5291e-03 | validation loss: 2.5801e-03\n",
      "Epoch: 59660 | training loss: 2.5304e-03 | validation loss: 2.5853e-03\n",
      "Epoch: 59670 | training loss: 2.5288e-03 | validation loss: 2.5780e-03\n",
      "Epoch: 59680 | training loss: 2.5289e-03 | validation loss: 2.5751e-03\n",
      "Epoch: 59690 | training loss: 2.5287e-03 | validation loss: 2.5778e-03\n",
      "Epoch: 59700 | training loss: 2.5286e-03 | validation loss: 2.5771e-03\n",
      "Epoch: 59710 | training loss: 2.5285e-03 | validation loss: 2.5766e-03\n",
      "Epoch: 59720 | training loss: 2.5284e-03 | validation loss: 2.5770e-03\n",
      "Epoch: 59730 | training loss: 2.5284e-03 | validation loss: 2.5764e-03\n",
      "Epoch: 59740 | training loss: 2.5283e-03 | validation loss: 2.5765e-03\n",
      "Epoch: 59750 | training loss: 2.5282e-03 | validation loss: 2.5763e-03\n",
      "Epoch: 59760 | training loss: 2.5281e-03 | validation loss: 2.5761e-03\n",
      "Epoch: 59770 | training loss: 2.5280e-03 | validation loss: 2.5760e-03\n",
      "Epoch: 59780 | training loss: 2.5280e-03 | validation loss: 2.5759e-03\n",
      "Epoch: 59790 | training loss: 2.5279e-03 | validation loss: 2.5758e-03\n",
      "Epoch: 59800 | training loss: 2.5278e-03 | validation loss: 2.5758e-03\n",
      "Epoch: 59810 | training loss: 2.5278e-03 | validation loss: 2.5769e-03\n",
      "Epoch: 59820 | training loss: 2.5358e-03 | validation loss: 2.5931e-03\n",
      "Epoch: 59830 | training loss: 3.6147e-03 | validation loss: 3.2442e-03\n",
      "Epoch: 59840 | training loss: 3.4551e-03 | validation loss: 2.8569e-03\n",
      "Epoch: 59850 | training loss: 2.7309e-03 | validation loss: 2.5873e-03\n",
      "Epoch: 59860 | training loss: 2.5539e-03 | validation loss: 2.5528e-03\n",
      "Epoch: 59870 | training loss: 2.5274e-03 | validation loss: 2.5713e-03\n",
      "Epoch: 59880 | training loss: 2.5311e-03 | validation loss: 2.5882e-03\n",
      "Epoch: 59890 | training loss: 2.5318e-03 | validation loss: 2.5884e-03\n",
      "Epoch: 59900 | training loss: 2.5283e-03 | validation loss: 2.5803e-03\n",
      "Epoch: 59910 | training loss: 2.5270e-03 | validation loss: 2.5735e-03\n",
      "Epoch: 59920 | training loss: 2.5271e-03 | validation loss: 2.5711e-03\n",
      "Epoch: 59930 | training loss: 2.5268e-03 | validation loss: 2.5732e-03\n",
      "Epoch: 59940 | training loss: 2.5268e-03 | validation loss: 2.5744e-03\n",
      "Epoch: 59950 | training loss: 2.5267e-03 | validation loss: 2.5731e-03\n",
      "Epoch: 59960 | training loss: 2.5266e-03 | validation loss: 2.5728e-03\n",
      "Epoch: 59970 | training loss: 2.5265e-03 | validation loss: 2.5732e-03\n",
      "Epoch: 59980 | training loss: 2.5264e-03 | validation loss: 2.5727e-03\n",
      "Epoch: 59990 | training loss: 2.5264e-03 | validation loss: 2.5727e-03\n",
      "Epoch: 60000 | training loss: 2.5263e-03 | validation loss: 2.5725e-03\n",
      "Epoch: 60010 | training loss: 2.5262e-03 | validation loss: 2.5723e-03\n",
      "Epoch: 60020 | training loss: 2.5261e-03 | validation loss: 2.5719e-03\n",
      "Epoch: 60030 | training loss: 2.5263e-03 | validation loss: 2.5698e-03\n",
      "Epoch: 60040 | training loss: 2.5657e-03 | validation loss: 2.5704e-03\n",
      "Epoch: 60050 | training loss: 2.6573e-03 | validation loss: 2.5971e-03\n",
      "Epoch: 60060 | training loss: 2.6816e-03 | validation loss: 2.6368e-03\n",
      "Epoch: 60070 | training loss: 2.5782e-03 | validation loss: 2.5873e-03\n",
      "Epoch: 60080 | training loss: 2.5377e-03 | validation loss: 2.5632e-03\n",
      "Epoch: 60090 | training loss: 2.5275e-03 | validation loss: 2.5622e-03\n",
      "Epoch: 60100 | training loss: 2.5283e-03 | validation loss: 2.5634e-03\n",
      "Epoch: 60110 | training loss: 2.5570e-03 | validation loss: 2.5544e-03\n",
      "Epoch: 60120 | training loss: 3.0760e-03 | validation loss: 2.6840e-03\n",
      "Epoch: 60130 | training loss: 2.6185e-03 | validation loss: 2.6685e-03\n",
      "Epoch: 60140 | training loss: 2.5933e-03 | validation loss: 2.5558e-03\n",
      "Epoch: 60150 | training loss: 2.5607e-03 | validation loss: 2.6191e-03\n",
      "Epoch: 60160 | training loss: 2.5395e-03 | validation loss: 2.5551e-03\n",
      "Epoch: 60170 | training loss: 2.5301e-03 | validation loss: 2.5841e-03\n",
      "Epoch: 60180 | training loss: 2.5259e-03 | validation loss: 2.5640e-03\n",
      "Epoch: 60190 | training loss: 2.5249e-03 | validation loss: 2.5682e-03\n",
      "Epoch: 60200 | training loss: 2.5252e-03 | validation loss: 2.5730e-03\n",
      "Epoch: 60210 | training loss: 2.5247e-03 | validation loss: 2.5699e-03\n",
      "Epoch: 60220 | training loss: 2.5246e-03 | validation loss: 2.5682e-03\n",
      "Epoch: 60230 | training loss: 2.5246e-03 | validation loss: 2.5668e-03\n",
      "Epoch: 60240 | training loss: 2.5270e-03 | validation loss: 2.5608e-03\n",
      "Epoch: 60250 | training loss: 2.6637e-03 | validation loss: 2.5673e-03\n",
      "Epoch: 60260 | training loss: 3.1092e-03 | validation loss: 2.7249e-03\n",
      "Epoch: 60270 | training loss: 2.5775e-03 | validation loss: 2.5456e-03\n",
      "Epoch: 60280 | training loss: 2.6238e-03 | validation loss: 2.6531e-03\n",
      "Epoch: 60290 | training loss: 2.5313e-03 | validation loss: 2.5824e-03\n",
      "Epoch: 60300 | training loss: 2.5380e-03 | validation loss: 2.5600e-03\n",
      "Epoch: 60310 | training loss: 2.5241e-03 | validation loss: 2.5699e-03\n",
      "Epoch: 60320 | training loss: 2.5254e-03 | validation loss: 2.5730e-03\n",
      "Epoch: 60330 | training loss: 2.5245e-03 | validation loss: 2.5626e-03\n",
      "Epoch: 60340 | training loss: 2.5239e-03 | validation loss: 2.5695e-03\n",
      "Epoch: 60350 | training loss: 2.5237e-03 | validation loss: 2.5661e-03\n",
      "Epoch: 60360 | training loss: 2.5236e-03 | validation loss: 2.5671e-03\n",
      "Epoch: 60370 | training loss: 2.5235e-03 | validation loss: 2.5662e-03\n",
      "Epoch: 60380 | training loss: 2.5234e-03 | validation loss: 2.5667e-03\n",
      "Epoch: 60390 | training loss: 2.5233e-03 | validation loss: 2.5663e-03\n",
      "Epoch: 60400 | training loss: 2.5233e-03 | validation loss: 2.5659e-03\n",
      "Epoch: 60410 | training loss: 2.5232e-03 | validation loss: 2.5658e-03\n",
      "Epoch: 60420 | training loss: 2.5231e-03 | validation loss: 2.5654e-03\n",
      "Epoch: 60430 | training loss: 2.5232e-03 | validation loss: 2.5640e-03\n",
      "Epoch: 60440 | training loss: 2.5294e-03 | validation loss: 2.5562e-03\n",
      "Epoch: 60450 | training loss: 3.0974e-03 | validation loss: 2.7179e-03\n",
      "Epoch: 60460 | training loss: 2.8910e-03 | validation loss: 2.8336e-03\n",
      "Epoch: 60470 | training loss: 2.6008e-03 | validation loss: 2.5592e-03\n",
      "Epoch: 60480 | training loss: 2.6265e-03 | validation loss: 2.5600e-03\n",
      "Epoch: 60490 | training loss: 2.5334e-03 | validation loss: 2.5515e-03\n",
      "Epoch: 60500 | training loss: 2.5269e-03 | validation loss: 2.5777e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60510 | training loss: 2.5267e-03 | validation loss: 2.5767e-03\n",
      "Epoch: 60520 | training loss: 2.5226e-03 | validation loss: 2.5621e-03\n",
      "Epoch: 60530 | training loss: 2.5228e-03 | validation loss: 2.5609e-03\n",
      "Epoch: 60540 | training loss: 2.5224e-03 | validation loss: 2.5662e-03\n",
      "Epoch: 60550 | training loss: 2.5221e-03 | validation loss: 2.5638e-03\n",
      "Epoch: 60560 | training loss: 2.5221e-03 | validation loss: 2.5632e-03\n",
      "Epoch: 60570 | training loss: 2.5220e-03 | validation loss: 2.5641e-03\n",
      "Epoch: 60580 | training loss: 2.5219e-03 | validation loss: 2.5631e-03\n",
      "Epoch: 60590 | training loss: 2.5218e-03 | validation loss: 2.5634e-03\n",
      "Epoch: 60600 | training loss: 2.5217e-03 | validation loss: 2.5630e-03\n",
      "Epoch: 60610 | training loss: 2.5217e-03 | validation loss: 2.5625e-03\n",
      "Epoch: 60620 | training loss: 2.5219e-03 | validation loss: 2.5601e-03\n",
      "Epoch: 60630 | training loss: 2.5638e-03 | validation loss: 2.5590e-03\n",
      "Epoch: 60640 | training loss: 2.6950e-03 | validation loss: 2.5880e-03\n",
      "Epoch: 60650 | training loss: 2.6676e-03 | validation loss: 2.6228e-03\n",
      "Epoch: 60660 | training loss: 2.5558e-03 | validation loss: 2.5767e-03\n",
      "Epoch: 60670 | training loss: 2.5231e-03 | validation loss: 2.5530e-03\n",
      "Epoch: 60680 | training loss: 2.5249e-03 | validation loss: 2.5579e-03\n",
      "Epoch: 60690 | training loss: 2.5289e-03 | validation loss: 2.5568e-03\n",
      "Epoch: 60700 | training loss: 2.5713e-03 | validation loss: 2.5447e-03\n",
      "Epoch: 60710 | training loss: 3.0382e-03 | validation loss: 2.6639e-03\n",
      "Epoch: 60720 | training loss: 2.6052e-03 | validation loss: 2.6486e-03\n",
      "Epoch: 60730 | training loss: 2.5470e-03 | validation loss: 2.5433e-03\n",
      "Epoch: 60740 | training loss: 2.5265e-03 | validation loss: 2.5765e-03\n",
      "Epoch: 60750 | training loss: 2.5206e-03 | validation loss: 2.5599e-03\n",
      "Epoch: 60760 | training loss: 2.5227e-03 | validation loss: 2.5532e-03\n",
      "Epoch: 60770 | training loss: 2.5225e-03 | validation loss: 2.5685e-03\n",
      "Epoch: 60780 | training loss: 2.5207e-03 | validation loss: 2.5630e-03\n",
      "Epoch: 60790 | training loss: 2.5204e-03 | validation loss: 2.5587e-03\n",
      "Epoch: 60800 | training loss: 2.5208e-03 | validation loss: 2.5557e-03\n",
      "Epoch: 60810 | training loss: 2.5296e-03 | validation loss: 2.5469e-03\n",
      "Epoch: 60820 | training loss: 2.8497e-03 | validation loss: 2.6114e-03\n",
      "Epoch: 60830 | training loss: 2.5348e-03 | validation loss: 2.5610e-03\n",
      "Epoch: 60840 | training loss: 2.6031e-03 | validation loss: 2.5492e-03\n",
      "Epoch: 60850 | training loss: 2.5922e-03 | validation loss: 2.6282e-03\n",
      "Epoch: 60860 | training loss: 2.5269e-03 | validation loss: 2.5484e-03\n",
      "Epoch: 60870 | training loss: 2.5202e-03 | validation loss: 2.5587e-03\n",
      "Epoch: 60880 | training loss: 2.5209e-03 | validation loss: 2.5643e-03\n",
      "Epoch: 60890 | training loss: 2.5203e-03 | validation loss: 2.5539e-03\n",
      "Epoch: 60900 | training loss: 2.5197e-03 | validation loss: 2.5611e-03\n",
      "Epoch: 60910 | training loss: 2.5194e-03 | validation loss: 2.5575e-03\n",
      "Epoch: 60920 | training loss: 2.5194e-03 | validation loss: 2.5570e-03\n",
      "Epoch: 60930 | training loss: 2.5193e-03 | validation loss: 2.5587e-03\n",
      "Epoch: 60940 | training loss: 2.5192e-03 | validation loss: 2.5583e-03\n",
      "Epoch: 60950 | training loss: 2.5191e-03 | validation loss: 2.5579e-03\n",
      "Epoch: 60960 | training loss: 2.5191e-03 | validation loss: 2.5583e-03\n",
      "Epoch: 60970 | training loss: 2.5197e-03 | validation loss: 2.5618e-03\n",
      "Epoch: 60980 | training loss: 2.5622e-03 | validation loss: 2.6092e-03\n",
      "Epoch: 60990 | training loss: 4.1331e-03 | validation loss: 3.5052e-03\n",
      "Epoch: 61000 | training loss: 2.6665e-03 | validation loss: 2.5719e-03\n",
      "Epoch: 61010 | training loss: 2.6849e-03 | validation loss: 2.5667e-03\n",
      "Epoch: 61020 | training loss: 2.5201e-03 | validation loss: 2.5474e-03\n",
      "Epoch: 61030 | training loss: 2.5376e-03 | validation loss: 2.5885e-03\n",
      "Epoch: 61040 | training loss: 2.5207e-03 | validation loss: 2.5655e-03\n",
      "Epoch: 61050 | training loss: 2.5208e-03 | validation loss: 2.5486e-03\n",
      "Epoch: 61060 | training loss: 2.5183e-03 | validation loss: 2.5551e-03\n",
      "Epoch: 61070 | training loss: 2.5186e-03 | validation loss: 2.5590e-03\n",
      "Epoch: 61080 | training loss: 2.5183e-03 | validation loss: 2.5539e-03\n",
      "Epoch: 61090 | training loss: 2.5181e-03 | validation loss: 2.5560e-03\n",
      "Epoch: 61100 | training loss: 2.5180e-03 | validation loss: 2.5550e-03\n",
      "Epoch: 61110 | training loss: 2.5179e-03 | validation loss: 2.5552e-03\n",
      "Epoch: 61120 | training loss: 2.5178e-03 | validation loss: 2.5548e-03\n",
      "Epoch: 61130 | training loss: 2.5178e-03 | validation loss: 2.5551e-03\n",
      "Epoch: 61140 | training loss: 2.5177e-03 | validation loss: 2.5548e-03\n",
      "Epoch: 61150 | training loss: 2.5177e-03 | validation loss: 2.5561e-03\n",
      "Epoch: 61160 | training loss: 2.5263e-03 | validation loss: 2.5757e-03\n",
      "Epoch: 61170 | training loss: 3.1031e-03 | validation loss: 3.0967e-03\n",
      "Epoch: 61180 | training loss: 2.5780e-03 | validation loss: 2.5944e-03\n",
      "Epoch: 61190 | training loss: 2.5824e-03 | validation loss: 2.5656e-03\n",
      "Epoch: 61200 | training loss: 2.5389e-03 | validation loss: 2.5382e-03\n",
      "Epoch: 61210 | training loss: 2.5284e-03 | validation loss: 2.5370e-03\n",
      "Epoch: 61220 | training loss: 2.5877e-03 | validation loss: 2.5416e-03\n",
      "Epoch: 61230 | training loss: 2.9106e-03 | validation loss: 2.6221e-03\n",
      "Epoch: 61240 | training loss: 2.5564e-03 | validation loss: 2.6065e-03\n",
      "Epoch: 61250 | training loss: 2.5170e-03 | validation loss: 2.5524e-03\n",
      "Epoch: 61260 | training loss: 2.5284e-03 | validation loss: 2.5376e-03\n",
      "Epoch: 61270 | training loss: 2.5259e-03 | validation loss: 2.5731e-03\n",
      "Epoch: 61280 | training loss: 2.5172e-03 | validation loss: 2.5568e-03\n",
      "Epoch: 61290 | training loss: 2.5178e-03 | validation loss: 2.5464e-03\n",
      "Epoch: 61300 | training loss: 2.5222e-03 | validation loss: 2.5410e-03\n",
      "Epoch: 61310 | training loss: 2.5736e-03 | validation loss: 2.5359e-03\n",
      "Epoch: 61320 | training loss: 3.1001e-03 | validation loss: 2.6887e-03\n",
      "Epoch: 61330 | training loss: 2.6421e-03 | validation loss: 2.6672e-03\n",
      "Epoch: 61340 | training loss: 2.5641e-03 | validation loss: 2.5403e-03\n",
      "Epoch: 61350 | training loss: 2.5320e-03 | validation loss: 2.5777e-03\n",
      "Epoch: 61360 | training loss: 2.5186e-03 | validation loss: 2.5432e-03\n",
      "Epoch: 61370 | training loss: 2.5162e-03 | validation loss: 2.5492e-03\n",
      "Epoch: 61380 | training loss: 2.5179e-03 | validation loss: 2.5586e-03\n",
      "Epoch: 61390 | training loss: 2.5159e-03 | validation loss: 2.5490e-03\n",
      "Epoch: 61400 | training loss: 2.5165e-03 | validation loss: 2.5464e-03\n",
      "Epoch: 61410 | training loss: 2.5180e-03 | validation loss: 2.5436e-03\n",
      "Epoch: 61420 | training loss: 2.5411e-03 | validation loss: 2.5366e-03\n",
      "Epoch: 61430 | training loss: 3.0283e-03 | validation loss: 2.6752e-03\n",
      "Epoch: 61440 | training loss: 2.5560e-03 | validation loss: 2.6035e-03\n",
      "Epoch: 61450 | training loss: 2.5762e-03 | validation loss: 2.5376e-03\n",
      "Epoch: 61460 | training loss: 2.5571e-03 | validation loss: 2.5985e-03\n",
      "Epoch: 61470 | training loss: 2.5333e-03 | validation loss: 2.5390e-03\n",
      "Epoch: 61480 | training loss: 2.5222e-03 | validation loss: 2.5637e-03\n",
      "Epoch: 61490 | training loss: 2.5175e-03 | validation loss: 2.5437e-03\n",
      "Epoch: 61500 | training loss: 2.5153e-03 | validation loss: 2.5512e-03\n",
      "Epoch: 61510 | training loss: 2.5151e-03 | validation loss: 2.5507e-03\n",
      "Epoch: 61520 | training loss: 2.5150e-03 | validation loss: 2.5469e-03\n",
      "Epoch: 61530 | training loss: 2.5150e-03 | validation loss: 2.5464e-03\n",
      "Epoch: 61540 | training loss: 2.5152e-03 | validation loss: 2.5452e-03\n",
      "Epoch: 61550 | training loss: 2.5206e-03 | validation loss: 2.5397e-03\n",
      "Epoch: 61560 | training loss: 2.7067e-03 | validation loss: 2.5749e-03\n",
      "Epoch: 61570 | training loss: 2.8396e-03 | validation loss: 2.6101e-03\n",
      "Epoch: 61580 | training loss: 2.5167e-03 | validation loss: 2.5511e-03\n",
      "Epoch: 61590 | training loss: 2.5831e-03 | validation loss: 2.6109e-03\n",
      "Epoch: 61600 | training loss: 2.5393e-03 | validation loss: 2.5407e-03\n",
      "Epoch: 61610 | training loss: 2.5153e-03 | validation loss: 2.5509e-03\n",
      "Epoch: 61620 | training loss: 2.5144e-03 | validation loss: 2.5499e-03\n",
      "Epoch: 61630 | training loss: 2.5144e-03 | validation loss: 2.5444e-03\n",
      "Epoch: 61640 | training loss: 2.5141e-03 | validation loss: 2.5480e-03\n",
      "Epoch: 61650 | training loss: 2.5139e-03 | validation loss: 2.5464e-03\n",
      "Epoch: 61660 | training loss: 2.5138e-03 | validation loss: 2.5459e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61670 | training loss: 2.5138e-03 | validation loss: 2.5478e-03\n",
      "Epoch: 61680 | training loss: 2.5140e-03 | validation loss: 2.5504e-03\n",
      "Epoch: 61690 | training loss: 2.5422e-03 | validation loss: 2.5994e-03\n",
      "Epoch: 61700 | training loss: 3.0189e-03 | validation loss: 3.0163e-03\n",
      "Epoch: 61710 | training loss: 2.5388e-03 | validation loss: 2.5822e-03\n",
      "Epoch: 61720 | training loss: 2.5395e-03 | validation loss: 2.5256e-03\n",
      "Epoch: 61730 | training loss: 2.5404e-03 | validation loss: 2.5594e-03\n",
      "Epoch: 61740 | training loss: 2.5162e-03 | validation loss: 2.5434e-03\n",
      "Epoch: 61750 | training loss: 2.5150e-03 | validation loss: 2.5424e-03\n",
      "Epoch: 61760 | training loss: 2.5186e-03 | validation loss: 2.5386e-03\n",
      "Epoch: 61770 | training loss: 2.5539e-03 | validation loss: 2.5261e-03\n",
      "Epoch: 61780 | training loss: 3.0135e-03 | validation loss: 2.6463e-03\n",
      "Epoch: 61790 | training loss: 2.5620e-03 | validation loss: 2.6049e-03\n",
      "Epoch: 61800 | training loss: 2.5299e-03 | validation loss: 2.5324e-03\n",
      "Epoch: 61810 | training loss: 2.5171e-03 | validation loss: 2.5567e-03\n",
      "Epoch: 61820 | training loss: 2.5127e-03 | validation loss: 2.5418e-03\n",
      "Epoch: 61830 | training loss: 2.5144e-03 | validation loss: 2.5371e-03\n",
      "Epoch: 61840 | training loss: 2.5149e-03 | validation loss: 2.5528e-03\n",
      "Epoch: 61850 | training loss: 2.5124e-03 | validation loss: 2.5438e-03\n",
      "Epoch: 61860 | training loss: 2.5128e-03 | validation loss: 2.5396e-03\n",
      "Epoch: 61870 | training loss: 2.5142e-03 | validation loss: 2.5367e-03\n",
      "Epoch: 61880 | training loss: 2.5330e-03 | validation loss: 2.5287e-03\n",
      "Epoch: 61890 | training loss: 2.9644e-03 | validation loss: 2.6423e-03\n",
      "Epoch: 61900 | training loss: 2.5241e-03 | validation loss: 2.5701e-03\n",
      "Epoch: 61910 | training loss: 2.5577e-03 | validation loss: 2.5284e-03\n",
      "Epoch: 61920 | training loss: 2.5532e-03 | validation loss: 2.5895e-03\n",
      "Epoch: 61930 | training loss: 2.5310e-03 | validation loss: 2.5320e-03\n",
      "Epoch: 61940 | training loss: 2.5192e-03 | validation loss: 2.5578e-03\n",
      "Epoch: 61950 | training loss: 2.5144e-03 | validation loss: 2.5351e-03\n",
      "Epoch: 61960 | training loss: 2.5121e-03 | validation loss: 2.5456e-03\n",
      "Epoch: 61970 | training loss: 2.5116e-03 | validation loss: 2.5423e-03\n",
      "Epoch: 61980 | training loss: 2.5117e-03 | validation loss: 2.5392e-03\n",
      "Epoch: 61990 | training loss: 2.5115e-03 | validation loss: 2.5399e-03\n",
      "Epoch: 62000 | training loss: 2.5114e-03 | validation loss: 2.5399e-03\n",
      "Epoch: 62010 | training loss: 2.5116e-03 | validation loss: 2.5381e-03\n",
      "Epoch: 62020 | training loss: 2.5229e-03 | validation loss: 2.5300e-03\n",
      "Epoch: 62030 | training loss: 3.1258e-03 | validation loss: 2.7107e-03\n",
      "Epoch: 62040 | training loss: 2.7528e-03 | validation loss: 2.7289e-03\n",
      "Epoch: 62050 | training loss: 2.6883e-03 | validation loss: 2.5588e-03\n",
      "Epoch: 62060 | training loss: 2.5254e-03 | validation loss: 2.5271e-03\n",
      "Epoch: 62070 | training loss: 2.5364e-03 | validation loss: 2.5759e-03\n",
      "Epoch: 62080 | training loss: 2.5108e-03 | validation loss: 2.5409e-03\n",
      "Epoch: 62090 | training loss: 2.5139e-03 | validation loss: 2.5326e-03\n",
      "Epoch: 62100 | training loss: 2.5118e-03 | validation loss: 2.5453e-03\n",
      "Epoch: 62110 | training loss: 2.5106e-03 | validation loss: 2.5377e-03\n",
      "Epoch: 62120 | training loss: 2.5104e-03 | validation loss: 2.5397e-03\n",
      "Epoch: 62130 | training loss: 2.5104e-03 | validation loss: 2.5388e-03\n",
      "Epoch: 62140 | training loss: 2.5103e-03 | validation loss: 2.5393e-03\n",
      "Epoch: 62150 | training loss: 2.5102e-03 | validation loss: 2.5383e-03\n",
      "Epoch: 62160 | training loss: 2.5101e-03 | validation loss: 2.5390e-03\n",
      "Epoch: 62170 | training loss: 2.5100e-03 | validation loss: 2.5389e-03\n",
      "Epoch: 62180 | training loss: 2.5101e-03 | validation loss: 2.5406e-03\n",
      "Epoch: 62190 | training loss: 2.5327e-03 | validation loss: 2.5806e-03\n",
      "Epoch: 62200 | training loss: 3.0178e-03 | validation loss: 3.0149e-03\n",
      "Epoch: 62210 | training loss: 2.6599e-03 | validation loss: 2.7101e-03\n",
      "Epoch: 62220 | training loss: 2.5439e-03 | validation loss: 2.5515e-03\n",
      "Epoch: 62230 | training loss: 2.5121e-03 | validation loss: 2.5356e-03\n",
      "Epoch: 62240 | training loss: 2.5116e-03 | validation loss: 2.5432e-03\n",
      "Epoch: 62250 | training loss: 2.5180e-03 | validation loss: 2.5517e-03\n",
      "Epoch: 62260 | training loss: 2.5676e-03 | validation loss: 2.6028e-03\n",
      "Epoch: 62270 | training loss: 2.9888e-03 | validation loss: 2.8774e-03\n",
      "Epoch: 62280 | training loss: 2.5773e-03 | validation loss: 2.5213e-03\n",
      "Epoch: 62290 | training loss: 2.5205e-03 | validation loss: 2.5599e-03\n",
      "Epoch: 62300 | training loss: 2.5092e-03 | validation loss: 2.5345e-03\n",
      "Epoch: 62310 | training loss: 2.5125e-03 | validation loss: 2.5274e-03\n",
      "Epoch: 62320 | training loss: 2.5138e-03 | validation loss: 2.5499e-03\n",
      "Epoch: 62330 | training loss: 2.5089e-03 | validation loss: 2.5354e-03\n",
      "Epoch: 62340 | training loss: 2.5101e-03 | validation loss: 2.5299e-03\n",
      "Epoch: 62350 | training loss: 2.5122e-03 | validation loss: 2.5268e-03\n",
      "Epoch: 62360 | training loss: 2.5400e-03 | validation loss: 2.5191e-03\n",
      "Epoch: 62370 | training loss: 2.9931e-03 | validation loss: 2.6389e-03\n",
      "Epoch: 62380 | training loss: 2.5369e-03 | validation loss: 2.5775e-03\n",
      "Epoch: 62390 | training loss: 2.5300e-03 | validation loss: 2.5225e-03\n",
      "Epoch: 62400 | training loss: 2.5210e-03 | validation loss: 2.5572e-03\n",
      "Epoch: 62410 | training loss: 2.5121e-03 | validation loss: 2.5255e-03\n",
      "Epoch: 62420 | training loss: 2.5084e-03 | validation loss: 2.5364e-03\n",
      "Epoch: 62430 | training loss: 2.5090e-03 | validation loss: 2.5393e-03\n",
      "Epoch: 62440 | training loss: 2.5092e-03 | validation loss: 2.5292e-03\n",
      "Epoch: 62450 | training loss: 2.5081e-03 | validation loss: 2.5327e-03\n",
      "Epoch: 62460 | training loss: 2.5080e-03 | validation loss: 2.5353e-03\n",
      "Epoch: 62470 | training loss: 2.5085e-03 | validation loss: 2.5379e-03\n",
      "Epoch: 62480 | training loss: 2.5177e-03 | validation loss: 2.5537e-03\n",
      "Epoch: 62490 | training loss: 2.8441e-03 | validation loss: 2.7803e-03\n",
      "Epoch: 62500 | training loss: 2.5191e-03 | validation loss: 2.5496e-03\n",
      "Epoch: 62510 | training loss: 2.5974e-03 | validation loss: 2.6220e-03\n",
      "Epoch: 62520 | training loss: 2.5793e-03 | validation loss: 2.5284e-03\n",
      "Epoch: 62530 | training loss: 2.5135e-03 | validation loss: 2.5437e-03\n",
      "Epoch: 62540 | training loss: 2.5080e-03 | validation loss: 2.5375e-03\n",
      "Epoch: 62550 | training loss: 2.5090e-03 | validation loss: 2.5275e-03\n",
      "Epoch: 62560 | training loss: 2.5082e-03 | validation loss: 2.5369e-03\n",
      "Epoch: 62570 | training loss: 2.5075e-03 | validation loss: 2.5301e-03\n",
      "Epoch: 62580 | training loss: 2.5071e-03 | validation loss: 2.5328e-03\n",
      "Epoch: 62590 | training loss: 2.5070e-03 | validation loss: 2.5325e-03\n",
      "Epoch: 62600 | training loss: 2.5070e-03 | validation loss: 2.5309e-03\n",
      "Epoch: 62610 | training loss: 2.5069e-03 | validation loss: 2.5312e-03\n",
      "Epoch: 62620 | training loss: 2.5068e-03 | validation loss: 2.5314e-03\n",
      "Epoch: 62630 | training loss: 2.5067e-03 | validation loss: 2.5315e-03\n",
      "Epoch: 62640 | training loss: 2.5067e-03 | validation loss: 2.5323e-03\n",
      "Epoch: 62650 | training loss: 2.5096e-03 | validation loss: 2.5403e-03\n",
      "Epoch: 62660 | training loss: 2.8322e-03 | validation loss: 2.7653e-03\n",
      "Epoch: 62670 | training loss: 2.5439e-03 | validation loss: 2.5404e-03\n",
      "Epoch: 62680 | training loss: 2.7472e-03 | validation loss: 2.7108e-03\n",
      "Epoch: 62690 | training loss: 2.6334e-03 | validation loss: 2.6265e-03\n",
      "Epoch: 62700 | training loss: 2.5267e-03 | validation loss: 2.5507e-03\n",
      "Epoch: 62710 | training loss: 2.5064e-03 | validation loss: 2.5305e-03\n",
      "Epoch: 62720 | training loss: 2.5109e-03 | validation loss: 2.5256e-03\n",
      "Epoch: 62730 | training loss: 2.5072e-03 | validation loss: 2.5270e-03\n",
      "Epoch: 62740 | training loss: 2.5061e-03 | validation loss: 2.5312e-03\n",
      "Epoch: 62750 | training loss: 2.5061e-03 | validation loss: 2.5308e-03\n",
      "Epoch: 62760 | training loss: 2.5058e-03 | validation loss: 2.5287e-03\n",
      "Epoch: 62770 | training loss: 2.5057e-03 | validation loss: 2.5289e-03\n",
      "Epoch: 62780 | training loss: 2.5056e-03 | validation loss: 2.5292e-03\n",
      "Epoch: 62790 | training loss: 2.5056e-03 | validation loss: 2.5286e-03\n",
      "Epoch: 62800 | training loss: 2.5055e-03 | validation loss: 2.5288e-03\n",
      "Epoch: 62810 | training loss: 2.5054e-03 | validation loss: 2.5285e-03\n",
      "Epoch: 62820 | training loss: 2.5053e-03 | validation loss: 2.5287e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62830 | training loss: 2.5053e-03 | validation loss: 2.5299e-03\n",
      "Epoch: 62840 | training loss: 2.5107e-03 | validation loss: 2.5476e-03\n",
      "Epoch: 62850 | training loss: 3.0729e-03 | validation loss: 3.0655e-03\n",
      "Epoch: 62860 | training loss: 2.6659e-03 | validation loss: 2.5682e-03\n",
      "Epoch: 62870 | training loss: 2.5900e-03 | validation loss: 2.5639e-03\n",
      "Epoch: 62880 | training loss: 2.5234e-03 | validation loss: 2.5135e-03\n",
      "Epoch: 62890 | training loss: 2.5056e-03 | validation loss: 2.5273e-03\n",
      "Epoch: 62900 | training loss: 2.5060e-03 | validation loss: 2.5338e-03\n",
      "Epoch: 62910 | training loss: 2.5068e-03 | validation loss: 2.5274e-03\n",
      "Epoch: 62920 | training loss: 2.5049e-03 | validation loss: 2.5298e-03\n",
      "Epoch: 62930 | training loss: 2.5049e-03 | validation loss: 2.5283e-03\n",
      "Epoch: 62940 | training loss: 2.5049e-03 | validation loss: 2.5290e-03\n",
      "Epoch: 62950 | training loss: 2.5072e-03 | validation loss: 2.5370e-03\n",
      "Epoch: 62960 | training loss: 2.5706e-03 | validation loss: 2.6020e-03\n",
      "Epoch: 62970 | training loss: 3.4255e-03 | validation loss: 3.1122e-03\n",
      "Epoch: 62980 | training loss: 2.7740e-03 | validation loss: 2.5744e-03\n",
      "Epoch: 62990 | training loss: 2.5234e-03 | validation loss: 2.5537e-03\n",
      "Epoch: 63000 | training loss: 2.5088e-03 | validation loss: 2.5343e-03\n",
      "Epoch: 63010 | training loss: 2.5135e-03 | validation loss: 2.5134e-03\n",
      "Epoch: 63020 | training loss: 2.5090e-03 | validation loss: 2.5395e-03\n",
      "Epoch: 63030 | training loss: 2.5058e-03 | validation loss: 2.5188e-03\n",
      "Epoch: 63040 | training loss: 2.5045e-03 | validation loss: 2.5286e-03\n",
      "Epoch: 63050 | training loss: 2.5038e-03 | validation loss: 2.5232e-03\n",
      "Epoch: 63060 | training loss: 2.5036e-03 | validation loss: 2.5235e-03\n",
      "Epoch: 63070 | training loss: 2.5036e-03 | validation loss: 2.5254e-03\n",
      "Epoch: 63080 | training loss: 2.5035e-03 | validation loss: 2.5251e-03\n",
      "Epoch: 63090 | training loss: 2.5035e-03 | validation loss: 2.5251e-03\n",
      "Epoch: 63100 | training loss: 2.5039e-03 | validation loss: 2.5277e-03\n",
      "Epoch: 63110 | training loss: 2.5206e-03 | validation loss: 2.5521e-03\n",
      "Epoch: 63120 | training loss: 3.3406e-03 | validation loss: 3.0544e-03\n",
      "Epoch: 63130 | training loss: 2.9399e-03 | validation loss: 2.6219e-03\n",
      "Epoch: 63140 | training loss: 2.5821e-03 | validation loss: 2.5956e-03\n",
      "Epoch: 63150 | training loss: 2.5425e-03 | validation loss: 2.5762e-03\n",
      "Epoch: 63160 | training loss: 2.5174e-03 | validation loss: 2.5133e-03\n",
      "Epoch: 63170 | training loss: 2.5048e-03 | validation loss: 2.5137e-03\n",
      "Epoch: 63180 | training loss: 2.5060e-03 | validation loss: 2.5350e-03\n",
      "Epoch: 63190 | training loss: 2.5034e-03 | validation loss: 2.5184e-03\n",
      "Epoch: 63200 | training loss: 2.5027e-03 | validation loss: 2.5230e-03\n",
      "Epoch: 63210 | training loss: 2.5025e-03 | validation loss: 2.5224e-03\n",
      "Epoch: 63220 | training loss: 2.5025e-03 | validation loss: 2.5220e-03\n",
      "Epoch: 63230 | training loss: 2.5024e-03 | validation loss: 2.5213e-03\n",
      "Epoch: 63240 | training loss: 2.5023e-03 | validation loss: 2.5223e-03\n",
      "Epoch: 63250 | training loss: 2.5023e-03 | validation loss: 2.5213e-03\n",
      "Epoch: 63260 | training loss: 2.5022e-03 | validation loss: 2.5212e-03\n",
      "Epoch: 63270 | training loss: 2.5021e-03 | validation loss: 2.5214e-03\n",
      "Epoch: 63280 | training loss: 2.5021e-03 | validation loss: 2.5222e-03\n",
      "Epoch: 63290 | training loss: 2.5063e-03 | validation loss: 2.5321e-03\n",
      "Epoch: 63300 | training loss: 2.8581e-03 | validation loss: 2.8472e-03\n",
      "Epoch: 63310 | training loss: 2.7226e-03 | validation loss: 2.6093e-03\n",
      "Epoch: 63320 | training loss: 2.6211e-03 | validation loss: 2.5046e-03\n",
      "Epoch: 63330 | training loss: 2.9589e-03 | validation loss: 2.6186e-03\n",
      "Epoch: 63340 | training loss: 2.6654e-03 | validation loss: 2.6783e-03\n",
      "Epoch: 63350 | training loss: 2.5619e-03 | validation loss: 2.5154e-03\n",
      "Epoch: 63360 | training loss: 2.5247e-03 | validation loss: 2.5608e-03\n",
      "Epoch: 63370 | training loss: 2.5096e-03 | validation loss: 2.5079e-03\n",
      "Epoch: 63380 | training loss: 2.5025e-03 | validation loss: 2.5243e-03\n",
      "Epoch: 63390 | training loss: 2.5018e-03 | validation loss: 2.5219e-03\n",
      "Epoch: 63400 | training loss: 2.5017e-03 | validation loss: 2.5151e-03\n",
      "Epoch: 63410 | training loss: 2.5019e-03 | validation loss: 2.5146e-03\n",
      "Epoch: 63420 | training loss: 2.5035e-03 | validation loss: 2.5110e-03\n",
      "Epoch: 63430 | training loss: 2.5336e-03 | validation loss: 2.5017e-03\n",
      "Epoch: 63440 | training loss: 3.1363e-03 | validation loss: 2.6727e-03\n",
      "Epoch: 63450 | training loss: 2.6453e-03 | validation loss: 2.6485e-03\n",
      "Epoch: 63460 | training loss: 2.6018e-03 | validation loss: 2.5185e-03\n",
      "Epoch: 63470 | training loss: 2.5439e-03 | validation loss: 2.5693e-03\n",
      "Epoch: 63480 | training loss: 2.5151e-03 | validation loss: 2.5039e-03\n",
      "Epoch: 63490 | training loss: 2.5060e-03 | validation loss: 2.5320e-03\n",
      "Epoch: 63500 | training loss: 2.5029e-03 | validation loss: 2.5113e-03\n",
      "Epoch: 63510 | training loss: 2.5011e-03 | validation loss: 2.5215e-03\n",
      "Epoch: 63520 | training loss: 2.5003e-03 | validation loss: 2.5177e-03\n",
      "Epoch: 63530 | training loss: 2.5005e-03 | validation loss: 2.5148e-03\n",
      "Epoch: 63540 | training loss: 2.5003e-03 | validation loss: 2.5152e-03\n",
      "Epoch: 63550 | training loss: 2.5003e-03 | validation loss: 2.5146e-03\n",
      "Epoch: 63560 | training loss: 2.5027e-03 | validation loss: 2.5099e-03\n",
      "Epoch: 63570 | training loss: 2.5905e-03 | validation loss: 2.5117e-03\n",
      "Epoch: 63580 | training loss: 3.4956e-03 | validation loss: 2.8314e-03\n",
      "Epoch: 63590 | training loss: 2.5888e-03 | validation loss: 2.5964e-03\n",
      "Epoch: 63600 | training loss: 2.5473e-03 | validation loss: 2.5752e-03\n",
      "Epoch: 63610 | training loss: 2.5368e-03 | validation loss: 2.5052e-03\n",
      "Epoch: 63620 | training loss: 2.5007e-03 | validation loss: 2.5155e-03\n",
      "Epoch: 63630 | training loss: 2.5019e-03 | validation loss: 2.5265e-03\n",
      "Epoch: 63640 | training loss: 2.5013e-03 | validation loss: 2.5086e-03\n",
      "Epoch: 63650 | training loss: 2.5001e-03 | validation loss: 2.5202e-03\n",
      "Epoch: 63660 | training loss: 2.4996e-03 | validation loss: 2.5124e-03\n",
      "Epoch: 63670 | training loss: 2.4994e-03 | validation loss: 2.5172e-03\n",
      "Epoch: 63680 | training loss: 2.4992e-03 | validation loss: 2.5136e-03\n",
      "Epoch: 63690 | training loss: 2.4991e-03 | validation loss: 2.5147e-03\n",
      "Epoch: 63700 | training loss: 2.4991e-03 | validation loss: 2.5153e-03\n",
      "Epoch: 63710 | training loss: 2.4990e-03 | validation loss: 2.5150e-03\n",
      "Epoch: 63720 | training loss: 2.4989e-03 | validation loss: 2.5152e-03\n",
      "Epoch: 63730 | training loss: 2.4992e-03 | validation loss: 2.5181e-03\n",
      "Epoch: 63740 | training loss: 2.5208e-03 | validation loss: 2.5550e-03\n",
      "Epoch: 63750 | training loss: 2.9430e-03 | validation loss: 2.9367e-03\n",
      "Epoch: 63760 | training loss: 2.7419e-03 | validation loss: 2.7045e-03\n",
      "Epoch: 63770 | training loss: 2.6218e-03 | validation loss: 2.5906e-03\n",
      "Epoch: 63780 | training loss: 2.5711e-03 | validation loss: 2.4918e-03\n",
      "Epoch: 63790 | training loss: 2.5303e-03 | validation loss: 2.5476e-03\n",
      "Epoch: 63800 | training loss: 2.5108e-03 | validation loss: 2.4996e-03\n",
      "Epoch: 63810 | training loss: 2.5000e-03 | validation loss: 2.5225e-03\n",
      "Epoch: 63820 | training loss: 2.5006e-03 | validation loss: 2.5240e-03\n",
      "Epoch: 63830 | training loss: 2.4981e-03 | validation loss: 2.5117e-03\n",
      "Epoch: 63840 | training loss: 2.4987e-03 | validation loss: 2.5072e-03\n",
      "Epoch: 63850 | training loss: 2.5034e-03 | validation loss: 2.5019e-03\n",
      "Epoch: 63860 | training loss: 2.6260e-03 | validation loss: 2.5101e-03\n",
      "Epoch: 63870 | training loss: 3.0895e-03 | validation loss: 2.6624e-03\n",
      "Epoch: 63880 | training loss: 2.5978e-03 | validation loss: 2.6023e-03\n",
      "Epoch: 63890 | training loss: 2.4979e-03 | validation loss: 2.5085e-03\n",
      "Epoch: 63900 | training loss: 2.5073e-03 | validation loss: 2.5006e-03\n",
      "Epoch: 63910 | training loss: 2.5059e-03 | validation loss: 2.5299e-03\n",
      "Epoch: 63920 | training loss: 2.5012e-03 | validation loss: 2.5033e-03\n",
      "Epoch: 63930 | training loss: 2.4986e-03 | validation loss: 2.5163e-03\n",
      "Epoch: 63940 | training loss: 2.4975e-03 | validation loss: 2.5090e-03\n",
      "Epoch: 63950 | training loss: 2.4973e-03 | validation loss: 2.5095e-03\n",
      "Epoch: 63960 | training loss: 2.4974e-03 | validation loss: 2.5121e-03\n",
      "Epoch: 63970 | training loss: 2.4972e-03 | validation loss: 2.5113e-03\n",
      "Epoch: 63980 | training loss: 2.4971e-03 | validation loss: 2.5106e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63990 | training loss: 2.4971e-03 | validation loss: 2.5113e-03\n",
      "Epoch: 64000 | training loss: 2.4987e-03 | validation loss: 2.5171e-03\n",
      "Epoch: 64010 | training loss: 2.6023e-03 | validation loss: 2.6086e-03\n",
      "Epoch: 64020 | training loss: 3.4578e-03 | validation loss: 3.1055e-03\n",
      "Epoch: 64030 | training loss: 2.5298e-03 | validation loss: 2.5579e-03\n",
      "Epoch: 64040 | training loss: 2.6006e-03 | validation loss: 2.5122e-03\n",
      "Epoch: 64050 | training loss: 2.5204e-03 | validation loss: 2.4914e-03\n",
      "Epoch: 64060 | training loss: 2.5037e-03 | validation loss: 2.5224e-03\n",
      "Epoch: 64070 | training loss: 2.5002e-03 | validation loss: 2.5224e-03\n",
      "Epoch: 64080 | training loss: 2.4979e-03 | validation loss: 2.5035e-03\n",
      "Epoch: 64090 | training loss: 2.4964e-03 | validation loss: 2.5068e-03\n",
      "Epoch: 64100 | training loss: 2.4965e-03 | validation loss: 2.5118e-03\n",
      "Epoch: 64110 | training loss: 2.4963e-03 | validation loss: 2.5061e-03\n",
      "Epoch: 64120 | training loss: 2.4961e-03 | validation loss: 2.5094e-03\n",
      "Epoch: 64130 | training loss: 2.4960e-03 | validation loss: 2.5071e-03\n",
      "Epoch: 64140 | training loss: 2.4960e-03 | validation loss: 2.5084e-03\n",
      "Epoch: 64150 | training loss: 2.4959e-03 | validation loss: 2.5074e-03\n",
      "Epoch: 64160 | training loss: 2.4958e-03 | validation loss: 2.5074e-03\n",
      "Epoch: 64170 | training loss: 2.4957e-03 | validation loss: 2.5075e-03\n",
      "Epoch: 64180 | training loss: 2.4957e-03 | validation loss: 2.5072e-03\n",
      "Epoch: 64190 | training loss: 2.4956e-03 | validation loss: 2.5070e-03\n",
      "Epoch: 64200 | training loss: 2.4957e-03 | validation loss: 2.5060e-03\n",
      "Epoch: 64210 | training loss: 2.5112e-03 | validation loss: 2.5058e-03\n",
      "Epoch: 64220 | training loss: 3.0701e-03 | validation loss: 2.8391e-03\n",
      "Epoch: 64230 | training loss: 2.7159e-03 | validation loss: 2.6849e-03\n",
      "Epoch: 64240 | training loss: 2.5973e-03 | validation loss: 2.6208e-03\n",
      "Epoch: 64250 | training loss: 2.5362e-03 | validation loss: 2.5006e-03\n",
      "Epoch: 64260 | training loss: 2.5125e-03 | validation loss: 2.5427e-03\n",
      "Epoch: 64270 | training loss: 2.5058e-03 | validation loss: 2.4999e-03\n",
      "Epoch: 64280 | training loss: 2.5020e-03 | validation loss: 2.5267e-03\n",
      "Epoch: 64290 | training loss: 2.4968e-03 | validation loss: 2.5021e-03\n",
      "Epoch: 64300 | training loss: 2.4952e-03 | validation loss: 2.5030e-03\n",
      "Epoch: 64310 | training loss: 2.4950e-03 | validation loss: 2.5075e-03\n",
      "Epoch: 64320 | training loss: 2.4956e-03 | validation loss: 2.5099e-03\n",
      "Epoch: 64330 | training loss: 2.5012e-03 | validation loss: 2.5218e-03\n",
      "Epoch: 64340 | training loss: 2.6304e-03 | validation loss: 2.6321e-03\n",
      "Epoch: 64350 | training loss: 2.9864e-03 | validation loss: 2.8471e-03\n",
      "Epoch: 64360 | training loss: 2.6002e-03 | validation loss: 2.5033e-03\n",
      "Epoch: 64370 | training loss: 2.5006e-03 | validation loss: 2.5209e-03\n",
      "Epoch: 64380 | training loss: 2.4946e-03 | validation loss: 2.5067e-03\n",
      "Epoch: 64390 | training loss: 2.4951e-03 | validation loss: 2.4986e-03\n",
      "Epoch: 64400 | training loss: 2.4943e-03 | validation loss: 2.5052e-03\n",
      "Epoch: 64410 | training loss: 2.4942e-03 | validation loss: 2.5053e-03\n",
      "Epoch: 64420 | training loss: 2.4945e-03 | validation loss: 2.4999e-03\n",
      "Epoch: 64430 | training loss: 2.4941e-03 | validation loss: 2.5051e-03\n",
      "Epoch: 64440 | training loss: 2.4940e-03 | validation loss: 2.5051e-03\n",
      "Epoch: 64450 | training loss: 2.4939e-03 | validation loss: 2.5042e-03\n",
      "Epoch: 64460 | training loss: 2.4940e-03 | validation loss: 2.5054e-03\n",
      "Epoch: 64470 | training loss: 2.4991e-03 | validation loss: 2.5169e-03\n",
      "Epoch: 64480 | training loss: 2.7443e-03 | validation loss: 2.6983e-03\n",
      "Epoch: 64490 | training loss: 2.5932e-03 | validation loss: 2.5899e-03\n",
      "Epoch: 64500 | training loss: 2.6228e-03 | validation loss: 2.6205e-03\n",
      "Epoch: 64510 | training loss: 2.5587e-03 | validation loss: 2.5025e-03\n",
      "Epoch: 64520 | training loss: 2.4976e-03 | validation loss: 2.4906e-03\n",
      "Epoch: 64530 | training loss: 2.5055e-03 | validation loss: 2.5212e-03\n",
      "Epoch: 64540 | training loss: 2.4951e-03 | validation loss: 2.4988e-03\n",
      "Epoch: 64550 | training loss: 2.4931e-03 | validation loss: 2.5008e-03\n",
      "Epoch: 64560 | training loss: 2.4932e-03 | validation loss: 2.5027e-03\n",
      "Epoch: 64570 | training loss: 2.4931e-03 | validation loss: 2.5003e-03\n",
      "Epoch: 64580 | training loss: 2.4929e-03 | validation loss: 2.5014e-03\n",
      "Epoch: 64590 | training loss: 2.4928e-03 | validation loss: 2.5010e-03\n",
      "Epoch: 64600 | training loss: 2.4928e-03 | validation loss: 2.5002e-03\n",
      "Epoch: 64610 | training loss: 2.4927e-03 | validation loss: 2.5008e-03\n",
      "Epoch: 64620 | training loss: 2.4926e-03 | validation loss: 2.5007e-03\n",
      "Epoch: 64630 | training loss: 2.4925e-03 | validation loss: 2.5005e-03\n",
      "Epoch: 64640 | training loss: 2.4925e-03 | validation loss: 2.5008e-03\n",
      "Epoch: 64650 | training loss: 2.4931e-03 | validation loss: 2.5038e-03\n",
      "Epoch: 64660 | training loss: 2.5356e-03 | validation loss: 2.5479e-03\n",
      "Epoch: 64670 | training loss: 4.1434e-03 | validation loss: 3.4593e-03\n",
      "Epoch: 64680 | training loss: 2.6152e-03 | validation loss: 2.4916e-03\n",
      "Epoch: 64690 | training loss: 2.6615e-03 | validation loss: 2.5581e-03\n",
      "Epoch: 64700 | training loss: 2.5157e-03 | validation loss: 2.5205e-03\n",
      "Epoch: 64710 | training loss: 2.4956e-03 | validation loss: 2.5008e-03\n",
      "Epoch: 64720 | training loss: 2.5000e-03 | validation loss: 2.5045e-03\n",
      "Epoch: 64730 | training loss: 2.4921e-03 | validation loss: 2.5011e-03\n",
      "Epoch: 64740 | training loss: 2.4928e-03 | validation loss: 2.4968e-03\n",
      "Epoch: 64750 | training loss: 2.4917e-03 | validation loss: 2.4992e-03\n",
      "Epoch: 64760 | training loss: 2.4918e-03 | validation loss: 2.4982e-03\n",
      "Epoch: 64770 | training loss: 2.4916e-03 | validation loss: 2.4981e-03\n",
      "Epoch: 64780 | training loss: 2.4915e-03 | validation loss: 2.4980e-03\n",
      "Epoch: 64790 | training loss: 2.4914e-03 | validation loss: 2.4976e-03\n",
      "Epoch: 64800 | training loss: 2.4914e-03 | validation loss: 2.4975e-03\n",
      "Epoch: 64810 | training loss: 2.4913e-03 | validation loss: 2.4973e-03\n",
      "Epoch: 64820 | training loss: 2.4912e-03 | validation loss: 2.4966e-03\n",
      "Epoch: 64830 | training loss: 2.4915e-03 | validation loss: 2.4933e-03\n",
      "Epoch: 64840 | training loss: 2.5172e-03 | validation loss: 2.4769e-03\n",
      "Epoch: 64850 | training loss: 3.4329e-03 | validation loss: 2.8028e-03\n",
      "Epoch: 64860 | training loss: 2.6160e-03 | validation loss: 2.5913e-03\n",
      "Epoch: 64870 | training loss: 2.5352e-03 | validation loss: 2.5139e-03\n",
      "Epoch: 64880 | training loss: 2.5168e-03 | validation loss: 2.5460e-03\n",
      "Epoch: 64890 | training loss: 2.5025e-03 | validation loss: 2.5114e-03\n",
      "Epoch: 64900 | training loss: 2.4936e-03 | validation loss: 2.5088e-03\n",
      "Epoch: 64910 | training loss: 2.4912e-03 | validation loss: 2.4916e-03\n",
      "Epoch: 64920 | training loss: 2.4911e-03 | validation loss: 2.4951e-03\n",
      "Epoch: 64930 | training loss: 2.4906e-03 | validation loss: 2.4944e-03\n",
      "Epoch: 64940 | training loss: 2.4905e-03 | validation loss: 2.4947e-03\n",
      "Epoch: 64950 | training loss: 2.4904e-03 | validation loss: 2.4939e-03\n",
      "Epoch: 64960 | training loss: 2.4909e-03 | validation loss: 2.4907e-03\n",
      "Epoch: 64970 | training loss: 2.5032e-03 | validation loss: 2.4825e-03\n",
      "Epoch: 64980 | training loss: 3.0092e-03 | validation loss: 2.6203e-03\n",
      "Epoch: 64990 | training loss: 2.5808e-03 | validation loss: 2.5862e-03\n",
      "Epoch: 65000 | training loss: 2.6837e-03 | validation loss: 2.5210e-03\n",
      "Epoch: 65010 | training loss: 2.5042e-03 | validation loss: 2.5176e-03\n",
      "Epoch: 65020 | training loss: 2.4990e-03 | validation loss: 2.5108e-03\n",
      "Epoch: 65030 | training loss: 2.4993e-03 | validation loss: 2.4830e-03\n",
      "Epoch: 65040 | training loss: 2.4919e-03 | validation loss: 2.5021e-03\n",
      "Epoch: 65050 | training loss: 2.4899e-03 | validation loss: 2.4914e-03\n",
      "Epoch: 65060 | training loss: 2.4896e-03 | validation loss: 2.4946e-03\n",
      "Epoch: 65070 | training loss: 2.4896e-03 | validation loss: 2.4922e-03\n",
      "Epoch: 65080 | training loss: 2.4895e-03 | validation loss: 2.4945e-03\n",
      "Epoch: 65090 | training loss: 2.4894e-03 | validation loss: 2.4922e-03\n",
      "Epoch: 65100 | training loss: 2.4893e-03 | validation loss: 2.4927e-03\n",
      "Epoch: 65110 | training loss: 2.4892e-03 | validation loss: 2.4931e-03\n",
      "Epoch: 65120 | training loss: 2.4892e-03 | validation loss: 2.4932e-03\n",
      "Epoch: 65130 | training loss: 2.4892e-03 | validation loss: 2.4944e-03\n",
      "Epoch: 65140 | training loss: 2.4933e-03 | validation loss: 2.5040e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65150 | training loss: 2.7468e-03 | validation loss: 2.6877e-03\n",
      "Epoch: 65160 | training loss: 2.5453e-03 | validation loss: 2.5524e-03\n",
      "Epoch: 65170 | training loss: 2.7461e-03 | validation loss: 2.6869e-03\n",
      "Epoch: 65180 | training loss: 2.4955e-03 | validation loss: 2.4838e-03\n",
      "Epoch: 65190 | training loss: 2.5266e-03 | validation loss: 2.4786e-03\n",
      "Epoch: 65200 | training loss: 2.4890e-03 | validation loss: 2.4937e-03\n",
      "Epoch: 65210 | training loss: 2.4930e-03 | validation loss: 2.5042e-03\n",
      "Epoch: 65220 | training loss: 2.4895e-03 | validation loss: 2.4866e-03\n",
      "Epoch: 65230 | training loss: 2.4884e-03 | validation loss: 2.4907e-03\n",
      "Epoch: 65240 | training loss: 2.4884e-03 | validation loss: 2.4927e-03\n",
      "Epoch: 65250 | training loss: 2.4883e-03 | validation loss: 2.4894e-03\n",
      "Epoch: 65260 | training loss: 2.4882e-03 | validation loss: 2.4915e-03\n",
      "Epoch: 65270 | training loss: 2.4881e-03 | validation loss: 2.4900e-03\n",
      "Epoch: 65280 | training loss: 2.4880e-03 | validation loss: 2.4905e-03\n",
      "Epoch: 65290 | training loss: 2.4880e-03 | validation loss: 2.4903e-03\n",
      "Epoch: 65300 | training loss: 2.4879e-03 | validation loss: 2.4899e-03\n",
      "Epoch: 65310 | training loss: 2.4878e-03 | validation loss: 2.4897e-03\n",
      "Epoch: 65320 | training loss: 2.4878e-03 | validation loss: 2.4893e-03\n",
      "Epoch: 65330 | training loss: 2.4879e-03 | validation loss: 2.4870e-03\n",
      "Epoch: 65340 | training loss: 2.5150e-03 | validation loss: 2.4785e-03\n",
      "Epoch: 65350 | training loss: 2.9567e-03 | validation loss: 2.6559e-03\n",
      "Epoch: 65360 | training loss: 2.7876e-03 | validation loss: 2.5718e-03\n",
      "Epoch: 65370 | training loss: 2.5901e-03 | validation loss: 2.5468e-03\n",
      "Epoch: 65380 | training loss: 2.5154e-03 | validation loss: 2.4906e-03\n",
      "Epoch: 65390 | training loss: 2.5040e-03 | validation loss: 2.4706e-03\n",
      "Epoch: 65400 | training loss: 2.4946e-03 | validation loss: 2.4737e-03\n",
      "Epoch: 65410 | training loss: 2.4931e-03 | validation loss: 2.4744e-03\n",
      "Epoch: 65420 | training loss: 2.5267e-03 | validation loss: 2.4689e-03\n",
      "Epoch: 65430 | training loss: 2.9989e-03 | validation loss: 2.5964e-03\n",
      "Epoch: 65440 | training loss: 2.5458e-03 | validation loss: 2.5538e-03\n",
      "Epoch: 65450 | training loss: 2.5134e-03 | validation loss: 2.4714e-03\n",
      "Epoch: 65460 | training loss: 2.4972e-03 | validation loss: 2.5081e-03\n",
      "Epoch: 65470 | training loss: 2.4884e-03 | validation loss: 2.4810e-03\n",
      "Epoch: 65480 | training loss: 2.4869e-03 | validation loss: 2.4849e-03\n",
      "Epoch: 65490 | training loss: 2.4885e-03 | validation loss: 2.4949e-03\n",
      "Epoch: 65500 | training loss: 2.4869e-03 | validation loss: 2.4836e-03\n",
      "Epoch: 65510 | training loss: 2.4872e-03 | validation loss: 2.4824e-03\n",
      "Epoch: 65520 | training loss: 2.4872e-03 | validation loss: 2.4820e-03\n",
      "Epoch: 65530 | training loss: 2.4916e-03 | validation loss: 2.4767e-03\n",
      "Epoch: 65540 | training loss: 2.6022e-03 | validation loss: 2.4833e-03\n",
      "Epoch: 65550 | training loss: 3.1468e-03 | validation loss: 2.6636e-03\n",
      "Epoch: 65560 | training loss: 2.6222e-03 | validation loss: 2.6031e-03\n",
      "Epoch: 65570 | training loss: 2.4888e-03 | validation loss: 2.4772e-03\n",
      "Epoch: 65580 | training loss: 2.4913e-03 | validation loss: 2.4760e-03\n",
      "Epoch: 65590 | training loss: 2.4923e-03 | validation loss: 2.5009e-03\n",
      "Epoch: 65600 | training loss: 2.4889e-03 | validation loss: 2.4787e-03\n",
      "Epoch: 65610 | training loss: 2.4867e-03 | validation loss: 2.4900e-03\n",
      "Epoch: 65620 | training loss: 2.4858e-03 | validation loss: 2.4840e-03\n",
      "Epoch: 65630 | training loss: 2.4858e-03 | validation loss: 2.4834e-03\n",
      "Epoch: 65640 | training loss: 2.4857e-03 | validation loss: 2.4865e-03\n",
      "Epoch: 65650 | training loss: 2.4856e-03 | validation loss: 2.4860e-03\n",
      "Epoch: 65660 | training loss: 2.4855e-03 | validation loss: 2.4857e-03\n",
      "Epoch: 65670 | training loss: 2.4858e-03 | validation loss: 2.4876e-03\n",
      "Epoch: 65680 | training loss: 2.4959e-03 | validation loss: 2.5048e-03\n",
      "Epoch: 65690 | training loss: 3.0099e-03 | validation loss: 2.8380e-03\n",
      "Epoch: 65700 | training loss: 2.6122e-03 | validation loss: 2.4862e-03\n",
      "Epoch: 65710 | training loss: 2.7013e-03 | validation loss: 2.6506e-03\n",
      "Epoch: 65720 | training loss: 2.4858e-03 | validation loss: 2.4890e-03\n",
      "Epoch: 65730 | training loss: 2.5134e-03 | validation loss: 2.4729e-03\n",
      "Epoch: 65740 | training loss: 2.4872e-03 | validation loss: 2.4899e-03\n",
      "Epoch: 65750 | training loss: 2.4858e-03 | validation loss: 2.4883e-03\n",
      "Epoch: 65760 | training loss: 2.4861e-03 | validation loss: 2.4790e-03\n",
      "Epoch: 65770 | training loss: 2.4853e-03 | validation loss: 2.4862e-03\n",
      "Epoch: 65780 | training loss: 2.4848e-03 | validation loss: 2.4813e-03\n",
      "Epoch: 65790 | training loss: 2.4847e-03 | validation loss: 2.4839e-03\n",
      "Epoch: 65800 | training loss: 2.4846e-03 | validation loss: 2.4819e-03\n",
      "Epoch: 65810 | training loss: 2.4845e-03 | validation loss: 2.4827e-03\n",
      "Epoch: 65820 | training loss: 2.4844e-03 | validation loss: 2.4825e-03\n",
      "Epoch: 65830 | training loss: 2.4843e-03 | validation loss: 2.4820e-03\n",
      "Epoch: 65840 | training loss: 2.4842e-03 | validation loss: 2.4817e-03\n",
      "Epoch: 65850 | training loss: 2.4842e-03 | validation loss: 2.4812e-03\n",
      "Epoch: 65860 | training loss: 2.4846e-03 | validation loss: 2.4788e-03\n",
      "Epoch: 65870 | training loss: 2.5118e-03 | validation loss: 2.4718e-03\n",
      "Epoch: 65880 | training loss: 3.9458e-03 | validation loss: 3.0008e-03\n",
      "Epoch: 65890 | training loss: 2.9203e-03 | validation loss: 2.7806e-03\n",
      "Epoch: 65900 | training loss: 2.5895e-03 | validation loss: 2.5613e-03\n",
      "Epoch: 65910 | training loss: 2.4912e-03 | validation loss: 2.4718e-03\n",
      "Epoch: 65920 | training loss: 2.5090e-03 | validation loss: 2.4823e-03\n",
      "Epoch: 65930 | training loss: 2.4841e-03 | validation loss: 2.4776e-03\n",
      "Epoch: 65940 | training loss: 2.4868e-03 | validation loss: 2.4878e-03\n",
      "Epoch: 65950 | training loss: 2.4835e-03 | validation loss: 2.4800e-03\n",
      "Epoch: 65960 | training loss: 2.4838e-03 | validation loss: 2.4791e-03\n",
      "Epoch: 65970 | training loss: 2.4835e-03 | validation loss: 2.4809e-03\n",
      "Epoch: 65980 | training loss: 2.4833e-03 | validation loss: 2.4797e-03\n",
      "Epoch: 65990 | training loss: 2.4832e-03 | validation loss: 2.4799e-03\n",
      "Epoch: 66000 | training loss: 2.4832e-03 | validation loss: 2.4797e-03\n",
      "Epoch: 66010 | training loss: 2.4831e-03 | validation loss: 2.4796e-03\n",
      "Epoch: 66020 | training loss: 2.4830e-03 | validation loss: 2.4796e-03\n",
      "Epoch: 66030 | training loss: 2.4831e-03 | validation loss: 2.4818e-03\n",
      "Epoch: 66040 | training loss: 2.4952e-03 | validation loss: 2.5104e-03\n",
      "Epoch: 66050 | training loss: 3.2415e-03 | validation loss: 3.1498e-03\n",
      "Epoch: 66060 | training loss: 2.5194e-03 | validation loss: 2.4573e-03\n",
      "Epoch: 66070 | training loss: 2.5373e-03 | validation loss: 2.5021e-03\n",
      "Epoch: 66080 | training loss: 2.5094e-03 | validation loss: 2.4683e-03\n",
      "Epoch: 66090 | training loss: 2.4886e-03 | validation loss: 2.4704e-03\n",
      "Epoch: 66100 | training loss: 2.4837e-03 | validation loss: 2.4829e-03\n",
      "Epoch: 66110 | training loss: 2.4837e-03 | validation loss: 2.4771e-03\n",
      "Epoch: 66120 | training loss: 2.4830e-03 | validation loss: 2.4773e-03\n",
      "Epoch: 66130 | training loss: 2.4823e-03 | validation loss: 2.4762e-03\n",
      "Epoch: 66140 | training loss: 2.4823e-03 | validation loss: 2.4756e-03\n",
      "Epoch: 66150 | training loss: 2.4823e-03 | validation loss: 2.4751e-03\n",
      "Epoch: 66160 | training loss: 2.4896e-03 | validation loss: 2.4656e-03\n",
      "Epoch: 66170 | training loss: 3.1099e-03 | validation loss: 2.6279e-03\n",
      "Epoch: 66180 | training loss: 2.8895e-03 | validation loss: 2.7753e-03\n",
      "Epoch: 66190 | training loss: 2.5667e-03 | validation loss: 2.4961e-03\n",
      "Epoch: 66200 | training loss: 2.5722e-03 | validation loss: 2.4852e-03\n",
      "Epoch: 66210 | training loss: 2.4834e-03 | validation loss: 2.4748e-03\n",
      "Epoch: 66220 | training loss: 2.4915e-03 | validation loss: 2.4952e-03\n",
      "Epoch: 66230 | training loss: 2.4825e-03 | validation loss: 2.4784e-03\n",
      "Epoch: 66240 | training loss: 2.4831e-03 | validation loss: 2.4692e-03\n",
      "Epoch: 66250 | training loss: 2.4815e-03 | validation loss: 2.4762e-03\n",
      "Epoch: 66260 | training loss: 2.4815e-03 | validation loss: 2.4776e-03\n",
      "Epoch: 66270 | training loss: 2.4814e-03 | validation loss: 2.4742e-03\n",
      "Epoch: 66280 | training loss: 2.4813e-03 | validation loss: 2.4759e-03\n",
      "Epoch: 66290 | training loss: 2.4812e-03 | validation loss: 2.4747e-03\n",
      "Epoch: 66300 | training loss: 2.4812e-03 | validation loss: 2.4754e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66310 | training loss: 2.4811e-03 | validation loss: 2.4746e-03\n",
      "Epoch: 66320 | training loss: 2.4810e-03 | validation loss: 2.4748e-03\n",
      "Epoch: 66330 | training loss: 2.4809e-03 | validation loss: 2.4746e-03\n",
      "Epoch: 66340 | training loss: 2.4809e-03 | validation loss: 2.4745e-03\n",
      "Epoch: 66350 | training loss: 2.4808e-03 | validation loss: 2.4743e-03\n",
      "Epoch: 66360 | training loss: 2.4807e-03 | validation loss: 2.4741e-03\n",
      "Epoch: 66370 | training loss: 2.4807e-03 | validation loss: 2.4737e-03\n",
      "Epoch: 66380 | training loss: 2.4808e-03 | validation loss: 2.4718e-03\n",
      "Epoch: 66390 | training loss: 2.5097e-03 | validation loss: 2.4616e-03\n",
      "Epoch: 66400 | training loss: 4.9603e-03 | validation loss: 3.3990e-03\n",
      "Epoch: 66410 | training loss: 2.5403e-03 | validation loss: 2.4539e-03\n",
      "Epoch: 66420 | training loss: 2.4833e-03 | validation loss: 2.4630e-03\n",
      "Epoch: 66430 | training loss: 2.4808e-03 | validation loss: 2.4722e-03\n",
      "Epoch: 66440 | training loss: 2.4816e-03 | validation loss: 2.4806e-03\n",
      "Epoch: 66450 | training loss: 2.4825e-03 | validation loss: 2.4833e-03\n",
      "Epoch: 66460 | training loss: 2.4826e-03 | validation loss: 2.4819e-03\n",
      "Epoch: 66470 | training loss: 2.4815e-03 | validation loss: 2.4797e-03\n",
      "Epoch: 66480 | training loss: 2.4803e-03 | validation loss: 2.4763e-03\n",
      "Epoch: 66490 | training loss: 2.4799e-03 | validation loss: 2.4724e-03\n",
      "Epoch: 66500 | training loss: 2.4799e-03 | validation loss: 2.4710e-03\n",
      "Epoch: 66510 | training loss: 2.4798e-03 | validation loss: 2.4715e-03\n",
      "Epoch: 66520 | training loss: 2.4797e-03 | validation loss: 2.4723e-03\n",
      "Epoch: 66530 | training loss: 2.4796e-03 | validation loss: 2.4720e-03\n",
      "Epoch: 66540 | training loss: 2.4795e-03 | validation loss: 2.4715e-03\n",
      "Epoch: 66550 | training loss: 2.4795e-03 | validation loss: 2.4716e-03\n",
      "Epoch: 66560 | training loss: 2.4794e-03 | validation loss: 2.4714e-03\n",
      "Epoch: 66570 | training loss: 2.4793e-03 | validation loss: 2.4712e-03\n",
      "Epoch: 66580 | training loss: 2.4793e-03 | validation loss: 2.4711e-03\n",
      "Epoch: 66590 | training loss: 2.4792e-03 | validation loss: 2.4709e-03\n",
      "Epoch: 66600 | training loss: 2.4791e-03 | validation loss: 2.4706e-03\n",
      "Epoch: 66610 | training loss: 2.4793e-03 | validation loss: 2.4686e-03\n",
      "Epoch: 66620 | training loss: 2.5284e-03 | validation loss: 2.4742e-03\n",
      "Epoch: 66630 | training loss: 2.5123e-03 | validation loss: 2.4574e-03\n",
      "Epoch: 66640 | training loss: 2.5837e-03 | validation loss: 2.5145e-03\n",
      "Epoch: 66650 | training loss: 2.5261e-03 | validation loss: 2.4757e-03\n",
      "Epoch: 66660 | training loss: 2.4981e-03 | validation loss: 2.4632e-03\n",
      "Epoch: 66670 | training loss: 2.4859e-03 | validation loss: 2.4660e-03\n",
      "Epoch: 66680 | training loss: 2.4808e-03 | validation loss: 2.4681e-03\n",
      "Epoch: 66690 | training loss: 2.4794e-03 | validation loss: 2.4724e-03\n",
      "Epoch: 66700 | training loss: 2.4870e-03 | validation loss: 2.4900e-03\n",
      "Epoch: 66710 | training loss: 2.7396e-03 | validation loss: 2.6819e-03\n",
      "Epoch: 66720 | training loss: 2.5410e-03 | validation loss: 2.5353e-03\n",
      "Epoch: 66730 | training loss: 2.4979e-03 | validation loss: 2.4993e-03\n",
      "Epoch: 66740 | training loss: 2.5343e-03 | validation loss: 2.4540e-03\n",
      "Epoch: 66750 | training loss: 2.5010e-03 | validation loss: 2.5034e-03\n",
      "Epoch: 66760 | training loss: 2.4826e-03 | validation loss: 2.4593e-03\n",
      "Epoch: 66770 | training loss: 2.4789e-03 | validation loss: 2.4735e-03\n",
      "Epoch: 66780 | training loss: 2.4784e-03 | validation loss: 2.4646e-03\n",
      "Epoch: 66790 | training loss: 2.4783e-03 | validation loss: 2.4709e-03\n",
      "Epoch: 66800 | training loss: 2.4780e-03 | validation loss: 2.4651e-03\n",
      "Epoch: 66810 | training loss: 2.4778e-03 | validation loss: 2.4676e-03\n",
      "Epoch: 66820 | training loss: 2.4777e-03 | validation loss: 2.4685e-03\n",
      "Epoch: 66830 | training loss: 2.4777e-03 | validation loss: 2.4682e-03\n",
      "Epoch: 66840 | training loss: 2.4777e-03 | validation loss: 2.4692e-03\n",
      "Epoch: 66850 | training loss: 2.4807e-03 | validation loss: 2.4775e-03\n",
      "Epoch: 66860 | training loss: 2.6182e-03 | validation loss: 2.5928e-03\n",
      "Epoch: 66870 | training loss: 3.0696e-03 | validation loss: 2.8568e-03\n",
      "Epoch: 66880 | training loss: 2.4880e-03 | validation loss: 2.4945e-03\n",
      "Epoch: 66890 | training loss: 2.5837e-03 | validation loss: 2.4745e-03\n",
      "Epoch: 66900 | training loss: 2.4809e-03 | validation loss: 2.4717e-03\n",
      "Epoch: 66910 | training loss: 2.4860e-03 | validation loss: 2.4807e-03\n",
      "Epoch: 66920 | training loss: 2.4817e-03 | validation loss: 2.4593e-03\n",
      "Epoch: 66930 | training loss: 2.4775e-03 | validation loss: 2.4701e-03\n",
      "Epoch: 66940 | training loss: 2.4769e-03 | validation loss: 2.4648e-03\n",
      "Epoch: 66950 | training loss: 2.4768e-03 | validation loss: 2.4652e-03\n",
      "Epoch: 66960 | training loss: 2.4768e-03 | validation loss: 2.4656e-03\n",
      "Epoch: 66970 | training loss: 2.4767e-03 | validation loss: 2.4654e-03\n",
      "Epoch: 66980 | training loss: 2.4766e-03 | validation loss: 2.4646e-03\n",
      "Epoch: 66990 | training loss: 2.4766e-03 | validation loss: 2.4651e-03\n",
      "Epoch: 67000 | training loss: 2.4765e-03 | validation loss: 2.4651e-03\n",
      "Epoch: 67010 | training loss: 2.4764e-03 | validation loss: 2.4650e-03\n",
      "Epoch: 67020 | training loss: 2.4764e-03 | validation loss: 2.4652e-03\n",
      "Epoch: 67030 | training loss: 2.4767e-03 | validation loss: 2.4676e-03\n",
      "Epoch: 67040 | training loss: 2.5010e-03 | validation loss: 2.4986e-03\n",
      "Epoch: 67050 | training loss: 3.9238e-03 | validation loss: 3.3163e-03\n",
      "Epoch: 67060 | training loss: 2.9642e-03 | validation loss: 2.5916e-03\n",
      "Epoch: 67070 | training loss: 2.5786e-03 | validation loss: 2.4639e-03\n",
      "Epoch: 67080 | training loss: 2.4844e-03 | validation loss: 2.4795e-03\n",
      "Epoch: 67090 | training loss: 2.5025e-03 | validation loss: 2.4994e-03\n",
      "Epoch: 67100 | training loss: 2.4763e-03 | validation loss: 2.4667e-03\n",
      "Epoch: 67110 | training loss: 2.4791e-03 | validation loss: 2.4565e-03\n",
      "Epoch: 67120 | training loss: 2.4757e-03 | validation loss: 2.4629e-03\n",
      "Epoch: 67130 | training loss: 2.4760e-03 | validation loss: 2.4662e-03\n",
      "Epoch: 67140 | training loss: 2.4757e-03 | validation loss: 2.4613e-03\n",
      "Epoch: 67150 | training loss: 2.4755e-03 | validation loss: 2.4631e-03\n",
      "Epoch: 67160 | training loss: 2.4754e-03 | validation loss: 2.4626e-03\n",
      "Epoch: 67170 | training loss: 2.4753e-03 | validation loss: 2.4623e-03\n",
      "Epoch: 67180 | training loss: 2.4753e-03 | validation loss: 2.4622e-03\n",
      "Epoch: 67190 | training loss: 2.4752e-03 | validation loss: 2.4622e-03\n",
      "Epoch: 67200 | training loss: 2.4751e-03 | validation loss: 2.4618e-03\n",
      "Epoch: 67210 | training loss: 2.4751e-03 | validation loss: 2.4615e-03\n",
      "Epoch: 67220 | training loss: 2.4755e-03 | validation loss: 2.4582e-03\n",
      "Epoch: 67230 | training loss: 2.6004e-03 | validation loss: 2.4920e-03\n",
      "Epoch: 67240 | training loss: 2.7371e-03 | validation loss: 2.6169e-03\n",
      "Epoch: 67250 | training loss: 2.4999e-03 | validation loss: 2.5104e-03\n",
      "Epoch: 67260 | training loss: 2.4790e-03 | validation loss: 2.4733e-03\n",
      "Epoch: 67270 | training loss: 2.4810e-03 | validation loss: 2.4563e-03\n",
      "Epoch: 67280 | training loss: 2.4751e-03 | validation loss: 2.4632e-03\n",
      "Epoch: 67290 | training loss: 2.4757e-03 | validation loss: 2.4671e-03\n",
      "Epoch: 67300 | training loss: 2.4801e-03 | validation loss: 2.4747e-03\n",
      "Epoch: 67310 | training loss: 2.5488e-03 | validation loss: 2.5406e-03\n",
      "Epoch: 67320 | training loss: 3.0991e-03 | validation loss: 2.8810e-03\n",
      "Epoch: 67330 | training loss: 2.6753e-03 | validation loss: 2.4758e-03\n",
      "Epoch: 67340 | training loss: 2.5506e-03 | validation loss: 2.5407e-03\n",
      "Epoch: 67350 | training loss: 2.5026e-03 | validation loss: 2.4439e-03\n",
      "Epoch: 67360 | training loss: 2.4852e-03 | validation loss: 2.4818e-03\n",
      "Epoch: 67370 | training loss: 2.4774e-03 | validation loss: 2.4509e-03\n",
      "Epoch: 67380 | training loss: 2.4741e-03 | validation loss: 2.4605e-03\n",
      "Epoch: 67390 | training loss: 2.4746e-03 | validation loss: 2.4634e-03\n",
      "Epoch: 67400 | training loss: 2.4739e-03 | validation loss: 2.4581e-03\n",
      "Epoch: 67410 | training loss: 2.4741e-03 | validation loss: 2.4558e-03\n",
      "Epoch: 67420 | training loss: 2.4757e-03 | validation loss: 2.4519e-03\n",
      "Epoch: 67430 | training loss: 2.5128e-03 | validation loss: 2.4433e-03\n",
      "Epoch: 67440 | training loss: 3.3326e-03 | validation loss: 2.7019e-03\n",
      "Epoch: 67450 | training loss: 2.7918e-03 | validation loss: 2.6912e-03\n",
      "Epoch: 67460 | training loss: 2.5728e-03 | validation loss: 2.4591e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67470 | training loss: 2.4777e-03 | validation loss: 2.4710e-03\n",
      "Epoch: 67480 | training loss: 2.4750e-03 | validation loss: 2.4645e-03\n",
      "Epoch: 67490 | training loss: 2.4760e-03 | validation loss: 2.4501e-03\n",
      "Epoch: 67500 | training loss: 2.4745e-03 | validation loss: 2.4634e-03\n",
      "Epoch: 67510 | training loss: 2.4735e-03 | validation loss: 2.4550e-03\n",
      "Epoch: 67520 | training loss: 2.4731e-03 | validation loss: 2.4574e-03\n",
      "Epoch: 67530 | training loss: 2.4731e-03 | validation loss: 2.4585e-03\n",
      "Epoch: 67540 | training loss: 2.4730e-03 | validation loss: 2.4560e-03\n",
      "Epoch: 67550 | training loss: 2.4729e-03 | validation loss: 2.4556e-03\n",
      "Epoch: 67560 | training loss: 2.4729e-03 | validation loss: 2.4554e-03\n",
      "Epoch: 67570 | training loss: 2.4733e-03 | validation loss: 2.4531e-03\n",
      "Epoch: 67580 | training loss: 2.4889e-03 | validation loss: 2.4448e-03\n",
      "Epoch: 67590 | training loss: 3.2142e-03 | validation loss: 2.6728e-03\n",
      "Epoch: 67600 | training loss: 2.8140e-03 | validation loss: 2.7044e-03\n",
      "Epoch: 67610 | training loss: 2.6038e-03 | validation loss: 2.4679e-03\n",
      "Epoch: 67620 | training loss: 2.4870e-03 | validation loss: 2.4402e-03\n",
      "Epoch: 67630 | training loss: 2.4963e-03 | validation loss: 2.4882e-03\n",
      "Epoch: 67640 | training loss: 2.4729e-03 | validation loss: 2.4559e-03\n",
      "Epoch: 67650 | training loss: 2.4738e-03 | validation loss: 2.4490e-03\n",
      "Epoch: 67660 | training loss: 2.4733e-03 | validation loss: 2.4613e-03\n",
      "Epoch: 67670 | training loss: 2.4725e-03 | validation loss: 2.4525e-03\n",
      "Epoch: 67680 | training loss: 2.4722e-03 | validation loss: 2.4568e-03\n",
      "Epoch: 67690 | training loss: 2.4720e-03 | validation loss: 2.4537e-03\n",
      "Epoch: 67700 | training loss: 2.4719e-03 | validation loss: 2.4557e-03\n",
      "Epoch: 67710 | training loss: 2.4718e-03 | validation loss: 2.4542e-03\n",
      "Epoch: 67720 | training loss: 2.4717e-03 | validation loss: 2.4541e-03\n",
      "Epoch: 67730 | training loss: 2.4717e-03 | validation loss: 2.4545e-03\n",
      "Epoch: 67740 | training loss: 2.4716e-03 | validation loss: 2.4546e-03\n",
      "Epoch: 67750 | training loss: 2.4716e-03 | validation loss: 2.4556e-03\n",
      "Epoch: 67760 | training loss: 2.4744e-03 | validation loss: 2.4644e-03\n",
      "Epoch: 67770 | training loss: 2.6712e-03 | validation loss: 2.6634e-03\n",
      "Epoch: 67780 | training loss: 2.6141e-03 | validation loss: 2.5400e-03\n",
      "Epoch: 67790 | training loss: 3.0883e-03 | validation loss: 2.9027e-03\n",
      "Epoch: 67800 | training loss: 2.6390e-03 | validation loss: 2.5095e-03\n",
      "Epoch: 67810 | training loss: 2.5426e-03 | validation loss: 2.5452e-03\n",
      "Epoch: 67820 | training loss: 2.4974e-03 | validation loss: 2.4463e-03\n",
      "Epoch: 67830 | training loss: 2.4789e-03 | validation loss: 2.4712e-03\n",
      "Epoch: 67840 | training loss: 2.4726e-03 | validation loss: 2.4447e-03\n",
      "Epoch: 67850 | training loss: 2.4713e-03 | validation loss: 2.4483e-03\n",
      "Epoch: 67860 | training loss: 2.4717e-03 | validation loss: 2.4571e-03\n",
      "Epoch: 67870 | training loss: 2.4708e-03 | validation loss: 2.4535e-03\n",
      "Epoch: 67880 | training loss: 2.4708e-03 | validation loss: 2.4505e-03\n",
      "Epoch: 67890 | training loss: 2.4714e-03 | validation loss: 2.4472e-03\n",
      "Epoch: 67900 | training loss: 2.4843e-03 | validation loss: 2.4383e-03\n",
      "Epoch: 67910 | training loss: 2.9563e-03 | validation loss: 2.5593e-03\n",
      "Epoch: 67920 | training loss: 2.5247e-03 | validation loss: 2.5194e-03\n",
      "Epoch: 67930 | training loss: 2.6285e-03 | validation loss: 2.4687e-03\n",
      "Epoch: 67940 | training loss: 2.5109e-03 | validation loss: 2.4975e-03\n",
      "Epoch: 67950 | training loss: 2.4705e-03 | validation loss: 2.4481e-03\n",
      "Epoch: 67960 | training loss: 2.4738e-03 | validation loss: 2.4433e-03\n",
      "Epoch: 67970 | training loss: 2.4729e-03 | validation loss: 2.4608e-03\n",
      "Epoch: 67980 | training loss: 2.4712e-03 | validation loss: 2.4460e-03\n",
      "Epoch: 67990 | training loss: 2.4704e-03 | validation loss: 2.4534e-03\n",
      "Epoch: 68000 | training loss: 2.4700e-03 | validation loss: 2.4492e-03\n",
      "Epoch: 68010 | training loss: 2.4698e-03 | validation loss: 2.4499e-03\n",
      "Epoch: 68020 | training loss: 2.4698e-03 | validation loss: 2.4510e-03\n",
      "Epoch: 68030 | training loss: 2.4697e-03 | validation loss: 2.4503e-03\n",
      "Epoch: 68040 | training loss: 2.4696e-03 | validation loss: 2.4498e-03\n",
      "Epoch: 68050 | training loss: 2.4696e-03 | validation loss: 2.4494e-03\n",
      "Epoch: 68060 | training loss: 2.4696e-03 | validation loss: 2.4484e-03\n",
      "Epoch: 68070 | training loss: 2.4733e-03 | validation loss: 2.4420e-03\n",
      "Epoch: 68080 | training loss: 2.8996e-03 | validation loss: 2.5526e-03\n",
      "Epoch: 68090 | training loss: 2.6914e-03 | validation loss: 2.6288e-03\n",
      "Epoch: 68100 | training loss: 2.5710e-03 | validation loss: 2.4545e-03\n",
      "Epoch: 68110 | training loss: 2.5819e-03 | validation loss: 2.4477e-03\n",
      "Epoch: 68120 | training loss: 2.5057e-03 | validation loss: 2.4307e-03\n",
      "Epoch: 68130 | training loss: 2.4705e-03 | validation loss: 2.4419e-03\n",
      "Epoch: 68140 | training loss: 2.4715e-03 | validation loss: 2.4588e-03\n",
      "Epoch: 68150 | training loss: 2.4708e-03 | validation loss: 2.4565e-03\n",
      "Epoch: 68160 | training loss: 2.4689e-03 | validation loss: 2.4474e-03\n",
      "Epoch: 68170 | training loss: 2.4691e-03 | validation loss: 2.4458e-03\n",
      "Epoch: 68180 | training loss: 2.4688e-03 | validation loss: 2.4485e-03\n",
      "Epoch: 68190 | training loss: 2.4687e-03 | validation loss: 2.4486e-03\n",
      "Epoch: 68200 | training loss: 2.4686e-03 | validation loss: 2.4470e-03\n",
      "Epoch: 68210 | training loss: 2.4685e-03 | validation loss: 2.4479e-03\n",
      "Epoch: 68220 | training loss: 2.4685e-03 | validation loss: 2.4473e-03\n",
      "Epoch: 68230 | training loss: 2.4684e-03 | validation loss: 2.4473e-03\n",
      "Epoch: 68240 | training loss: 2.4683e-03 | validation loss: 2.4471e-03\n",
      "Epoch: 68250 | training loss: 2.4683e-03 | validation loss: 2.4470e-03\n",
      "Epoch: 68260 | training loss: 2.4682e-03 | validation loss: 2.4468e-03\n",
      "Epoch: 68270 | training loss: 2.4681e-03 | validation loss: 2.4462e-03\n",
      "Epoch: 68280 | training loss: 2.4689e-03 | validation loss: 2.4430e-03\n",
      "Epoch: 68290 | training loss: 2.5930e-03 | validation loss: 2.4857e-03\n",
      "Epoch: 68300 | training loss: 2.5785e-03 | validation loss: 2.5284e-03\n",
      "Epoch: 68310 | training loss: 2.4840e-03 | validation loss: 2.4331e-03\n",
      "Epoch: 68320 | training loss: 2.4923e-03 | validation loss: 2.4479e-03\n",
      "Epoch: 68330 | training loss: 2.4856e-03 | validation loss: 2.4525e-03\n",
      "Epoch: 68340 | training loss: 2.4929e-03 | validation loss: 2.4755e-03\n",
      "Epoch: 68350 | training loss: 2.7462e-03 | validation loss: 2.6639e-03\n",
      "Epoch: 68360 | training loss: 2.5091e-03 | validation loss: 2.4971e-03\n",
      "Epoch: 68370 | training loss: 2.4909e-03 | validation loss: 2.4298e-03\n",
      "Epoch: 68380 | training loss: 2.4804e-03 | validation loss: 2.4706e-03\n",
      "Epoch: 68390 | training loss: 2.4777e-03 | validation loss: 2.4330e-03\n",
      "Epoch: 68400 | training loss: 2.4739e-03 | validation loss: 2.4614e-03\n",
      "Epoch: 68410 | training loss: 2.4679e-03 | validation loss: 2.4401e-03\n",
      "Epoch: 68420 | training loss: 2.4683e-03 | validation loss: 2.4389e-03\n",
      "Epoch: 68430 | training loss: 2.4671e-03 | validation loss: 2.4433e-03\n",
      "Epoch: 68440 | training loss: 2.4671e-03 | validation loss: 2.4454e-03\n",
      "Epoch: 68450 | training loss: 2.4681e-03 | validation loss: 2.4498e-03\n",
      "Epoch: 68460 | training loss: 2.5021e-03 | validation loss: 2.4910e-03\n",
      "Epoch: 68470 | training loss: 3.5795e-03 | validation loss: 3.1282e-03\n",
      "Epoch: 68480 | training loss: 2.8882e-03 | validation loss: 2.5462e-03\n",
      "Epoch: 68490 | training loss: 2.4752e-03 | validation loss: 2.4540e-03\n",
      "Epoch: 68500 | training loss: 2.5084e-03 | validation loss: 2.4875e-03\n",
      "Epoch: 68510 | training loss: 2.4789e-03 | validation loss: 2.4307e-03\n",
      "Epoch: 68520 | training loss: 2.4666e-03 | validation loss: 2.4438e-03\n",
      "Epoch: 68530 | training loss: 2.4678e-03 | validation loss: 2.4496e-03\n",
      "Epoch: 68540 | training loss: 2.4673e-03 | validation loss: 2.4382e-03\n",
      "Epoch: 68550 | training loss: 2.4667e-03 | validation loss: 2.4454e-03\n",
      "Epoch: 68560 | training loss: 2.4664e-03 | validation loss: 2.4410e-03\n",
      "Epoch: 68570 | training loss: 2.4662e-03 | validation loss: 2.4430e-03\n",
      "Epoch: 68580 | training loss: 2.4661e-03 | validation loss: 2.4422e-03\n",
      "Epoch: 68590 | training loss: 2.4661e-03 | validation loss: 2.4415e-03\n",
      "Epoch: 68600 | training loss: 2.4660e-03 | validation loss: 2.4417e-03\n",
      "Epoch: 68610 | training loss: 2.4660e-03 | validation loss: 2.4418e-03\n",
      "Epoch: 68620 | training loss: 2.4659e-03 | validation loss: 2.4419e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68630 | training loss: 2.4659e-03 | validation loss: 2.4432e-03\n",
      "Epoch: 68640 | training loss: 2.4732e-03 | validation loss: 2.4575e-03\n",
      "Epoch: 68650 | training loss: 3.2036e-03 | validation loss: 2.9120e-03\n",
      "Epoch: 68660 | training loss: 3.1049e-03 | validation loss: 2.6178e-03\n",
      "Epoch: 68670 | training loss: 2.4683e-03 | validation loss: 2.4372e-03\n",
      "Epoch: 68680 | training loss: 2.5270e-03 | validation loss: 2.5051e-03\n",
      "Epoch: 68690 | training loss: 2.5014e-03 | validation loss: 2.4900e-03\n",
      "Epoch: 68700 | training loss: 2.4673e-03 | validation loss: 2.4495e-03\n",
      "Epoch: 68710 | training loss: 2.4678e-03 | validation loss: 2.4325e-03\n",
      "Epoch: 68720 | training loss: 2.4665e-03 | validation loss: 2.4354e-03\n",
      "Epoch: 68730 | training loss: 2.4654e-03 | validation loss: 2.4426e-03\n",
      "Epoch: 68740 | training loss: 2.4652e-03 | validation loss: 2.4419e-03\n",
      "Epoch: 68750 | training loss: 2.4651e-03 | validation loss: 2.4385e-03\n",
      "Epoch: 68760 | training loss: 2.4650e-03 | validation loss: 2.4401e-03\n",
      "Epoch: 68770 | training loss: 2.4649e-03 | validation loss: 2.4398e-03\n",
      "Epoch: 68780 | training loss: 2.4649e-03 | validation loss: 2.4393e-03\n",
      "Epoch: 68790 | training loss: 2.4648e-03 | validation loss: 2.4395e-03\n",
      "Epoch: 68800 | training loss: 2.4647e-03 | validation loss: 2.4391e-03\n",
      "Epoch: 68810 | training loss: 2.4647e-03 | validation loss: 2.4390e-03\n",
      "Epoch: 68820 | training loss: 2.4646e-03 | validation loss: 2.4389e-03\n",
      "Epoch: 68830 | training loss: 2.4645e-03 | validation loss: 2.4380e-03\n",
      "Epoch: 68840 | training loss: 2.4667e-03 | validation loss: 2.4334e-03\n",
      "Epoch: 68850 | training loss: 2.7520e-03 | validation loss: 2.5685e-03\n",
      "Epoch: 68860 | training loss: 2.7454e-03 | validation loss: 2.6697e-03\n",
      "Epoch: 68870 | training loss: 2.4861e-03 | validation loss: 2.4655e-03\n",
      "Epoch: 68880 | training loss: 2.4666e-03 | validation loss: 2.4414e-03\n",
      "Epoch: 68890 | training loss: 2.4760e-03 | validation loss: 2.4462e-03\n",
      "Epoch: 68900 | training loss: 2.5028e-03 | validation loss: 2.4811e-03\n",
      "Epoch: 68910 | training loss: 2.8763e-03 | validation loss: 2.7367e-03\n",
      "Epoch: 68920 | training loss: 2.4732e-03 | validation loss: 2.4244e-03\n",
      "Epoch: 68930 | training loss: 2.4644e-03 | validation loss: 2.4413e-03\n",
      "Epoch: 68940 | training loss: 2.4644e-03 | validation loss: 2.4414e-03\n",
      "Epoch: 68950 | training loss: 2.4676e-03 | validation loss: 2.4283e-03\n",
      "Epoch: 68960 | training loss: 2.4690e-03 | validation loss: 2.4510e-03\n",
      "Epoch: 68970 | training loss: 2.4643e-03 | validation loss: 2.4321e-03\n",
      "Epoch: 68980 | training loss: 2.4648e-03 | validation loss: 2.4310e-03\n",
      "Epoch: 68990 | training loss: 2.4639e-03 | validation loss: 2.4331e-03\n",
      "Epoch: 69000 | training loss: 2.4643e-03 | validation loss: 2.4314e-03\n",
      "Epoch: 69010 | training loss: 2.4771e-03 | validation loss: 2.4227e-03\n",
      "Epoch: 69020 | training loss: 2.9102e-03 | validation loss: 2.5306e-03\n",
      "Epoch: 69030 | training loss: 2.4880e-03 | validation loss: 2.4787e-03\n",
      "Epoch: 69040 | training loss: 2.5884e-03 | validation loss: 2.4424e-03\n",
      "Epoch: 69050 | training loss: 2.5196e-03 | validation loss: 2.4926e-03\n",
      "Epoch: 69060 | training loss: 2.4670e-03 | validation loss: 2.4256e-03\n",
      "Epoch: 69070 | training loss: 2.4633e-03 | validation loss: 2.4341e-03\n",
      "Epoch: 69080 | training loss: 2.4637e-03 | validation loss: 2.4402e-03\n",
      "Epoch: 69090 | training loss: 2.4632e-03 | validation loss: 2.4317e-03\n",
      "Epoch: 69100 | training loss: 2.4628e-03 | validation loss: 2.4358e-03\n",
      "Epoch: 69110 | training loss: 2.4628e-03 | validation loss: 2.4356e-03\n",
      "Epoch: 69120 | training loss: 2.4628e-03 | validation loss: 2.4331e-03\n",
      "Epoch: 69130 | training loss: 2.4626e-03 | validation loss: 2.4345e-03\n",
      "Epoch: 69140 | training loss: 2.4626e-03 | validation loss: 2.4350e-03\n",
      "Epoch: 69150 | training loss: 2.4626e-03 | validation loss: 2.4356e-03\n",
      "Epoch: 69160 | training loss: 2.4636e-03 | validation loss: 2.4396e-03\n",
      "Epoch: 69170 | training loss: 2.5038e-03 | validation loss: 2.4831e-03\n",
      "Epoch: 69180 | training loss: 3.7140e-03 | validation loss: 3.1817e-03\n",
      "Epoch: 69190 | training loss: 2.8455e-03 | validation loss: 2.5253e-03\n",
      "Epoch: 69200 | training loss: 2.4685e-03 | validation loss: 2.4197e-03\n",
      "Epoch: 69210 | training loss: 2.5167e-03 | validation loss: 2.4937e-03\n",
      "Epoch: 69220 | training loss: 2.4631e-03 | validation loss: 2.4315e-03\n",
      "Epoch: 69230 | training loss: 2.4667e-03 | validation loss: 2.4228e-03\n",
      "Epoch: 69240 | training loss: 2.4645e-03 | validation loss: 2.4430e-03\n",
      "Epoch: 69250 | training loss: 2.4622e-03 | validation loss: 2.4294e-03\n",
      "Epoch: 69260 | training loss: 2.4618e-03 | validation loss: 2.4339e-03\n",
      "Epoch: 69270 | training loss: 2.4617e-03 | validation loss: 2.4320e-03\n",
      "Epoch: 69280 | training loss: 2.4617e-03 | validation loss: 2.4330e-03\n",
      "Epoch: 69290 | training loss: 2.4616e-03 | validation loss: 2.4317e-03\n",
      "Epoch: 69300 | training loss: 2.4615e-03 | validation loss: 2.4327e-03\n",
      "Epoch: 69310 | training loss: 2.4614e-03 | validation loss: 2.4322e-03\n",
      "Epoch: 69320 | training loss: 2.4614e-03 | validation loss: 2.4318e-03\n",
      "Epoch: 69330 | training loss: 2.4613e-03 | validation loss: 2.4320e-03\n",
      "Epoch: 69340 | training loss: 2.4630e-03 | validation loss: 2.4376e-03\n",
      "Epoch: 69350 | training loss: 2.7023e-03 | validation loss: 2.6688e-03\n",
      "Epoch: 69360 | training loss: 2.7445e-03 | validation loss: 2.5949e-03\n",
      "Epoch: 69370 | training loss: 2.6432e-03 | validation loss: 2.5758e-03\n",
      "Epoch: 69380 | training loss: 2.5075e-03 | validation loss: 2.4900e-03\n",
      "Epoch: 69390 | training loss: 2.5026e-03 | validation loss: 2.4191e-03\n",
      "Epoch: 69400 | training loss: 2.4660e-03 | validation loss: 2.4305e-03\n",
      "Epoch: 69410 | training loss: 2.4692e-03 | validation loss: 2.4536e-03\n",
      "Epoch: 69420 | training loss: 2.4792e-03 | validation loss: 2.4649e-03\n",
      "Epoch: 69430 | training loss: 2.5469e-03 | validation loss: 2.5209e-03\n",
      "Epoch: 69440 | training loss: 2.7605e-03 | validation loss: 2.6599e-03\n",
      "Epoch: 69450 | training loss: 2.4797e-03 | validation loss: 2.4143e-03\n",
      "Epoch: 69460 | training loss: 2.4728e-03 | validation loss: 2.4161e-03\n",
      "Epoch: 69470 | training loss: 2.4795e-03 | validation loss: 2.4609e-03\n",
      "Epoch: 69480 | training loss: 2.4618e-03 | validation loss: 2.4362e-03\n",
      "Epoch: 69490 | training loss: 2.4619e-03 | validation loss: 2.4231e-03\n",
      "Epoch: 69500 | training loss: 2.4705e-03 | validation loss: 2.4168e-03\n",
      "Epoch: 69510 | training loss: 2.5753e-03 | validation loss: 2.4253e-03\n",
      "Epoch: 69520 | training loss: 2.9275e-03 | validation loss: 2.5355e-03\n",
      "Epoch: 69530 | training loss: 2.6235e-03 | validation loss: 2.5650e-03\n",
      "Epoch: 69540 | training loss: 2.5190e-03 | validation loss: 2.4182e-03\n",
      "Epoch: 69550 | training loss: 2.4790e-03 | validation loss: 2.4588e-03\n",
      "Epoch: 69560 | training loss: 2.4624e-03 | validation loss: 2.4215e-03\n",
      "Epoch: 69570 | training loss: 2.4604e-03 | validation loss: 2.4244e-03\n",
      "Epoch: 69580 | training loss: 2.4616e-03 | validation loss: 2.4356e-03\n",
      "Epoch: 69590 | training loss: 2.4602e-03 | validation loss: 2.4318e-03\n",
      "Epoch: 69600 | training loss: 2.4598e-03 | validation loss: 2.4300e-03\n",
      "Epoch: 69610 | training loss: 2.4605e-03 | validation loss: 2.4329e-03\n",
      "Epoch: 69620 | training loss: 2.4837e-03 | validation loss: 2.4625e-03\n",
      "Epoch: 69630 | training loss: 3.3165e-03 | validation loss: 2.9657e-03\n",
      "Epoch: 69640 | training loss: 2.8354e-03 | validation loss: 2.5100e-03\n",
      "Epoch: 69650 | training loss: 2.5635e-03 | validation loss: 2.5164e-03\n",
      "Epoch: 69660 | training loss: 2.4647e-03 | validation loss: 2.4428e-03\n",
      "Epoch: 69670 | training loss: 2.4797e-03 | validation loss: 2.4175e-03\n",
      "Epoch: 69680 | training loss: 2.4641e-03 | validation loss: 2.4365e-03\n",
      "Epoch: 69690 | training loss: 2.4593e-03 | validation loss: 2.4261e-03\n",
      "Epoch: 69700 | training loss: 2.4590e-03 | validation loss: 2.4258e-03\n",
      "Epoch: 69710 | training loss: 2.4589e-03 | validation loss: 2.4270e-03\n",
      "Epoch: 69720 | training loss: 2.4588e-03 | validation loss: 2.4264e-03\n",
      "Epoch: 69730 | training loss: 2.4588e-03 | validation loss: 2.4255e-03\n",
      "Epoch: 69740 | training loss: 2.4587e-03 | validation loss: 2.4267e-03\n",
      "Epoch: 69750 | training loss: 2.4586e-03 | validation loss: 2.4260e-03\n",
      "Epoch: 69760 | training loss: 2.4586e-03 | validation loss: 2.4254e-03\n",
      "Epoch: 69770 | training loss: 2.4585e-03 | validation loss: 2.4249e-03\n",
      "Epoch: 69780 | training loss: 2.4587e-03 | validation loss: 2.4235e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69790 | training loss: 2.4664e-03 | validation loss: 2.4173e-03\n",
      "Epoch: 69800 | training loss: 2.9509e-03 | validation loss: 2.5666e-03\n",
      "Epoch: 69810 | training loss: 2.5974e-03 | validation loss: 2.5278e-03\n",
      "Epoch: 69820 | training loss: 2.6689e-03 | validation loss: 2.4706e-03\n",
      "Epoch: 69830 | training loss: 2.4907e-03 | validation loss: 2.4429e-03\n",
      "Epoch: 69840 | training loss: 2.4731e-03 | validation loss: 2.4382e-03\n",
      "Epoch: 69850 | training loss: 2.4647e-03 | validation loss: 2.4318e-03\n",
      "Epoch: 69860 | training loss: 2.4603e-03 | validation loss: 2.4241e-03\n",
      "Epoch: 69870 | training loss: 2.4581e-03 | validation loss: 2.4230e-03\n",
      "Epoch: 69880 | training loss: 2.4584e-03 | validation loss: 2.4258e-03\n",
      "Epoch: 69890 | training loss: 2.4579e-03 | validation loss: 2.4237e-03\n",
      "Epoch: 69900 | training loss: 2.4577e-03 | validation loss: 2.4237e-03\n",
      "Epoch: 69910 | training loss: 2.4576e-03 | validation loss: 2.4235e-03\n",
      "Epoch: 69920 | training loss: 2.4576e-03 | validation loss: 2.4237e-03\n",
      "Epoch: 69930 | training loss: 2.4575e-03 | validation loss: 2.4236e-03\n",
      "Epoch: 69940 | training loss: 2.4574e-03 | validation loss: 2.4243e-03\n",
      "Epoch: 69950 | training loss: 2.4582e-03 | validation loss: 2.4300e-03\n",
      "Epoch: 69960 | training loss: 2.5366e-03 | validation loss: 2.5337e-03\n",
      "Epoch: 69970 | training loss: 2.7973e-03 | validation loss: 2.6966e-03\n",
      "Epoch: 69980 | training loss: 2.6734e-03 | validation loss: 2.5796e-03\n",
      "Epoch: 69990 | training loss: 2.5086e-03 | validation loss: 2.5012e-03\n",
      "Epoch: 70000 | training loss: 2.4656e-03 | validation loss: 2.4172e-03\n",
      "Epoch: 70010 | training loss: 2.4618e-03 | validation loss: 2.4282e-03\n",
      "Epoch: 70020 | training loss: 2.4605e-03 | validation loss: 2.4116e-03\n",
      "Epoch: 70030 | training loss: 2.4573e-03 | validation loss: 2.4193e-03\n",
      "Epoch: 70040 | training loss: 2.4572e-03 | validation loss: 2.4255e-03\n",
      "Epoch: 70050 | training loss: 2.4569e-03 | validation loss: 2.4242e-03\n",
      "Epoch: 70060 | training loss: 2.4567e-03 | validation loss: 2.4219e-03\n",
      "Epoch: 70070 | training loss: 2.4569e-03 | validation loss: 2.4236e-03\n",
      "Epoch: 70080 | training loss: 2.4664e-03 | validation loss: 2.4417e-03\n",
      "Epoch: 70090 | training loss: 3.0380e-03 | validation loss: 2.8095e-03\n",
      "Epoch: 70100 | training loss: 2.6902e-03 | validation loss: 2.4489e-03\n",
      "Epoch: 70110 | training loss: 2.6358e-03 | validation loss: 2.5582e-03\n",
      "Epoch: 70120 | training loss: 2.4800e-03 | validation loss: 2.4508e-03\n",
      "Epoch: 70130 | training loss: 2.4776e-03 | validation loss: 2.4094e-03\n",
      "Epoch: 70140 | training loss: 2.4578e-03 | validation loss: 2.4174e-03\n",
      "Epoch: 70150 | training loss: 2.4601e-03 | validation loss: 2.4322e-03\n",
      "Epoch: 70160 | training loss: 2.4565e-03 | validation loss: 2.4167e-03\n",
      "Epoch: 70170 | training loss: 2.4560e-03 | validation loss: 2.4191e-03\n",
      "Epoch: 70180 | training loss: 2.4560e-03 | validation loss: 2.4211e-03\n",
      "Epoch: 70190 | training loss: 2.4559e-03 | validation loss: 2.4189e-03\n",
      "Epoch: 70200 | training loss: 2.4558e-03 | validation loss: 2.4199e-03\n",
      "Epoch: 70210 | training loss: 2.4557e-03 | validation loss: 2.4194e-03\n",
      "Epoch: 70220 | training loss: 2.4557e-03 | validation loss: 2.4190e-03\n",
      "Epoch: 70230 | training loss: 2.4556e-03 | validation loss: 2.4193e-03\n",
      "Epoch: 70240 | training loss: 2.4556e-03 | validation loss: 2.4192e-03\n",
      "Epoch: 70250 | training loss: 2.4555e-03 | validation loss: 2.4190e-03\n",
      "Epoch: 70260 | training loss: 2.4554e-03 | validation loss: 2.4191e-03\n",
      "Epoch: 70270 | training loss: 2.4555e-03 | validation loss: 2.4205e-03\n",
      "Epoch: 70280 | training loss: 2.4645e-03 | validation loss: 2.4367e-03\n",
      "Epoch: 70290 | training loss: 3.3534e-03 | validation loss: 2.9747e-03\n",
      "Epoch: 70300 | training loss: 3.2089e-03 | validation loss: 2.6426e-03\n",
      "Epoch: 70310 | training loss: 2.4705e-03 | validation loss: 2.4004e-03\n",
      "Epoch: 70320 | training loss: 2.4873e-03 | validation loss: 2.4569e-03\n",
      "Epoch: 70330 | training loss: 2.4902e-03 | validation loss: 2.4645e-03\n",
      "Epoch: 70340 | training loss: 2.4596e-03 | validation loss: 2.4314e-03\n",
      "Epoch: 70350 | training loss: 2.4561e-03 | validation loss: 2.4121e-03\n",
      "Epoch: 70360 | training loss: 2.4563e-03 | validation loss: 2.4124e-03\n",
      "Epoch: 70370 | training loss: 2.4548e-03 | validation loss: 2.4187e-03\n",
      "Epoch: 70380 | training loss: 2.4548e-03 | validation loss: 2.4194e-03\n",
      "Epoch: 70390 | training loss: 2.4547e-03 | validation loss: 2.4159e-03\n",
      "Epoch: 70400 | training loss: 2.4546e-03 | validation loss: 2.4171e-03\n",
      "Epoch: 70410 | training loss: 2.4545e-03 | validation loss: 2.4171e-03\n",
      "Epoch: 70420 | training loss: 2.4544e-03 | validation loss: 2.4165e-03\n",
      "Epoch: 70430 | training loss: 2.4544e-03 | validation loss: 2.4168e-03\n",
      "Epoch: 70440 | training loss: 2.4543e-03 | validation loss: 2.4164e-03\n",
      "Epoch: 70450 | training loss: 2.4542e-03 | validation loss: 2.4164e-03\n",
      "Epoch: 70460 | training loss: 2.4542e-03 | validation loss: 2.4163e-03\n",
      "Epoch: 70470 | training loss: 2.4541e-03 | validation loss: 2.4167e-03\n",
      "Epoch: 70480 | training loss: 2.4559e-03 | validation loss: 2.4239e-03\n",
      "Epoch: 70490 | training loss: 2.7708e-03 | validation loss: 2.7286e-03\n",
      "Epoch: 70500 | training loss: 2.7850e-03 | validation loss: 2.6048e-03\n",
      "Epoch: 70510 | training loss: 2.5195e-03 | validation loss: 2.4281e-03\n",
      "Epoch: 70520 | training loss: 2.4675e-03 | validation loss: 2.3998e-03\n",
      "Epoch: 70530 | training loss: 2.4558e-03 | validation loss: 2.4063e-03\n",
      "Epoch: 70540 | training loss: 2.4576e-03 | validation loss: 2.4104e-03\n",
      "Epoch: 70550 | training loss: 2.4849e-03 | validation loss: 2.4026e-03\n",
      "Epoch: 70560 | training loss: 2.9685e-03 | validation loss: 2.5271e-03\n",
      "Epoch: 70570 | training loss: 2.5167e-03 | validation loss: 2.4871e-03\n",
      "Epoch: 70580 | training loss: 2.5032e-03 | validation loss: 2.4009e-03\n",
      "Epoch: 70590 | training loss: 2.4819e-03 | validation loss: 2.4548e-03\n",
      "Epoch: 70600 | training loss: 2.4649e-03 | validation loss: 2.4013e-03\n",
      "Epoch: 70610 | training loss: 2.4566e-03 | validation loss: 2.4245e-03\n",
      "Epoch: 70620 | training loss: 2.4534e-03 | validation loss: 2.4113e-03\n",
      "Epoch: 70630 | training loss: 2.4535e-03 | validation loss: 2.4103e-03\n",
      "Epoch: 70640 | training loss: 2.4534e-03 | validation loss: 2.4166e-03\n",
      "Epoch: 70650 | training loss: 2.4533e-03 | validation loss: 2.4161e-03\n",
      "Epoch: 70660 | training loss: 2.4532e-03 | validation loss: 2.4161e-03\n",
      "Epoch: 70670 | training loss: 2.4554e-03 | validation loss: 2.4223e-03\n",
      "Epoch: 70680 | training loss: 2.5206e-03 | validation loss: 2.4863e-03\n",
      "Epoch: 70690 | training loss: 3.4516e-03 | validation loss: 3.0308e-03\n",
      "Epoch: 70700 | training loss: 2.7108e-03 | validation loss: 2.4654e-03\n",
      "Epoch: 70710 | training loss: 2.4555e-03 | validation loss: 2.4185e-03\n",
      "Epoch: 70720 | training loss: 2.4735e-03 | validation loss: 2.4394e-03\n",
      "Epoch: 70730 | training loss: 2.4676e-03 | validation loss: 2.4009e-03\n",
      "Epoch: 70740 | training loss: 2.4567e-03 | validation loss: 2.4249e-03\n",
      "Epoch: 70750 | training loss: 2.4534e-03 | validation loss: 2.4077e-03\n",
      "Epoch: 70760 | training loss: 2.4527e-03 | validation loss: 2.4147e-03\n",
      "Epoch: 70770 | training loss: 2.4525e-03 | validation loss: 2.4098e-03\n",
      "Epoch: 70780 | training loss: 2.4523e-03 | validation loss: 2.4133e-03\n",
      "Epoch: 70790 | training loss: 2.4521e-03 | validation loss: 2.4114e-03\n",
      "Epoch: 70800 | training loss: 2.4521e-03 | validation loss: 2.4106e-03\n",
      "Epoch: 70810 | training loss: 2.4520e-03 | validation loss: 2.4106e-03\n",
      "Epoch: 70820 | training loss: 2.4520e-03 | validation loss: 2.4099e-03\n",
      "Epoch: 70830 | training loss: 2.4529e-03 | validation loss: 2.4067e-03\n",
      "Epoch: 70840 | training loss: 2.4982e-03 | validation loss: 2.4011e-03\n",
      "Epoch: 70850 | training loss: 3.8383e-03 | validation loss: 2.8894e-03\n",
      "Epoch: 70860 | training loss: 2.7188e-03 | validation loss: 2.6072e-03\n",
      "Epoch: 70870 | training loss: 2.5159e-03 | validation loss: 2.4808e-03\n",
      "Epoch: 70880 | training loss: 2.4826e-03 | validation loss: 2.3993e-03\n",
      "Epoch: 70890 | training loss: 2.4595e-03 | validation loss: 2.3976e-03\n",
      "Epoch: 70900 | training loss: 2.4577e-03 | validation loss: 2.4261e-03\n",
      "Epoch: 70910 | training loss: 2.4514e-03 | validation loss: 2.4097e-03\n",
      "Epoch: 70920 | training loss: 2.4518e-03 | validation loss: 2.4062e-03\n",
      "Epoch: 70930 | training loss: 2.4516e-03 | validation loss: 2.4132e-03\n",
      "Epoch: 70940 | training loss: 2.4513e-03 | validation loss: 2.4079e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70950 | training loss: 2.4512e-03 | validation loss: 2.4106e-03\n",
      "Epoch: 70960 | training loss: 2.4511e-03 | validation loss: 2.4087e-03\n",
      "Epoch: 70970 | training loss: 2.4510e-03 | validation loss: 2.4097e-03\n",
      "Epoch: 70980 | training loss: 2.4509e-03 | validation loss: 2.4092e-03\n",
      "Epoch: 70990 | training loss: 2.4509e-03 | validation loss: 2.4088e-03\n",
      "Epoch: 71000 | training loss: 2.4508e-03 | validation loss: 2.4091e-03\n",
      "Epoch: 71010 | training loss: 2.4512e-03 | validation loss: 2.4121e-03\n",
      "Epoch: 71020 | training loss: 2.5106e-03 | validation loss: 2.4865e-03\n",
      "Epoch: 71030 | training loss: 2.5130e-03 | validation loss: 2.4972e-03\n",
      "Epoch: 71040 | training loss: 2.6744e-03 | validation loss: 2.6425e-03\n",
      "Epoch: 71050 | training loss: 2.5266e-03 | validation loss: 2.5092e-03\n",
      "Epoch: 71060 | training loss: 2.4525e-03 | validation loss: 2.4102e-03\n",
      "Epoch: 71070 | training loss: 2.4650e-03 | validation loss: 2.3904e-03\n",
      "Epoch: 71080 | training loss: 2.5158e-03 | validation loss: 2.3889e-03\n",
      "Epoch: 71090 | training loss: 2.7300e-03 | validation loss: 2.4420e-03\n",
      "Epoch: 71100 | training loss: 2.4502e-03 | validation loss: 2.4060e-03\n",
      "Epoch: 71110 | training loss: 2.4881e-03 | validation loss: 2.4579e-03\n",
      "Epoch: 71120 | training loss: 2.4620e-03 | validation loss: 2.3951e-03\n",
      "Epoch: 71130 | training loss: 2.4579e-03 | validation loss: 2.3956e-03\n",
      "Epoch: 71140 | training loss: 2.4502e-03 | validation loss: 2.4036e-03\n",
      "Epoch: 71150 | training loss: 2.4499e-03 | validation loss: 2.4078e-03\n",
      "Epoch: 71160 | training loss: 2.4516e-03 | validation loss: 2.4143e-03\n",
      "Epoch: 71170 | training loss: 2.5168e-03 | validation loss: 2.4798e-03\n",
      "Epoch: 71180 | training loss: 3.5833e-03 | validation loss: 3.0981e-03\n",
      "Epoch: 71190 | training loss: 2.6224e-03 | validation loss: 2.4374e-03\n",
      "Epoch: 71200 | training loss: 2.4787e-03 | validation loss: 2.3953e-03\n",
      "Epoch: 71210 | training loss: 2.4930e-03 | validation loss: 2.4534e-03\n",
      "Epoch: 71220 | training loss: 2.4507e-03 | validation loss: 2.3988e-03\n",
      "Epoch: 71230 | training loss: 2.4517e-03 | validation loss: 2.3991e-03\n",
      "Epoch: 71240 | training loss: 2.4515e-03 | validation loss: 2.4141e-03\n",
      "Epoch: 71250 | training loss: 2.4501e-03 | validation loss: 2.4017e-03\n",
      "Epoch: 71260 | training loss: 2.4495e-03 | validation loss: 2.4075e-03\n",
      "Epoch: 71270 | training loss: 2.4493e-03 | validation loss: 2.4035e-03\n",
      "Epoch: 71280 | training loss: 2.4491e-03 | validation loss: 2.4059e-03\n",
      "Epoch: 71290 | training loss: 2.4490e-03 | validation loss: 2.4046e-03\n",
      "Epoch: 71300 | training loss: 2.4490e-03 | validation loss: 2.4042e-03\n",
      "Epoch: 71310 | training loss: 2.4489e-03 | validation loss: 2.4044e-03\n",
      "Epoch: 71320 | training loss: 2.4488e-03 | validation loss: 2.4044e-03\n",
      "Epoch: 71330 | training loss: 2.4488e-03 | validation loss: 2.4044e-03\n",
      "Epoch: 71340 | training loss: 2.4487e-03 | validation loss: 2.4050e-03\n",
      "Epoch: 71350 | training loss: 2.4512e-03 | validation loss: 2.4125e-03\n",
      "Epoch: 71360 | training loss: 2.7829e-03 | validation loss: 2.6441e-03\n",
      "Epoch: 71370 | training loss: 2.5418e-03 | validation loss: 2.3959e-03\n",
      "Epoch: 71380 | training loss: 2.5861e-03 | validation loss: 2.5154e-03\n",
      "Epoch: 71390 | training loss: 2.5637e-03 | validation loss: 2.5086e-03\n",
      "Epoch: 71400 | training loss: 2.4983e-03 | validation loss: 2.4649e-03\n",
      "Epoch: 71410 | training loss: 2.4592e-03 | validation loss: 2.4271e-03\n",
      "Epoch: 71420 | training loss: 2.4483e-03 | validation loss: 2.4041e-03\n",
      "Epoch: 71430 | training loss: 2.4494e-03 | validation loss: 2.3975e-03\n",
      "Epoch: 71440 | training loss: 2.4488e-03 | validation loss: 2.3992e-03\n",
      "Epoch: 71450 | training loss: 2.4480e-03 | validation loss: 2.4030e-03\n",
      "Epoch: 71460 | training loss: 2.4481e-03 | validation loss: 2.4046e-03\n",
      "Epoch: 71470 | training loss: 2.4479e-03 | validation loss: 2.4023e-03\n",
      "Epoch: 71480 | training loss: 2.4479e-03 | validation loss: 2.4019e-03\n",
      "Epoch: 71490 | training loss: 2.4478e-03 | validation loss: 2.4027e-03\n",
      "Epoch: 71500 | training loss: 2.4477e-03 | validation loss: 2.4021e-03\n",
      "Epoch: 71510 | training loss: 2.4477e-03 | validation loss: 2.4021e-03\n",
      "Epoch: 71520 | training loss: 2.4476e-03 | validation loss: 2.4019e-03\n",
      "Epoch: 71530 | training loss: 2.4475e-03 | validation loss: 2.4018e-03\n",
      "Epoch: 71540 | training loss: 2.4475e-03 | validation loss: 2.4016e-03\n",
      "Epoch: 71550 | training loss: 2.4474e-03 | validation loss: 2.4014e-03\n",
      "Epoch: 71560 | training loss: 2.4474e-03 | validation loss: 2.4003e-03\n",
      "Epoch: 71570 | training loss: 2.4544e-03 | validation loss: 2.3945e-03\n",
      "Epoch: 71580 | training loss: 3.0901e-03 | validation loss: 2.7484e-03\n",
      "Epoch: 71590 | training loss: 2.4699e-03 | validation loss: 2.4209e-03\n",
      "Epoch: 71600 | training loss: 2.4982e-03 | validation loss: 2.4667e-03\n",
      "Epoch: 71610 | training loss: 2.4748e-03 | validation loss: 2.4433e-03\n",
      "Epoch: 71620 | training loss: 2.4551e-03 | validation loss: 2.4199e-03\n",
      "Epoch: 71630 | training loss: 2.4508e-03 | validation loss: 2.4144e-03\n",
      "Epoch: 71640 | training loss: 2.5102e-03 | validation loss: 2.4707e-03\n",
      "Epoch: 71650 | training loss: 3.2101e-03 | validation loss: 2.8965e-03\n",
      "Epoch: 71660 | training loss: 2.7013e-03 | validation loss: 2.4336e-03\n",
      "Epoch: 71670 | training loss: 2.5157e-03 | validation loss: 2.4736e-03\n",
      "Epoch: 71680 | training loss: 2.4569e-03 | validation loss: 2.3872e-03\n",
      "Epoch: 71690 | training loss: 2.4478e-03 | validation loss: 2.4053e-03\n",
      "Epoch: 71700 | training loss: 2.4470e-03 | validation loss: 2.3957e-03\n",
      "Epoch: 71710 | training loss: 2.4471e-03 | validation loss: 2.4035e-03\n",
      "Epoch: 71720 | training loss: 2.4471e-03 | validation loss: 2.3947e-03\n",
      "Epoch: 71730 | training loss: 2.4464e-03 | validation loss: 2.4007e-03\n",
      "Epoch: 71740 | training loss: 2.4464e-03 | validation loss: 2.4005e-03\n",
      "Epoch: 71750 | training loss: 2.4462e-03 | validation loss: 2.3989e-03\n",
      "Epoch: 71760 | training loss: 2.4461e-03 | validation loss: 2.3984e-03\n",
      "Epoch: 71770 | training loss: 2.4461e-03 | validation loss: 2.3983e-03\n",
      "Epoch: 71780 | training loss: 2.4460e-03 | validation loss: 2.3986e-03\n",
      "Epoch: 71790 | training loss: 2.4468e-03 | validation loss: 2.4032e-03\n",
      "Epoch: 71800 | training loss: 2.6042e-03 | validation loss: 2.5346e-03\n",
      "Epoch: 71810 | training loss: 2.6181e-03 | validation loss: 2.5237e-03\n",
      "Epoch: 71820 | training loss: 2.8193e-03 | validation loss: 2.6501e-03\n",
      "Epoch: 71830 | training loss: 2.5854e-03 | validation loss: 2.5110e-03\n",
      "Epoch: 71840 | training loss: 2.4900e-03 | validation loss: 2.4467e-03\n",
      "Epoch: 71850 | training loss: 2.4581e-03 | validation loss: 2.4220e-03\n",
      "Epoch: 71860 | training loss: 2.4490e-03 | validation loss: 2.4117e-03\n",
      "Epoch: 71870 | training loss: 2.4472e-03 | validation loss: 2.4060e-03\n",
      "Epoch: 71880 | training loss: 2.4463e-03 | validation loss: 2.4014e-03\n",
      "Epoch: 71890 | training loss: 2.4455e-03 | validation loss: 2.3977e-03\n",
      "Epoch: 71900 | training loss: 2.4453e-03 | validation loss: 2.3959e-03\n",
      "Epoch: 71910 | training loss: 2.4453e-03 | validation loss: 2.3957e-03\n",
      "Epoch: 71920 | training loss: 2.4452e-03 | validation loss: 2.3963e-03\n",
      "Epoch: 71930 | training loss: 2.4451e-03 | validation loss: 2.3967e-03\n",
      "Epoch: 71940 | training loss: 2.4451e-03 | validation loss: 2.3963e-03\n",
      "Epoch: 71950 | training loss: 2.4450e-03 | validation loss: 2.3960e-03\n",
      "Epoch: 71960 | training loss: 2.4449e-03 | validation loss: 2.3961e-03\n",
      "Epoch: 71970 | training loss: 2.4449e-03 | validation loss: 2.3959e-03\n",
      "Epoch: 71980 | training loss: 2.4448e-03 | validation loss: 2.3958e-03\n",
      "Epoch: 71990 | training loss: 2.4447e-03 | validation loss: 2.3957e-03\n",
      "Epoch: 72000 | training loss: 2.4447e-03 | validation loss: 2.3955e-03\n",
      "Epoch: 72010 | training loss: 2.4446e-03 | validation loss: 2.3954e-03\n",
      "Epoch: 72020 | training loss: 2.4446e-03 | validation loss: 2.3952e-03\n",
      "Epoch: 72030 | training loss: 2.4445e-03 | validation loss: 2.3951e-03\n",
      "Epoch: 72040 | training loss: 2.4444e-03 | validation loss: 2.3949e-03\n",
      "Epoch: 72050 | training loss: 2.4444e-03 | validation loss: 2.3948e-03\n",
      "Epoch: 72060 | training loss: 2.4443e-03 | validation loss: 2.3946e-03\n",
      "Epoch: 72070 | training loss: 2.4443e-03 | validation loss: 2.3943e-03\n",
      "Epoch: 72080 | training loss: 2.4652e-03 | validation loss: 2.4030e-03\n",
      "Epoch: 72090 | training loss: 4.2524e-03 | validation loss: 3.1597e-03\n",
      "Epoch: 72100 | training loss: 3.0700e-03 | validation loss: 2.9213e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72110 | training loss: 2.5455e-03 | validation loss: 2.4858e-03\n",
      "Epoch: 72120 | training loss: 2.5064e-03 | validation loss: 2.4160e-03\n",
      "Epoch: 72130 | training loss: 2.4472e-03 | validation loss: 2.3836e-03\n",
      "Epoch: 72140 | training loss: 2.4557e-03 | validation loss: 2.3990e-03\n",
      "Epoch: 72150 | training loss: 2.4445e-03 | validation loss: 2.3933e-03\n",
      "Epoch: 72160 | training loss: 2.4447e-03 | validation loss: 2.3943e-03\n",
      "Epoch: 72170 | training loss: 2.4440e-03 | validation loss: 2.3920e-03\n",
      "Epoch: 72180 | training loss: 2.4437e-03 | validation loss: 2.3908e-03\n",
      "Epoch: 72190 | training loss: 2.4436e-03 | validation loss: 2.3932e-03\n",
      "Epoch: 72200 | training loss: 2.4435e-03 | validation loss: 2.3933e-03\n",
      "Epoch: 72210 | training loss: 2.4434e-03 | validation loss: 2.3923e-03\n",
      "Epoch: 72220 | training loss: 2.4434e-03 | validation loss: 2.3926e-03\n",
      "Epoch: 72230 | training loss: 2.4433e-03 | validation loss: 2.3920e-03\n",
      "Epoch: 72240 | training loss: 2.4433e-03 | validation loss: 2.3921e-03\n",
      "Epoch: 72250 | training loss: 2.4432e-03 | validation loss: 2.3919e-03\n",
      "Epoch: 72260 | training loss: 2.4431e-03 | validation loss: 2.3918e-03\n",
      "Epoch: 72270 | training loss: 2.4431e-03 | validation loss: 2.3917e-03\n",
      "Epoch: 72280 | training loss: 2.4430e-03 | validation loss: 2.3916e-03\n",
      "Epoch: 72290 | training loss: 2.4430e-03 | validation loss: 2.3915e-03\n",
      "Epoch: 72300 | training loss: 2.4429e-03 | validation loss: 2.3915e-03\n",
      "Epoch: 72310 | training loss: 2.4429e-03 | validation loss: 2.3923e-03\n",
      "Epoch: 72320 | training loss: 2.4465e-03 | validation loss: 2.4028e-03\n",
      "Epoch: 72330 | training loss: 3.0252e-03 | validation loss: 2.7829e-03\n",
      "Epoch: 72340 | training loss: 3.1315e-03 | validation loss: 2.5753e-03\n",
      "Epoch: 72350 | training loss: 2.4591e-03 | validation loss: 2.3722e-03\n",
      "Epoch: 72360 | training loss: 2.4487e-03 | validation loss: 2.4031e-03\n",
      "Epoch: 72370 | training loss: 2.4591e-03 | validation loss: 2.4185e-03\n",
      "Epoch: 72380 | training loss: 2.4564e-03 | validation loss: 2.4165e-03\n",
      "Epoch: 72390 | training loss: 2.4476e-03 | validation loss: 2.4051e-03\n",
      "Epoch: 72400 | training loss: 2.4427e-03 | validation loss: 2.3939e-03\n",
      "Epoch: 72410 | training loss: 2.4426e-03 | validation loss: 2.3884e-03\n",
      "Epoch: 72420 | training loss: 2.4424e-03 | validation loss: 2.3882e-03\n",
      "Epoch: 72430 | training loss: 2.4422e-03 | validation loss: 2.3903e-03\n",
      "Epoch: 72440 | training loss: 2.4421e-03 | validation loss: 2.3906e-03\n",
      "Epoch: 72450 | training loss: 2.4420e-03 | validation loss: 2.3893e-03\n",
      "Epoch: 72460 | training loss: 2.4420e-03 | validation loss: 2.3894e-03\n",
      "Epoch: 72470 | training loss: 2.4419e-03 | validation loss: 2.3897e-03\n",
      "Epoch: 72480 | training loss: 2.4419e-03 | validation loss: 2.3893e-03\n",
      "Epoch: 72490 | training loss: 2.4418e-03 | validation loss: 2.3893e-03\n",
      "Epoch: 72500 | training loss: 2.4417e-03 | validation loss: 2.3891e-03\n",
      "Epoch: 72510 | training loss: 2.4417e-03 | validation loss: 2.3890e-03\n",
      "Epoch: 72520 | training loss: 2.4416e-03 | validation loss: 2.3888e-03\n",
      "Epoch: 72530 | training loss: 2.4415e-03 | validation loss: 2.3887e-03\n",
      "Epoch: 72540 | training loss: 2.4415e-03 | validation loss: 2.3886e-03\n",
      "Epoch: 72550 | training loss: 2.4414e-03 | validation loss: 2.3885e-03\n",
      "Epoch: 72560 | training loss: 2.4414e-03 | validation loss: 2.3883e-03\n",
      "Epoch: 72570 | training loss: 2.4413e-03 | validation loss: 2.3883e-03\n",
      "Epoch: 72580 | training loss: 2.4412e-03 | validation loss: 2.3886e-03\n",
      "Epoch: 72590 | training loss: 2.4427e-03 | validation loss: 2.3944e-03\n",
      "Epoch: 72600 | training loss: 2.7372e-03 | validation loss: 2.6056e-03\n",
      "Epoch: 72610 | training loss: 2.5769e-03 | validation loss: 2.3876e-03\n",
      "Epoch: 72620 | training loss: 2.4705e-03 | validation loss: 2.4116e-03\n",
      "Epoch: 72630 | training loss: 2.4582e-03 | validation loss: 2.4062e-03\n",
      "Epoch: 72640 | training loss: 2.4475e-03 | validation loss: 2.4003e-03\n",
      "Epoch: 72650 | training loss: 2.4451e-03 | validation loss: 2.4013e-03\n",
      "Epoch: 72660 | training loss: 2.4442e-03 | validation loss: 2.4003e-03\n",
      "Epoch: 72670 | training loss: 2.4430e-03 | validation loss: 2.3962e-03\n",
      "Epoch: 72680 | training loss: 2.4416e-03 | validation loss: 2.3922e-03\n",
      "Epoch: 72690 | training loss: 2.4407e-03 | validation loss: 2.3890e-03\n",
      "Epoch: 72700 | training loss: 2.4405e-03 | validation loss: 2.3863e-03\n",
      "Epoch: 72710 | training loss: 2.4405e-03 | validation loss: 2.3853e-03\n",
      "Epoch: 72720 | training loss: 2.4404e-03 | validation loss: 2.3861e-03\n",
      "Epoch: 72730 | training loss: 2.4404e-03 | validation loss: 2.3866e-03\n",
      "Epoch: 72740 | training loss: 2.4403e-03 | validation loss: 2.3863e-03\n",
      "Epoch: 72750 | training loss: 2.4402e-03 | validation loss: 2.3859e-03\n",
      "Epoch: 72760 | training loss: 2.4402e-03 | validation loss: 2.3860e-03\n",
      "Epoch: 72770 | training loss: 2.4401e-03 | validation loss: 2.3858e-03\n",
      "Epoch: 72780 | training loss: 2.4400e-03 | validation loss: 2.3856e-03\n",
      "Epoch: 72790 | training loss: 2.4400e-03 | validation loss: 2.3855e-03\n",
      "Epoch: 72800 | training loss: 2.4399e-03 | validation loss: 2.3854e-03\n",
      "Epoch: 72810 | training loss: 2.4399e-03 | validation loss: 2.3852e-03\n",
      "Epoch: 72820 | training loss: 2.4398e-03 | validation loss: 2.3851e-03\n",
      "Epoch: 72830 | training loss: 2.4397e-03 | validation loss: 2.3849e-03\n",
      "Epoch: 72840 | training loss: 2.4398e-03 | validation loss: 2.3836e-03\n",
      "Epoch: 72850 | training loss: 2.4695e-03 | validation loss: 2.3838e-03\n",
      "Epoch: 72860 | training loss: 2.5825e-03 | validation loss: 2.4379e-03\n",
      "Epoch: 72870 | training loss: 2.5432e-03 | validation loss: 2.4219e-03\n",
      "Epoch: 72880 | training loss: 2.4547e-03 | validation loss: 2.3830e-03\n",
      "Epoch: 72890 | training loss: 2.4396e-03 | validation loss: 2.3823e-03\n",
      "Epoch: 72900 | training loss: 2.4398e-03 | validation loss: 2.3860e-03\n",
      "Epoch: 72910 | training loss: 2.4398e-03 | validation loss: 2.3865e-03\n",
      "Epoch: 72920 | training loss: 2.4397e-03 | validation loss: 2.3875e-03\n",
      "Epoch: 72930 | training loss: 2.4511e-03 | validation loss: 2.4085e-03\n",
      "Epoch: 72940 | training loss: 3.3804e-03 | validation loss: 2.9828e-03\n",
      "Epoch: 72950 | training loss: 3.0635e-03 | validation loss: 2.5444e-03\n",
      "Epoch: 72960 | training loss: 2.4419e-03 | validation loss: 2.3798e-03\n",
      "Epoch: 72970 | training loss: 2.4989e-03 | validation loss: 2.4511e-03\n",
      "Epoch: 72980 | training loss: 2.4551e-03 | validation loss: 2.4108e-03\n",
      "Epoch: 72990 | training loss: 2.4414e-03 | validation loss: 2.3759e-03\n",
      "Epoch: 73000 | training loss: 2.4416e-03 | validation loss: 2.3752e-03\n",
      "Epoch: 73010 | training loss: 2.4393e-03 | validation loss: 2.3862e-03\n",
      "Epoch: 73020 | training loss: 2.4387e-03 | validation loss: 2.3835e-03\n",
      "Epoch: 73030 | training loss: 2.4387e-03 | validation loss: 2.3799e-03\n",
      "Epoch: 73040 | training loss: 2.4386e-03 | validation loss: 2.3831e-03\n",
      "Epoch: 73050 | training loss: 2.4385e-03 | validation loss: 2.3812e-03\n",
      "Epoch: 73060 | training loss: 2.4384e-03 | validation loss: 2.3819e-03\n",
      "Epoch: 73070 | training loss: 2.4383e-03 | validation loss: 2.3812e-03\n",
      "Epoch: 73080 | training loss: 2.4383e-03 | validation loss: 2.3816e-03\n",
      "Epoch: 73090 | training loss: 2.4382e-03 | validation loss: 2.3812e-03\n",
      "Epoch: 73100 | training loss: 2.4382e-03 | validation loss: 2.3811e-03\n",
      "Epoch: 73110 | training loss: 2.4381e-03 | validation loss: 2.3810e-03\n",
      "Epoch: 73120 | training loss: 2.4380e-03 | validation loss: 2.3809e-03\n",
      "Epoch: 73130 | training loss: 2.4380e-03 | validation loss: 2.3809e-03\n",
      "Epoch: 73140 | training loss: 2.4379e-03 | validation loss: 2.3814e-03\n",
      "Epoch: 73150 | training loss: 2.4397e-03 | validation loss: 2.3882e-03\n",
      "Epoch: 73160 | training loss: 2.7108e-03 | validation loss: 2.5883e-03\n",
      "Epoch: 73170 | training loss: 2.4675e-03 | validation loss: 2.3598e-03\n",
      "Epoch: 73180 | training loss: 2.6432e-03 | validation loss: 2.5267e-03\n",
      "Epoch: 73190 | training loss: 2.5661e-03 | validation loss: 2.4820e-03\n",
      "Epoch: 73200 | training loss: 2.4862e-03 | validation loss: 2.4291e-03\n",
      "Epoch: 73210 | training loss: 2.4478e-03 | validation loss: 2.3987e-03\n",
      "Epoch: 73220 | training loss: 2.4379e-03 | validation loss: 2.3842e-03\n",
      "Epoch: 73230 | training loss: 2.4386e-03 | validation loss: 2.3788e-03\n",
      "Epoch: 73240 | training loss: 2.4382e-03 | validation loss: 2.3778e-03\n",
      "Epoch: 73250 | training loss: 2.4373e-03 | validation loss: 2.3793e-03\n",
      "Epoch: 73260 | training loss: 2.4373e-03 | validation loss: 2.3801e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73270 | training loss: 2.4372e-03 | validation loss: 2.3791e-03\n",
      "Epoch: 73280 | training loss: 2.4371e-03 | validation loss: 2.3788e-03\n",
      "Epoch: 73290 | training loss: 2.4370e-03 | validation loss: 2.3792e-03\n",
      "Epoch: 73300 | training loss: 2.4370e-03 | validation loss: 2.3788e-03\n",
      "Epoch: 73310 | training loss: 2.4369e-03 | validation loss: 2.3787e-03\n",
      "Epoch: 73320 | training loss: 2.4369e-03 | validation loss: 2.3786e-03\n",
      "Epoch: 73330 | training loss: 2.4368e-03 | validation loss: 2.3784e-03\n",
      "Epoch: 73340 | training loss: 2.4367e-03 | validation loss: 2.3783e-03\n",
      "Epoch: 73350 | training loss: 2.4367e-03 | validation loss: 2.3782e-03\n",
      "Epoch: 73360 | training loss: 2.4366e-03 | validation loss: 2.3781e-03\n",
      "Epoch: 73370 | training loss: 2.4366e-03 | validation loss: 2.3779e-03\n",
      "Epoch: 73380 | training loss: 2.4365e-03 | validation loss: 2.3778e-03\n",
      "Epoch: 73390 | training loss: 2.4364e-03 | validation loss: 2.3777e-03\n",
      "Epoch: 73400 | training loss: 2.4364e-03 | validation loss: 2.3775e-03\n",
      "Epoch: 73410 | training loss: 2.4363e-03 | validation loss: 2.3775e-03\n",
      "Epoch: 73420 | training loss: 2.4365e-03 | validation loss: 2.3792e-03\n",
      "Epoch: 73430 | training loss: 2.4905e-03 | validation loss: 2.4305e-03\n",
      "Epoch: 73440 | training loss: 4.4482e-03 | validation loss: 3.5206e-03\n",
      "Epoch: 73450 | training loss: 3.0712e-03 | validation loss: 2.7639e-03\n",
      "Epoch: 73460 | training loss: 2.5875e-03 | validation loss: 2.4691e-03\n",
      "Epoch: 73470 | training loss: 2.4390e-03 | validation loss: 2.3722e-03\n",
      "Epoch: 73480 | training loss: 2.4486e-03 | validation loss: 2.3792e-03\n",
      "Epoch: 73490 | training loss: 2.4467e-03 | validation loss: 2.3794e-03\n",
      "Epoch: 73500 | training loss: 2.4381e-03 | validation loss: 2.3765e-03\n",
      "Epoch: 73510 | training loss: 2.4359e-03 | validation loss: 2.3776e-03\n",
      "Epoch: 73520 | training loss: 2.4358e-03 | validation loss: 2.3774e-03\n",
      "Epoch: 73530 | training loss: 2.4357e-03 | validation loss: 2.3755e-03\n",
      "Epoch: 73540 | training loss: 2.4356e-03 | validation loss: 2.3748e-03\n",
      "Epoch: 73550 | training loss: 2.4355e-03 | validation loss: 2.3756e-03\n",
      "Epoch: 73560 | training loss: 2.4355e-03 | validation loss: 2.3755e-03\n",
      "Epoch: 73570 | training loss: 2.4354e-03 | validation loss: 2.3752e-03\n",
      "Epoch: 73580 | training loss: 2.4353e-03 | validation loss: 2.3753e-03\n",
      "Epoch: 73590 | training loss: 2.4353e-03 | validation loss: 2.3750e-03\n",
      "Epoch: 73600 | training loss: 2.4352e-03 | validation loss: 2.3750e-03\n",
      "Epoch: 73610 | training loss: 2.4352e-03 | validation loss: 2.3748e-03\n",
      "Epoch: 73620 | training loss: 2.4351e-03 | validation loss: 2.3747e-03\n",
      "Epoch: 73630 | training loss: 2.4350e-03 | validation loss: 2.3746e-03\n",
      "Epoch: 73640 | training loss: 2.4350e-03 | validation loss: 2.3744e-03\n",
      "Epoch: 73650 | training loss: 2.4349e-03 | validation loss: 2.3743e-03\n",
      "Epoch: 73660 | training loss: 2.4349e-03 | validation loss: 2.3741e-03\n",
      "Epoch: 73670 | training loss: 2.4348e-03 | validation loss: 2.3739e-03\n",
      "Epoch: 73680 | training loss: 2.4347e-03 | validation loss: 2.3733e-03\n",
      "Epoch: 73690 | training loss: 2.4357e-03 | validation loss: 2.3678e-03\n",
      "Epoch: 73700 | training loss: 2.6546e-03 | validation loss: 2.3905e-03\n",
      "Epoch: 73710 | training loss: 2.6438e-03 | validation loss: 2.5993e-03\n",
      "Epoch: 73720 | training loss: 2.6295e-03 | validation loss: 2.5014e-03\n",
      "Epoch: 73730 | training loss: 2.5162e-03 | validation loss: 2.4543e-03\n",
      "Epoch: 73740 | training loss: 2.4686e-03 | validation loss: 2.4288e-03\n",
      "Epoch: 73750 | training loss: 2.4475e-03 | validation loss: 2.4036e-03\n",
      "Epoch: 73760 | training loss: 2.4359e-03 | validation loss: 2.3810e-03\n",
      "Epoch: 73770 | training loss: 2.4349e-03 | validation loss: 2.3717e-03\n",
      "Epoch: 73780 | training loss: 2.4342e-03 | validation loss: 2.3727e-03\n",
      "Epoch: 73790 | training loss: 2.4342e-03 | validation loss: 2.3736e-03\n",
      "Epoch: 73800 | training loss: 2.4341e-03 | validation loss: 2.3719e-03\n",
      "Epoch: 73810 | training loss: 2.4340e-03 | validation loss: 2.3725e-03\n",
      "Epoch: 73820 | training loss: 2.4339e-03 | validation loss: 2.3722e-03\n",
      "Epoch: 73830 | training loss: 2.4339e-03 | validation loss: 2.3719e-03\n",
      "Epoch: 73840 | training loss: 2.4338e-03 | validation loss: 2.3716e-03\n",
      "Epoch: 73850 | training loss: 2.4338e-03 | validation loss: 2.3714e-03\n",
      "Epoch: 73860 | training loss: 2.4337e-03 | validation loss: 2.3712e-03\n",
      "Epoch: 73870 | training loss: 2.4336e-03 | validation loss: 2.3711e-03\n",
      "Epoch: 73880 | training loss: 2.4336e-03 | validation loss: 2.3711e-03\n",
      "Epoch: 73890 | training loss: 2.4335e-03 | validation loss: 2.3709e-03\n",
      "Epoch: 73900 | training loss: 2.4335e-03 | validation loss: 2.3709e-03\n",
      "Epoch: 73910 | training loss: 2.4334e-03 | validation loss: 2.3713e-03\n",
      "Epoch: 73920 | training loss: 2.4353e-03 | validation loss: 2.3771e-03\n",
      "Epoch: 73930 | training loss: 2.7231e-03 | validation loss: 2.5781e-03\n",
      "Epoch: 73940 | training loss: 2.4870e-03 | validation loss: 2.3788e-03\n",
      "Epoch: 73950 | training loss: 2.5763e-03 | validation loss: 2.4988e-03\n",
      "Epoch: 73960 | training loss: 2.5408e-03 | validation loss: 2.4723e-03\n",
      "Epoch: 73970 | training loss: 2.4850e-03 | validation loss: 2.4294e-03\n",
      "Epoch: 73980 | training loss: 2.4511e-03 | validation loss: 2.3994e-03\n",
      "Epoch: 73990 | training loss: 2.4358e-03 | validation loss: 2.3797e-03\n",
      "Epoch: 74000 | training loss: 2.4329e-03 | validation loss: 2.3693e-03\n",
      "Epoch: 74010 | training loss: 2.4335e-03 | validation loss: 2.3661e-03\n",
      "Epoch: 74020 | training loss: 2.4330e-03 | validation loss: 2.3673e-03\n",
      "Epoch: 74030 | training loss: 2.4327e-03 | validation loss: 2.3698e-03\n",
      "Epoch: 74040 | training loss: 2.4327e-03 | validation loss: 2.3701e-03\n",
      "Epoch: 74050 | training loss: 2.4326e-03 | validation loss: 2.3690e-03\n",
      "Epoch: 74060 | training loss: 2.4325e-03 | validation loss: 2.3689e-03\n",
      "Epoch: 74070 | training loss: 2.4325e-03 | validation loss: 2.3691e-03\n",
      "Epoch: 74080 | training loss: 2.4324e-03 | validation loss: 2.3687e-03\n",
      "Epoch: 74090 | training loss: 2.4324e-03 | validation loss: 2.3688e-03\n",
      "Epoch: 74100 | training loss: 2.4323e-03 | validation loss: 2.3686e-03\n",
      "Epoch: 74110 | training loss: 2.4322e-03 | validation loss: 2.3685e-03\n",
      "Epoch: 74120 | training loss: 2.4322e-03 | validation loss: 2.3683e-03\n",
      "Epoch: 74130 | training loss: 2.4321e-03 | validation loss: 2.3682e-03\n",
      "Epoch: 74140 | training loss: 2.4321e-03 | validation loss: 2.3681e-03\n",
      "Epoch: 74150 | training loss: 2.4320e-03 | validation loss: 2.3679e-03\n",
      "Epoch: 74160 | training loss: 2.4319e-03 | validation loss: 2.3678e-03\n",
      "Epoch: 74170 | training loss: 2.4319e-03 | validation loss: 2.3676e-03\n",
      "Epoch: 74180 | training loss: 2.4318e-03 | validation loss: 2.3674e-03\n",
      "Epoch: 74190 | training loss: 2.4318e-03 | validation loss: 2.3664e-03\n",
      "Epoch: 74200 | training loss: 2.4406e-03 | validation loss: 2.3590e-03\n",
      "Epoch: 74210 | training loss: 4.1307e-03 | validation loss: 2.9945e-03\n",
      "Epoch: 74220 | training loss: 2.9541e-03 | validation loss: 2.7161e-03\n",
      "Epoch: 74230 | training loss: 2.6484e-03 | validation loss: 2.5250e-03\n",
      "Epoch: 74240 | training loss: 2.4913e-03 | validation loss: 2.4170e-03\n",
      "Epoch: 74250 | training loss: 2.4474e-03 | validation loss: 2.3822e-03\n",
      "Epoch: 74260 | training loss: 2.4366e-03 | validation loss: 2.3735e-03\n",
      "Epoch: 74270 | training loss: 2.4337e-03 | validation loss: 2.3719e-03\n",
      "Epoch: 74280 | training loss: 2.4326e-03 | validation loss: 2.3704e-03\n",
      "Epoch: 74290 | training loss: 2.4319e-03 | validation loss: 2.3684e-03\n",
      "Epoch: 74300 | training loss: 2.4314e-03 | validation loss: 2.3673e-03\n",
      "Epoch: 74310 | training loss: 2.4311e-03 | validation loss: 2.3664e-03\n",
      "Epoch: 74320 | training loss: 2.4310e-03 | validation loss: 2.3655e-03\n",
      "Epoch: 74330 | training loss: 2.4310e-03 | validation loss: 2.3655e-03\n",
      "Epoch: 74340 | training loss: 2.4309e-03 | validation loss: 2.3655e-03\n",
      "Epoch: 74350 | training loss: 2.4308e-03 | validation loss: 2.3656e-03\n",
      "Epoch: 74360 | training loss: 2.4308e-03 | validation loss: 2.3654e-03\n",
      "Epoch: 74370 | training loss: 2.4307e-03 | validation loss: 2.3652e-03\n",
      "Epoch: 74380 | training loss: 2.4307e-03 | validation loss: 2.3651e-03\n",
      "Epoch: 74390 | training loss: 2.4306e-03 | validation loss: 2.3650e-03\n",
      "Epoch: 74400 | training loss: 2.4305e-03 | validation loss: 2.3648e-03\n",
      "Epoch: 74410 | training loss: 2.4305e-03 | validation loss: 2.3647e-03\n",
      "Epoch: 74420 | training loss: 2.4304e-03 | validation loss: 2.3647e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74430 | training loss: 2.4304e-03 | validation loss: 2.3662e-03\n",
      "Epoch: 74440 | training loss: 2.4436e-03 | validation loss: 2.3964e-03\n",
      "Epoch: 74450 | training loss: 3.4591e-03 | validation loss: 3.2099e-03\n",
      "Epoch: 74460 | training loss: 2.6681e-03 | validation loss: 2.4834e-03\n",
      "Epoch: 74470 | training loss: 2.4923e-03 | validation loss: 2.4510e-03\n",
      "Epoch: 74480 | training loss: 2.4472e-03 | validation loss: 2.3815e-03\n",
      "Epoch: 74490 | training loss: 2.4358e-03 | validation loss: 2.3758e-03\n",
      "Epoch: 74500 | training loss: 2.4318e-03 | validation loss: 2.3702e-03\n",
      "Epoch: 74510 | training loss: 2.4303e-03 | validation loss: 2.3657e-03\n",
      "Epoch: 74520 | training loss: 2.4300e-03 | validation loss: 2.3612e-03\n",
      "Epoch: 74530 | training loss: 2.4299e-03 | validation loss: 2.3640e-03\n",
      "Epoch: 74540 | training loss: 2.4298e-03 | validation loss: 2.3616e-03\n",
      "Epoch: 74550 | training loss: 2.4297e-03 | validation loss: 2.3608e-03\n",
      "Epoch: 74560 | training loss: 2.4297e-03 | validation loss: 2.3606e-03\n",
      "Epoch: 74570 | training loss: 2.4304e-03 | validation loss: 2.3578e-03\n",
      "Epoch: 74580 | training loss: 2.4536e-03 | validation loss: 2.3471e-03\n",
      "Epoch: 74590 | training loss: 3.3261e-03 | validation loss: 2.6172e-03\n",
      "Epoch: 74600 | training loss: 2.8360e-03 | validation loss: 2.6461e-03\n",
      "Epoch: 74610 | training loss: 2.5025e-03 | validation loss: 2.3595e-03\n",
      "Epoch: 74620 | training loss: 2.4426e-03 | validation loss: 2.3538e-03\n",
      "Epoch: 74630 | training loss: 2.4503e-03 | validation loss: 2.3940e-03\n",
      "Epoch: 74640 | training loss: 2.4318e-03 | validation loss: 2.3550e-03\n",
      "Epoch: 74650 | training loss: 2.4291e-03 | validation loss: 2.3601e-03\n",
      "Epoch: 74660 | training loss: 2.4293e-03 | validation loss: 2.3630e-03\n",
      "Epoch: 74670 | training loss: 2.4291e-03 | validation loss: 2.3595e-03\n",
      "Epoch: 74680 | training loss: 2.4289e-03 | validation loss: 2.3617e-03\n",
      "Epoch: 74690 | training loss: 2.4288e-03 | validation loss: 2.3608e-03\n",
      "Epoch: 74700 | training loss: 2.4288e-03 | validation loss: 2.3599e-03\n",
      "Epoch: 74710 | training loss: 2.4287e-03 | validation loss: 2.3608e-03\n",
      "Epoch: 74720 | training loss: 2.4287e-03 | validation loss: 2.3608e-03\n",
      "Epoch: 74730 | training loss: 2.4286e-03 | validation loss: 2.3609e-03\n",
      "Epoch: 74740 | training loss: 2.4287e-03 | validation loss: 2.3619e-03\n",
      "Epoch: 74750 | training loss: 2.4324e-03 | validation loss: 2.3710e-03\n",
      "Epoch: 74760 | training loss: 2.6692e-03 | validation loss: 2.5439e-03\n",
      "Epoch: 74770 | training loss: 2.5186e-03 | validation loss: 2.4389e-03\n",
      "Epoch: 74780 | training loss: 2.6786e-03 | validation loss: 2.5502e-03\n",
      "Epoch: 74790 | training loss: 2.4392e-03 | validation loss: 2.3601e-03\n",
      "Epoch: 74800 | training loss: 2.4654e-03 | validation loss: 2.3464e-03\n",
      "Epoch: 74810 | training loss: 2.4294e-03 | validation loss: 2.3573e-03\n",
      "Epoch: 74820 | training loss: 2.4326e-03 | validation loss: 2.3731e-03\n",
      "Epoch: 74830 | training loss: 2.4288e-03 | validation loss: 2.3566e-03\n",
      "Epoch: 74840 | training loss: 2.4281e-03 | validation loss: 2.3565e-03\n",
      "Epoch: 74850 | training loss: 2.4281e-03 | validation loss: 2.3620e-03\n",
      "Epoch: 74860 | training loss: 2.4279e-03 | validation loss: 2.3568e-03\n",
      "Epoch: 74870 | training loss: 2.4278e-03 | validation loss: 2.3599e-03\n",
      "Epoch: 74880 | training loss: 2.4277e-03 | validation loss: 2.3577e-03\n",
      "Epoch: 74890 | training loss: 2.4277e-03 | validation loss: 2.3588e-03\n",
      "Epoch: 74900 | training loss: 2.4276e-03 | validation loss: 2.3583e-03\n",
      "Epoch: 74910 | training loss: 2.4276e-03 | validation loss: 2.3579e-03\n",
      "Epoch: 74920 | training loss: 2.4275e-03 | validation loss: 2.3579e-03\n",
      "Epoch: 74930 | training loss: 2.4274e-03 | validation loss: 2.3577e-03\n",
      "Epoch: 74940 | training loss: 2.4274e-03 | validation loss: 2.3571e-03\n",
      "Epoch: 74950 | training loss: 2.4318e-03 | validation loss: 2.3547e-03\n",
      "Epoch: 74960 | training loss: 2.8233e-03 | validation loss: 2.5768e-03\n",
      "Epoch: 74970 | training loss: 2.8418e-03 | validation loss: 2.7442e-03\n",
      "Epoch: 74980 | training loss: 2.6476e-03 | validation loss: 2.5757e-03\n",
      "Epoch: 74990 | training loss: 2.5023e-03 | validation loss: 2.3668e-03\n",
      "Epoch: 75000 | training loss: 2.4534e-03 | validation loss: 2.4011e-03\n",
      "Epoch: 75010 | training loss: 2.4395e-03 | validation loss: 2.3437e-03\n",
      "Epoch: 75020 | training loss: 2.4333e-03 | validation loss: 2.3704e-03\n",
      "Epoch: 75030 | training loss: 2.4280e-03 | validation loss: 2.3499e-03\n",
      "Epoch: 75040 | training loss: 2.4277e-03 | validation loss: 2.3503e-03\n",
      "Epoch: 75050 | training loss: 2.4268e-03 | validation loss: 2.3569e-03\n",
      "Epoch: 75060 | training loss: 2.4272e-03 | validation loss: 2.3605e-03\n",
      "Epoch: 75070 | training loss: 2.4311e-03 | validation loss: 2.3694e-03\n",
      "Epoch: 75080 | training loss: 2.5292e-03 | validation loss: 2.4564e-03\n",
      "Epoch: 75090 | training loss: 3.1226e-03 | validation loss: 2.8092e-03\n",
      "Epoch: 75100 | training loss: 2.5841e-03 | validation loss: 2.3732e-03\n",
      "Epoch: 75110 | training loss: 2.4324e-03 | validation loss: 2.3708e-03\n",
      "Epoch: 75120 | training loss: 2.4298e-03 | validation loss: 2.3644e-03\n",
      "Epoch: 75130 | training loss: 2.4316e-03 | validation loss: 2.3454e-03\n",
      "Epoch: 75140 | training loss: 2.4289e-03 | validation loss: 2.3638e-03\n",
      "Epoch: 75150 | training loss: 2.4269e-03 | validation loss: 2.3515e-03\n",
      "Epoch: 75160 | training loss: 2.4261e-03 | validation loss: 2.3554e-03\n",
      "Epoch: 75170 | training loss: 2.4262e-03 | validation loss: 2.3564e-03\n",
      "Epoch: 75180 | training loss: 2.4261e-03 | validation loss: 2.3532e-03\n",
      "Epoch: 75190 | training loss: 2.4260e-03 | validation loss: 2.3531e-03\n",
      "Epoch: 75200 | training loss: 2.4259e-03 | validation loss: 2.3530e-03\n",
      "Epoch: 75210 | training loss: 2.4263e-03 | validation loss: 2.3511e-03\n",
      "Epoch: 75220 | training loss: 2.4380e-03 | validation loss: 2.3431e-03\n",
      "Epoch: 75230 | training loss: 3.0071e-03 | validation loss: 2.5101e-03\n",
      "Epoch: 75240 | training loss: 2.6064e-03 | validation loss: 2.5060e-03\n",
      "Epoch: 75250 | training loss: 2.6204e-03 | validation loss: 2.3858e-03\n",
      "Epoch: 75260 | training loss: 2.4287e-03 | validation loss: 2.3432e-03\n",
      "Epoch: 75270 | training loss: 2.4506e-03 | validation loss: 2.3869e-03\n",
      "Epoch: 75280 | training loss: 2.4292e-03 | validation loss: 2.3520e-03\n",
      "Epoch: 75290 | training loss: 2.4259e-03 | validation loss: 2.3484e-03\n",
      "Epoch: 75300 | training loss: 2.4261e-03 | validation loss: 2.3581e-03\n",
      "Epoch: 75310 | training loss: 2.4256e-03 | validation loss: 2.3507e-03\n",
      "Epoch: 75320 | training loss: 2.4253e-03 | validation loss: 2.3543e-03\n",
      "Epoch: 75330 | training loss: 2.4252e-03 | validation loss: 2.3519e-03\n",
      "Epoch: 75340 | training loss: 2.4251e-03 | validation loss: 2.3532e-03\n",
      "Epoch: 75350 | training loss: 2.4250e-03 | validation loss: 2.3523e-03\n",
      "Epoch: 75360 | training loss: 2.4249e-03 | validation loss: 2.3523e-03\n",
      "Epoch: 75370 | training loss: 2.4249e-03 | validation loss: 2.3523e-03\n",
      "Epoch: 75380 | training loss: 2.4248e-03 | validation loss: 2.3522e-03\n",
      "Epoch: 75390 | training loss: 2.4248e-03 | validation loss: 2.3520e-03\n",
      "Epoch: 75400 | training loss: 2.4273e-03 | validation loss: 2.3525e-03\n",
      "Epoch: 75410 | training loss: 2.6012e-03 | validation loss: 2.4408e-03\n",
      "Epoch: 75420 | training loss: 3.3522e-03 | validation loss: 2.6386e-03\n",
      "Epoch: 75430 | training loss: 2.4710e-03 | validation loss: 2.4187e-03\n",
      "Epoch: 75440 | training loss: 2.4710e-03 | validation loss: 2.4181e-03\n",
      "Epoch: 75450 | training loss: 2.4441e-03 | validation loss: 2.3539e-03\n",
      "Epoch: 75460 | training loss: 2.4276e-03 | validation loss: 2.3588e-03\n",
      "Epoch: 75470 | training loss: 2.4282e-03 | validation loss: 2.3657e-03\n",
      "Epoch: 75480 | training loss: 2.4263e-03 | validation loss: 2.3515e-03\n",
      "Epoch: 75490 | training loss: 2.4249e-03 | validation loss: 2.3562e-03\n",
      "Epoch: 75500 | training loss: 2.4243e-03 | validation loss: 2.3500e-03\n",
      "Epoch: 75510 | training loss: 2.4241e-03 | validation loss: 2.3511e-03\n",
      "Epoch: 75520 | training loss: 2.4241e-03 | validation loss: 2.3488e-03\n",
      "Epoch: 75530 | training loss: 2.4240e-03 | validation loss: 2.3500e-03\n",
      "Epoch: 75540 | training loss: 2.4239e-03 | validation loss: 2.3503e-03\n",
      "Epoch: 75550 | training loss: 2.4238e-03 | validation loss: 2.3498e-03\n",
      "Epoch: 75560 | training loss: 2.4238e-03 | validation loss: 2.3494e-03\n",
      "Epoch: 75570 | training loss: 2.4237e-03 | validation loss: 2.3492e-03\n",
      "Epoch: 75580 | training loss: 2.4238e-03 | validation loss: 2.3475e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75590 | training loss: 2.4348e-03 | validation loss: 2.3380e-03\n",
      "Epoch: 75600 | training loss: 3.5691e-03 | validation loss: 2.7140e-03\n",
      "Epoch: 75610 | training loss: 3.2043e-03 | validation loss: 2.8388e-03\n",
      "Epoch: 75620 | training loss: 2.5288e-03 | validation loss: 2.4518e-03\n",
      "Epoch: 75630 | training loss: 2.4294e-03 | validation loss: 2.3598e-03\n",
      "Epoch: 75640 | training loss: 2.4456e-03 | validation loss: 2.3478e-03\n",
      "Epoch: 75650 | training loss: 2.4338e-03 | validation loss: 2.3424e-03\n",
      "Epoch: 75660 | training loss: 2.4233e-03 | validation loss: 2.3463e-03\n",
      "Epoch: 75670 | training loss: 2.4246e-03 | validation loss: 2.3520e-03\n",
      "Epoch: 75680 | training loss: 2.4232e-03 | validation loss: 2.3492e-03\n",
      "Epoch: 75690 | training loss: 2.4232e-03 | validation loss: 2.3471e-03\n",
      "Epoch: 75700 | training loss: 2.4230e-03 | validation loss: 2.3484e-03\n",
      "Epoch: 75710 | training loss: 2.4230e-03 | validation loss: 2.3483e-03\n",
      "Epoch: 75720 | training loss: 2.4229e-03 | validation loss: 2.3475e-03\n",
      "Epoch: 75730 | training loss: 2.4228e-03 | validation loss: 2.3480e-03\n",
      "Epoch: 75740 | training loss: 2.4228e-03 | validation loss: 2.3475e-03\n",
      "Epoch: 75750 | training loss: 2.4227e-03 | validation loss: 2.3476e-03\n",
      "Epoch: 75760 | training loss: 2.4226e-03 | validation loss: 2.3473e-03\n",
      "Epoch: 75770 | training loss: 2.4226e-03 | validation loss: 2.3473e-03\n",
      "Epoch: 75780 | training loss: 2.4225e-03 | validation loss: 2.3472e-03\n",
      "Epoch: 75790 | training loss: 2.4225e-03 | validation loss: 2.3470e-03\n",
      "Epoch: 75800 | training loss: 2.4224e-03 | validation loss: 2.3469e-03\n",
      "Epoch: 75810 | training loss: 2.4223e-03 | validation loss: 2.3467e-03\n",
      "Epoch: 75820 | training loss: 2.4223e-03 | validation loss: 2.3463e-03\n",
      "Epoch: 75830 | training loss: 2.4225e-03 | validation loss: 2.3444e-03\n",
      "Epoch: 75840 | training loss: 2.4651e-03 | validation loss: 2.3414e-03\n",
      "Epoch: 75850 | training loss: 4.6259e-03 | validation loss: 3.1794e-03\n",
      "Epoch: 75860 | training loss: 2.6211e-03 | validation loss: 2.4498e-03\n",
      "Epoch: 75870 | training loss: 2.4676e-03 | validation loss: 2.4010e-03\n",
      "Epoch: 75880 | training loss: 2.4363e-03 | validation loss: 2.3686e-03\n",
      "Epoch: 75890 | training loss: 2.4254e-03 | validation loss: 2.3473e-03\n",
      "Epoch: 75900 | training loss: 2.4230e-03 | validation loss: 2.3414e-03\n",
      "Epoch: 75910 | training loss: 2.4218e-03 | validation loss: 2.3447e-03\n",
      "Epoch: 75920 | training loss: 2.4219e-03 | validation loss: 2.3471e-03\n",
      "Epoch: 75930 | training loss: 2.4219e-03 | validation loss: 2.3450e-03\n",
      "Epoch: 75940 | training loss: 2.4218e-03 | validation loss: 2.3449e-03\n",
      "Epoch: 75950 | training loss: 2.4216e-03 | validation loss: 2.3453e-03\n",
      "Epoch: 75960 | training loss: 2.4215e-03 | validation loss: 2.3446e-03\n",
      "Epoch: 75970 | training loss: 2.4214e-03 | validation loss: 2.3449e-03\n",
      "Epoch: 75980 | training loss: 2.4214e-03 | validation loss: 2.3446e-03\n",
      "Epoch: 75990 | training loss: 2.4213e-03 | validation loss: 2.3446e-03\n",
      "Epoch: 76000 | training loss: 2.4213e-03 | validation loss: 2.3444e-03\n",
      "Epoch: 76010 | training loss: 2.4212e-03 | validation loss: 2.3443e-03\n",
      "Epoch: 76020 | training loss: 2.4211e-03 | validation loss: 2.3442e-03\n",
      "Epoch: 76030 | training loss: 2.4211e-03 | validation loss: 2.3440e-03\n",
      "Epoch: 76040 | training loss: 2.4210e-03 | validation loss: 2.3439e-03\n",
      "Epoch: 76050 | training loss: 2.4210e-03 | validation loss: 2.3438e-03\n",
      "Epoch: 76060 | training loss: 2.4209e-03 | validation loss: 2.3440e-03\n",
      "Epoch: 76070 | training loss: 2.4212e-03 | validation loss: 2.3476e-03\n",
      "Epoch: 76080 | training loss: 2.5118e-03 | validation loss: 2.4598e-03\n",
      "Epoch: 76090 | training loss: 2.7243e-03 | validation loss: 2.5528e-03\n",
      "Epoch: 76100 | training loss: 2.4589e-03 | validation loss: 2.3901e-03\n",
      "Epoch: 76110 | training loss: 2.4739e-03 | validation loss: 2.3269e-03\n",
      "Epoch: 76120 | training loss: 2.4623e-03 | validation loss: 2.3302e-03\n",
      "Epoch: 76130 | training loss: 2.4308e-03 | validation loss: 2.3429e-03\n",
      "Epoch: 76140 | training loss: 2.4207e-03 | validation loss: 2.3423e-03\n",
      "Epoch: 76150 | training loss: 2.4212e-03 | validation loss: 2.3414e-03\n",
      "Epoch: 76160 | training loss: 2.4208e-03 | validation loss: 2.3462e-03\n",
      "Epoch: 76170 | training loss: 2.4205e-03 | validation loss: 2.3432e-03\n",
      "Epoch: 76180 | training loss: 2.4203e-03 | validation loss: 2.3429e-03\n",
      "Epoch: 76190 | training loss: 2.4202e-03 | validation loss: 2.3421e-03\n",
      "Epoch: 76200 | training loss: 2.4201e-03 | validation loss: 2.3419e-03\n",
      "Epoch: 76210 | training loss: 2.4201e-03 | validation loss: 2.3412e-03\n",
      "Epoch: 76220 | training loss: 2.4200e-03 | validation loss: 2.3412e-03\n",
      "Epoch: 76230 | training loss: 2.4200e-03 | validation loss: 2.3410e-03\n",
      "Epoch: 76240 | training loss: 2.4199e-03 | validation loss: 2.3409e-03\n",
      "Epoch: 76250 | training loss: 2.4199e-03 | validation loss: 2.3408e-03\n",
      "Epoch: 76260 | training loss: 2.4198e-03 | validation loss: 2.3409e-03\n",
      "Epoch: 76270 | training loss: 2.4199e-03 | validation loss: 2.3427e-03\n",
      "Epoch: 76280 | training loss: 2.4407e-03 | validation loss: 2.3709e-03\n",
      "Epoch: 76290 | training loss: 4.3856e-03 | validation loss: 3.4548e-03\n",
      "Epoch: 76300 | training loss: 2.5885e-03 | validation loss: 2.3638e-03\n",
      "Epoch: 76310 | training loss: 2.6222e-03 | validation loss: 2.3720e-03\n",
      "Epoch: 76320 | training loss: 2.5088e-03 | validation loss: 2.3391e-03\n",
      "Epoch: 76330 | training loss: 2.4438e-03 | validation loss: 2.3291e-03\n",
      "Epoch: 76340 | training loss: 2.4213e-03 | validation loss: 2.3347e-03\n",
      "Epoch: 76350 | training loss: 2.4199e-03 | validation loss: 2.3434e-03\n",
      "Epoch: 76360 | training loss: 2.4207e-03 | validation loss: 2.3456e-03\n",
      "Epoch: 76370 | training loss: 2.4194e-03 | validation loss: 2.3416e-03\n",
      "Epoch: 76380 | training loss: 2.4192e-03 | validation loss: 2.3382e-03\n",
      "Epoch: 76390 | training loss: 2.4191e-03 | validation loss: 2.3386e-03\n",
      "Epoch: 76400 | training loss: 2.4191e-03 | validation loss: 2.3399e-03\n",
      "Epoch: 76410 | training loss: 2.4190e-03 | validation loss: 2.3392e-03\n",
      "Epoch: 76420 | training loss: 2.4189e-03 | validation loss: 2.3388e-03\n",
      "Epoch: 76430 | training loss: 2.4189e-03 | validation loss: 2.3391e-03\n",
      "Epoch: 76440 | training loss: 2.4188e-03 | validation loss: 2.3387e-03\n",
      "Epoch: 76450 | training loss: 2.4187e-03 | validation loss: 2.3388e-03\n",
      "Epoch: 76460 | training loss: 2.4187e-03 | validation loss: 2.3386e-03\n",
      "Epoch: 76470 | training loss: 2.4186e-03 | validation loss: 2.3385e-03\n",
      "Epoch: 76480 | training loss: 2.4186e-03 | validation loss: 2.3384e-03\n",
      "Epoch: 76490 | training loss: 2.4185e-03 | validation loss: 2.3382e-03\n",
      "Epoch: 76500 | training loss: 2.4185e-03 | validation loss: 2.3381e-03\n",
      "Epoch: 76510 | training loss: 2.4184e-03 | validation loss: 2.3380e-03\n",
      "Epoch: 76520 | training loss: 2.4183e-03 | validation loss: 2.3378e-03\n",
      "Epoch: 76530 | training loss: 2.4183e-03 | validation loss: 2.3377e-03\n",
      "Epoch: 76540 | training loss: 2.4182e-03 | validation loss: 2.3373e-03\n",
      "Epoch: 76550 | training loss: 2.4189e-03 | validation loss: 2.3341e-03\n",
      "Epoch: 76560 | training loss: 2.6240e-03 | validation loss: 2.3710e-03\n",
      "Epoch: 76570 | training loss: 2.4582e-03 | validation loss: 2.3853e-03\n",
      "Epoch: 76580 | training loss: 2.4185e-03 | validation loss: 2.3386e-03\n",
      "Epoch: 76590 | training loss: 2.4563e-03 | validation loss: 2.3806e-03\n",
      "Epoch: 76600 | training loss: 2.4658e-03 | validation loss: 2.3881e-03\n",
      "Epoch: 76610 | training loss: 2.4433e-03 | validation loss: 2.3703e-03\n",
      "Epoch: 76620 | training loss: 2.4274e-03 | validation loss: 2.3545e-03\n",
      "Epoch: 76630 | training loss: 2.4210e-03 | validation loss: 2.3458e-03\n",
      "Epoch: 76640 | training loss: 2.4189e-03 | validation loss: 2.3417e-03\n",
      "Epoch: 76650 | training loss: 2.4181e-03 | validation loss: 2.3395e-03\n",
      "Epoch: 76660 | training loss: 2.4177e-03 | validation loss: 2.3380e-03\n",
      "Epoch: 76670 | training loss: 2.4176e-03 | validation loss: 2.3369e-03\n",
      "Epoch: 76680 | training loss: 2.4175e-03 | validation loss: 2.3362e-03\n",
      "Epoch: 76690 | training loss: 2.4174e-03 | validation loss: 2.3357e-03\n",
      "Epoch: 76700 | training loss: 2.4173e-03 | validation loss: 2.3356e-03\n",
      "Epoch: 76710 | training loss: 2.4173e-03 | validation loss: 2.3357e-03\n",
      "Epoch: 76720 | training loss: 2.4172e-03 | validation loss: 2.3357e-03\n",
      "Epoch: 76730 | training loss: 2.4172e-03 | validation loss: 2.3355e-03\n",
      "Epoch: 76740 | training loss: 2.4171e-03 | validation loss: 2.3354e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76750 | training loss: 2.4171e-03 | validation loss: 2.3353e-03\n",
      "Epoch: 76760 | training loss: 2.4170e-03 | validation loss: 2.3351e-03\n",
      "Epoch: 76770 | training loss: 2.4169e-03 | validation loss: 2.3350e-03\n",
      "Epoch: 76780 | training loss: 2.4169e-03 | validation loss: 2.3349e-03\n",
      "Epoch: 76790 | training loss: 2.4168e-03 | validation loss: 2.3347e-03\n",
      "Epoch: 76800 | training loss: 2.4168e-03 | validation loss: 2.3346e-03\n",
      "Epoch: 76810 | training loss: 2.4167e-03 | validation loss: 2.3345e-03\n",
      "Epoch: 76820 | training loss: 2.4166e-03 | validation loss: 2.3343e-03\n",
      "Epoch: 76830 | training loss: 2.4166e-03 | validation loss: 2.3338e-03\n",
      "Epoch: 76840 | training loss: 2.4183e-03 | validation loss: 2.3284e-03\n",
      "Epoch: 76850 | training loss: 3.2426e-03 | validation loss: 2.7712e-03\n",
      "Epoch: 76860 | training loss: 2.4210e-03 | validation loss: 2.3244e-03\n",
      "Epoch: 76870 | training loss: 2.4817e-03 | validation loss: 2.3605e-03\n",
      "Epoch: 76880 | training loss: 2.4704e-03 | validation loss: 2.3336e-03\n",
      "Epoch: 76890 | training loss: 2.4245e-03 | validation loss: 2.3387e-03\n",
      "Epoch: 76900 | training loss: 2.4175e-03 | validation loss: 2.3309e-03\n",
      "Epoch: 76910 | training loss: 2.4180e-03 | validation loss: 2.3352e-03\n",
      "Epoch: 76920 | training loss: 2.4170e-03 | validation loss: 2.3372e-03\n",
      "Epoch: 76930 | training loss: 2.4167e-03 | validation loss: 2.3376e-03\n",
      "Epoch: 76940 | training loss: 2.4201e-03 | validation loss: 2.3454e-03\n",
      "Epoch: 76950 | training loss: 2.5141e-03 | validation loss: 2.4302e-03\n",
      "Epoch: 76960 | training loss: 3.1137e-03 | validation loss: 2.7891e-03\n",
      "Epoch: 76970 | training loss: 2.5522e-03 | validation loss: 2.3339e-03\n",
      "Epoch: 76980 | training loss: 2.4163e-03 | validation loss: 2.3346e-03\n",
      "Epoch: 76990 | training loss: 2.4266e-03 | validation loss: 2.3523e-03\n",
      "Epoch: 77000 | training loss: 2.4254e-03 | validation loss: 2.3196e-03\n",
      "Epoch: 77010 | training loss: 2.4200e-03 | validation loss: 2.3437e-03\n",
      "Epoch: 77020 | training loss: 2.4172e-03 | validation loss: 2.3253e-03\n",
      "Epoch: 77030 | training loss: 2.4160e-03 | validation loss: 2.3348e-03\n",
      "Epoch: 77040 | training loss: 2.4154e-03 | validation loss: 2.3304e-03\n",
      "Epoch: 77050 | training loss: 2.4155e-03 | validation loss: 2.3294e-03\n",
      "Epoch: 77060 | training loss: 2.4153e-03 | validation loss: 2.3313e-03\n",
      "Epoch: 77070 | training loss: 2.4153e-03 | validation loss: 2.3320e-03\n",
      "Epoch: 77080 | training loss: 2.4155e-03 | validation loss: 2.3334e-03\n",
      "Epoch: 77090 | training loss: 2.4199e-03 | validation loss: 2.3436e-03\n",
      "Epoch: 77100 | training loss: 2.6063e-03 | validation loss: 2.4879e-03\n",
      "Epoch: 77110 | training loss: 2.6947e-03 | validation loss: 2.5359e-03\n",
      "Epoch: 77120 | training loss: 2.4429e-03 | validation loss: 2.3671e-03\n",
      "Epoch: 77130 | training loss: 2.5077e-03 | validation loss: 2.3285e-03\n",
      "Epoch: 77140 | training loss: 2.4202e-03 | validation loss: 2.3434e-03\n",
      "Epoch: 77150 | training loss: 2.4188e-03 | validation loss: 2.3416e-03\n",
      "Epoch: 77160 | training loss: 2.4192e-03 | validation loss: 2.3226e-03\n",
      "Epoch: 77170 | training loss: 2.4163e-03 | validation loss: 2.3361e-03\n",
      "Epoch: 77180 | training loss: 2.4151e-03 | validation loss: 2.3265e-03\n",
      "Epoch: 77190 | training loss: 2.4148e-03 | validation loss: 2.3314e-03\n",
      "Epoch: 77200 | training loss: 2.4146e-03 | validation loss: 2.3279e-03\n",
      "Epoch: 77210 | training loss: 2.4145e-03 | validation loss: 2.3300e-03\n",
      "Epoch: 77220 | training loss: 2.4144e-03 | validation loss: 2.3292e-03\n",
      "Epoch: 77230 | training loss: 2.4144e-03 | validation loss: 2.3285e-03\n",
      "Epoch: 77240 | training loss: 2.4143e-03 | validation loss: 2.3284e-03\n",
      "Epoch: 77250 | training loss: 2.4143e-03 | validation loss: 2.3277e-03\n",
      "Epoch: 77260 | training loss: 2.4152e-03 | validation loss: 2.3246e-03\n",
      "Epoch: 77270 | training loss: 2.4591e-03 | validation loss: 2.3186e-03\n",
      "Epoch: 77280 | training loss: 3.8581e-03 | validation loss: 2.8309e-03\n",
      "Epoch: 77290 | training loss: 2.6545e-03 | validation loss: 2.5052e-03\n",
      "Epoch: 77300 | training loss: 2.5064e-03 | validation loss: 2.4154e-03\n",
      "Epoch: 77310 | training loss: 2.4322e-03 | validation loss: 2.3220e-03\n",
      "Epoch: 77320 | training loss: 2.4288e-03 | validation loss: 2.3173e-03\n",
      "Epoch: 77330 | training loss: 2.4173e-03 | validation loss: 2.3353e-03\n",
      "Epoch: 77340 | training loss: 2.4144e-03 | validation loss: 2.3320e-03\n",
      "Epoch: 77350 | training loss: 2.4147e-03 | validation loss: 2.3243e-03\n",
      "Epoch: 77360 | training loss: 2.4139e-03 | validation loss: 2.3292e-03\n",
      "Epoch: 77370 | training loss: 2.4136e-03 | validation loss: 2.3270e-03\n",
      "Epoch: 77380 | training loss: 2.4135e-03 | validation loss: 2.3274e-03\n",
      "Epoch: 77390 | training loss: 2.4135e-03 | validation loss: 2.3270e-03\n",
      "Epoch: 77400 | training loss: 2.4134e-03 | validation loss: 2.3272e-03\n",
      "Epoch: 77410 | training loss: 2.4134e-03 | validation loss: 2.3267e-03\n",
      "Epoch: 77420 | training loss: 2.4133e-03 | validation loss: 2.3268e-03\n",
      "Epoch: 77430 | training loss: 2.4132e-03 | validation loss: 2.3268e-03\n",
      "Epoch: 77440 | training loss: 2.4132e-03 | validation loss: 2.3267e-03\n",
      "Epoch: 77450 | training loss: 2.4131e-03 | validation loss: 2.3268e-03\n",
      "Epoch: 77460 | training loss: 2.4134e-03 | validation loss: 2.3283e-03\n",
      "Epoch: 77470 | training loss: 2.4308e-03 | validation loss: 2.3495e-03\n",
      "Epoch: 77480 | training loss: 3.7093e-03 | validation loss: 3.0820e-03\n",
      "Epoch: 77490 | training loss: 2.9741e-03 | validation loss: 2.5054e-03\n",
      "Epoch: 77500 | training loss: 2.5746e-03 | validation loss: 2.4603e-03\n",
      "Epoch: 77510 | training loss: 2.4526e-03 | validation loss: 2.3833e-03\n",
      "Epoch: 77520 | training loss: 2.4158e-03 | validation loss: 2.3214e-03\n",
      "Epoch: 77530 | training loss: 2.4135e-03 | validation loss: 2.3204e-03\n",
      "Epoch: 77540 | training loss: 2.4150e-03 | validation loss: 2.3290e-03\n",
      "Epoch: 77550 | training loss: 2.4138e-03 | validation loss: 2.3219e-03\n",
      "Epoch: 77560 | training loss: 2.4126e-03 | validation loss: 2.3253e-03\n",
      "Epoch: 77570 | training loss: 2.4125e-03 | validation loss: 2.3249e-03\n",
      "Epoch: 77580 | training loss: 2.4124e-03 | validation loss: 2.3254e-03\n",
      "Epoch: 77590 | training loss: 2.4124e-03 | validation loss: 2.3241e-03\n",
      "Epoch: 77600 | training loss: 2.4123e-03 | validation loss: 2.3246e-03\n",
      "Epoch: 77610 | training loss: 2.4122e-03 | validation loss: 2.3245e-03\n",
      "Epoch: 77620 | training loss: 2.4122e-03 | validation loss: 2.3241e-03\n",
      "Epoch: 77630 | training loss: 2.4121e-03 | validation loss: 2.3239e-03\n",
      "Epoch: 77640 | training loss: 2.4121e-03 | validation loss: 2.3234e-03\n",
      "Epoch: 77650 | training loss: 2.4122e-03 | validation loss: 2.3211e-03\n",
      "Epoch: 77660 | training loss: 2.4245e-03 | validation loss: 2.3075e-03\n",
      "Epoch: 77670 | training loss: 3.3696e-03 | validation loss: 2.6137e-03\n",
      "Epoch: 77680 | training loss: 2.7987e-03 | validation loss: 2.6348e-03\n",
      "Epoch: 77690 | training loss: 2.5548e-03 | validation loss: 2.4639e-03\n",
      "Epoch: 77700 | training loss: 2.4350e-03 | validation loss: 2.3588e-03\n",
      "Epoch: 77710 | training loss: 2.4181e-03 | validation loss: 2.3389e-03\n",
      "Epoch: 77720 | training loss: 2.4141e-03 | validation loss: 2.3136e-03\n",
      "Epoch: 77730 | training loss: 2.4136e-03 | validation loss: 2.3175e-03\n",
      "Epoch: 77740 | training loss: 2.4119e-03 | validation loss: 2.3195e-03\n",
      "Epoch: 77750 | training loss: 2.4115e-03 | validation loss: 2.3232e-03\n",
      "Epoch: 77760 | training loss: 2.4115e-03 | validation loss: 2.3238e-03\n",
      "Epoch: 77770 | training loss: 2.4113e-03 | validation loss: 2.3224e-03\n",
      "Epoch: 77780 | training loss: 2.4113e-03 | validation loss: 2.3212e-03\n",
      "Epoch: 77790 | training loss: 2.4112e-03 | validation loss: 2.3221e-03\n",
      "Epoch: 77800 | training loss: 2.4112e-03 | validation loss: 2.3219e-03\n",
      "Epoch: 77810 | training loss: 2.4111e-03 | validation loss: 2.3219e-03\n",
      "Epoch: 77820 | training loss: 2.4111e-03 | validation loss: 2.3226e-03\n",
      "Epoch: 77830 | training loss: 2.4132e-03 | validation loss: 2.3281e-03\n",
      "Epoch: 77840 | training loss: 2.5833e-03 | validation loss: 2.4542e-03\n",
      "Epoch: 77850 | training loss: 2.7272e-03 | validation loss: 2.5563e-03\n",
      "Epoch: 77860 | training loss: 2.6932e-03 | validation loss: 2.5224e-03\n",
      "Epoch: 77870 | training loss: 2.4171e-03 | validation loss: 2.3087e-03\n",
      "Epoch: 77880 | training loss: 2.4517e-03 | validation loss: 2.3039e-03\n",
      "Epoch: 77890 | training loss: 2.4168e-03 | validation loss: 2.3126e-03\n",
      "Epoch: 77900 | training loss: 2.4139e-03 | validation loss: 2.3328e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77910 | training loss: 2.4116e-03 | validation loss: 2.3263e-03\n",
      "Epoch: 77920 | training loss: 2.4112e-03 | validation loss: 2.3162e-03\n",
      "Epoch: 77930 | training loss: 2.4105e-03 | validation loss: 2.3203e-03\n",
      "Epoch: 77940 | training loss: 2.4105e-03 | validation loss: 2.3217e-03\n",
      "Epoch: 77950 | training loss: 2.4104e-03 | validation loss: 2.3191e-03\n",
      "Epoch: 77960 | training loss: 2.4103e-03 | validation loss: 2.3206e-03\n",
      "Epoch: 77970 | training loss: 2.4102e-03 | validation loss: 2.3196e-03\n",
      "Epoch: 77980 | training loss: 2.4102e-03 | validation loss: 2.3200e-03\n",
      "Epoch: 77990 | training loss: 2.4101e-03 | validation loss: 2.3196e-03\n",
      "Epoch: 78000 | training loss: 2.4101e-03 | validation loss: 2.3195e-03\n",
      "Epoch: 78010 | training loss: 2.4100e-03 | validation loss: 2.3195e-03\n",
      "Epoch: 78020 | training loss: 2.4099e-03 | validation loss: 2.3194e-03\n",
      "Epoch: 78030 | training loss: 2.4099e-03 | validation loss: 2.3193e-03\n",
      "Epoch: 78040 | training loss: 2.4098e-03 | validation loss: 2.3196e-03\n",
      "Epoch: 78050 | training loss: 2.4104e-03 | validation loss: 2.3228e-03\n",
      "Epoch: 78060 | training loss: 2.4684e-03 | validation loss: 2.3812e-03\n",
      "Epoch: 78070 | training loss: 4.1706e-03 | validation loss: 3.3281e-03\n",
      "Epoch: 78080 | training loss: 2.4860e-03 | validation loss: 2.4091e-03\n",
      "Epoch: 78090 | training loss: 2.4356e-03 | validation loss: 2.3039e-03\n",
      "Epoch: 78100 | training loss: 2.4598e-03 | validation loss: 2.3007e-03\n",
      "Epoch: 78110 | training loss: 2.4332e-03 | validation loss: 2.3045e-03\n",
      "Epoch: 78120 | training loss: 2.4115e-03 | validation loss: 2.3107e-03\n",
      "Epoch: 78130 | training loss: 2.4102e-03 | validation loss: 2.3234e-03\n",
      "Epoch: 78140 | training loss: 2.4104e-03 | validation loss: 2.3238e-03\n",
      "Epoch: 78150 | training loss: 2.4092e-03 | validation loss: 2.3179e-03\n",
      "Epoch: 78160 | training loss: 2.4093e-03 | validation loss: 2.3155e-03\n",
      "Epoch: 78170 | training loss: 2.4091e-03 | validation loss: 2.3180e-03\n",
      "Epoch: 78180 | training loss: 2.4091e-03 | validation loss: 2.3180e-03\n",
      "Epoch: 78190 | training loss: 2.4090e-03 | validation loss: 2.3169e-03\n",
      "Epoch: 78200 | training loss: 2.4089e-03 | validation loss: 2.3176e-03\n",
      "Epoch: 78210 | training loss: 2.4089e-03 | validation loss: 2.3176e-03\n",
      "Epoch: 78220 | training loss: 2.4099e-03 | validation loss: 2.3222e-03\n",
      "Epoch: 78230 | training loss: 2.5155e-03 | validation loss: 2.4363e-03\n",
      "Epoch: 78240 | training loss: 2.4247e-03 | validation loss: 2.3182e-03\n",
      "Epoch: 78250 | training loss: 2.4876e-03 | validation loss: 2.4012e-03\n",
      "Epoch: 78260 | training loss: 2.4298e-03 | validation loss: 2.3488e-03\n",
      "Epoch: 78270 | training loss: 2.4092e-03 | validation loss: 2.3120e-03\n",
      "Epoch: 78280 | training loss: 2.4129e-03 | validation loss: 2.3117e-03\n",
      "Epoch: 78290 | training loss: 2.4085e-03 | validation loss: 2.3152e-03\n",
      "Epoch: 78300 | training loss: 2.4089e-03 | validation loss: 2.3187e-03\n",
      "Epoch: 78310 | training loss: 2.4083e-03 | validation loss: 2.3152e-03\n",
      "Epoch: 78320 | training loss: 2.4083e-03 | validation loss: 2.3147e-03\n",
      "Epoch: 78330 | training loss: 2.4083e-03 | validation loss: 2.3167e-03\n",
      "Epoch: 78340 | training loss: 2.4102e-03 | validation loss: 2.3235e-03\n",
      "Epoch: 78350 | training loss: 2.7160e-03 | validation loss: 2.5524e-03\n",
      "Epoch: 78360 | training loss: 2.5478e-03 | validation loss: 2.3154e-03\n",
      "Epoch: 78370 | training loss: 2.5923e-03 | validation loss: 2.4321e-03\n",
      "Epoch: 78380 | training loss: 2.4973e-03 | validation loss: 2.3802e-03\n",
      "Epoch: 78390 | training loss: 2.4253e-03 | validation loss: 2.3307e-03\n",
      "Epoch: 78400 | training loss: 2.4080e-03 | validation loss: 2.3126e-03\n",
      "Epoch: 78410 | training loss: 2.4098e-03 | validation loss: 2.3107e-03\n",
      "Epoch: 78420 | training loss: 2.4088e-03 | validation loss: 2.3136e-03\n",
      "Epoch: 78430 | training loss: 2.4080e-03 | validation loss: 2.3176e-03\n",
      "Epoch: 78440 | training loss: 2.4079e-03 | validation loss: 2.3175e-03\n",
      "Epoch: 78450 | training loss: 2.4076e-03 | validation loss: 2.3141e-03\n",
      "Epoch: 78460 | training loss: 2.4075e-03 | validation loss: 2.3130e-03\n",
      "Epoch: 78470 | training loss: 2.4075e-03 | validation loss: 2.3135e-03\n",
      "Epoch: 78480 | training loss: 2.4074e-03 | validation loss: 2.3134e-03\n",
      "Epoch: 78490 | training loss: 2.4073e-03 | validation loss: 2.3136e-03\n",
      "Epoch: 78500 | training loss: 2.4073e-03 | validation loss: 2.3132e-03\n",
      "Epoch: 78510 | training loss: 2.4072e-03 | validation loss: 2.3131e-03\n",
      "Epoch: 78520 | training loss: 2.4072e-03 | validation loss: 2.3130e-03\n",
      "Epoch: 78530 | training loss: 2.4071e-03 | validation loss: 2.3129e-03\n",
      "Epoch: 78540 | training loss: 2.4071e-03 | validation loss: 2.3128e-03\n",
      "Epoch: 78550 | training loss: 2.4070e-03 | validation loss: 2.3127e-03\n",
      "Epoch: 78560 | training loss: 2.4069e-03 | validation loss: 2.3125e-03\n",
      "Epoch: 78570 | training loss: 2.4069e-03 | validation loss: 2.3124e-03\n",
      "Epoch: 78580 | training loss: 2.4068e-03 | validation loss: 2.3121e-03\n",
      "Epoch: 78590 | training loss: 2.4071e-03 | validation loss: 2.3103e-03\n",
      "Epoch: 78600 | training loss: 2.4506e-03 | validation loss: 2.3088e-03\n",
      "Epoch: 78610 | training loss: 4.6092e-03 | validation loss: 3.1472e-03\n",
      "Epoch: 78620 | training loss: 2.6779e-03 | validation loss: 2.4429e-03\n",
      "Epoch: 78630 | training loss: 2.5002e-03 | validation loss: 2.4059e-03\n",
      "Epoch: 78640 | training loss: 2.4616e-03 | validation loss: 2.3843e-03\n",
      "Epoch: 78650 | training loss: 2.4320e-03 | validation loss: 2.3542e-03\n",
      "Epoch: 78660 | training loss: 2.4142e-03 | validation loss: 2.3283e-03\n",
      "Epoch: 78670 | training loss: 2.4086e-03 | validation loss: 2.3131e-03\n",
      "Epoch: 78680 | training loss: 2.4073e-03 | validation loss: 2.3088e-03\n",
      "Epoch: 78690 | training loss: 2.4064e-03 | validation loss: 2.3107e-03\n",
      "Epoch: 78700 | training loss: 2.4062e-03 | validation loss: 2.3123e-03\n",
      "Epoch: 78710 | training loss: 2.4062e-03 | validation loss: 2.3106e-03\n",
      "Epoch: 78720 | training loss: 2.4061e-03 | validation loss: 2.3100e-03\n",
      "Epoch: 78730 | training loss: 2.4060e-03 | validation loss: 2.3108e-03\n",
      "Epoch: 78740 | training loss: 2.4060e-03 | validation loss: 2.3104e-03\n",
      "Epoch: 78750 | training loss: 2.4059e-03 | validation loss: 2.3104e-03\n",
      "Epoch: 78760 | training loss: 2.4059e-03 | validation loss: 2.3102e-03\n",
      "Epoch: 78770 | training loss: 2.4058e-03 | validation loss: 2.3101e-03\n",
      "Epoch: 78780 | training loss: 2.4058e-03 | validation loss: 2.3100e-03\n",
      "Epoch: 78790 | training loss: 2.4057e-03 | validation loss: 2.3099e-03\n",
      "Epoch: 78800 | training loss: 2.4056e-03 | validation loss: 2.3098e-03\n",
      "Epoch: 78810 | training loss: 2.4056e-03 | validation loss: 2.3096e-03\n",
      "Epoch: 78820 | training loss: 2.4055e-03 | validation loss: 2.3095e-03\n",
      "Epoch: 78830 | training loss: 2.4055e-03 | validation loss: 2.3093e-03\n",
      "Epoch: 78840 | training loss: 2.4054e-03 | validation loss: 2.3089e-03\n",
      "Epoch: 78850 | training loss: 2.4055e-03 | validation loss: 2.3065e-03\n",
      "Epoch: 78860 | training loss: 2.4348e-03 | validation loss: 2.2895e-03\n",
      "Epoch: 78870 | training loss: 4.1604e-03 | validation loss: 2.9084e-03\n",
      "Epoch: 78880 | training loss: 2.8119e-03 | validation loss: 2.5039e-03\n",
      "Epoch: 78890 | training loss: 2.5727e-03 | validation loss: 2.3654e-03\n",
      "Epoch: 78900 | training loss: 2.4601e-03 | validation loss: 2.2966e-03\n",
      "Epoch: 78910 | training loss: 2.4139e-03 | validation loss: 2.2944e-03\n",
      "Epoch: 78920 | training loss: 2.4059e-03 | validation loss: 2.3083e-03\n",
      "Epoch: 78930 | training loss: 2.4050e-03 | validation loss: 2.3088e-03\n",
      "Epoch: 78940 | training loss: 2.4051e-03 | validation loss: 2.3084e-03\n",
      "Epoch: 78950 | training loss: 2.4049e-03 | validation loss: 2.3093e-03\n",
      "Epoch: 78960 | training loss: 2.4048e-03 | validation loss: 2.3074e-03\n",
      "Epoch: 78970 | training loss: 2.4047e-03 | validation loss: 2.3075e-03\n",
      "Epoch: 78980 | training loss: 2.4047e-03 | validation loss: 2.3067e-03\n",
      "Epoch: 78990 | training loss: 2.4046e-03 | validation loss: 2.3067e-03\n",
      "Epoch: 79000 | training loss: 2.4046e-03 | validation loss: 2.3066e-03\n",
      "Epoch: 79010 | training loss: 2.4045e-03 | validation loss: 2.3066e-03\n",
      "Epoch: 79020 | training loss: 2.4044e-03 | validation loss: 2.3067e-03\n",
      "Epoch: 79030 | training loss: 2.4044e-03 | validation loss: 2.3065e-03\n",
      "Epoch: 79040 | training loss: 2.4043e-03 | validation loss: 2.3062e-03\n",
      "Epoch: 79050 | training loss: 2.4044e-03 | validation loss: 2.3052e-03\n",
      "Epoch: 79060 | training loss: 2.4134e-03 | validation loss: 2.2999e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79070 | training loss: 3.5183e-03 | validation loss: 2.7226e-03\n",
      "Epoch: 79080 | training loss: 3.2254e-03 | validation loss: 2.8198e-03\n",
      "Epoch: 79090 | training loss: 2.5399e-03 | validation loss: 2.4235e-03\n",
      "Epoch: 79100 | training loss: 2.4073e-03 | validation loss: 2.3179e-03\n",
      "Epoch: 79110 | training loss: 2.4154e-03 | validation loss: 2.3028e-03\n",
      "Epoch: 79120 | training loss: 2.4164e-03 | validation loss: 2.3008e-03\n",
      "Epoch: 79130 | training loss: 2.4060e-03 | validation loss: 2.3027e-03\n",
      "Epoch: 79140 | training loss: 2.4042e-03 | validation loss: 2.3088e-03\n",
      "Epoch: 79150 | training loss: 2.4043e-03 | validation loss: 2.3091e-03\n",
      "Epoch: 79160 | training loss: 2.4037e-03 | validation loss: 2.3049e-03\n",
      "Epoch: 79170 | training loss: 2.4037e-03 | validation loss: 2.3040e-03\n",
      "Epoch: 79180 | training loss: 2.4036e-03 | validation loss: 2.3054e-03\n",
      "Epoch: 79190 | training loss: 2.4035e-03 | validation loss: 2.3049e-03\n",
      "Epoch: 79200 | training loss: 2.4035e-03 | validation loss: 2.3047e-03\n",
      "Epoch: 79210 | training loss: 2.4034e-03 | validation loss: 2.3049e-03\n",
      "Epoch: 79220 | training loss: 2.4034e-03 | validation loss: 2.3045e-03\n",
      "Epoch: 79230 | training loss: 2.4033e-03 | validation loss: 2.3046e-03\n",
      "Epoch: 79240 | training loss: 2.4033e-03 | validation loss: 2.3044e-03\n",
      "Epoch: 79250 | training loss: 2.4032e-03 | validation loss: 2.3043e-03\n",
      "Epoch: 79260 | training loss: 2.4032e-03 | validation loss: 2.3042e-03\n",
      "Epoch: 79270 | training loss: 2.4031e-03 | validation loss: 2.3041e-03\n",
      "Epoch: 79280 | training loss: 2.4030e-03 | validation loss: 2.3040e-03\n",
      "Epoch: 79290 | training loss: 2.4030e-03 | validation loss: 2.3039e-03\n",
      "Epoch: 79300 | training loss: 2.4029e-03 | validation loss: 2.3042e-03\n",
      "Epoch: 79310 | training loss: 2.4042e-03 | validation loss: 2.3089e-03\n",
      "Epoch: 79320 | training loss: 2.6103e-03 | validation loss: 2.4598e-03\n",
      "Epoch: 79330 | training loss: 2.4290e-03 | validation loss: 2.3491e-03\n",
      "Epoch: 79340 | training loss: 2.6935e-03 | validation loss: 2.5167e-03\n",
      "Epoch: 79350 | training loss: 2.5406e-03 | validation loss: 2.4151e-03\n",
      "Epoch: 79360 | training loss: 2.4558e-03 | validation loss: 2.3499e-03\n",
      "Epoch: 79370 | training loss: 2.4222e-03 | validation loss: 2.3191e-03\n",
      "Epoch: 79380 | training loss: 2.4083e-03 | validation loss: 2.3055e-03\n",
      "Epoch: 79390 | training loss: 2.4030e-03 | validation loss: 2.3023e-03\n",
      "Epoch: 79400 | training loss: 2.4026e-03 | validation loss: 2.3033e-03\n",
      "Epoch: 79410 | training loss: 2.4027e-03 | validation loss: 2.3025e-03\n",
      "Epoch: 79420 | training loss: 2.4024e-03 | validation loss: 2.3014e-03\n",
      "Epoch: 79430 | training loss: 2.4023e-03 | validation loss: 2.3024e-03\n",
      "Epoch: 79440 | training loss: 2.4022e-03 | validation loss: 2.3025e-03\n",
      "Epoch: 79450 | training loss: 2.4021e-03 | validation loss: 2.3018e-03\n",
      "Epoch: 79460 | training loss: 2.4021e-03 | validation loss: 2.3020e-03\n",
      "Epoch: 79470 | training loss: 2.4020e-03 | validation loss: 2.3018e-03\n",
      "Epoch: 79480 | training loss: 2.4020e-03 | validation loss: 2.3017e-03\n",
      "Epoch: 79490 | training loss: 2.4019e-03 | validation loss: 2.3016e-03\n",
      "Epoch: 79500 | training loss: 2.4019e-03 | validation loss: 2.3014e-03\n",
      "Epoch: 79510 | training loss: 2.4018e-03 | validation loss: 2.3013e-03\n",
      "Epoch: 79520 | training loss: 2.4017e-03 | validation loss: 2.3012e-03\n",
      "Epoch: 79530 | training loss: 2.4017e-03 | validation loss: 2.3011e-03\n",
      "Epoch: 79540 | training loss: 2.4016e-03 | validation loss: 2.3010e-03\n",
      "Epoch: 79550 | training loss: 2.4016e-03 | validation loss: 2.3010e-03\n",
      "Epoch: 79560 | training loss: 2.4016e-03 | validation loss: 2.3025e-03\n",
      "Epoch: 79570 | training loss: 2.4152e-03 | validation loss: 2.3330e-03\n",
      "Epoch: 79580 | training loss: 3.6397e-03 | validation loss: 3.2592e-03\n",
      "Epoch: 79590 | training loss: 2.6207e-03 | validation loss: 2.4572e-03\n",
      "Epoch: 79600 | training loss: 2.5295e-03 | validation loss: 2.4464e-03\n",
      "Epoch: 79610 | training loss: 2.4572e-03 | validation loss: 2.3440e-03\n",
      "Epoch: 79620 | training loss: 2.4154e-03 | validation loss: 2.3299e-03\n",
      "Epoch: 79630 | training loss: 2.4039e-03 | validation loss: 2.2991e-03\n",
      "Epoch: 79640 | training loss: 2.4019e-03 | validation loss: 2.3044e-03\n",
      "Epoch: 79650 | training loss: 2.4012e-03 | validation loss: 2.2966e-03\n",
      "Epoch: 79660 | training loss: 2.4010e-03 | validation loss: 2.2983e-03\n",
      "Epoch: 79670 | training loss: 2.4010e-03 | validation loss: 2.3001e-03\n",
      "Epoch: 79680 | training loss: 2.4009e-03 | validation loss: 2.3000e-03\n",
      "Epoch: 79690 | training loss: 2.4010e-03 | validation loss: 2.3006e-03\n",
      "Epoch: 79700 | training loss: 2.4026e-03 | validation loss: 2.3061e-03\n",
      "Epoch: 79710 | training loss: 2.4719e-03 | validation loss: 2.3718e-03\n",
      "Epoch: 79720 | training loss: 3.5457e-03 | validation loss: 2.9861e-03\n",
      "Epoch: 79730 | training loss: 2.5261e-03 | validation loss: 2.3069e-03\n",
      "Epoch: 79740 | training loss: 2.4558e-03 | validation loss: 2.2901e-03\n",
      "Epoch: 79750 | training loss: 2.4361e-03 | validation loss: 2.3412e-03\n",
      "Epoch: 79760 | training loss: 2.4008e-03 | validation loss: 2.3006e-03\n",
      "Epoch: 79770 | training loss: 2.4056e-03 | validation loss: 2.2900e-03\n",
      "Epoch: 79780 | training loss: 2.4024e-03 | validation loss: 2.3051e-03\n",
      "Epoch: 79790 | training loss: 2.4007e-03 | validation loss: 2.2951e-03\n",
      "Epoch: 79800 | training loss: 2.4003e-03 | validation loss: 2.2987e-03\n",
      "Epoch: 79810 | training loss: 2.4002e-03 | validation loss: 2.2965e-03\n",
      "Epoch: 79820 | training loss: 2.4002e-03 | validation loss: 2.2980e-03\n",
      "Epoch: 79830 | training loss: 2.4001e-03 | validation loss: 2.2964e-03\n",
      "Epoch: 79840 | training loss: 2.4000e-03 | validation loss: 2.2970e-03\n",
      "Epoch: 79850 | training loss: 2.4000e-03 | validation loss: 2.2972e-03\n",
      "Epoch: 79860 | training loss: 2.3999e-03 | validation loss: 2.2971e-03\n",
      "Epoch: 79870 | training loss: 2.3999e-03 | validation loss: 2.2974e-03\n",
      "Epoch: 79880 | training loss: 2.4003e-03 | validation loss: 2.2998e-03\n",
      "Epoch: 79890 | training loss: 2.4229e-03 | validation loss: 2.3289e-03\n",
      "Epoch: 79900 | training loss: 3.7146e-03 | validation loss: 3.0710e-03\n",
      "Epoch: 79910 | training loss: 2.9482e-03 | validation loss: 2.4530e-03\n",
      "Epoch: 79920 | training loss: 2.4451e-03 | validation loss: 2.2855e-03\n",
      "Epoch: 79930 | training loss: 2.4325e-03 | validation loss: 2.3333e-03\n",
      "Epoch: 79940 | training loss: 2.4199e-03 | validation loss: 2.3235e-03\n",
      "Epoch: 79950 | training loss: 2.4008e-03 | validation loss: 2.2911e-03\n",
      "Epoch: 79960 | training loss: 2.4021e-03 | validation loss: 2.2902e-03\n",
      "Epoch: 79970 | training loss: 2.4000e-03 | validation loss: 2.2997e-03\n",
      "Epoch: 79980 | training loss: 2.3993e-03 | validation loss: 2.2961e-03\n",
      "Epoch: 79990 | training loss: 2.3993e-03 | validation loss: 2.2939e-03\n",
      "Epoch: 80000 | training loss: 2.3992e-03 | validation loss: 2.2965e-03\n",
      "Epoch: 80010 | training loss: 2.3991e-03 | validation loss: 2.2945e-03\n",
      "Epoch: 80020 | training loss: 2.3991e-03 | validation loss: 2.2955e-03\n",
      "Epoch: 80030 | training loss: 2.3990e-03 | validation loss: 2.2947e-03\n",
      "Epoch: 80040 | training loss: 2.3989e-03 | validation loss: 2.2949e-03\n",
      "Epoch: 80050 | training loss: 2.3989e-03 | validation loss: 2.2948e-03\n",
      "Epoch: 80060 | training loss: 2.3988e-03 | validation loss: 2.2946e-03\n",
      "Epoch: 80070 | training loss: 2.3988e-03 | validation loss: 2.2944e-03\n",
      "Epoch: 80080 | training loss: 2.3987e-03 | validation loss: 2.2942e-03\n",
      "Epoch: 80090 | training loss: 2.3987e-03 | validation loss: 2.2933e-03\n",
      "Epoch: 80100 | training loss: 2.4009e-03 | validation loss: 2.2887e-03\n",
      "Epoch: 80110 | training loss: 2.6313e-03 | validation loss: 2.3378e-03\n",
      "Epoch: 80120 | training loss: 2.4328e-03 | validation loss: 2.2847e-03\n",
      "Epoch: 80130 | training loss: 2.7795e-03 | validation loss: 2.3874e-03\n",
      "Epoch: 80140 | training loss: 2.4950e-03 | validation loss: 2.2960e-03\n",
      "Epoch: 80150 | training loss: 2.4002e-03 | validation loss: 2.2878e-03\n",
      "Epoch: 80160 | training loss: 2.4060e-03 | validation loss: 2.3085e-03\n",
      "Epoch: 80170 | training loss: 2.4045e-03 | validation loss: 2.3068e-03\n",
      "Epoch: 80180 | training loss: 2.3982e-03 | validation loss: 2.2939e-03\n",
      "Epoch: 80190 | training loss: 2.3989e-03 | validation loss: 2.2897e-03\n",
      "Epoch: 80200 | training loss: 2.3981e-03 | validation loss: 2.2926e-03\n",
      "Epoch: 80210 | training loss: 2.3981e-03 | validation loss: 2.2943e-03\n",
      "Epoch: 80220 | training loss: 2.3980e-03 | validation loss: 2.2922e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80230 | training loss: 2.3979e-03 | validation loss: 2.2925e-03\n",
      "Epoch: 80240 | training loss: 2.3979e-03 | validation loss: 2.2927e-03\n",
      "Epoch: 80250 | training loss: 2.3978e-03 | validation loss: 2.2922e-03\n",
      "Epoch: 80260 | training loss: 2.3977e-03 | validation loss: 2.2924e-03\n",
      "Epoch: 80270 | training loss: 2.3977e-03 | validation loss: 2.2921e-03\n",
      "Epoch: 80280 | training loss: 2.3976e-03 | validation loss: 2.2919e-03\n",
      "Epoch: 80290 | training loss: 2.3976e-03 | validation loss: 2.2914e-03\n",
      "Epoch: 80300 | training loss: 2.3995e-03 | validation loss: 2.2860e-03\n",
      "Epoch: 80310 | training loss: 2.8364e-03 | validation loss: 2.4962e-03\n",
      "Epoch: 80320 | training loss: 2.6803e-03 | validation loss: 2.5151e-03\n",
      "Epoch: 80330 | training loss: 2.5224e-03 | validation loss: 2.4330e-03\n",
      "Epoch: 80340 | training loss: 2.4418e-03 | validation loss: 2.3259e-03\n",
      "Epoch: 80350 | training loss: 2.4108e-03 | validation loss: 2.3130e-03\n",
      "Epoch: 80360 | training loss: 2.4039e-03 | validation loss: 2.3089e-03\n",
      "Epoch: 80370 | training loss: 2.3992e-03 | validation loss: 2.2987e-03\n",
      "Epoch: 80380 | training loss: 2.3976e-03 | validation loss: 2.2941e-03\n",
      "Epoch: 80390 | training loss: 2.3974e-03 | validation loss: 2.2942e-03\n",
      "Epoch: 80400 | training loss: 2.4083e-03 | validation loss: 2.3126e-03\n",
      "Epoch: 80410 | training loss: 2.9975e-03 | validation loss: 2.6938e-03\n",
      "Epoch: 80420 | training loss: 2.6425e-03 | validation loss: 2.3217e-03\n",
      "Epoch: 80430 | training loss: 2.5616e-03 | validation loss: 2.4229e-03\n",
      "Epoch: 80440 | training loss: 2.4055e-03 | validation loss: 2.3048e-03\n",
      "Epoch: 80450 | training loss: 2.4225e-03 | validation loss: 2.2753e-03\n",
      "Epoch: 80460 | training loss: 2.3972e-03 | validation loss: 2.2916e-03\n",
      "Epoch: 80470 | training loss: 2.3984e-03 | validation loss: 2.2957e-03\n",
      "Epoch: 80480 | training loss: 2.3979e-03 | validation loss: 2.2841e-03\n",
      "Epoch: 80490 | training loss: 2.3969e-03 | validation loss: 2.2923e-03\n",
      "Epoch: 80500 | training loss: 2.3966e-03 | validation loss: 2.2875e-03\n",
      "Epoch: 80510 | training loss: 2.3965e-03 | validation loss: 2.2901e-03\n",
      "Epoch: 80520 | training loss: 2.3964e-03 | validation loss: 2.2880e-03\n",
      "Epoch: 80530 | training loss: 2.3963e-03 | validation loss: 2.2890e-03\n",
      "Epoch: 80540 | training loss: 2.3962e-03 | validation loss: 2.2888e-03\n",
      "Epoch: 80550 | training loss: 2.3962e-03 | validation loss: 2.2883e-03\n",
      "Epoch: 80560 | training loss: 2.3961e-03 | validation loss: 2.2880e-03\n",
      "Epoch: 80570 | training loss: 2.3961e-03 | validation loss: 2.2874e-03\n",
      "Epoch: 80580 | training loss: 2.3966e-03 | validation loss: 2.2847e-03\n",
      "Epoch: 80590 | training loss: 2.4297e-03 | validation loss: 2.2763e-03\n",
      "Epoch: 80600 | training loss: 3.9342e-03 | validation loss: 2.8221e-03\n",
      "Epoch: 80610 | training loss: 2.7046e-03 | validation loss: 2.5026e-03\n",
      "Epoch: 80620 | training loss: 2.5203e-03 | validation loss: 2.3977e-03\n",
      "Epoch: 80630 | training loss: 2.4031e-03 | validation loss: 2.2917e-03\n",
      "Epoch: 80640 | training loss: 2.4184e-03 | validation loss: 2.2804e-03\n",
      "Epoch: 80650 | training loss: 2.3962e-03 | validation loss: 2.2833e-03\n",
      "Epoch: 80660 | training loss: 2.3986e-03 | validation loss: 2.2941e-03\n",
      "Epoch: 80670 | training loss: 2.3958e-03 | validation loss: 2.2873e-03\n",
      "Epoch: 80680 | training loss: 2.3956e-03 | validation loss: 2.2857e-03\n",
      "Epoch: 80690 | training loss: 2.3956e-03 | validation loss: 2.2880e-03\n",
      "Epoch: 80700 | training loss: 2.3954e-03 | validation loss: 2.2864e-03\n",
      "Epoch: 80710 | training loss: 2.3954e-03 | validation loss: 2.2870e-03\n",
      "Epoch: 80720 | training loss: 2.3953e-03 | validation loss: 2.2864e-03\n",
      "Epoch: 80730 | training loss: 2.3952e-03 | validation loss: 2.2866e-03\n",
      "Epoch: 80740 | training loss: 2.3952e-03 | validation loss: 2.2863e-03\n",
      "Epoch: 80750 | training loss: 2.3951e-03 | validation loss: 2.2862e-03\n",
      "Epoch: 80760 | training loss: 2.3951e-03 | validation loss: 2.2862e-03\n",
      "Epoch: 80770 | training loss: 2.3950e-03 | validation loss: 2.2861e-03\n",
      "Epoch: 80780 | training loss: 2.3949e-03 | validation loss: 2.2861e-03\n",
      "Epoch: 80790 | training loss: 2.3949e-03 | validation loss: 2.2866e-03\n",
      "Epoch: 80800 | training loss: 2.3980e-03 | validation loss: 2.2932e-03\n",
      "Epoch: 80810 | training loss: 2.7283e-03 | validation loss: 2.5099e-03\n",
      "Epoch: 80820 | training loss: 2.5326e-03 | validation loss: 2.4204e-03\n",
      "Epoch: 80830 | training loss: 2.5477e-03 | validation loss: 2.4159e-03\n",
      "Epoch: 80840 | training loss: 2.4376e-03 | validation loss: 2.3017e-03\n",
      "Epoch: 80850 | training loss: 2.4232e-03 | validation loss: 2.2771e-03\n",
      "Epoch: 80860 | training loss: 2.4033e-03 | validation loss: 2.2845e-03\n",
      "Epoch: 80870 | training loss: 2.3996e-03 | validation loss: 2.2900e-03\n",
      "Epoch: 80880 | training loss: 2.3962e-03 | validation loss: 2.2805e-03\n",
      "Epoch: 80890 | training loss: 2.3948e-03 | validation loss: 2.2832e-03\n",
      "Epoch: 80900 | training loss: 2.3944e-03 | validation loss: 2.2851e-03\n",
      "Epoch: 80910 | training loss: 2.3943e-03 | validation loss: 2.2838e-03\n",
      "Epoch: 80920 | training loss: 2.3942e-03 | validation loss: 2.2849e-03\n",
      "Epoch: 80930 | training loss: 2.3942e-03 | validation loss: 2.2838e-03\n",
      "Epoch: 80940 | training loss: 2.3941e-03 | validation loss: 2.2838e-03\n",
      "Epoch: 80950 | training loss: 2.3940e-03 | validation loss: 2.2839e-03\n",
      "Epoch: 80960 | training loss: 2.3940e-03 | validation loss: 2.2837e-03\n",
      "Epoch: 80970 | training loss: 2.3939e-03 | validation loss: 2.2834e-03\n",
      "Epoch: 80980 | training loss: 2.3939e-03 | validation loss: 2.2833e-03\n",
      "Epoch: 80990 | training loss: 2.3938e-03 | validation loss: 2.2826e-03\n",
      "Epoch: 81000 | training loss: 2.3943e-03 | validation loss: 2.2790e-03\n",
      "Epoch: 81010 | training loss: 2.4398e-03 | validation loss: 2.2642e-03\n",
      "Epoch: 81020 | training loss: 3.6907e-03 | validation loss: 2.6930e-03\n",
      "Epoch: 81030 | training loss: 2.5471e-03 | validation loss: 2.3526e-03\n",
      "Epoch: 81040 | training loss: 2.4249e-03 | validation loss: 2.3099e-03\n",
      "Epoch: 81050 | training loss: 2.4037e-03 | validation loss: 2.2868e-03\n",
      "Epoch: 81060 | training loss: 2.3997e-03 | validation loss: 2.2948e-03\n",
      "Epoch: 81070 | training loss: 2.3994e-03 | validation loss: 2.3011e-03\n",
      "Epoch: 81080 | training loss: 2.3952e-03 | validation loss: 2.2878e-03\n",
      "Epoch: 81090 | training loss: 2.3934e-03 | validation loss: 2.2836e-03\n",
      "Epoch: 81100 | training loss: 2.3933e-03 | validation loss: 2.2806e-03\n",
      "Epoch: 81110 | training loss: 2.3933e-03 | validation loss: 2.2801e-03\n",
      "Epoch: 81120 | training loss: 2.3931e-03 | validation loss: 2.2817e-03\n",
      "Epoch: 81130 | training loss: 2.3931e-03 | validation loss: 2.2822e-03\n",
      "Epoch: 81140 | training loss: 2.3930e-03 | validation loss: 2.2810e-03\n",
      "Epoch: 81150 | training loss: 2.3930e-03 | validation loss: 2.2811e-03\n",
      "Epoch: 81160 | training loss: 2.3929e-03 | validation loss: 2.2813e-03\n",
      "Epoch: 81170 | training loss: 2.3929e-03 | validation loss: 2.2811e-03\n",
      "Epoch: 81180 | training loss: 2.3928e-03 | validation loss: 2.2815e-03\n",
      "Epoch: 81190 | training loss: 2.3937e-03 | validation loss: 2.2848e-03\n",
      "Epoch: 81200 | training loss: 2.4704e-03 | validation loss: 2.3504e-03\n",
      "Epoch: 81210 | training loss: 3.7259e-03 | validation loss: 3.0609e-03\n",
      "Epoch: 81220 | training loss: 2.4588e-03 | validation loss: 2.3278e-03\n",
      "Epoch: 81230 | training loss: 2.4695e-03 | validation loss: 2.2637e-03\n",
      "Epoch: 81240 | training loss: 2.4449e-03 | validation loss: 2.2667e-03\n",
      "Epoch: 81250 | training loss: 2.3946e-03 | validation loss: 2.2794e-03\n",
      "Epoch: 81260 | training loss: 2.3981e-03 | validation loss: 2.2973e-03\n",
      "Epoch: 81270 | training loss: 2.3934e-03 | validation loss: 2.2857e-03\n",
      "Epoch: 81280 | training loss: 2.3930e-03 | validation loss: 2.2750e-03\n",
      "Epoch: 81290 | training loss: 2.3923e-03 | validation loss: 2.2787e-03\n",
      "Epoch: 81300 | training loss: 2.3923e-03 | validation loss: 2.2818e-03\n",
      "Epoch: 81310 | training loss: 2.3922e-03 | validation loss: 2.2785e-03\n",
      "Epoch: 81320 | training loss: 2.3921e-03 | validation loss: 2.2796e-03\n",
      "Epoch: 81330 | training loss: 2.3920e-03 | validation loss: 2.2793e-03\n",
      "Epoch: 81340 | training loss: 2.3920e-03 | validation loss: 2.2792e-03\n",
      "Epoch: 81350 | training loss: 2.3919e-03 | validation loss: 2.2790e-03\n",
      "Epoch: 81360 | training loss: 2.3919e-03 | validation loss: 2.2790e-03\n",
      "Epoch: 81370 | training loss: 2.3918e-03 | validation loss: 2.2789e-03\n",
      "Epoch: 81380 | training loss: 2.3918e-03 | validation loss: 2.2787e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81390 | training loss: 2.3917e-03 | validation loss: 2.2786e-03\n",
      "Epoch: 81400 | training loss: 2.3916e-03 | validation loss: 2.2784e-03\n",
      "Epoch: 81410 | training loss: 2.3916e-03 | validation loss: 2.2779e-03\n",
      "Epoch: 81420 | training loss: 2.3919e-03 | validation loss: 2.2753e-03\n",
      "Epoch: 81430 | training loss: 2.4361e-03 | validation loss: 2.2668e-03\n",
      "Epoch: 81440 | training loss: 4.4304e-03 | validation loss: 3.0298e-03\n",
      "Epoch: 81450 | training loss: 2.4847e-03 | validation loss: 2.2663e-03\n",
      "Epoch: 81460 | training loss: 2.3973e-03 | validation loss: 2.2919e-03\n",
      "Epoch: 81470 | training loss: 2.4118e-03 | validation loss: 2.3169e-03\n",
      "Epoch: 81480 | training loss: 2.4092e-03 | validation loss: 2.3092e-03\n",
      "Epoch: 81490 | training loss: 2.4007e-03 | validation loss: 2.3008e-03\n",
      "Epoch: 81500 | training loss: 2.3934e-03 | validation loss: 2.2876e-03\n",
      "Epoch: 81510 | training loss: 2.3911e-03 | validation loss: 2.2773e-03\n",
      "Epoch: 81520 | training loss: 2.3913e-03 | validation loss: 2.2741e-03\n",
      "Epoch: 81530 | training loss: 2.3911e-03 | validation loss: 2.2751e-03\n",
      "Epoch: 81540 | training loss: 2.3909e-03 | validation loss: 2.2775e-03\n",
      "Epoch: 81550 | training loss: 2.3909e-03 | validation loss: 2.2776e-03\n",
      "Epoch: 81560 | training loss: 2.3908e-03 | validation loss: 2.2763e-03\n",
      "Epoch: 81570 | training loss: 2.3907e-03 | validation loss: 2.2765e-03\n",
      "Epoch: 81580 | training loss: 2.3907e-03 | validation loss: 2.2766e-03\n",
      "Epoch: 81590 | training loss: 2.3906e-03 | validation loss: 2.2763e-03\n",
      "Epoch: 81600 | training loss: 2.3906e-03 | validation loss: 2.2768e-03\n",
      "Epoch: 81610 | training loss: 2.3937e-03 | validation loss: 2.2835e-03\n",
      "Epoch: 81620 | training loss: 2.8571e-03 | validation loss: 2.6598e-03\n",
      "Epoch: 81630 | training loss: 2.5763e-03 | validation loss: 2.3633e-03\n",
      "Epoch: 81640 | training loss: 2.4792e-03 | validation loss: 2.2976e-03\n",
      "Epoch: 81650 | training loss: 2.4162e-03 | validation loss: 2.2760e-03\n",
      "Epoch: 81660 | training loss: 2.3948e-03 | validation loss: 2.2716e-03\n",
      "Epoch: 81670 | training loss: 2.3906e-03 | validation loss: 2.2710e-03\n",
      "Epoch: 81680 | training loss: 2.3908e-03 | validation loss: 2.2796e-03\n",
      "Epoch: 81690 | training loss: 2.3907e-03 | validation loss: 2.2791e-03\n",
      "Epoch: 81700 | training loss: 2.3901e-03 | validation loss: 2.2758e-03\n",
      "Epoch: 81710 | training loss: 2.3900e-03 | validation loss: 2.2744e-03\n",
      "Epoch: 81720 | training loss: 2.3901e-03 | validation loss: 2.2763e-03\n",
      "Epoch: 81730 | training loss: 2.3989e-03 | validation loss: 2.2943e-03\n",
      "Epoch: 81740 | training loss: 3.0967e-03 | validation loss: 2.7363e-03\n",
      "Epoch: 81750 | training loss: 2.8555e-03 | validation loss: 2.3792e-03\n",
      "Epoch: 81760 | training loss: 2.4532e-03 | validation loss: 2.3236e-03\n",
      "Epoch: 81770 | training loss: 2.4734e-03 | validation loss: 2.3445e-03\n",
      "Epoch: 81780 | training loss: 2.3906e-03 | validation loss: 2.2711e-03\n",
      "Epoch: 81790 | training loss: 2.4001e-03 | validation loss: 2.2631e-03\n",
      "Epoch: 81800 | training loss: 2.3896e-03 | validation loss: 2.2743e-03\n",
      "Epoch: 81810 | training loss: 2.3909e-03 | validation loss: 2.2806e-03\n",
      "Epoch: 81820 | training loss: 2.3897e-03 | validation loss: 2.2716e-03\n",
      "Epoch: 81830 | training loss: 2.3894e-03 | validation loss: 2.2729e-03\n",
      "Epoch: 81840 | training loss: 2.3893e-03 | validation loss: 2.2735e-03\n",
      "Epoch: 81850 | training loss: 2.3893e-03 | validation loss: 2.2727e-03\n",
      "Epoch: 81860 | training loss: 2.3892e-03 | validation loss: 2.2733e-03\n",
      "Epoch: 81870 | training loss: 2.3892e-03 | validation loss: 2.2727e-03\n",
      "Epoch: 81880 | training loss: 2.3891e-03 | validation loss: 2.2727e-03\n",
      "Epoch: 81890 | training loss: 2.3891e-03 | validation loss: 2.2727e-03\n",
      "Epoch: 81900 | training loss: 2.3890e-03 | validation loss: 2.2725e-03\n",
      "Epoch: 81910 | training loss: 2.3889e-03 | validation loss: 2.2724e-03\n",
      "Epoch: 81920 | training loss: 2.3889e-03 | validation loss: 2.2722e-03\n",
      "Epoch: 81930 | training loss: 2.3888e-03 | validation loss: 2.2718e-03\n",
      "Epoch: 81940 | training loss: 2.3890e-03 | validation loss: 2.2700e-03\n",
      "Epoch: 81950 | training loss: 2.4133e-03 | validation loss: 2.2612e-03\n",
      "Epoch: 81960 | training loss: 4.5052e-03 | validation loss: 3.0569e-03\n",
      "Epoch: 81970 | training loss: 2.4528e-03 | validation loss: 2.3268e-03\n",
      "Epoch: 81980 | training loss: 2.5259e-03 | validation loss: 2.3853e-03\n",
      "Epoch: 81990 | training loss: 2.4658e-03 | validation loss: 2.3520e-03\n",
      "Epoch: 82000 | training loss: 2.4197e-03 | validation loss: 2.3175e-03\n",
      "Epoch: 82010 | training loss: 2.3950e-03 | validation loss: 2.2878e-03\n",
      "Epoch: 82020 | training loss: 2.3885e-03 | validation loss: 2.2716e-03\n",
      "Epoch: 82030 | training loss: 2.3890e-03 | validation loss: 2.2673e-03\n",
      "Epoch: 82040 | training loss: 2.3887e-03 | validation loss: 2.2677e-03\n",
      "Epoch: 82050 | training loss: 2.3882e-03 | validation loss: 2.2708e-03\n",
      "Epoch: 82060 | training loss: 2.3882e-03 | validation loss: 2.2723e-03\n",
      "Epoch: 82070 | training loss: 2.3881e-03 | validation loss: 2.2705e-03\n",
      "Epoch: 82080 | training loss: 2.3881e-03 | validation loss: 2.2701e-03\n",
      "Epoch: 82090 | training loss: 2.3880e-03 | validation loss: 2.2707e-03\n",
      "Epoch: 82100 | training loss: 2.3879e-03 | validation loss: 2.2703e-03\n",
      "Epoch: 82110 | training loss: 2.3879e-03 | validation loss: 2.2702e-03\n",
      "Epoch: 82120 | training loss: 2.3878e-03 | validation loss: 2.2701e-03\n",
      "Epoch: 82130 | training loss: 2.3878e-03 | validation loss: 2.2700e-03\n",
      "Epoch: 82140 | training loss: 2.3877e-03 | validation loss: 2.2699e-03\n",
      "Epoch: 82150 | training loss: 2.3877e-03 | validation loss: 2.2701e-03\n",
      "Epoch: 82160 | training loss: 2.3883e-03 | validation loss: 2.2733e-03\n",
      "Epoch: 82170 | training loss: 2.4861e-03 | validation loss: 2.3800e-03\n",
      "Epoch: 82180 | training loss: 2.4199e-03 | validation loss: 2.2820e-03\n",
      "Epoch: 82190 | training loss: 2.4323e-03 | validation loss: 2.3209e-03\n",
      "Epoch: 82200 | training loss: 2.4263e-03 | validation loss: 2.3135e-03\n",
      "Epoch: 82210 | training loss: 2.4020e-03 | validation loss: 2.2907e-03\n",
      "Epoch: 82220 | training loss: 2.3893e-03 | validation loss: 2.2753e-03\n",
      "Epoch: 82230 | training loss: 2.3879e-03 | validation loss: 2.2712e-03\n",
      "Epoch: 82240 | training loss: 2.4003e-03 | validation loss: 2.2903e-03\n",
      "Epoch: 82250 | training loss: 2.9277e-03 | validation loss: 2.6386e-03\n",
      "Epoch: 82260 | training loss: 2.5308e-03 | validation loss: 2.2699e-03\n",
      "Epoch: 82270 | training loss: 2.5704e-03 | validation loss: 2.4125e-03\n",
      "Epoch: 82280 | training loss: 2.3928e-03 | validation loss: 2.2567e-03\n",
      "Epoch: 82290 | training loss: 2.3995e-03 | validation loss: 2.2557e-03\n",
      "Epoch: 82300 | training loss: 2.3956e-03 | validation loss: 2.2857e-03\n",
      "Epoch: 82310 | training loss: 2.3884e-03 | validation loss: 2.2629e-03\n",
      "Epoch: 82320 | training loss: 2.3869e-03 | validation loss: 2.2693e-03\n",
      "Epoch: 82330 | training loss: 2.3867e-03 | validation loss: 2.2666e-03\n",
      "Epoch: 82340 | training loss: 2.3867e-03 | validation loss: 2.2678e-03\n",
      "Epoch: 82350 | training loss: 2.3867e-03 | validation loss: 2.2660e-03\n",
      "Epoch: 82360 | training loss: 2.3866e-03 | validation loss: 2.2678e-03\n",
      "Epoch: 82370 | training loss: 2.3865e-03 | validation loss: 2.2669e-03\n",
      "Epoch: 82380 | training loss: 2.3865e-03 | validation loss: 2.2663e-03\n",
      "Epoch: 82390 | training loss: 2.3864e-03 | validation loss: 2.2660e-03\n",
      "Epoch: 82400 | training loss: 2.3865e-03 | validation loss: 2.2646e-03\n",
      "Epoch: 82410 | training loss: 2.3908e-03 | validation loss: 2.2584e-03\n",
      "Epoch: 82420 | training loss: 2.6589e-03 | validation loss: 2.3151e-03\n",
      "Epoch: 82430 | training loss: 2.4252e-03 | validation loss: 2.2716e-03\n",
      "Epoch: 82440 | training loss: 2.6331e-03 | validation loss: 2.3074e-03\n",
      "Epoch: 82450 | training loss: 2.3996e-03 | validation loss: 2.2731e-03\n",
      "Epoch: 82460 | training loss: 2.4185e-03 | validation loss: 2.3053e-03\n",
      "Epoch: 82470 | training loss: 2.3890e-03 | validation loss: 2.2694e-03\n",
      "Epoch: 82480 | training loss: 2.3888e-03 | validation loss: 2.2584e-03\n",
      "Epoch: 82490 | training loss: 2.3873e-03 | validation loss: 2.2686e-03\n",
      "Epoch: 82500 | training loss: 2.3861e-03 | validation loss: 2.2668e-03\n",
      "Epoch: 82510 | training loss: 2.3858e-03 | validation loss: 2.2640e-03\n",
      "Epoch: 82520 | training loss: 2.3858e-03 | validation loss: 2.2661e-03\n",
      "Epoch: 82530 | training loss: 2.3857e-03 | validation loss: 2.2646e-03\n",
      "Epoch: 82540 | training loss: 2.3856e-03 | validation loss: 2.2652e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82550 | training loss: 2.3856e-03 | validation loss: 2.2648e-03\n",
      "Epoch: 82560 | training loss: 2.3855e-03 | validation loss: 2.2648e-03\n",
      "Epoch: 82570 | training loss: 2.3855e-03 | validation loss: 2.2647e-03\n",
      "Epoch: 82580 | training loss: 2.3854e-03 | validation loss: 2.2645e-03\n",
      "Epoch: 82590 | training loss: 2.3854e-03 | validation loss: 2.2643e-03\n",
      "Epoch: 82600 | training loss: 2.3853e-03 | validation loss: 2.2639e-03\n",
      "Epoch: 82610 | training loss: 2.3858e-03 | validation loss: 2.2622e-03\n",
      "Epoch: 82620 | training loss: 2.4410e-03 | validation loss: 2.2719e-03\n",
      "Epoch: 82630 | training loss: 4.0183e-03 | validation loss: 2.8538e-03\n",
      "Epoch: 82640 | training loss: 2.7369e-03 | validation loss: 2.6021e-03\n",
      "Epoch: 82650 | training loss: 2.5755e-03 | validation loss: 2.4702e-03\n",
      "Epoch: 82660 | training loss: 2.4187e-03 | validation loss: 2.3077e-03\n",
      "Epoch: 82670 | training loss: 2.3971e-03 | validation loss: 2.2596e-03\n",
      "Epoch: 82680 | training loss: 2.3854e-03 | validation loss: 2.2608e-03\n",
      "Epoch: 82690 | training loss: 2.3860e-03 | validation loss: 2.2687e-03\n",
      "Epoch: 82700 | training loss: 2.3849e-03 | validation loss: 2.2614e-03\n",
      "Epoch: 82710 | training loss: 2.3848e-03 | validation loss: 2.2610e-03\n",
      "Epoch: 82720 | training loss: 2.3847e-03 | validation loss: 2.2639e-03\n",
      "Epoch: 82730 | training loss: 2.3847e-03 | validation loss: 2.2619e-03\n",
      "Epoch: 82740 | training loss: 2.3846e-03 | validation loss: 2.2632e-03\n",
      "Epoch: 82750 | training loss: 2.3845e-03 | validation loss: 2.2623e-03\n",
      "Epoch: 82760 | training loss: 2.3845e-03 | validation loss: 2.2626e-03\n",
      "Epoch: 82770 | training loss: 2.3844e-03 | validation loss: 2.2621e-03\n",
      "Epoch: 82780 | training loss: 2.3844e-03 | validation loss: 2.2619e-03\n",
      "Epoch: 82790 | training loss: 2.3843e-03 | validation loss: 2.2619e-03\n",
      "Epoch: 82800 | training loss: 2.3843e-03 | validation loss: 2.2619e-03\n",
      "Epoch: 82810 | training loss: 2.3842e-03 | validation loss: 2.2618e-03\n",
      "Epoch: 82820 | training loss: 2.3842e-03 | validation loss: 2.2618e-03\n",
      "Epoch: 82830 | training loss: 2.3842e-03 | validation loss: 2.2631e-03\n",
      "Epoch: 82840 | training loss: 2.3957e-03 | validation loss: 2.2858e-03\n",
      "Epoch: 82850 | training loss: 4.1760e-03 | validation loss: 3.3317e-03\n",
      "Epoch: 82860 | training loss: 2.7180e-03 | validation loss: 2.3901e-03\n",
      "Epoch: 82870 | training loss: 2.5402e-03 | validation loss: 2.3163e-03\n",
      "Epoch: 82880 | training loss: 2.4315e-03 | validation loss: 2.2881e-03\n",
      "Epoch: 82890 | training loss: 2.3994e-03 | validation loss: 2.2824e-03\n",
      "Epoch: 82900 | training loss: 2.3902e-03 | validation loss: 2.2793e-03\n",
      "Epoch: 82910 | training loss: 2.3876e-03 | validation loss: 2.2751e-03\n",
      "Epoch: 82920 | training loss: 2.3851e-03 | validation loss: 2.2687e-03\n",
      "Epoch: 82930 | training loss: 2.3839e-03 | validation loss: 2.2631e-03\n",
      "Epoch: 82940 | training loss: 2.3837e-03 | validation loss: 2.2608e-03\n",
      "Epoch: 82950 | training loss: 2.3835e-03 | validation loss: 2.2608e-03\n",
      "Epoch: 82960 | training loss: 2.3835e-03 | validation loss: 2.2604e-03\n",
      "Epoch: 82970 | training loss: 2.3834e-03 | validation loss: 2.2595e-03\n",
      "Epoch: 82980 | training loss: 2.3834e-03 | validation loss: 2.2596e-03\n",
      "Epoch: 82990 | training loss: 2.3833e-03 | validation loss: 2.2597e-03\n",
      "Epoch: 83000 | training loss: 2.3833e-03 | validation loss: 2.2596e-03\n",
      "Epoch: 83010 | training loss: 2.3832e-03 | validation loss: 2.2595e-03\n",
      "Epoch: 83020 | training loss: 2.3831e-03 | validation loss: 2.2593e-03\n",
      "Epoch: 83030 | training loss: 2.3831e-03 | validation loss: 2.2593e-03\n",
      "Epoch: 83040 | training loss: 2.3830e-03 | validation loss: 2.2592e-03\n",
      "Epoch: 83050 | training loss: 2.3830e-03 | validation loss: 2.2590e-03\n",
      "Epoch: 83060 | training loss: 2.3829e-03 | validation loss: 2.2589e-03\n",
      "Epoch: 83070 | training loss: 2.3829e-03 | validation loss: 2.2588e-03\n",
      "Epoch: 83080 | training loss: 2.3828e-03 | validation loss: 2.2586e-03\n",
      "Epoch: 83090 | training loss: 2.3828e-03 | validation loss: 2.2583e-03\n",
      "Epoch: 83100 | training loss: 2.3832e-03 | validation loss: 2.2561e-03\n",
      "Epoch: 83110 | training loss: 2.4731e-03 | validation loss: 2.2662e-03\n",
      "Epoch: 83120 | training loss: 3.3596e-03 | validation loss: 2.5636e-03\n",
      "Epoch: 83130 | training loss: 2.9544e-03 | validation loss: 2.4234e-03\n",
      "Epoch: 83140 | training loss: 2.5526e-03 | validation loss: 2.2854e-03\n",
      "Epoch: 83150 | training loss: 2.4256e-03 | validation loss: 2.2522e-03\n",
      "Epoch: 83160 | training loss: 2.3909e-03 | validation loss: 2.2521e-03\n",
      "Epoch: 83170 | training loss: 2.3837e-03 | validation loss: 2.2581e-03\n",
      "Epoch: 83180 | training loss: 2.3831e-03 | validation loss: 2.2624e-03\n",
      "Epoch: 83190 | training loss: 2.3830e-03 | validation loss: 2.2629e-03\n",
      "Epoch: 83200 | training loss: 2.3825e-03 | validation loss: 2.2602e-03\n",
      "Epoch: 83210 | training loss: 2.3823e-03 | validation loss: 2.2573e-03\n",
      "Epoch: 83220 | training loss: 2.3822e-03 | validation loss: 2.2564e-03\n",
      "Epoch: 83230 | training loss: 2.3821e-03 | validation loss: 2.2571e-03\n",
      "Epoch: 83240 | training loss: 2.3820e-03 | validation loss: 2.2573e-03\n",
      "Epoch: 83250 | training loss: 2.3820e-03 | validation loss: 2.2567e-03\n",
      "Epoch: 83260 | training loss: 2.3819e-03 | validation loss: 2.2568e-03\n",
      "Epoch: 83270 | training loss: 2.3819e-03 | validation loss: 2.2568e-03\n",
      "Epoch: 83280 | training loss: 2.3818e-03 | validation loss: 2.2565e-03\n",
      "Epoch: 83290 | training loss: 2.3818e-03 | validation loss: 2.2565e-03\n",
      "Epoch: 83300 | training loss: 2.3817e-03 | validation loss: 2.2564e-03\n",
      "Epoch: 83310 | training loss: 2.3817e-03 | validation loss: 2.2563e-03\n",
      "Epoch: 83320 | training loss: 2.3816e-03 | validation loss: 2.2561e-03\n",
      "Epoch: 83330 | training loss: 2.3816e-03 | validation loss: 2.2560e-03\n",
      "Epoch: 83340 | training loss: 2.3815e-03 | validation loss: 2.2559e-03\n",
      "Epoch: 83350 | training loss: 2.3814e-03 | validation loss: 2.2558e-03\n",
      "Epoch: 83360 | training loss: 2.3814e-03 | validation loss: 2.2556e-03\n",
      "Epoch: 83370 | training loss: 2.3813e-03 | validation loss: 2.2554e-03\n",
      "Epoch: 83380 | training loss: 2.3813e-03 | validation loss: 2.2545e-03\n",
      "Epoch: 83390 | training loss: 2.3841e-03 | validation loss: 2.2460e-03\n",
      "Epoch: 83400 | training loss: 3.0678e-03 | validation loss: 2.4488e-03\n",
      "Epoch: 83410 | training loss: 3.1100e-03 | validation loss: 2.8664e-03\n",
      "Epoch: 83420 | training loss: 2.5774e-03 | validation loss: 2.4449e-03\n",
      "Epoch: 83430 | training loss: 2.3815e-03 | validation loss: 2.2516e-03\n",
      "Epoch: 83440 | training loss: 2.4089e-03 | validation loss: 2.2525e-03\n",
      "Epoch: 83450 | training loss: 2.3921e-03 | validation loss: 2.2446e-03\n",
      "Epoch: 83460 | training loss: 2.3818e-03 | validation loss: 2.2480e-03\n",
      "Epoch: 83470 | training loss: 2.3810e-03 | validation loss: 2.2565e-03\n",
      "Epoch: 83480 | training loss: 2.3811e-03 | validation loss: 2.2568e-03\n",
      "Epoch: 83490 | training loss: 2.3809e-03 | validation loss: 2.2556e-03\n",
      "Epoch: 83500 | training loss: 2.3808e-03 | validation loss: 2.2550e-03\n",
      "Epoch: 83510 | training loss: 2.3807e-03 | validation loss: 2.2540e-03\n",
      "Epoch: 83520 | training loss: 2.3806e-03 | validation loss: 2.2537e-03\n",
      "Epoch: 83530 | training loss: 2.3805e-03 | validation loss: 2.2534e-03\n",
      "Epoch: 83540 | training loss: 2.3805e-03 | validation loss: 2.2531e-03\n",
      "Epoch: 83550 | training loss: 2.3804e-03 | validation loss: 2.2529e-03\n",
      "Epoch: 83560 | training loss: 2.3804e-03 | validation loss: 2.2528e-03\n",
      "Epoch: 83570 | training loss: 2.3803e-03 | validation loss: 2.2527e-03\n",
      "Epoch: 83580 | training loss: 2.3803e-03 | validation loss: 2.2526e-03\n",
      "Epoch: 83590 | training loss: 2.3802e-03 | validation loss: 2.2528e-03\n",
      "Epoch: 83600 | training loss: 2.3813e-03 | validation loss: 2.2567e-03\n",
      "Epoch: 83610 | training loss: 2.6143e-03 | validation loss: 2.4210e-03\n",
      "Epoch: 83620 | training loss: 2.4209e-03 | validation loss: 2.2736e-03\n",
      "Epoch: 83630 | training loss: 2.4622e-03 | validation loss: 2.3370e-03\n",
      "Epoch: 83640 | training loss: 2.4176e-03 | validation loss: 2.2920e-03\n",
      "Epoch: 83650 | training loss: 2.3967e-03 | validation loss: 2.2695e-03\n",
      "Epoch: 83660 | training loss: 2.3885e-03 | validation loss: 2.2639e-03\n",
      "Epoch: 83670 | training loss: 2.3848e-03 | validation loss: 2.2619e-03\n",
      "Epoch: 83680 | training loss: 2.3824e-03 | validation loss: 2.2595e-03\n",
      "Epoch: 83690 | training loss: 2.3806e-03 | validation loss: 2.2563e-03\n",
      "Epoch: 83700 | training loss: 2.3798e-03 | validation loss: 2.2531e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83710 | training loss: 2.3797e-03 | validation loss: 2.2511e-03\n",
      "Epoch: 83720 | training loss: 2.3796e-03 | validation loss: 2.2507e-03\n",
      "Epoch: 83730 | training loss: 2.3795e-03 | validation loss: 2.2513e-03\n",
      "Epoch: 83740 | training loss: 2.3795e-03 | validation loss: 2.2516e-03\n",
      "Epoch: 83750 | training loss: 2.3794e-03 | validation loss: 2.2512e-03\n",
      "Epoch: 83760 | training loss: 2.3794e-03 | validation loss: 2.2509e-03\n",
      "Epoch: 83770 | training loss: 2.3793e-03 | validation loss: 2.2510e-03\n",
      "Epoch: 83780 | training loss: 2.3793e-03 | validation loss: 2.2508e-03\n",
      "Epoch: 83790 | training loss: 2.3792e-03 | validation loss: 2.2507e-03\n",
      "Epoch: 83800 | training loss: 2.3792e-03 | validation loss: 2.2507e-03\n",
      "Epoch: 83810 | training loss: 2.3791e-03 | validation loss: 2.2505e-03\n",
      "Epoch: 83820 | training loss: 2.3791e-03 | validation loss: 2.2504e-03\n",
      "Epoch: 83830 | training loss: 2.3790e-03 | validation loss: 2.2503e-03\n",
      "Epoch: 83840 | training loss: 2.3790e-03 | validation loss: 2.2502e-03\n",
      "Epoch: 83850 | training loss: 2.3789e-03 | validation loss: 2.2501e-03\n",
      "Epoch: 83860 | training loss: 2.3789e-03 | validation loss: 2.2500e-03\n",
      "Epoch: 83870 | training loss: 2.3788e-03 | validation loss: 2.2498e-03\n",
      "Epoch: 83880 | training loss: 2.3788e-03 | validation loss: 2.2497e-03\n",
      "Epoch: 83890 | training loss: 2.3787e-03 | validation loss: 2.2492e-03\n",
      "Epoch: 83900 | training loss: 2.3797e-03 | validation loss: 2.2459e-03\n",
      "Epoch: 83910 | training loss: 2.6418e-03 | validation loss: 2.3086e-03\n",
      "Epoch: 83920 | training loss: 2.5374e-03 | validation loss: 2.3679e-03\n",
      "Epoch: 83930 | training loss: 2.3926e-03 | validation loss: 2.2429e-03\n",
      "Epoch: 83940 | training loss: 2.4034e-03 | validation loss: 2.2589e-03\n",
      "Epoch: 83950 | training loss: 2.4042e-03 | validation loss: 2.2632e-03\n",
      "Epoch: 83960 | training loss: 2.3935e-03 | validation loss: 2.2575e-03\n",
      "Epoch: 83970 | training loss: 2.3849e-03 | validation loss: 2.2520e-03\n",
      "Epoch: 83980 | training loss: 2.3806e-03 | validation loss: 2.2497e-03\n",
      "Epoch: 83990 | training loss: 2.3788e-03 | validation loss: 2.2495e-03\n",
      "Epoch: 84000 | training loss: 2.3783e-03 | validation loss: 2.2497e-03\n",
      "Epoch: 84010 | training loss: 2.3781e-03 | validation loss: 2.2491e-03\n",
      "Epoch: 84020 | training loss: 2.3781e-03 | validation loss: 2.2481e-03\n",
      "Epoch: 84030 | training loss: 2.3780e-03 | validation loss: 2.2477e-03\n",
      "Epoch: 84040 | training loss: 2.3780e-03 | validation loss: 2.2480e-03\n",
      "Epoch: 84050 | training loss: 2.3779e-03 | validation loss: 2.2481e-03\n",
      "Epoch: 84060 | training loss: 2.3779e-03 | validation loss: 2.2479e-03\n",
      "Epoch: 84070 | training loss: 2.3778e-03 | validation loss: 2.2478e-03\n",
      "Epoch: 84080 | training loss: 2.3778e-03 | validation loss: 2.2476e-03\n",
      "Epoch: 84090 | training loss: 2.3777e-03 | validation loss: 2.2475e-03\n",
      "Epoch: 84100 | training loss: 2.3777e-03 | validation loss: 2.2474e-03\n",
      "Epoch: 84110 | training loss: 2.3776e-03 | validation loss: 2.2473e-03\n",
      "Epoch: 84120 | training loss: 2.3776e-03 | validation loss: 2.2472e-03\n",
      "Epoch: 84130 | training loss: 2.3775e-03 | validation loss: 2.2471e-03\n",
      "Epoch: 84140 | training loss: 2.3774e-03 | validation loss: 2.2469e-03\n",
      "Epoch: 84150 | training loss: 2.3774e-03 | validation loss: 2.2468e-03\n",
      "Epoch: 84160 | training loss: 2.3773e-03 | validation loss: 2.2467e-03\n",
      "Epoch: 84170 | training loss: 2.3773e-03 | validation loss: 2.2466e-03\n",
      "Epoch: 84180 | training loss: 2.3772e-03 | validation loss: 2.2468e-03\n",
      "Epoch: 84190 | training loss: 2.3777e-03 | validation loss: 2.2513e-03\n",
      "Epoch: 84200 | training loss: 2.5227e-03 | validation loss: 2.4160e-03\n",
      "Epoch: 84210 | training loss: 2.8721e-03 | validation loss: 2.5519e-03\n",
      "Epoch: 84220 | training loss: 2.4958e-03 | validation loss: 2.2618e-03\n",
      "Epoch: 84230 | training loss: 2.4585e-03 | validation loss: 2.2601e-03\n",
      "Epoch: 84240 | training loss: 2.3994e-03 | validation loss: 2.2512e-03\n",
      "Epoch: 84250 | training loss: 2.3798e-03 | validation loss: 2.2362e-03\n",
      "Epoch: 84260 | training loss: 2.3788e-03 | validation loss: 2.2545e-03\n",
      "Epoch: 84270 | training loss: 2.3781e-03 | validation loss: 2.2463e-03\n",
      "Epoch: 84280 | training loss: 2.3772e-03 | validation loss: 2.2481e-03\n",
      "Epoch: 84290 | training loss: 2.3769e-03 | validation loss: 2.2470e-03\n",
      "Epoch: 84300 | training loss: 2.3767e-03 | validation loss: 2.2445e-03\n",
      "Epoch: 84310 | training loss: 2.3766e-03 | validation loss: 2.2441e-03\n",
      "Epoch: 84320 | training loss: 2.3766e-03 | validation loss: 2.2435e-03\n",
      "Epoch: 84330 | training loss: 2.3769e-03 | validation loss: 2.2414e-03\n",
      "Epoch: 84340 | training loss: 2.3882e-03 | validation loss: 2.2330e-03\n",
      "Epoch: 84350 | training loss: 3.0064e-03 | validation loss: 2.4137e-03\n",
      "Epoch: 84360 | training loss: 2.6482e-03 | validation loss: 2.4460e-03\n",
      "Epoch: 84370 | training loss: 2.5353e-03 | validation loss: 2.2598e-03\n",
      "Epoch: 84380 | training loss: 2.3943e-03 | validation loss: 2.2318e-03\n",
      "Epoch: 84390 | training loss: 2.3998e-03 | validation loss: 2.2755e-03\n",
      "Epoch: 84400 | training loss: 2.3763e-03 | validation loss: 2.2448e-03\n",
      "Epoch: 84410 | training loss: 2.3793e-03 | validation loss: 2.2366e-03\n",
      "Epoch: 84420 | training loss: 2.3772e-03 | validation loss: 2.2484e-03\n",
      "Epoch: 84430 | training loss: 2.3761e-03 | validation loss: 2.2417e-03\n",
      "Epoch: 84440 | training loss: 2.3759e-03 | validation loss: 2.2435e-03\n",
      "Epoch: 84450 | training loss: 2.3759e-03 | validation loss: 2.2428e-03\n",
      "Epoch: 84460 | training loss: 2.3758e-03 | validation loss: 2.2433e-03\n",
      "Epoch: 84470 | training loss: 2.3758e-03 | validation loss: 2.2423e-03\n",
      "Epoch: 84480 | training loss: 2.3757e-03 | validation loss: 2.2429e-03\n",
      "Epoch: 84490 | training loss: 2.3757e-03 | validation loss: 2.2428e-03\n",
      "Epoch: 84500 | training loss: 2.3756e-03 | validation loss: 2.2425e-03\n",
      "Epoch: 84510 | training loss: 2.3756e-03 | validation loss: 2.2423e-03\n",
      "Epoch: 84520 | training loss: 2.3755e-03 | validation loss: 2.2420e-03\n",
      "Epoch: 84530 | training loss: 2.3756e-03 | validation loss: 2.2407e-03\n",
      "Epoch: 84540 | training loss: 2.3859e-03 | validation loss: 2.2325e-03\n",
      "Epoch: 84550 | training loss: 3.6702e-03 | validation loss: 2.6853e-03\n",
      "Epoch: 84560 | training loss: 3.1777e-03 | validation loss: 2.7383e-03\n",
      "Epoch: 84570 | training loss: 2.6195e-03 | validation loss: 2.4226e-03\n",
      "Epoch: 84580 | training loss: 2.4148e-03 | validation loss: 2.2875e-03\n",
      "Epoch: 84590 | training loss: 2.3761e-03 | validation loss: 2.2472e-03\n",
      "Epoch: 84600 | training loss: 2.3784e-03 | validation loss: 2.2374e-03\n",
      "Epoch: 84610 | training loss: 2.3796e-03 | validation loss: 2.2357e-03\n",
      "Epoch: 84620 | training loss: 2.3760e-03 | validation loss: 2.2380e-03\n",
      "Epoch: 84630 | training loss: 2.3751e-03 | validation loss: 2.2421e-03\n",
      "Epoch: 84640 | training loss: 2.3752e-03 | validation loss: 2.2430e-03\n",
      "Epoch: 84650 | training loss: 2.3749e-03 | validation loss: 2.2409e-03\n",
      "Epoch: 84660 | training loss: 2.3749e-03 | validation loss: 2.2404e-03\n",
      "Epoch: 84670 | training loss: 2.3748e-03 | validation loss: 2.2411e-03\n",
      "Epoch: 84680 | training loss: 2.3747e-03 | validation loss: 2.2407e-03\n",
      "Epoch: 84690 | training loss: 2.3747e-03 | validation loss: 2.2405e-03\n",
      "Epoch: 84700 | training loss: 2.3746e-03 | validation loss: 2.2406e-03\n",
      "Epoch: 84710 | training loss: 2.3746e-03 | validation loss: 2.2404e-03\n",
      "Epoch: 84720 | training loss: 2.3745e-03 | validation loss: 2.2403e-03\n",
      "Epoch: 84730 | training loss: 2.3745e-03 | validation loss: 2.2402e-03\n",
      "Epoch: 84740 | training loss: 2.3744e-03 | validation loss: 2.2401e-03\n",
      "Epoch: 84750 | training loss: 2.3744e-03 | validation loss: 2.2400e-03\n",
      "Epoch: 84760 | training loss: 2.3743e-03 | validation loss: 2.2399e-03\n",
      "Epoch: 84770 | training loss: 2.3743e-03 | validation loss: 2.2397e-03\n",
      "Epoch: 84780 | training loss: 2.3742e-03 | validation loss: 2.2396e-03\n",
      "Epoch: 84790 | training loss: 2.3742e-03 | validation loss: 2.2398e-03\n",
      "Epoch: 84800 | training loss: 2.3745e-03 | validation loss: 2.2422e-03\n",
      "Epoch: 84810 | training loss: 2.4452e-03 | validation loss: 2.3060e-03\n",
      "Epoch: 84820 | training loss: 3.9235e-03 | validation loss: 3.1357e-03\n",
      "Epoch: 84830 | training loss: 2.9435e-03 | validation loss: 2.5908e-03\n",
      "Epoch: 84840 | training loss: 2.5536e-03 | validation loss: 2.3557e-03\n",
      "Epoch: 84850 | training loss: 2.4385e-03 | validation loss: 2.2813e-03\n",
      "Epoch: 84860 | training loss: 2.3958e-03 | validation loss: 2.2569e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84870 | training loss: 2.3805e-03 | validation loss: 2.2477e-03\n",
      "Epoch: 84880 | training loss: 2.3751e-03 | validation loss: 2.2411e-03\n",
      "Epoch: 84890 | training loss: 2.3737e-03 | validation loss: 2.2380e-03\n",
      "Epoch: 84900 | training loss: 2.3737e-03 | validation loss: 2.2381e-03\n",
      "Epoch: 84910 | training loss: 2.3737e-03 | validation loss: 2.2376e-03\n",
      "Epoch: 84920 | training loss: 2.3735e-03 | validation loss: 2.2376e-03\n",
      "Epoch: 84930 | training loss: 2.3735e-03 | validation loss: 2.2380e-03\n",
      "Epoch: 84940 | training loss: 2.3734e-03 | validation loss: 2.2379e-03\n",
      "Epoch: 84950 | training loss: 2.3734e-03 | validation loss: 2.2377e-03\n",
      "Epoch: 84960 | training loss: 2.3733e-03 | validation loss: 2.2375e-03\n",
      "Epoch: 84970 | training loss: 2.3732e-03 | validation loss: 2.2375e-03\n",
      "Epoch: 84980 | training loss: 2.3732e-03 | validation loss: 2.2374e-03\n",
      "Epoch: 84990 | training loss: 2.3731e-03 | validation loss: 2.2373e-03\n",
      "Epoch: 85000 | training loss: 2.3731e-03 | validation loss: 2.2372e-03\n",
      "Epoch: 85010 | training loss: 2.3730e-03 | validation loss: 2.2371e-03\n",
      "Epoch: 85020 | training loss: 2.3730e-03 | validation loss: 2.2374e-03\n",
      "Epoch: 85030 | training loss: 2.3736e-03 | validation loss: 2.2425e-03\n",
      "Epoch: 85040 | training loss: 2.5073e-03 | validation loss: 2.3973e-03\n",
      "Epoch: 85050 | training loss: 2.5781e-03 | validation loss: 2.3677e-03\n",
      "Epoch: 85060 | training loss: 2.3784e-03 | validation loss: 2.2488e-03\n",
      "Epoch: 85070 | training loss: 2.3988e-03 | validation loss: 2.2209e-03\n",
      "Epoch: 85080 | training loss: 2.3829e-03 | validation loss: 2.2526e-03\n",
      "Epoch: 85090 | training loss: 2.3742e-03 | validation loss: 2.2287e-03\n",
      "Epoch: 85100 | training loss: 2.3727e-03 | validation loss: 2.2373e-03\n",
      "Epoch: 85110 | training loss: 2.3726e-03 | validation loss: 2.2353e-03\n",
      "Epoch: 85120 | training loss: 2.3727e-03 | validation loss: 2.2380e-03\n",
      "Epoch: 85130 | training loss: 2.3726e-03 | validation loss: 2.2350e-03\n",
      "Epoch: 85140 | training loss: 2.3724e-03 | validation loss: 2.2365e-03\n",
      "Epoch: 85150 | training loss: 2.3723e-03 | validation loss: 2.2355e-03\n",
      "Epoch: 85160 | training loss: 2.3723e-03 | validation loss: 2.2345e-03\n",
      "Epoch: 85170 | training loss: 2.3722e-03 | validation loss: 2.2343e-03\n",
      "Epoch: 85180 | training loss: 2.3722e-03 | validation loss: 2.2337e-03\n",
      "Epoch: 85190 | training loss: 2.3737e-03 | validation loss: 2.2290e-03\n",
      "Epoch: 85200 | training loss: 2.5031e-03 | validation loss: 2.2383e-03\n",
      "Epoch: 85210 | training loss: 2.9675e-03 | validation loss: 2.4083e-03\n",
      "Epoch: 85220 | training loss: 2.5998e-03 | validation loss: 2.2688e-03\n",
      "Epoch: 85230 | training loss: 2.3815e-03 | validation loss: 2.2436e-03\n",
      "Epoch: 85240 | training loss: 2.4190e-03 | validation loss: 2.2800e-03\n",
      "Epoch: 85250 | training loss: 2.3787e-03 | validation loss: 2.2442e-03\n",
      "Epoch: 85260 | training loss: 2.3748e-03 | validation loss: 2.2258e-03\n",
      "Epoch: 85270 | training loss: 2.3730e-03 | validation loss: 2.2290e-03\n",
      "Epoch: 85280 | training loss: 2.3722e-03 | validation loss: 2.2377e-03\n",
      "Epoch: 85290 | training loss: 2.3717e-03 | validation loss: 2.2346e-03\n",
      "Epoch: 85300 | training loss: 2.3717e-03 | validation loss: 2.2321e-03\n",
      "Epoch: 85310 | training loss: 2.3716e-03 | validation loss: 2.2341e-03\n",
      "Epoch: 85320 | training loss: 2.3715e-03 | validation loss: 2.2328e-03\n",
      "Epoch: 85330 | training loss: 2.3714e-03 | validation loss: 2.2335e-03\n",
      "Epoch: 85340 | training loss: 2.3714e-03 | validation loss: 2.2328e-03\n",
      "Epoch: 85350 | training loss: 2.3713e-03 | validation loss: 2.2331e-03\n",
      "Epoch: 85360 | training loss: 2.3713e-03 | validation loss: 2.2328e-03\n",
      "Epoch: 85370 | training loss: 2.3712e-03 | validation loss: 2.2327e-03\n",
      "Epoch: 85380 | training loss: 2.3712e-03 | validation loss: 2.2326e-03\n",
      "Epoch: 85390 | training loss: 2.3711e-03 | validation loss: 2.2325e-03\n",
      "Epoch: 85400 | training loss: 2.3711e-03 | validation loss: 2.2324e-03\n",
      "Epoch: 85410 | training loss: 2.3710e-03 | validation loss: 2.2326e-03\n",
      "Epoch: 85420 | training loss: 2.3713e-03 | validation loss: 2.2350e-03\n",
      "Epoch: 85430 | training loss: 2.4291e-03 | validation loss: 2.2926e-03\n",
      "Epoch: 85440 | training loss: 4.2741e-03 | validation loss: 3.3116e-03\n",
      "Epoch: 85450 | training loss: 2.8220e-03 | validation loss: 2.5430e-03\n",
      "Epoch: 85460 | training loss: 2.4748e-03 | validation loss: 2.3338e-03\n",
      "Epoch: 85470 | training loss: 2.3972e-03 | validation loss: 2.2709e-03\n",
      "Epoch: 85480 | training loss: 2.3752e-03 | validation loss: 2.2436e-03\n",
      "Epoch: 85490 | training loss: 2.3708e-03 | validation loss: 2.2318e-03\n",
      "Epoch: 85500 | training loss: 2.3711e-03 | validation loss: 2.2281e-03\n",
      "Epoch: 85510 | training loss: 2.3715e-03 | validation loss: 2.2271e-03\n",
      "Epoch: 85520 | training loss: 2.3709e-03 | validation loss: 2.2279e-03\n",
      "Epoch: 85530 | training loss: 2.3704e-03 | validation loss: 2.2306e-03\n",
      "Epoch: 85540 | training loss: 2.3704e-03 | validation loss: 2.2319e-03\n",
      "Epoch: 85550 | training loss: 2.3703e-03 | validation loss: 2.2314e-03\n",
      "Epoch: 85560 | training loss: 2.3703e-03 | validation loss: 2.2304e-03\n",
      "Epoch: 85570 | training loss: 2.3702e-03 | validation loss: 2.2305e-03\n",
      "Epoch: 85580 | training loss: 2.3702e-03 | validation loss: 2.2307e-03\n",
      "Epoch: 85590 | training loss: 2.3701e-03 | validation loss: 2.2304e-03\n",
      "Epoch: 85600 | training loss: 2.3701e-03 | validation loss: 2.2303e-03\n",
      "Epoch: 85610 | training loss: 2.3700e-03 | validation loss: 2.2303e-03\n",
      "Epoch: 85620 | training loss: 2.3700e-03 | validation loss: 2.2301e-03\n",
      "Epoch: 85630 | training loss: 2.3699e-03 | validation loss: 2.2301e-03\n",
      "Epoch: 85640 | training loss: 2.3699e-03 | validation loss: 2.2310e-03\n",
      "Epoch: 85650 | training loss: 2.3777e-03 | validation loss: 2.2475e-03\n",
      "Epoch: 85660 | training loss: 3.0529e-03 | validation loss: 2.8240e-03\n",
      "Epoch: 85670 | training loss: 2.3706e-03 | validation loss: 2.2270e-03\n",
      "Epoch: 85680 | training loss: 2.3951e-03 | validation loss: 2.2261e-03\n",
      "Epoch: 85690 | training loss: 2.3903e-03 | validation loss: 2.2319e-03\n",
      "Epoch: 85700 | training loss: 2.3790e-03 | validation loss: 2.2248e-03\n",
      "Epoch: 85710 | training loss: 2.3719e-03 | validation loss: 2.2229e-03\n",
      "Epoch: 85720 | training loss: 2.3700e-03 | validation loss: 2.2243e-03\n",
      "Epoch: 85730 | training loss: 2.3745e-03 | validation loss: 2.2206e-03\n",
      "Epoch: 85740 | training loss: 2.4970e-03 | validation loss: 2.2286e-03\n",
      "Epoch: 85750 | training loss: 2.9025e-03 | validation loss: 2.3600e-03\n",
      "Epoch: 85760 | training loss: 2.4337e-03 | validation loss: 2.2926e-03\n",
      "Epoch: 85770 | training loss: 2.3729e-03 | validation loss: 2.2356e-03\n",
      "Epoch: 85780 | training loss: 2.3870e-03 | validation loss: 2.2142e-03\n",
      "Epoch: 85790 | training loss: 2.3799e-03 | validation loss: 2.2481e-03\n",
      "Epoch: 85800 | training loss: 2.3733e-03 | validation loss: 2.2203e-03\n",
      "Epoch: 85810 | training loss: 2.3707e-03 | validation loss: 2.2347e-03\n",
      "Epoch: 85820 | training loss: 2.3695e-03 | validation loss: 2.2242e-03\n",
      "Epoch: 85830 | training loss: 2.3689e-03 | validation loss: 2.2280e-03\n",
      "Epoch: 85840 | training loss: 2.3689e-03 | validation loss: 2.2288e-03\n",
      "Epoch: 85850 | training loss: 2.3688e-03 | validation loss: 2.2268e-03\n",
      "Epoch: 85860 | training loss: 2.3688e-03 | validation loss: 2.2259e-03\n",
      "Epoch: 85870 | training loss: 2.3690e-03 | validation loss: 2.2244e-03\n",
      "Epoch: 85880 | training loss: 2.3749e-03 | validation loss: 2.2179e-03\n",
      "Epoch: 85890 | training loss: 2.6398e-03 | validation loss: 2.2744e-03\n",
      "Epoch: 85900 | training loss: 2.4352e-03 | validation loss: 2.2347e-03\n",
      "Epoch: 85910 | training loss: 2.4863e-03 | validation loss: 2.2272e-03\n",
      "Epoch: 85920 | training loss: 2.4384e-03 | validation loss: 2.2872e-03\n",
      "Epoch: 85930 | training loss: 2.3697e-03 | validation loss: 2.2339e-03\n",
      "Epoch: 85940 | training loss: 2.3779e-03 | validation loss: 2.2203e-03\n",
      "Epoch: 85950 | training loss: 2.3721e-03 | validation loss: 2.2332e-03\n",
      "Epoch: 85960 | training loss: 2.3687e-03 | validation loss: 2.2241e-03\n",
      "Epoch: 85970 | training loss: 2.3682e-03 | validation loss: 2.2271e-03\n",
      "Epoch: 85980 | training loss: 2.3682e-03 | validation loss: 2.2250e-03\n",
      "Epoch: 85990 | training loss: 2.3681e-03 | validation loss: 2.2267e-03\n",
      "Epoch: 86000 | training loss: 2.3681e-03 | validation loss: 2.2248e-03\n",
      "Epoch: 86010 | training loss: 2.3680e-03 | validation loss: 2.2259e-03\n",
      "Epoch: 86020 | training loss: 2.3679e-03 | validation loss: 2.2258e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86030 | training loss: 2.3679e-03 | validation loss: 2.2254e-03\n",
      "Epoch: 86040 | training loss: 2.3678e-03 | validation loss: 2.2253e-03\n",
      "Epoch: 86050 | training loss: 2.3678e-03 | validation loss: 2.2254e-03\n",
      "Epoch: 86060 | training loss: 2.3680e-03 | validation loss: 2.2270e-03\n",
      "Epoch: 86070 | training loss: 2.3898e-03 | validation loss: 2.2538e-03\n",
      "Epoch: 86080 | training loss: 4.2776e-03 | validation loss: 3.3019e-03\n",
      "Epoch: 86090 | training loss: 2.5829e-03 | validation loss: 2.2551e-03\n",
      "Epoch: 86100 | training loss: 2.5792e-03 | validation loss: 2.2820e-03\n",
      "Epoch: 86110 | training loss: 2.4525e-03 | validation loss: 2.2545e-03\n",
      "Epoch: 86120 | training loss: 2.3880e-03 | validation loss: 2.2296e-03\n",
      "Epoch: 86130 | training loss: 2.3686e-03 | validation loss: 2.2209e-03\n",
      "Epoch: 86140 | training loss: 2.3681e-03 | validation loss: 2.2258e-03\n",
      "Epoch: 86150 | training loss: 2.3687e-03 | validation loss: 2.2276e-03\n",
      "Epoch: 86160 | training loss: 2.3674e-03 | validation loss: 2.2243e-03\n",
      "Epoch: 86170 | training loss: 2.3673e-03 | validation loss: 2.2236e-03\n",
      "Epoch: 86180 | training loss: 2.3672e-03 | validation loss: 2.2232e-03\n",
      "Epoch: 86190 | training loss: 2.3671e-03 | validation loss: 2.2240e-03\n",
      "Epoch: 86200 | training loss: 2.3670e-03 | validation loss: 2.2235e-03\n",
      "Epoch: 86210 | training loss: 2.3670e-03 | validation loss: 2.2233e-03\n",
      "Epoch: 86220 | training loss: 2.3669e-03 | validation loss: 2.2234e-03\n",
      "Epoch: 86230 | training loss: 2.3669e-03 | validation loss: 2.2232e-03\n",
      "Epoch: 86240 | training loss: 2.3668e-03 | validation loss: 2.2231e-03\n",
      "Epoch: 86250 | training loss: 2.3668e-03 | validation loss: 2.2230e-03\n",
      "Epoch: 86260 | training loss: 2.3667e-03 | validation loss: 2.2229e-03\n",
      "Epoch: 86270 | training loss: 2.3667e-03 | validation loss: 2.2227e-03\n",
      "Epoch: 86280 | training loss: 2.3666e-03 | validation loss: 2.2217e-03\n",
      "Epoch: 86290 | training loss: 2.3733e-03 | validation loss: 2.2108e-03\n",
      "Epoch: 86300 | training loss: 3.7928e-03 | validation loss: 2.8641e-03\n",
      "Epoch: 86310 | training loss: 2.7203e-03 | validation loss: 2.4352e-03\n",
      "Epoch: 86320 | training loss: 2.5295e-03 | validation loss: 2.2601e-03\n",
      "Epoch: 86330 | training loss: 2.3860e-03 | validation loss: 2.2121e-03\n",
      "Epoch: 86340 | training loss: 2.3739e-03 | validation loss: 2.2411e-03\n",
      "Epoch: 86350 | training loss: 2.3740e-03 | validation loss: 2.2271e-03\n",
      "Epoch: 86360 | training loss: 2.3685e-03 | validation loss: 2.2305e-03\n",
      "Epoch: 86370 | training loss: 2.3665e-03 | validation loss: 2.2207e-03\n",
      "Epoch: 86380 | training loss: 2.3662e-03 | validation loss: 2.2220e-03\n",
      "Epoch: 86390 | training loss: 2.3662e-03 | validation loss: 2.2192e-03\n",
      "Epoch: 86400 | training loss: 2.3661e-03 | validation loss: 2.2208e-03\n",
      "Epoch: 86410 | training loss: 2.3660e-03 | validation loss: 2.2206e-03\n",
      "Epoch: 86420 | training loss: 2.3659e-03 | validation loss: 2.2201e-03\n",
      "Epoch: 86430 | training loss: 2.3659e-03 | validation loss: 2.2199e-03\n",
      "Epoch: 86440 | training loss: 2.3659e-03 | validation loss: 2.2190e-03\n",
      "Epoch: 86450 | training loss: 2.3677e-03 | validation loss: 2.2144e-03\n",
      "Epoch: 86460 | training loss: 2.4793e-03 | validation loss: 2.2206e-03\n",
      "Epoch: 86470 | training loss: 3.1632e-03 | validation loss: 2.4607e-03\n",
      "Epoch: 86480 | training loss: 2.4130e-03 | validation loss: 2.2107e-03\n",
      "Epoch: 86490 | training loss: 2.4601e-03 | validation loss: 2.3083e-03\n",
      "Epoch: 86500 | training loss: 2.3870e-03 | validation loss: 2.2509e-03\n",
      "Epoch: 86510 | training loss: 2.3737e-03 | validation loss: 2.2113e-03\n",
      "Epoch: 86520 | training loss: 2.3679e-03 | validation loss: 2.2142e-03\n",
      "Epoch: 86530 | training loss: 2.3674e-03 | validation loss: 2.2270e-03\n",
      "Epoch: 86540 | training loss: 2.3654e-03 | validation loss: 2.2188e-03\n",
      "Epoch: 86550 | training loss: 2.3654e-03 | validation loss: 2.2181e-03\n",
      "Epoch: 86560 | training loss: 2.3654e-03 | validation loss: 2.2207e-03\n",
      "Epoch: 86570 | training loss: 2.3653e-03 | validation loss: 2.2182e-03\n",
      "Epoch: 86580 | training loss: 2.3652e-03 | validation loss: 2.2196e-03\n",
      "Epoch: 86590 | training loss: 2.3651e-03 | validation loss: 2.2187e-03\n",
      "Epoch: 86600 | training loss: 2.3651e-03 | validation loss: 2.2189e-03\n",
      "Epoch: 86610 | training loss: 2.3650e-03 | validation loss: 2.2189e-03\n",
      "Epoch: 86620 | training loss: 2.3650e-03 | validation loss: 2.2187e-03\n",
      "Epoch: 86630 | training loss: 2.3649e-03 | validation loss: 2.2185e-03\n",
      "Epoch: 86640 | training loss: 2.3649e-03 | validation loss: 2.2183e-03\n",
      "Epoch: 86650 | training loss: 2.3648e-03 | validation loss: 2.2175e-03\n",
      "Epoch: 86660 | training loss: 2.3667e-03 | validation loss: 2.2130e-03\n",
      "Epoch: 86670 | training loss: 2.5707e-03 | validation loss: 2.2502e-03\n",
      "Epoch: 86680 | training loss: 2.4534e-03 | validation loss: 2.2257e-03\n",
      "Epoch: 86690 | training loss: 2.7690e-03 | validation loss: 2.3235e-03\n",
      "Epoch: 86700 | training loss: 2.4592e-03 | validation loss: 2.2175e-03\n",
      "Epoch: 86710 | training loss: 2.3671e-03 | validation loss: 2.2085e-03\n",
      "Epoch: 86720 | training loss: 2.3716e-03 | validation loss: 2.2291e-03\n",
      "Epoch: 86730 | training loss: 2.3710e-03 | validation loss: 2.2305e-03\n",
      "Epoch: 86740 | training loss: 2.3646e-03 | validation loss: 2.2200e-03\n",
      "Epoch: 86750 | training loss: 2.3650e-03 | validation loss: 2.2151e-03\n",
      "Epoch: 86760 | training loss: 2.3643e-03 | validation loss: 2.2163e-03\n",
      "Epoch: 86770 | training loss: 2.3643e-03 | validation loss: 2.2184e-03\n",
      "Epoch: 86780 | training loss: 2.3642e-03 | validation loss: 2.2170e-03\n",
      "Epoch: 86790 | training loss: 2.3641e-03 | validation loss: 2.2168e-03\n",
      "Epoch: 86800 | training loss: 2.3641e-03 | validation loss: 2.2172e-03\n",
      "Epoch: 86810 | training loss: 2.3640e-03 | validation loss: 2.2167e-03\n",
      "Epoch: 86820 | training loss: 2.3640e-03 | validation loss: 2.2168e-03\n",
      "Epoch: 86830 | training loss: 2.3639e-03 | validation loss: 2.2166e-03\n",
      "Epoch: 86840 | training loss: 2.3639e-03 | validation loss: 2.2165e-03\n",
      "Epoch: 86850 | training loss: 2.3638e-03 | validation loss: 2.2164e-03\n",
      "Epoch: 86860 | training loss: 2.3638e-03 | validation loss: 2.2163e-03\n",
      "Epoch: 86870 | training loss: 2.3637e-03 | validation loss: 2.2162e-03\n",
      "Epoch: 86880 | training loss: 2.3637e-03 | validation loss: 2.2160e-03\n",
      "Epoch: 86890 | training loss: 2.3636e-03 | validation loss: 2.2158e-03\n",
      "Epoch: 86900 | training loss: 2.3636e-03 | validation loss: 2.2152e-03\n",
      "Epoch: 86910 | training loss: 2.3660e-03 | validation loss: 2.2113e-03\n",
      "Epoch: 86920 | training loss: 2.7505e-03 | validation loss: 2.3310e-03\n",
      "Epoch: 86930 | training loss: 2.6869e-03 | validation loss: 2.4163e-03\n",
      "Epoch: 86940 | training loss: 2.4243e-03 | validation loss: 2.2030e-03\n",
      "Epoch: 86950 | training loss: 2.3792e-03 | validation loss: 2.1993e-03\n",
      "Epoch: 86960 | training loss: 2.3685e-03 | validation loss: 2.2157e-03\n",
      "Epoch: 86970 | training loss: 2.3691e-03 | validation loss: 2.2229e-03\n",
      "Epoch: 86980 | training loss: 2.3654e-03 | validation loss: 2.2158e-03\n",
      "Epoch: 86990 | training loss: 2.3640e-03 | validation loss: 2.2115e-03\n",
      "Epoch: 87000 | training loss: 2.3632e-03 | validation loss: 2.2145e-03\n",
      "Epoch: 87010 | training loss: 2.3631e-03 | validation loss: 2.2153e-03\n",
      "Epoch: 87020 | training loss: 2.3630e-03 | validation loss: 2.2140e-03\n",
      "Epoch: 87030 | training loss: 2.3630e-03 | validation loss: 2.2148e-03\n",
      "Epoch: 87040 | training loss: 2.3629e-03 | validation loss: 2.2142e-03\n",
      "Epoch: 87050 | training loss: 2.3628e-03 | validation loss: 2.2143e-03\n",
      "Epoch: 87060 | training loss: 2.3628e-03 | validation loss: 2.2139e-03\n",
      "Epoch: 87070 | training loss: 2.3627e-03 | validation loss: 2.2140e-03\n",
      "Epoch: 87080 | training loss: 2.3627e-03 | validation loss: 2.2139e-03\n",
      "Epoch: 87090 | training loss: 2.3626e-03 | validation loss: 2.2137e-03\n",
      "Epoch: 87100 | training loss: 2.3626e-03 | validation loss: 2.2136e-03\n",
      "Epoch: 87110 | training loss: 2.3625e-03 | validation loss: 2.2135e-03\n",
      "Epoch: 87120 | training loss: 2.3625e-03 | validation loss: 2.2131e-03\n",
      "Epoch: 87130 | training loss: 2.3626e-03 | validation loss: 2.2110e-03\n",
      "Epoch: 87140 | training loss: 2.3788e-03 | validation loss: 2.1965e-03\n",
      "Epoch: 87150 | training loss: 3.8002e-03 | validation loss: 2.7235e-03\n",
      "Epoch: 87160 | training loss: 2.5676e-03 | validation loss: 2.3548e-03\n",
      "Epoch: 87170 | training loss: 2.3790e-03 | validation loss: 2.2448e-03\n",
      "Epoch: 87180 | training loss: 2.3794e-03 | validation loss: 2.2123e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87190 | training loss: 2.3638e-03 | validation loss: 2.2131e-03\n",
      "Epoch: 87200 | training loss: 2.3652e-03 | validation loss: 2.2235e-03\n",
      "Epoch: 87210 | training loss: 2.3628e-03 | validation loss: 2.2146e-03\n",
      "Epoch: 87220 | training loss: 2.3625e-03 | validation loss: 2.2147e-03\n",
      "Epoch: 87230 | training loss: 2.3623e-03 | validation loss: 2.2156e-03\n",
      "Epoch: 87240 | training loss: 2.3620e-03 | validation loss: 2.2120e-03\n",
      "Epoch: 87250 | training loss: 2.3619e-03 | validation loss: 2.2121e-03\n",
      "Epoch: 87260 | training loss: 2.3618e-03 | validation loss: 2.2108e-03\n",
      "Epoch: 87270 | training loss: 2.3617e-03 | validation loss: 2.2113e-03\n",
      "Epoch: 87280 | training loss: 2.3617e-03 | validation loss: 2.2116e-03\n",
      "Epoch: 87290 | training loss: 2.3616e-03 | validation loss: 2.2112e-03\n",
      "Epoch: 87300 | training loss: 2.3616e-03 | validation loss: 2.2110e-03\n",
      "Epoch: 87310 | training loss: 2.3615e-03 | validation loss: 2.2108e-03\n",
      "Epoch: 87320 | training loss: 2.3615e-03 | validation loss: 2.2101e-03\n",
      "Epoch: 87330 | training loss: 2.3632e-03 | validation loss: 2.2062e-03\n",
      "Epoch: 87340 | training loss: 2.5249e-03 | validation loss: 2.2357e-03\n",
      "Epoch: 87350 | training loss: 2.6954e-03 | validation loss: 2.2847e-03\n",
      "Epoch: 87360 | training loss: 2.7339e-03 | validation loss: 2.3025e-03\n",
      "Epoch: 87370 | training loss: 2.3780e-03 | validation loss: 2.2031e-03\n",
      "Epoch: 87380 | training loss: 2.3759e-03 | validation loss: 2.2352e-03\n",
      "Epoch: 87390 | training loss: 2.3797e-03 | validation loss: 2.2384e-03\n",
      "Epoch: 87400 | training loss: 2.3621e-03 | validation loss: 2.2149e-03\n",
      "Epoch: 87410 | training loss: 2.3627e-03 | validation loss: 2.2048e-03\n",
      "Epoch: 87420 | training loss: 2.3613e-03 | validation loss: 2.2075e-03\n",
      "Epoch: 87430 | training loss: 2.3612e-03 | validation loss: 2.2124e-03\n",
      "Epoch: 87440 | training loss: 2.3609e-03 | validation loss: 2.2099e-03\n",
      "Epoch: 87450 | training loss: 2.3609e-03 | validation loss: 2.2089e-03\n",
      "Epoch: 87460 | training loss: 2.3608e-03 | validation loss: 2.2102e-03\n",
      "Epoch: 87470 | training loss: 2.3608e-03 | validation loss: 2.2092e-03\n",
      "Epoch: 87480 | training loss: 2.3607e-03 | validation loss: 2.2096e-03\n",
      "Epoch: 87490 | training loss: 2.3607e-03 | validation loss: 2.2092e-03\n",
      "Epoch: 87500 | training loss: 2.3606e-03 | validation loss: 2.2093e-03\n",
      "Epoch: 87510 | training loss: 2.3606e-03 | validation loss: 2.2091e-03\n",
      "Epoch: 87520 | training loss: 2.3605e-03 | validation loss: 2.2090e-03\n",
      "Epoch: 87530 | training loss: 2.3605e-03 | validation loss: 2.2089e-03\n",
      "Epoch: 87540 | training loss: 2.3604e-03 | validation loss: 2.2088e-03\n",
      "Epoch: 87550 | training loss: 2.3604e-03 | validation loss: 2.2086e-03\n",
      "Epoch: 87560 | training loss: 2.3603e-03 | validation loss: 2.2084e-03\n",
      "Epoch: 87570 | training loss: 2.3604e-03 | validation loss: 2.2071e-03\n",
      "Epoch: 87580 | training loss: 2.3787e-03 | validation loss: 2.1995e-03\n",
      "Epoch: 87590 | training loss: 4.8133e-03 | validation loss: 3.1579e-03\n",
      "Epoch: 87600 | training loss: 2.3647e-03 | validation loss: 2.2244e-03\n",
      "Epoch: 87610 | training loss: 2.3710e-03 | validation loss: 2.2278e-03\n",
      "Epoch: 87620 | training loss: 2.3620e-03 | validation loss: 2.2123e-03\n",
      "Epoch: 87630 | training loss: 2.3606e-03 | validation loss: 2.2092e-03\n",
      "Epoch: 87640 | training loss: 2.3606e-03 | validation loss: 2.2106e-03\n",
      "Epoch: 87650 | training loss: 2.3608e-03 | validation loss: 2.2116e-03\n",
      "Epoch: 87660 | training loss: 2.3608e-03 | validation loss: 2.2112e-03\n",
      "Epoch: 87670 | training loss: 2.3604e-03 | validation loss: 2.2104e-03\n",
      "Epoch: 87680 | training loss: 2.3599e-03 | validation loss: 2.2088e-03\n",
      "Epoch: 87690 | training loss: 2.3597e-03 | validation loss: 2.2072e-03\n",
      "Epoch: 87700 | training loss: 2.3597e-03 | validation loss: 2.2067e-03\n",
      "Epoch: 87710 | training loss: 2.3596e-03 | validation loss: 2.2068e-03\n",
      "Epoch: 87720 | training loss: 2.3595e-03 | validation loss: 2.2071e-03\n",
      "Epoch: 87730 | training loss: 2.3595e-03 | validation loss: 2.2069e-03\n",
      "Epoch: 87740 | training loss: 2.3594e-03 | validation loss: 2.2067e-03\n",
      "Epoch: 87750 | training loss: 2.3594e-03 | validation loss: 2.2066e-03\n",
      "Epoch: 87760 | training loss: 2.3593e-03 | validation loss: 2.2065e-03\n",
      "Epoch: 87770 | training loss: 2.3593e-03 | validation loss: 2.2064e-03\n",
      "Epoch: 87780 | training loss: 2.3592e-03 | validation loss: 2.2062e-03\n",
      "Epoch: 87790 | training loss: 2.3592e-03 | validation loss: 2.2056e-03\n",
      "Epoch: 87800 | training loss: 2.3609e-03 | validation loss: 2.1999e-03\n",
      "Epoch: 87810 | training loss: 2.7631e-03 | validation loss: 2.3773e-03\n",
      "Epoch: 87820 | training loss: 2.6273e-03 | validation loss: 2.4609e-03\n",
      "Epoch: 87830 | training loss: 2.4748e-03 | validation loss: 2.2986e-03\n",
      "Epoch: 87840 | training loss: 2.4040e-03 | validation loss: 2.2694e-03\n",
      "Epoch: 87850 | training loss: 2.3755e-03 | validation loss: 2.2173e-03\n",
      "Epoch: 87860 | training loss: 2.3643e-03 | validation loss: 2.2202e-03\n",
      "Epoch: 87870 | training loss: 2.3601e-03 | validation loss: 2.2086e-03\n",
      "Epoch: 87880 | training loss: 2.3591e-03 | validation loss: 2.2040e-03\n",
      "Epoch: 87890 | training loss: 2.3588e-03 | validation loss: 2.2061e-03\n",
      "Epoch: 87900 | training loss: 2.3588e-03 | validation loss: 2.2055e-03\n",
      "Epoch: 87910 | training loss: 2.3588e-03 | validation loss: 2.2058e-03\n",
      "Epoch: 87920 | training loss: 2.3597e-03 | validation loss: 2.2103e-03\n",
      "Epoch: 87930 | training loss: 2.3900e-03 | validation loss: 2.2482e-03\n",
      "Epoch: 87940 | training loss: 3.3179e-03 | validation loss: 2.8067e-03\n",
      "Epoch: 87950 | training loss: 2.7489e-03 | validation loss: 2.2930e-03\n",
      "Epoch: 87960 | training loss: 2.4058e-03 | validation loss: 2.2541e-03\n",
      "Epoch: 87970 | training loss: 2.3700e-03 | validation loss: 2.2212e-03\n",
      "Epoch: 87980 | training loss: 2.3767e-03 | validation loss: 2.1906e-03\n",
      "Epoch: 87990 | training loss: 2.3626e-03 | validation loss: 2.2152e-03\n",
      "Epoch: 88000 | training loss: 2.3586e-03 | validation loss: 2.2014e-03\n",
      "Epoch: 88010 | training loss: 2.3581e-03 | validation loss: 2.2044e-03\n",
      "Epoch: 88020 | training loss: 2.3581e-03 | validation loss: 2.2024e-03\n",
      "Epoch: 88030 | training loss: 2.3581e-03 | validation loss: 2.2043e-03\n",
      "Epoch: 88040 | training loss: 2.3580e-03 | validation loss: 2.2021e-03\n",
      "Epoch: 88050 | training loss: 2.3579e-03 | validation loss: 2.2033e-03\n",
      "Epoch: 88060 | training loss: 2.3579e-03 | validation loss: 2.2035e-03\n",
      "Epoch: 88070 | training loss: 2.3578e-03 | validation loss: 2.2033e-03\n",
      "Epoch: 88080 | training loss: 2.3578e-03 | validation loss: 2.2037e-03\n",
      "Epoch: 88090 | training loss: 2.3586e-03 | validation loss: 2.2072e-03\n",
      "Epoch: 88100 | training loss: 2.4004e-03 | validation loss: 2.2521e-03\n",
      "Epoch: 88110 | training loss: 3.8413e-03 | validation loss: 3.0659e-03\n",
      "Epoch: 88120 | training loss: 2.6010e-03 | validation loss: 2.2611e-03\n",
      "Epoch: 88130 | training loss: 2.4621e-03 | validation loss: 2.1974e-03\n",
      "Epoch: 88140 | training loss: 2.3722e-03 | validation loss: 2.2143e-03\n",
      "Epoch: 88150 | training loss: 2.3747e-03 | validation loss: 2.2329e-03\n",
      "Epoch: 88160 | training loss: 2.3593e-03 | validation loss: 2.2025e-03\n",
      "Epoch: 88170 | training loss: 2.3593e-03 | validation loss: 2.1938e-03\n",
      "Epoch: 88180 | training loss: 2.3579e-03 | validation loss: 2.2062e-03\n",
      "Epoch: 88190 | training loss: 2.3573e-03 | validation loss: 2.2018e-03\n",
      "Epoch: 88200 | training loss: 2.3572e-03 | validation loss: 2.2002e-03\n",
      "Epoch: 88210 | training loss: 2.3572e-03 | validation loss: 2.2026e-03\n",
      "Epoch: 88220 | training loss: 2.3571e-03 | validation loss: 2.2006e-03\n",
      "Epoch: 88230 | training loss: 2.3570e-03 | validation loss: 2.2014e-03\n",
      "Epoch: 88240 | training loss: 2.3570e-03 | validation loss: 2.2012e-03\n",
      "Epoch: 88250 | training loss: 2.3569e-03 | validation loss: 2.2008e-03\n",
      "Epoch: 88260 | training loss: 2.3569e-03 | validation loss: 2.2009e-03\n",
      "Epoch: 88270 | training loss: 2.3568e-03 | validation loss: 2.2009e-03\n",
      "Epoch: 88280 | training loss: 2.3568e-03 | validation loss: 2.2007e-03\n",
      "Epoch: 88290 | training loss: 2.3571e-03 | validation loss: 2.2002e-03\n",
      "Epoch: 88300 | training loss: 2.3983e-03 | validation loss: 2.2182e-03\n",
      "Epoch: 88310 | training loss: 2.7655e-03 | validation loss: 2.4574e-03\n",
      "Epoch: 88320 | training loss: 2.6492e-03 | validation loss: 2.3730e-03\n",
      "Epoch: 88330 | training loss: 2.4452e-03 | validation loss: 2.2156e-03\n",
      "Epoch: 88340 | training loss: 2.3848e-03 | validation loss: 2.1987e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88350 | training loss: 2.3656e-03 | validation loss: 2.1974e-03\n",
      "Epoch: 88360 | training loss: 2.3594e-03 | validation loss: 2.1959e-03\n",
      "Epoch: 88370 | training loss: 2.3580e-03 | validation loss: 2.1930e-03\n",
      "Epoch: 88380 | training loss: 2.3576e-03 | validation loss: 2.2023e-03\n",
      "Epoch: 88390 | training loss: 2.3565e-03 | validation loss: 2.1966e-03\n",
      "Epoch: 88400 | training loss: 2.3565e-03 | validation loss: 2.1959e-03\n",
      "Epoch: 88410 | training loss: 2.3564e-03 | validation loss: 2.1961e-03\n",
      "Epoch: 88420 | training loss: 2.3580e-03 | validation loss: 2.1928e-03\n",
      "Epoch: 88430 | training loss: 2.3973e-03 | validation loss: 2.1850e-03\n",
      "Epoch: 88440 | training loss: 3.2478e-03 | validation loss: 2.4603e-03\n",
      "Epoch: 88450 | training loss: 2.6882e-03 | validation loss: 2.4361e-03\n",
      "Epoch: 88460 | training loss: 2.4265e-03 | validation loss: 2.1914e-03\n",
      "Epoch: 88470 | training loss: 2.3559e-03 | validation loss: 2.1998e-03\n",
      "Epoch: 88480 | training loss: 2.3616e-03 | validation loss: 2.2127e-03\n",
      "Epoch: 88490 | training loss: 2.3605e-03 | validation loss: 2.1908e-03\n",
      "Epoch: 88500 | training loss: 2.3578e-03 | validation loss: 2.2058e-03\n",
      "Epoch: 88510 | training loss: 2.3564e-03 | validation loss: 2.1942e-03\n",
      "Epoch: 88520 | training loss: 2.3557e-03 | validation loss: 2.1995e-03\n",
      "Epoch: 88530 | training loss: 2.3556e-03 | validation loss: 2.1984e-03\n",
      "Epoch: 88540 | training loss: 2.3556e-03 | validation loss: 2.1965e-03\n",
      "Epoch: 88550 | training loss: 2.3555e-03 | validation loss: 2.1971e-03\n",
      "Epoch: 88560 | training loss: 2.3554e-03 | validation loss: 2.1975e-03\n",
      "Epoch: 88570 | training loss: 2.3554e-03 | validation loss: 2.1975e-03\n",
      "Epoch: 88580 | training loss: 2.3553e-03 | validation loss: 2.1977e-03\n",
      "Epoch: 88590 | training loss: 2.3557e-03 | validation loss: 2.2004e-03\n",
      "Epoch: 88600 | training loss: 2.4079e-03 | validation loss: 2.2552e-03\n",
      "Epoch: 88610 | training loss: 4.3110e-03 | validation loss: 3.3071e-03\n",
      "Epoch: 88620 | training loss: 2.5680e-03 | validation loss: 2.3775e-03\n",
      "Epoch: 88630 | training loss: 2.3784e-03 | validation loss: 2.2406e-03\n",
      "Epoch: 88640 | training loss: 2.3690e-03 | validation loss: 2.2116e-03\n",
      "Epoch: 88650 | training loss: 2.3643e-03 | validation loss: 2.1948e-03\n",
      "Epoch: 88660 | training loss: 2.3603e-03 | validation loss: 2.1868e-03\n",
      "Epoch: 88670 | training loss: 2.3579e-03 | validation loss: 2.1870e-03\n",
      "Epoch: 88680 | training loss: 2.3555e-03 | validation loss: 2.1920e-03\n",
      "Epoch: 88690 | training loss: 2.3548e-03 | validation loss: 2.1977e-03\n",
      "Epoch: 88700 | training loss: 2.3549e-03 | validation loss: 2.1989e-03\n",
      "Epoch: 88710 | training loss: 2.3547e-03 | validation loss: 2.1962e-03\n",
      "Epoch: 88720 | training loss: 2.3547e-03 | validation loss: 2.1951e-03\n",
      "Epoch: 88730 | training loss: 2.3546e-03 | validation loss: 2.1961e-03\n",
      "Epoch: 88740 | training loss: 2.3545e-03 | validation loss: 2.1960e-03\n",
      "Epoch: 88750 | training loss: 2.3545e-03 | validation loss: 2.1956e-03\n",
      "Epoch: 88760 | training loss: 2.3544e-03 | validation loss: 2.1957e-03\n",
      "Epoch: 88770 | training loss: 2.3544e-03 | validation loss: 2.1955e-03\n",
      "Epoch: 88780 | training loss: 2.3543e-03 | validation loss: 2.1954e-03\n",
      "Epoch: 88790 | training loss: 2.3543e-03 | validation loss: 2.1953e-03\n",
      "Epoch: 88800 | training loss: 2.3542e-03 | validation loss: 2.1952e-03\n",
      "Epoch: 88810 | training loss: 2.3542e-03 | validation loss: 2.1951e-03\n",
      "Epoch: 88820 | training loss: 2.3541e-03 | validation loss: 2.1950e-03\n",
      "Epoch: 88830 | training loss: 2.3541e-03 | validation loss: 2.1949e-03\n",
      "Epoch: 88840 | training loss: 2.3541e-03 | validation loss: 2.1950e-03\n",
      "Epoch: 88850 | training loss: 2.3565e-03 | validation loss: 2.1993e-03\n",
      "Epoch: 88860 | training loss: 2.7528e-03 | validation loss: 2.4923e-03\n",
      "Epoch: 88870 | training loss: 3.1926e-03 | validation loss: 2.5075e-03\n",
      "Epoch: 88880 | training loss: 2.6268e-03 | validation loss: 2.3645e-03\n",
      "Epoch: 88890 | training loss: 2.4408e-03 | validation loss: 2.1857e-03\n",
      "Epoch: 88900 | training loss: 2.3784e-03 | validation loss: 2.2204e-03\n",
      "Epoch: 88910 | training loss: 2.3614e-03 | validation loss: 2.1808e-03\n",
      "Epoch: 88920 | training loss: 2.3558e-03 | validation loss: 2.2015e-03\n",
      "Epoch: 88930 | training loss: 2.3537e-03 | validation loss: 2.1922e-03\n",
      "Epoch: 88940 | training loss: 2.3540e-03 | validation loss: 2.1898e-03\n",
      "Epoch: 88950 | training loss: 2.3535e-03 | validation loss: 2.1934e-03\n",
      "Epoch: 88960 | training loss: 2.3537e-03 | validation loss: 2.1951e-03\n",
      "Epoch: 88970 | training loss: 2.3547e-03 | validation loss: 2.1990e-03\n",
      "Epoch: 88980 | training loss: 2.3778e-03 | validation loss: 2.2292e-03\n",
      "Epoch: 88990 | training loss: 3.0421e-03 | validation loss: 2.6437e-03\n",
      "Epoch: 89000 | training loss: 2.5770e-03 | validation loss: 2.2238e-03\n",
      "Epoch: 89010 | training loss: 2.4917e-03 | validation loss: 2.3112e-03\n",
      "Epoch: 89020 | training loss: 2.3709e-03 | validation loss: 2.1797e-03\n",
      "Epoch: 89030 | training loss: 2.3533e-03 | validation loss: 2.1901e-03\n",
      "Epoch: 89040 | training loss: 2.3551e-03 | validation loss: 2.1993e-03\n",
      "Epoch: 89050 | training loss: 2.3543e-03 | validation loss: 2.1873e-03\n",
      "Epoch: 89060 | training loss: 2.3534e-03 | validation loss: 2.1953e-03\n",
      "Epoch: 89070 | training loss: 2.3530e-03 | validation loss: 2.1915e-03\n",
      "Epoch: 89080 | training loss: 2.3530e-03 | validation loss: 2.1910e-03\n",
      "Epoch: 89090 | training loss: 2.3529e-03 | validation loss: 2.1931e-03\n",
      "Epoch: 89100 | training loss: 2.3528e-03 | validation loss: 2.1924e-03\n",
      "Epoch: 89110 | training loss: 2.3528e-03 | validation loss: 2.1917e-03\n",
      "Epoch: 89120 | training loss: 2.3527e-03 | validation loss: 2.1915e-03\n",
      "Epoch: 89130 | training loss: 2.3527e-03 | validation loss: 2.1909e-03\n",
      "Epoch: 89140 | training loss: 2.3533e-03 | validation loss: 2.1877e-03\n",
      "Epoch: 89150 | training loss: 2.4329e-03 | validation loss: 2.1866e-03\n",
      "Epoch: 89160 | training loss: 3.6732e-03 | validation loss: 2.6538e-03\n",
      "Epoch: 89170 | training loss: 2.6421e-03 | validation loss: 2.2405e-03\n",
      "Epoch: 89180 | training loss: 2.3697e-03 | validation loss: 2.1744e-03\n",
      "Epoch: 89190 | training loss: 2.3705e-03 | validation loss: 2.2013e-03\n",
      "Epoch: 89200 | training loss: 2.3703e-03 | validation loss: 2.2106e-03\n",
      "Epoch: 89210 | training loss: 2.3578e-03 | validation loss: 2.2036e-03\n",
      "Epoch: 89220 | training loss: 2.3529e-03 | validation loss: 2.1954e-03\n",
      "Epoch: 89230 | training loss: 2.3531e-03 | validation loss: 2.1902e-03\n",
      "Epoch: 89240 | training loss: 2.3522e-03 | validation loss: 2.1888e-03\n",
      "Epoch: 89250 | training loss: 2.3522e-03 | validation loss: 2.1903e-03\n",
      "Epoch: 89260 | training loss: 2.3520e-03 | validation loss: 2.1907e-03\n",
      "Epoch: 89270 | training loss: 2.3520e-03 | validation loss: 2.1901e-03\n",
      "Epoch: 89280 | training loss: 2.3520e-03 | validation loss: 2.1900e-03\n",
      "Epoch: 89290 | training loss: 2.3519e-03 | validation loss: 2.1900e-03\n",
      "Epoch: 89300 | training loss: 2.3518e-03 | validation loss: 2.1899e-03\n",
      "Epoch: 89310 | training loss: 2.3518e-03 | validation loss: 2.1898e-03\n",
      "Epoch: 89320 | training loss: 2.3517e-03 | validation loss: 2.1897e-03\n",
      "Epoch: 89330 | training loss: 2.3517e-03 | validation loss: 2.1896e-03\n",
      "Epoch: 89340 | training loss: 2.3516e-03 | validation loss: 2.1895e-03\n",
      "Epoch: 89350 | training loss: 2.3516e-03 | validation loss: 2.1894e-03\n",
      "Epoch: 89360 | training loss: 2.3515e-03 | validation loss: 2.1893e-03\n",
      "Epoch: 89370 | training loss: 2.3515e-03 | validation loss: 2.1891e-03\n",
      "Epoch: 89380 | training loss: 2.3514e-03 | validation loss: 2.1890e-03\n",
      "Epoch: 89390 | training loss: 2.3514e-03 | validation loss: 2.1891e-03\n",
      "Epoch: 89400 | training loss: 2.3528e-03 | validation loss: 2.1919e-03\n",
      "Epoch: 89410 | training loss: 2.6307e-03 | validation loss: 2.3692e-03\n",
      "Epoch: 89420 | training loss: 2.9694e-03 | validation loss: 2.7563e-03\n",
      "Epoch: 89430 | training loss: 2.8143e-03 | validation loss: 2.6067e-03\n",
      "Epoch: 89440 | training loss: 2.4436e-03 | validation loss: 2.3009e-03\n",
      "Epoch: 89450 | training loss: 2.3529e-03 | validation loss: 2.1858e-03\n",
      "Epoch: 89460 | training loss: 2.3663e-03 | validation loss: 2.1751e-03\n",
      "Epoch: 89470 | training loss: 2.3566e-03 | validation loss: 2.1764e-03\n",
      "Epoch: 89480 | training loss: 2.3510e-03 | validation loss: 2.1867e-03\n",
      "Epoch: 89490 | training loss: 2.3515e-03 | validation loss: 2.1924e-03\n",
      "Epoch: 89500 | training loss: 2.3511e-03 | validation loss: 2.1892e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89510 | training loss: 2.3510e-03 | validation loss: 2.1872e-03\n",
      "Epoch: 89520 | training loss: 2.3508e-03 | validation loss: 2.1882e-03\n",
      "Epoch: 89530 | training loss: 2.3508e-03 | validation loss: 2.1875e-03\n",
      "Epoch: 89540 | training loss: 2.3507e-03 | validation loss: 2.1871e-03\n",
      "Epoch: 89550 | training loss: 2.3507e-03 | validation loss: 2.1872e-03\n",
      "Epoch: 89560 | training loss: 2.3506e-03 | validation loss: 2.1869e-03\n",
      "Epoch: 89570 | training loss: 2.3506e-03 | validation loss: 2.1869e-03\n",
      "Epoch: 89580 | training loss: 2.3505e-03 | validation loss: 2.1867e-03\n",
      "Epoch: 89590 | training loss: 2.3505e-03 | validation loss: 2.1866e-03\n",
      "Epoch: 89600 | training loss: 2.3504e-03 | validation loss: 2.1865e-03\n",
      "Epoch: 89610 | training loss: 2.3504e-03 | validation loss: 2.1864e-03\n",
      "Epoch: 89620 | training loss: 2.3503e-03 | validation loss: 2.1864e-03\n",
      "Epoch: 89630 | training loss: 2.3503e-03 | validation loss: 2.1864e-03\n",
      "Epoch: 89640 | training loss: 2.3502e-03 | validation loss: 2.1870e-03\n",
      "Epoch: 89650 | training loss: 2.3531e-03 | validation loss: 2.1959e-03\n",
      "Epoch: 89660 | training loss: 2.8601e-03 | validation loss: 2.5357e-03\n",
      "Epoch: 89670 | training loss: 2.9734e-03 | validation loss: 2.3460e-03\n",
      "Epoch: 89680 | training loss: 2.4014e-03 | validation loss: 2.1764e-03\n",
      "Epoch: 89690 | training loss: 2.3833e-03 | validation loss: 2.1944e-03\n",
      "Epoch: 89700 | training loss: 2.3746e-03 | validation loss: 2.1999e-03\n",
      "Epoch: 89710 | training loss: 2.3650e-03 | validation loss: 2.1984e-03\n",
      "Epoch: 89720 | training loss: 2.3558e-03 | validation loss: 2.1917e-03\n",
      "Epoch: 89730 | training loss: 2.3507e-03 | validation loss: 2.1850e-03\n",
      "Epoch: 89740 | training loss: 2.3501e-03 | validation loss: 2.1819e-03\n",
      "Epoch: 89750 | training loss: 2.3500e-03 | validation loss: 2.1826e-03\n",
      "Epoch: 89760 | training loss: 2.3497e-03 | validation loss: 2.1850e-03\n",
      "Epoch: 89770 | training loss: 2.3497e-03 | validation loss: 2.1860e-03\n",
      "Epoch: 89780 | training loss: 2.3496e-03 | validation loss: 2.1850e-03\n",
      "Epoch: 89790 | training loss: 2.3495e-03 | validation loss: 2.1845e-03\n",
      "Epoch: 89800 | training loss: 2.3495e-03 | validation loss: 2.1847e-03\n",
      "Epoch: 89810 | training loss: 2.3494e-03 | validation loss: 2.1845e-03\n",
      "Epoch: 89820 | training loss: 2.3494e-03 | validation loss: 2.1845e-03\n",
      "Epoch: 89830 | training loss: 2.3493e-03 | validation loss: 2.1843e-03\n",
      "Epoch: 89840 | training loss: 2.3493e-03 | validation loss: 2.1843e-03\n",
      "Epoch: 89850 | training loss: 2.3492e-03 | validation loss: 2.1841e-03\n",
      "Epoch: 89860 | training loss: 2.3492e-03 | validation loss: 2.1840e-03\n",
      "Epoch: 89870 | training loss: 2.3491e-03 | validation loss: 2.1839e-03\n",
      "Epoch: 89880 | training loss: 2.3491e-03 | validation loss: 2.1838e-03\n",
      "Epoch: 89890 | training loss: 2.3490e-03 | validation loss: 2.1837e-03\n",
      "Epoch: 89900 | training loss: 2.3490e-03 | validation loss: 2.1836e-03\n",
      "Epoch: 89910 | training loss: 2.3489e-03 | validation loss: 2.1834e-03\n",
      "Epoch: 89920 | training loss: 2.3489e-03 | validation loss: 2.1825e-03\n",
      "Epoch: 89930 | training loss: 2.3574e-03 | validation loss: 2.1755e-03\n",
      "Epoch: 89940 | training loss: 4.1414e-03 | validation loss: 2.8572e-03\n",
      "Epoch: 89950 | training loss: 2.7848e-03 | validation loss: 2.4800e-03\n",
      "Epoch: 89960 | training loss: 2.5064e-03 | validation loss: 2.3078e-03\n",
      "Epoch: 89970 | training loss: 2.3774e-03 | validation loss: 2.2152e-03\n",
      "Epoch: 89980 | training loss: 2.3520e-03 | validation loss: 2.1890e-03\n",
      "Epoch: 89990 | training loss: 2.3491e-03 | validation loss: 2.1831e-03\n",
      "Epoch: 90000 | training loss: 2.3487e-03 | validation loss: 2.1828e-03\n",
      "Epoch: 90010 | training loss: 2.3487e-03 | validation loss: 2.1841e-03\n",
      "Epoch: 90020 | training loss: 2.3487e-03 | validation loss: 2.1844e-03\n",
      "Epoch: 90030 | training loss: 2.3486e-03 | validation loss: 2.1836e-03\n",
      "Epoch: 90040 | training loss: 2.3484e-03 | validation loss: 2.1830e-03\n",
      "Epoch: 90050 | training loss: 2.3483e-03 | validation loss: 2.1824e-03\n",
      "Epoch: 90060 | training loss: 2.3482e-03 | validation loss: 2.1818e-03\n",
      "Epoch: 90070 | training loss: 2.3482e-03 | validation loss: 2.1818e-03\n",
      "Epoch: 90080 | training loss: 2.3481e-03 | validation loss: 2.1818e-03\n",
      "Epoch: 90090 | training loss: 2.3481e-03 | validation loss: 2.1818e-03\n",
      "Epoch: 90100 | training loss: 2.3480e-03 | validation loss: 2.1816e-03\n",
      "Epoch: 90110 | training loss: 2.3480e-03 | validation loss: 2.1815e-03\n",
      "Epoch: 90120 | training loss: 2.3479e-03 | validation loss: 2.1815e-03\n",
      "Epoch: 90130 | training loss: 2.3479e-03 | validation loss: 2.1813e-03\n",
      "Epoch: 90140 | training loss: 2.3478e-03 | validation loss: 2.1812e-03\n",
      "Epoch: 90150 | training loss: 2.3478e-03 | validation loss: 2.1811e-03\n",
      "Epoch: 90160 | training loss: 2.3477e-03 | validation loss: 2.1809e-03\n",
      "Epoch: 90170 | training loss: 2.3477e-03 | validation loss: 2.1804e-03\n",
      "Epoch: 90180 | training loss: 2.3487e-03 | validation loss: 2.1755e-03\n",
      "Epoch: 90190 | training loss: 2.5905e-03 | validation loss: 2.2564e-03\n",
      "Epoch: 90200 | training loss: 2.7336e-03 | validation loss: 2.4851e-03\n",
      "Epoch: 90210 | training loss: 2.4451e-03 | validation loss: 2.2866e-03\n",
      "Epoch: 90220 | training loss: 2.3876e-03 | validation loss: 2.2355e-03\n",
      "Epoch: 90230 | training loss: 2.3639e-03 | validation loss: 2.1985e-03\n",
      "Epoch: 90240 | training loss: 2.3532e-03 | validation loss: 2.1936e-03\n",
      "Epoch: 90250 | training loss: 2.3492e-03 | validation loss: 2.1862e-03\n",
      "Epoch: 90260 | training loss: 2.3481e-03 | validation loss: 2.1792e-03\n",
      "Epoch: 90270 | training loss: 2.3474e-03 | validation loss: 2.1819e-03\n",
      "Epoch: 90280 | training loss: 2.3473e-03 | validation loss: 2.1807e-03\n",
      "Epoch: 90290 | training loss: 2.3472e-03 | validation loss: 2.1797e-03\n",
      "Epoch: 90300 | training loss: 2.3473e-03 | validation loss: 2.1809e-03\n",
      "Epoch: 90310 | training loss: 2.3512e-03 | validation loss: 2.1909e-03\n",
      "Epoch: 90320 | training loss: 2.5479e-03 | validation loss: 2.3425e-03\n",
      "Epoch: 90330 | training loss: 2.5542e-03 | validation loss: 2.3365e-03\n",
      "Epoch: 90340 | training loss: 2.4324e-03 | validation loss: 2.2596e-03\n",
      "Epoch: 90350 | training loss: 2.4276e-03 | validation loss: 2.1760e-03\n",
      "Epoch: 90360 | training loss: 2.3481e-03 | validation loss: 2.1749e-03\n",
      "Epoch: 90370 | training loss: 2.3591e-03 | validation loss: 2.2009e-03\n",
      "Epoch: 90380 | training loss: 2.3492e-03 | validation loss: 2.1731e-03\n",
      "Epoch: 90390 | training loss: 2.3467e-03 | validation loss: 2.1787e-03\n",
      "Epoch: 90400 | training loss: 2.3467e-03 | validation loss: 2.1793e-03\n",
      "Epoch: 90410 | training loss: 2.3466e-03 | validation loss: 2.1766e-03\n",
      "Epoch: 90420 | training loss: 2.3465e-03 | validation loss: 2.1785e-03\n",
      "Epoch: 90430 | training loss: 2.3465e-03 | validation loss: 2.1778e-03\n",
      "Epoch: 90440 | training loss: 2.3464e-03 | validation loss: 2.1772e-03\n",
      "Epoch: 90450 | training loss: 2.3464e-03 | validation loss: 2.1779e-03\n",
      "Epoch: 90460 | training loss: 2.3463e-03 | validation loss: 2.1778e-03\n",
      "Epoch: 90470 | training loss: 2.3463e-03 | validation loss: 2.1776e-03\n",
      "Epoch: 90480 | training loss: 2.3462e-03 | validation loss: 2.1778e-03\n",
      "Epoch: 90490 | training loss: 2.3465e-03 | validation loss: 2.1797e-03\n",
      "Epoch: 90500 | training loss: 2.3631e-03 | validation loss: 2.2033e-03\n",
      "Epoch: 90510 | training loss: 3.6060e-03 | validation loss: 2.9215e-03\n",
      "Epoch: 90520 | training loss: 3.0030e-03 | validation loss: 2.3770e-03\n",
      "Epoch: 90530 | training loss: 2.4168e-03 | validation loss: 2.1640e-03\n",
      "Epoch: 90540 | training loss: 2.3635e-03 | validation loss: 2.1928e-03\n",
      "Epoch: 90550 | training loss: 2.3731e-03 | validation loss: 2.2130e-03\n",
      "Epoch: 90560 | training loss: 2.3487e-03 | validation loss: 2.1888e-03\n",
      "Epoch: 90570 | training loss: 2.3481e-03 | validation loss: 2.1710e-03\n",
      "Epoch: 90580 | training loss: 2.3464e-03 | validation loss: 2.1716e-03\n",
      "Epoch: 90590 | training loss: 2.3461e-03 | validation loss: 2.1799e-03\n",
      "Epoch: 90600 | training loss: 2.3457e-03 | validation loss: 2.1765e-03\n",
      "Epoch: 90610 | training loss: 2.3457e-03 | validation loss: 2.1747e-03\n",
      "Epoch: 90620 | training loss: 2.3456e-03 | validation loss: 2.1770e-03\n",
      "Epoch: 90630 | training loss: 2.3455e-03 | validation loss: 2.1753e-03\n",
      "Epoch: 90640 | training loss: 2.3455e-03 | validation loss: 2.1762e-03\n",
      "Epoch: 90650 | training loss: 2.3454e-03 | validation loss: 2.1755e-03\n",
      "Epoch: 90660 | training loss: 2.3454e-03 | validation loss: 2.1757e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90670 | training loss: 2.3453e-03 | validation loss: 2.1755e-03\n",
      "Epoch: 90680 | training loss: 2.3453e-03 | validation loss: 2.1753e-03\n",
      "Epoch: 90690 | training loss: 2.3452e-03 | validation loss: 2.1753e-03\n",
      "Epoch: 90700 | training loss: 2.3452e-03 | validation loss: 2.1753e-03\n",
      "Epoch: 90710 | training loss: 2.3454e-03 | validation loss: 2.1766e-03\n",
      "Epoch: 90720 | training loss: 2.3754e-03 | validation loss: 2.2105e-03\n",
      "Epoch: 90730 | training loss: 2.7041e-03 | validation loss: 2.4240e-03\n",
      "Epoch: 90740 | training loss: 2.9064e-03 | validation loss: 2.3797e-03\n",
      "Epoch: 90750 | training loss: 2.4387e-03 | validation loss: 2.2863e-03\n",
      "Epoch: 90760 | training loss: 2.3660e-03 | validation loss: 2.1750e-03\n",
      "Epoch: 90770 | training loss: 2.3472e-03 | validation loss: 2.1836e-03\n",
      "Epoch: 90780 | training loss: 2.3464e-03 | validation loss: 2.1820e-03\n",
      "Epoch: 90790 | training loss: 2.3485e-03 | validation loss: 2.1648e-03\n",
      "Epoch: 90800 | training loss: 2.3450e-03 | validation loss: 2.1738e-03\n",
      "Epoch: 90810 | training loss: 2.3460e-03 | validation loss: 2.1784e-03\n",
      "Epoch: 90820 | training loss: 2.3471e-03 | validation loss: 2.1823e-03\n",
      "Epoch: 90830 | training loss: 2.3652e-03 | validation loss: 2.2063e-03\n",
      "Epoch: 90840 | training loss: 2.7114e-03 | validation loss: 2.4402e-03\n",
      "Epoch: 90850 | training loss: 2.3460e-03 | validation loss: 2.1755e-03\n",
      "Epoch: 90860 | training loss: 2.3470e-03 | validation loss: 2.1802e-03\n",
      "Epoch: 90870 | training loss: 2.3499e-03 | validation loss: 2.1640e-03\n",
      "Epoch: 90880 | training loss: 2.3468e-03 | validation loss: 2.1818e-03\n",
      "Epoch: 90890 | training loss: 2.3444e-03 | validation loss: 2.1716e-03\n",
      "Epoch: 90900 | training loss: 2.3448e-03 | validation loss: 2.1696e-03\n",
      "Epoch: 90910 | training loss: 2.3454e-03 | validation loss: 2.1783e-03\n",
      "Epoch: 90920 | training loss: 2.3441e-03 | validation loss: 2.1721e-03\n",
      "Epoch: 90930 | training loss: 2.3444e-03 | validation loss: 2.1699e-03\n",
      "Epoch: 90940 | training loss: 2.3450e-03 | validation loss: 2.1683e-03\n",
      "Epoch: 90950 | training loss: 2.3528e-03 | validation loss: 2.1626e-03\n",
      "Epoch: 90960 | training loss: 2.5643e-03 | validation loss: 2.2054e-03\n",
      "Epoch: 90970 | training loss: 2.5554e-03 | validation loss: 2.2127e-03\n",
      "Epoch: 90980 | training loss: 2.3485e-03 | validation loss: 2.1759e-03\n",
      "Epoch: 90990 | training loss: 2.3636e-03 | validation loss: 2.1992e-03\n",
      "Epoch: 91000 | training loss: 2.3656e-03 | validation loss: 2.1647e-03\n",
      "Epoch: 91010 | training loss: 2.3536e-03 | validation loss: 2.1891e-03\n",
      "Epoch: 91020 | training loss: 2.3475e-03 | validation loss: 2.1648e-03\n",
      "Epoch: 91030 | training loss: 2.3450e-03 | validation loss: 2.1779e-03\n",
      "Epoch: 91040 | training loss: 2.3438e-03 | validation loss: 2.1691e-03\n",
      "Epoch: 91050 | training loss: 2.3435e-03 | validation loss: 2.1711e-03\n",
      "Epoch: 91060 | training loss: 2.3436e-03 | validation loss: 2.1728e-03\n",
      "Epoch: 91070 | training loss: 2.3435e-03 | validation loss: 2.1723e-03\n",
      "Epoch: 91080 | training loss: 2.3434e-03 | validation loss: 2.1721e-03\n",
      "Epoch: 91090 | training loss: 2.3437e-03 | validation loss: 2.1739e-03\n",
      "Epoch: 91100 | training loss: 2.3559e-03 | validation loss: 2.1919e-03\n",
      "Epoch: 91110 | training loss: 3.0181e-03 | validation loss: 2.5970e-03\n",
      "Epoch: 91120 | training loss: 2.6545e-03 | validation loss: 2.2407e-03\n",
      "Epoch: 91130 | training loss: 2.4895e-03 | validation loss: 2.2888e-03\n",
      "Epoch: 91140 | training loss: 2.3699e-03 | validation loss: 2.2061e-03\n",
      "Epoch: 91150 | training loss: 2.3637e-03 | validation loss: 2.1594e-03\n",
      "Epoch: 91160 | training loss: 2.3439e-03 | validation loss: 2.1663e-03\n",
      "Epoch: 91170 | training loss: 2.3467e-03 | validation loss: 2.1806e-03\n",
      "Epoch: 91180 | training loss: 2.3437e-03 | validation loss: 2.1665e-03\n",
      "Epoch: 91190 | training loss: 2.3429e-03 | validation loss: 2.1707e-03\n",
      "Epoch: 91200 | training loss: 2.3428e-03 | validation loss: 2.1704e-03\n",
      "Epoch: 91210 | training loss: 2.3427e-03 | validation loss: 2.1695e-03\n",
      "Epoch: 91220 | training loss: 2.3427e-03 | validation loss: 2.1697e-03\n",
      "Epoch: 91230 | training loss: 2.3426e-03 | validation loss: 2.1699e-03\n",
      "Epoch: 91240 | training loss: 2.3426e-03 | validation loss: 2.1692e-03\n",
      "Epoch: 91250 | training loss: 2.3425e-03 | validation loss: 2.1691e-03\n",
      "Epoch: 91260 | training loss: 2.3428e-03 | validation loss: 2.1673e-03\n",
      "Epoch: 91270 | training loss: 2.3759e-03 | validation loss: 2.1667e-03\n",
      "Epoch: 91280 | training loss: 2.6460e-03 | validation loss: 2.2782e-03\n",
      "Epoch: 91290 | training loss: 2.4942e-03 | validation loss: 2.2095e-03\n",
      "Epoch: 91300 | training loss: 2.3591e-03 | validation loss: 2.1784e-03\n",
      "Epoch: 91310 | training loss: 2.3517e-03 | validation loss: 2.1897e-03\n",
      "Epoch: 91320 | training loss: 2.3545e-03 | validation loss: 2.1959e-03\n",
      "Epoch: 91330 | training loss: 2.3796e-03 | validation loss: 2.2217e-03\n",
      "Epoch: 91340 | training loss: 2.6522e-03 | validation loss: 2.4027e-03\n",
      "Epoch: 91350 | training loss: 2.3518e-03 | validation loss: 2.1858e-03\n",
      "Epoch: 91360 | training loss: 2.3742e-03 | validation loss: 2.1532e-03\n",
      "Epoch: 91370 | training loss: 2.3707e-03 | validation loss: 2.2057e-03\n",
      "Epoch: 91380 | training loss: 2.3519e-03 | validation loss: 2.1568e-03\n",
      "Epoch: 91390 | training loss: 2.3422e-03 | validation loss: 2.1655e-03\n",
      "Epoch: 91400 | training loss: 2.3446e-03 | validation loss: 2.1770e-03\n",
      "Epoch: 91410 | training loss: 2.3455e-03 | validation loss: 2.1784e-03\n",
      "Epoch: 91420 | training loss: 2.3564e-03 | validation loss: 2.1925e-03\n",
      "Epoch: 91430 | training loss: 2.5241e-03 | validation loss: 2.3166e-03\n",
      "Epoch: 91440 | training loss: 2.6086e-03 | validation loss: 2.3644e-03\n",
      "Epoch: 91450 | training loss: 2.4532e-03 | validation loss: 2.1734e-03\n",
      "Epoch: 91460 | training loss: 2.3867e-03 | validation loss: 2.2181e-03\n",
      "Epoch: 91470 | training loss: 2.3618e-03 | validation loss: 2.1553e-03\n",
      "Epoch: 91480 | training loss: 2.3482e-03 | validation loss: 2.1815e-03\n",
      "Epoch: 91490 | training loss: 2.3415e-03 | validation loss: 2.1656e-03\n",
      "Epoch: 91500 | training loss: 2.3429e-03 | validation loss: 2.1616e-03\n",
      "Epoch: 91510 | training loss: 2.3414e-03 | validation loss: 2.1656e-03\n",
      "Epoch: 91520 | training loss: 2.3413e-03 | validation loss: 2.1676e-03\n",
      "Epoch: 91530 | training loss: 2.3421e-03 | validation loss: 2.1710e-03\n",
      "Epoch: 91540 | training loss: 2.3656e-03 | validation loss: 2.1995e-03\n",
      "Epoch: 91550 | training loss: 3.2340e-03 | validation loss: 2.7141e-03\n",
      "Epoch: 91560 | training loss: 2.7461e-03 | validation loss: 2.2673e-03\n",
      "Epoch: 91570 | training loss: 2.4289e-03 | validation loss: 2.2409e-03\n",
      "Epoch: 91580 | training loss: 2.3533e-03 | validation loss: 2.1900e-03\n",
      "Epoch: 91590 | training loss: 2.3622e-03 | validation loss: 2.1568e-03\n",
      "Epoch: 91600 | training loss: 2.3439e-03 | validation loss: 2.1710e-03\n",
      "Epoch: 91610 | training loss: 2.3411e-03 | validation loss: 2.1687e-03\n",
      "Epoch: 91620 | training loss: 2.3411e-03 | validation loss: 2.1622e-03\n",
      "Epoch: 91630 | training loss: 2.3409e-03 | validation loss: 2.1679e-03\n",
      "Epoch: 91640 | training loss: 2.3407e-03 | validation loss: 2.1640e-03\n",
      "Epoch: 91650 | training loss: 2.3407e-03 | validation loss: 2.1658e-03\n",
      "Epoch: 91660 | training loss: 2.3406e-03 | validation loss: 2.1652e-03\n",
      "Epoch: 91670 | training loss: 2.3405e-03 | validation loss: 2.1646e-03\n",
      "Epoch: 91680 | training loss: 2.3405e-03 | validation loss: 2.1649e-03\n",
      "Epoch: 91690 | training loss: 2.3405e-03 | validation loss: 2.1649e-03\n",
      "Epoch: 91700 | training loss: 2.3405e-03 | validation loss: 2.1651e-03\n",
      "Epoch: 91710 | training loss: 2.3443e-03 | validation loss: 2.1681e-03\n",
      "Epoch: 91720 | training loss: 2.5603e-03 | validation loss: 2.2994e-03\n",
      "Epoch: 91730 | training loss: 3.3108e-03 | validation loss: 2.4986e-03\n",
      "Epoch: 91740 | training loss: 2.4836e-03 | validation loss: 2.2872e-03\n",
      "Epoch: 91750 | training loss: 2.3491e-03 | validation loss: 2.1735e-03\n",
      "Epoch: 91760 | training loss: 2.3526e-03 | validation loss: 2.1667e-03\n",
      "Epoch: 91770 | training loss: 2.3503e-03 | validation loss: 2.1883e-03\n",
      "Epoch: 91780 | training loss: 2.3448e-03 | validation loss: 2.1648e-03\n",
      "Epoch: 91790 | training loss: 2.3420e-03 | validation loss: 2.1731e-03\n",
      "Epoch: 91800 | training loss: 2.3406e-03 | validation loss: 2.1630e-03\n",
      "Epoch: 91810 | training loss: 2.3400e-03 | validation loss: 2.1654e-03\n",
      "Epoch: 91820 | training loss: 2.3399e-03 | validation loss: 2.1620e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91830 | training loss: 2.3398e-03 | validation loss: 2.1619e-03\n",
      "Epoch: 91840 | training loss: 2.3397e-03 | validation loss: 2.1633e-03\n",
      "Epoch: 91850 | training loss: 2.3397e-03 | validation loss: 2.1637e-03\n",
      "Epoch: 91860 | training loss: 2.3398e-03 | validation loss: 2.1643e-03\n",
      "Epoch: 91870 | training loss: 2.3422e-03 | validation loss: 2.1714e-03\n",
      "Epoch: 91880 | training loss: 2.4835e-03 | validation loss: 2.2852e-03\n",
      "Epoch: 91890 | training loss: 2.9020e-03 | validation loss: 2.5267e-03\n",
      "Epoch: 91900 | training loss: 2.4080e-03 | validation loss: 2.2345e-03\n",
      "Epoch: 91910 | training loss: 2.4291e-03 | validation loss: 2.1709e-03\n",
      "Epoch: 91920 | training loss: 2.3531e-03 | validation loss: 2.1554e-03\n",
      "Epoch: 91930 | training loss: 2.3511e-03 | validation loss: 2.1820e-03\n",
      "Epoch: 91940 | training loss: 2.3397e-03 | validation loss: 2.1635e-03\n",
      "Epoch: 91950 | training loss: 2.3412e-03 | validation loss: 2.1563e-03\n",
      "Epoch: 91960 | training loss: 2.3397e-03 | validation loss: 2.1656e-03\n",
      "Epoch: 91970 | training loss: 2.3392e-03 | validation loss: 2.1613e-03\n",
      "Epoch: 91980 | training loss: 2.3391e-03 | validation loss: 2.1617e-03\n",
      "Epoch: 91990 | training loss: 2.3390e-03 | validation loss: 2.1617e-03\n",
      "Epoch: 92000 | training loss: 2.3390e-03 | validation loss: 2.1618e-03\n",
      "Epoch: 92010 | training loss: 2.3390e-03 | validation loss: 2.1611e-03\n",
      "Epoch: 92020 | training loss: 2.3389e-03 | validation loss: 2.1616e-03\n",
      "Epoch: 92030 | training loss: 2.3389e-03 | validation loss: 2.1614e-03\n",
      "Epoch: 92040 | training loss: 2.3388e-03 | validation loss: 2.1612e-03\n",
      "Epoch: 92050 | training loss: 2.3388e-03 | validation loss: 2.1610e-03\n",
      "Epoch: 92060 | training loss: 2.3387e-03 | validation loss: 2.1607e-03\n",
      "Epoch: 92070 | training loss: 2.3389e-03 | validation loss: 2.1591e-03\n",
      "Epoch: 92080 | training loss: 2.3571e-03 | validation loss: 2.1512e-03\n",
      "Epoch: 92090 | training loss: 4.0992e-03 | validation loss: 2.8097e-03\n",
      "Epoch: 92100 | training loss: 2.7100e-03 | validation loss: 2.4143e-03\n",
      "Epoch: 92110 | training loss: 2.5935e-03 | validation loss: 2.3494e-03\n",
      "Epoch: 92120 | training loss: 2.4101e-03 | validation loss: 2.2328e-03\n",
      "Epoch: 92130 | training loss: 2.3454e-03 | validation loss: 2.1758e-03\n",
      "Epoch: 92140 | training loss: 2.3392e-03 | validation loss: 2.1559e-03\n",
      "Epoch: 92150 | training loss: 2.3419e-03 | validation loss: 2.1532e-03\n",
      "Epoch: 92160 | training loss: 2.3392e-03 | validation loss: 2.1563e-03\n",
      "Epoch: 92170 | training loss: 2.3383e-03 | validation loss: 2.1612e-03\n",
      "Epoch: 92180 | training loss: 2.3383e-03 | validation loss: 2.1620e-03\n",
      "Epoch: 92190 | training loss: 2.3381e-03 | validation loss: 2.1592e-03\n",
      "Epoch: 92200 | training loss: 2.3381e-03 | validation loss: 2.1592e-03\n",
      "Epoch: 92210 | training loss: 2.3380e-03 | validation loss: 2.1600e-03\n",
      "Epoch: 92220 | training loss: 2.3380e-03 | validation loss: 2.1593e-03\n",
      "Epoch: 92230 | training loss: 2.3379e-03 | validation loss: 2.1594e-03\n",
      "Epoch: 92240 | training loss: 2.3379e-03 | validation loss: 2.1593e-03\n",
      "Epoch: 92250 | training loss: 2.3378e-03 | validation loss: 2.1592e-03\n",
      "Epoch: 92260 | training loss: 2.3378e-03 | validation loss: 2.1592e-03\n",
      "Epoch: 92270 | training loss: 2.3377e-03 | validation loss: 2.1595e-03\n",
      "Epoch: 92280 | training loss: 2.3386e-03 | validation loss: 2.1640e-03\n",
      "Epoch: 92290 | training loss: 2.4957e-03 | validation loss: 2.3285e-03\n",
      "Epoch: 92300 | training loss: 2.5270e-03 | validation loss: 2.2657e-03\n",
      "Epoch: 92310 | training loss: 2.3445e-03 | validation loss: 2.1494e-03\n",
      "Epoch: 92320 | training loss: 2.3483e-03 | validation loss: 2.1747e-03\n",
      "Epoch: 92330 | training loss: 2.3502e-03 | validation loss: 2.1834e-03\n",
      "Epoch: 92340 | training loss: 2.3431e-03 | validation loss: 2.1726e-03\n",
      "Epoch: 92350 | training loss: 2.3385e-03 | validation loss: 2.1644e-03\n",
      "Epoch: 92360 | training loss: 2.3411e-03 | validation loss: 2.1685e-03\n",
      "Epoch: 92370 | training loss: 2.4414e-03 | validation loss: 2.2564e-03\n",
      "Epoch: 92380 | training loss: 3.0382e-03 | validation loss: 2.6104e-03\n",
      "Epoch: 92390 | training loss: 2.4394e-03 | validation loss: 2.1607e-03\n",
      "Epoch: 92400 | training loss: 2.3402e-03 | validation loss: 2.1531e-03\n",
      "Epoch: 92410 | training loss: 2.3598e-03 | validation loss: 2.1909e-03\n",
      "Epoch: 92420 | training loss: 2.3494e-03 | validation loss: 2.1465e-03\n",
      "Epoch: 92430 | training loss: 2.3411e-03 | validation loss: 2.1678e-03\n",
      "Epoch: 92440 | training loss: 2.3384e-03 | validation loss: 2.1516e-03\n",
      "Epoch: 92450 | training loss: 2.3376e-03 | validation loss: 2.1609e-03\n",
      "Epoch: 92460 | training loss: 2.3371e-03 | validation loss: 2.1546e-03\n",
      "Epoch: 92470 | training loss: 2.3368e-03 | validation loss: 2.1568e-03\n",
      "Epoch: 92480 | training loss: 2.3368e-03 | validation loss: 2.1576e-03\n",
      "Epoch: 92490 | training loss: 2.3367e-03 | validation loss: 2.1569e-03\n",
      "Epoch: 92500 | training loss: 2.3366e-03 | validation loss: 2.1565e-03\n",
      "Epoch: 92510 | training loss: 2.3366e-03 | validation loss: 2.1568e-03\n",
      "Epoch: 92520 | training loss: 2.3370e-03 | validation loss: 2.1594e-03\n",
      "Epoch: 92530 | training loss: 2.3694e-03 | validation loss: 2.1979e-03\n",
      "Epoch: 92540 | training loss: 4.1368e-03 | validation loss: 3.1862e-03\n",
      "Epoch: 92550 | training loss: 2.4879e-03 | validation loss: 2.1984e-03\n",
      "Epoch: 92560 | training loss: 2.5240e-03 | validation loss: 2.1845e-03\n",
      "Epoch: 92570 | training loss: 2.3707e-03 | validation loss: 2.1365e-03\n",
      "Epoch: 92580 | training loss: 2.3437e-03 | validation loss: 2.1536e-03\n",
      "Epoch: 92590 | training loss: 2.3421e-03 | validation loss: 2.1680e-03\n",
      "Epoch: 92600 | training loss: 2.3381e-03 | validation loss: 2.1647e-03\n",
      "Epoch: 92610 | training loss: 2.3366e-03 | validation loss: 2.1543e-03\n",
      "Epoch: 92620 | training loss: 2.3365e-03 | validation loss: 2.1523e-03\n",
      "Epoch: 92630 | training loss: 2.3361e-03 | validation loss: 2.1562e-03\n",
      "Epoch: 92640 | training loss: 2.3360e-03 | validation loss: 2.1557e-03\n",
      "Epoch: 92650 | training loss: 2.3360e-03 | validation loss: 2.1542e-03\n",
      "Epoch: 92660 | training loss: 2.3359e-03 | validation loss: 2.1553e-03\n",
      "Epoch: 92670 | training loss: 2.3359e-03 | validation loss: 2.1546e-03\n",
      "Epoch: 92680 | training loss: 2.3358e-03 | validation loss: 2.1548e-03\n",
      "Epoch: 92690 | training loss: 2.3358e-03 | validation loss: 2.1545e-03\n",
      "Epoch: 92700 | training loss: 2.3357e-03 | validation loss: 2.1546e-03\n",
      "Epoch: 92710 | training loss: 2.3357e-03 | validation loss: 2.1544e-03\n",
      "Epoch: 92720 | training loss: 2.3356e-03 | validation loss: 2.1543e-03\n",
      "Epoch: 92730 | training loss: 2.3356e-03 | validation loss: 2.1542e-03\n",
      "Epoch: 92740 | training loss: 2.3355e-03 | validation loss: 2.1541e-03\n",
      "Epoch: 92750 | training loss: 2.3358e-03 | validation loss: 2.1544e-03\n",
      "Epoch: 92760 | training loss: 2.3764e-03 | validation loss: 2.1813e-03\n",
      "Epoch: 92770 | training loss: 3.2006e-03 | validation loss: 2.4783e-03\n",
      "Epoch: 92780 | training loss: 2.8002e-03 | validation loss: 2.5611e-03\n",
      "Epoch: 92790 | training loss: 2.4737e-03 | validation loss: 2.2286e-03\n",
      "Epoch: 92800 | training loss: 2.3422e-03 | validation loss: 2.1622e-03\n",
      "Epoch: 92810 | training loss: 2.3451e-03 | validation loss: 2.1702e-03\n",
      "Epoch: 92820 | training loss: 2.3402e-03 | validation loss: 2.1415e-03\n",
      "Epoch: 92830 | training loss: 2.3367e-03 | validation loss: 2.1525e-03\n",
      "Epoch: 92840 | training loss: 2.3356e-03 | validation loss: 2.1496e-03\n",
      "Epoch: 92850 | training loss: 2.3352e-03 | validation loss: 2.1515e-03\n",
      "Epoch: 92860 | training loss: 2.3351e-03 | validation loss: 2.1509e-03\n",
      "Epoch: 92870 | training loss: 2.3350e-03 | validation loss: 2.1528e-03\n",
      "Epoch: 92880 | training loss: 2.3349e-03 | validation loss: 2.1517e-03\n",
      "Epoch: 92890 | training loss: 2.3348e-03 | validation loss: 2.1519e-03\n",
      "Epoch: 92900 | training loss: 2.3348e-03 | validation loss: 2.1522e-03\n",
      "Epoch: 92910 | training loss: 2.3347e-03 | validation loss: 2.1522e-03\n",
      "Epoch: 92920 | training loss: 2.3347e-03 | validation loss: 2.1525e-03\n",
      "Epoch: 92930 | training loss: 2.3350e-03 | validation loss: 2.1550e-03\n",
      "Epoch: 92940 | training loss: 2.3668e-03 | validation loss: 2.1943e-03\n",
      "Epoch: 92950 | training loss: 4.2258e-03 | validation loss: 3.2352e-03\n",
      "Epoch: 92960 | training loss: 2.4065e-03 | validation loss: 2.1705e-03\n",
      "Epoch: 92970 | training loss: 2.5180e-03 | validation loss: 2.1927e-03\n",
      "Epoch: 92980 | training loss: 2.3972e-03 | validation loss: 2.1538e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92990 | training loss: 2.3376e-03 | validation loss: 2.1503e-03\n",
      "Epoch: 93000 | training loss: 2.3385e-03 | validation loss: 2.1643e-03\n",
      "Epoch: 93010 | training loss: 2.3375e-03 | validation loss: 2.1617e-03\n",
      "Epoch: 93020 | training loss: 2.3343e-03 | validation loss: 2.1512e-03\n",
      "Epoch: 93030 | training loss: 2.3347e-03 | validation loss: 2.1485e-03\n",
      "Epoch: 93040 | training loss: 2.3342e-03 | validation loss: 2.1514e-03\n",
      "Epoch: 93050 | training loss: 2.3342e-03 | validation loss: 2.1515e-03\n",
      "Epoch: 93060 | training loss: 2.3341e-03 | validation loss: 2.1502e-03\n",
      "Epoch: 93070 | training loss: 2.3340e-03 | validation loss: 2.1511e-03\n",
      "Epoch: 93080 | training loss: 2.3340e-03 | validation loss: 2.1506e-03\n",
      "Epoch: 93090 | training loss: 2.3339e-03 | validation loss: 2.1506e-03\n",
      "Epoch: 93100 | training loss: 2.3339e-03 | validation loss: 2.1504e-03\n",
      "Epoch: 93110 | training loss: 2.3338e-03 | validation loss: 2.1505e-03\n",
      "Epoch: 93120 | training loss: 2.3338e-03 | validation loss: 2.1503e-03\n",
      "Epoch: 93130 | training loss: 2.3337e-03 | validation loss: 2.1502e-03\n",
      "Epoch: 93140 | training loss: 2.3337e-03 | validation loss: 2.1501e-03\n",
      "Epoch: 93150 | training loss: 2.3336e-03 | validation loss: 2.1500e-03\n",
      "Epoch: 93160 | training loss: 2.3336e-03 | validation loss: 2.1499e-03\n",
      "Epoch: 93170 | training loss: 2.3336e-03 | validation loss: 2.1501e-03\n",
      "Epoch: 93180 | training loss: 2.3341e-03 | validation loss: 2.1532e-03\n",
      "Epoch: 93190 | training loss: 2.4231e-03 | validation loss: 2.2325e-03\n",
      "Epoch: 93200 | training loss: 3.4328e-03 | validation loss: 2.8041e-03\n",
      "Epoch: 93210 | training loss: 2.9190e-03 | validation loss: 2.5321e-03\n",
      "Epoch: 93220 | training loss: 2.5160e-03 | validation loss: 2.3030e-03\n",
      "Epoch: 93230 | training loss: 2.3920e-03 | validation loss: 2.2182e-03\n",
      "Epoch: 93240 | training loss: 2.3492e-03 | validation loss: 2.1781e-03\n",
      "Epoch: 93250 | training loss: 2.3358e-03 | validation loss: 2.1576e-03\n",
      "Epoch: 93260 | training loss: 2.3332e-03 | validation loss: 2.1488e-03\n",
      "Epoch: 93270 | training loss: 2.3335e-03 | validation loss: 2.1463e-03\n",
      "Epoch: 93280 | training loss: 2.3335e-03 | validation loss: 2.1458e-03\n",
      "Epoch: 93290 | training loss: 2.3331e-03 | validation loss: 2.1473e-03\n",
      "Epoch: 93300 | training loss: 2.3330e-03 | validation loss: 2.1493e-03\n",
      "Epoch: 93310 | training loss: 2.3329e-03 | validation loss: 2.1491e-03\n",
      "Epoch: 93320 | training loss: 2.3329e-03 | validation loss: 2.1483e-03\n",
      "Epoch: 93330 | training loss: 2.3328e-03 | validation loss: 2.1481e-03\n",
      "Epoch: 93340 | training loss: 2.3328e-03 | validation loss: 2.1484e-03\n",
      "Epoch: 93350 | training loss: 2.3327e-03 | validation loss: 2.1481e-03\n",
      "Epoch: 93360 | training loss: 2.3327e-03 | validation loss: 2.1480e-03\n",
      "Epoch: 93370 | training loss: 2.3326e-03 | validation loss: 2.1480e-03\n",
      "Epoch: 93380 | training loss: 2.3326e-03 | validation loss: 2.1478e-03\n",
      "Epoch: 93390 | training loss: 2.3325e-03 | validation loss: 2.1477e-03\n",
      "Epoch: 93400 | training loss: 2.3325e-03 | validation loss: 2.1476e-03\n",
      "Epoch: 93410 | training loss: 2.3324e-03 | validation loss: 2.1475e-03\n",
      "Epoch: 93420 | training loss: 2.3324e-03 | validation loss: 2.1477e-03\n",
      "Epoch: 93430 | training loss: 2.3340e-03 | validation loss: 2.1535e-03\n",
      "Epoch: 93440 | training loss: 2.9588e-03 | validation loss: 2.6889e-03\n",
      "Epoch: 93450 | training loss: 2.3986e-03 | validation loss: 2.1756e-03\n",
      "Epoch: 93460 | training loss: 2.3341e-03 | validation loss: 2.1409e-03\n",
      "Epoch: 93470 | training loss: 2.3424e-03 | validation loss: 2.1611e-03\n",
      "Epoch: 93480 | training loss: 2.3446e-03 | validation loss: 2.1660e-03\n",
      "Epoch: 93490 | training loss: 2.3383e-03 | validation loss: 2.1600e-03\n",
      "Epoch: 93500 | training loss: 2.3363e-03 | validation loss: 2.1602e-03\n",
      "Epoch: 93510 | training loss: 2.3849e-03 | validation loss: 2.2101e-03\n",
      "Epoch: 93520 | training loss: 3.2035e-03 | validation loss: 2.6996e-03\n",
      "Epoch: 93530 | training loss: 2.6254e-03 | validation loss: 2.1979e-03\n",
      "Epoch: 93540 | training loss: 2.3739e-03 | validation loss: 2.1967e-03\n",
      "Epoch: 93550 | training loss: 2.3321e-03 | validation loss: 2.1477e-03\n",
      "Epoch: 93560 | training loss: 2.3375e-03 | validation loss: 2.1359e-03\n",
      "Epoch: 93570 | training loss: 2.3358e-03 | validation loss: 2.1567e-03\n",
      "Epoch: 93580 | training loss: 2.3333e-03 | validation loss: 2.1396e-03\n",
      "Epoch: 93590 | training loss: 2.3321e-03 | validation loss: 2.1487e-03\n",
      "Epoch: 93600 | training loss: 2.3316e-03 | validation loss: 2.1444e-03\n",
      "Epoch: 93610 | training loss: 2.3316e-03 | validation loss: 2.1438e-03\n",
      "Epoch: 93620 | training loss: 2.3315e-03 | validation loss: 2.1460e-03\n",
      "Epoch: 93630 | training loss: 2.3315e-03 | validation loss: 2.1461e-03\n",
      "Epoch: 93640 | training loss: 2.3315e-03 | validation loss: 2.1465e-03\n",
      "Epoch: 93650 | training loss: 2.3324e-03 | validation loss: 2.1503e-03\n",
      "Epoch: 93660 | training loss: 2.3654e-03 | validation loss: 2.1888e-03\n",
      "Epoch: 93670 | training loss: 3.4258e-03 | validation loss: 2.8099e-03\n",
      "Epoch: 93680 | training loss: 2.7580e-03 | validation loss: 2.2547e-03\n",
      "Epoch: 93690 | training loss: 2.3402e-03 | validation loss: 2.1616e-03\n",
      "Epoch: 93700 | training loss: 2.3714e-03 | validation loss: 2.1899e-03\n",
      "Epoch: 93710 | training loss: 2.3440e-03 | validation loss: 2.1331e-03\n",
      "Epoch: 93720 | training loss: 2.3311e-03 | validation loss: 2.1431e-03\n",
      "Epoch: 93730 | training loss: 2.3323e-03 | validation loss: 2.1493e-03\n",
      "Epoch: 93740 | training loss: 2.3319e-03 | validation loss: 2.1403e-03\n",
      "Epoch: 93750 | training loss: 2.3313e-03 | validation loss: 2.1470e-03\n",
      "Epoch: 93760 | training loss: 2.3310e-03 | validation loss: 2.1423e-03\n",
      "Epoch: 93770 | training loss: 2.3308e-03 | validation loss: 2.1445e-03\n",
      "Epoch: 93780 | training loss: 2.3308e-03 | validation loss: 2.1437e-03\n",
      "Epoch: 93790 | training loss: 2.3307e-03 | validation loss: 2.1431e-03\n",
      "Epoch: 93800 | training loss: 2.3307e-03 | validation loss: 2.1434e-03\n",
      "Epoch: 93810 | training loss: 2.3306e-03 | validation loss: 2.1435e-03\n",
      "Epoch: 93820 | training loss: 2.3306e-03 | validation loss: 2.1436e-03\n",
      "Epoch: 93830 | training loss: 2.3307e-03 | validation loss: 2.1450e-03\n",
      "Epoch: 93840 | training loss: 2.3399e-03 | validation loss: 2.1609e-03\n",
      "Epoch: 93850 | training loss: 3.2031e-03 | validation loss: 2.6798e-03\n",
      "Epoch: 93860 | training loss: 3.0408e-03 | validation loss: 2.3593e-03\n",
      "Epoch: 93870 | training loss: 2.3377e-03 | validation loss: 2.1284e-03\n",
      "Epoch: 93880 | training loss: 2.3800e-03 | validation loss: 2.1900e-03\n",
      "Epoch: 93890 | training loss: 2.3624e-03 | validation loss: 2.1843e-03\n",
      "Epoch: 93900 | training loss: 2.3324e-03 | validation loss: 2.1531e-03\n",
      "Epoch: 93910 | training loss: 2.3328e-03 | validation loss: 2.1367e-03\n",
      "Epoch: 93920 | training loss: 2.3310e-03 | validation loss: 2.1369e-03\n",
      "Epoch: 93930 | training loss: 2.3304e-03 | validation loss: 2.1453e-03\n",
      "Epoch: 93940 | training loss: 2.3301e-03 | validation loss: 2.1437e-03\n",
      "Epoch: 93950 | training loss: 2.3301e-03 | validation loss: 2.1404e-03\n",
      "Epoch: 93960 | training loss: 2.3300e-03 | validation loss: 2.1428e-03\n",
      "Epoch: 93970 | training loss: 2.3299e-03 | validation loss: 2.1417e-03\n",
      "Epoch: 93980 | training loss: 2.3298e-03 | validation loss: 2.1419e-03\n",
      "Epoch: 93990 | training loss: 2.3298e-03 | validation loss: 2.1417e-03\n",
      "Epoch: 94000 | training loss: 2.3297e-03 | validation loss: 2.1417e-03\n",
      "Epoch: 94010 | training loss: 2.3297e-03 | validation loss: 2.1415e-03\n",
      "Epoch: 94020 | training loss: 2.3297e-03 | validation loss: 2.1415e-03\n",
      "Epoch: 94030 | training loss: 2.3296e-03 | validation loss: 2.1414e-03\n",
      "Epoch: 94040 | training loss: 2.3296e-03 | validation loss: 2.1412e-03\n",
      "Epoch: 94050 | training loss: 2.3295e-03 | validation loss: 2.1410e-03\n",
      "Epoch: 94060 | training loss: 2.3296e-03 | validation loss: 2.1400e-03\n",
      "Epoch: 94070 | training loss: 2.3492e-03 | validation loss: 2.1409e-03\n",
      "Epoch: 94080 | training loss: 2.8556e-03 | validation loss: 2.4414e-03\n",
      "Epoch: 94090 | training loss: 2.6313e-03 | validation loss: 2.3338e-03\n",
      "Epoch: 94100 | training loss: 2.5308e-03 | validation loss: 2.2817e-03\n",
      "Epoch: 94110 | training loss: 2.4294e-03 | validation loss: 2.1311e-03\n",
      "Epoch: 94120 | training loss: 2.3740e-03 | validation loss: 2.1817e-03\n",
      "Epoch: 94130 | training loss: 2.3465e-03 | validation loss: 2.1235e-03\n",
      "Epoch: 94140 | training loss: 2.3317e-03 | validation loss: 2.1434e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94150 | training loss: 2.3306e-03 | validation loss: 2.1442e-03\n",
      "Epoch: 94160 | training loss: 2.3296e-03 | validation loss: 2.1356e-03\n",
      "Epoch: 94170 | training loss: 2.3305e-03 | validation loss: 2.1340e-03\n",
      "Epoch: 94180 | training loss: 2.3362e-03 | validation loss: 2.1297e-03\n",
      "Epoch: 94190 | training loss: 2.4363e-03 | validation loss: 2.1375e-03\n",
      "Epoch: 94200 | training loss: 2.8796e-03 | validation loss: 2.2810e-03\n",
      "Epoch: 94210 | training loss: 2.5119e-03 | validation loss: 2.2866e-03\n",
      "Epoch: 94220 | training loss: 2.3863e-03 | validation loss: 2.1289e-03\n",
      "Epoch: 94230 | training loss: 2.3483e-03 | validation loss: 2.1681e-03\n",
      "Epoch: 94240 | training loss: 2.3374e-03 | validation loss: 2.1290e-03\n",
      "Epoch: 94250 | training loss: 2.3327e-03 | validation loss: 2.1503e-03\n",
      "Epoch: 94260 | training loss: 2.3292e-03 | validation loss: 2.1355e-03\n",
      "Epoch: 94270 | training loss: 2.3288e-03 | validation loss: 2.1362e-03\n",
      "Epoch: 94280 | training loss: 2.3286e-03 | validation loss: 2.1403e-03\n",
      "Epoch: 94290 | training loss: 2.3289e-03 | validation loss: 2.1421e-03\n",
      "Epoch: 94300 | training loss: 2.3316e-03 | validation loss: 2.1483e-03\n",
      "Epoch: 94310 | training loss: 2.3924e-03 | validation loss: 2.2054e-03\n",
      "Epoch: 94320 | training loss: 3.2185e-03 | validation loss: 2.6884e-03\n",
      "Epoch: 94330 | training loss: 2.6214e-03 | validation loss: 2.2044e-03\n",
      "Epoch: 94340 | training loss: 2.3812e-03 | validation loss: 2.1911e-03\n",
      "Epoch: 94350 | training loss: 2.3288e-03 | validation loss: 2.1330e-03\n",
      "Epoch: 94360 | training loss: 2.3299e-03 | validation loss: 2.1338e-03\n",
      "Epoch: 94370 | training loss: 2.3297e-03 | validation loss: 2.1445e-03\n",
      "Epoch: 94380 | training loss: 2.3285e-03 | validation loss: 2.1345e-03\n",
      "Epoch: 94390 | training loss: 2.3280e-03 | validation loss: 2.1382e-03\n",
      "Epoch: 94400 | training loss: 2.3281e-03 | validation loss: 2.1393e-03\n",
      "Epoch: 94410 | training loss: 2.3280e-03 | validation loss: 2.1359e-03\n",
      "Epoch: 94420 | training loss: 2.3278e-03 | validation loss: 2.1367e-03\n",
      "Epoch: 94430 | training loss: 2.3278e-03 | validation loss: 2.1372e-03\n",
      "Epoch: 94440 | training loss: 2.3277e-03 | validation loss: 2.1375e-03\n",
      "Epoch: 94450 | training loss: 2.3278e-03 | validation loss: 2.1387e-03\n",
      "Epoch: 94460 | training loss: 2.3341e-03 | validation loss: 2.1508e-03\n",
      "Epoch: 94470 | training loss: 2.9022e-03 | validation loss: 2.5081e-03\n",
      "Epoch: 94480 | training loss: 2.6935e-03 | validation loss: 2.2240e-03\n",
      "Epoch: 94490 | training loss: 2.4074e-03 | validation loss: 2.2073e-03\n",
      "Epoch: 94500 | training loss: 2.4313e-03 | validation loss: 2.2301e-03\n",
      "Epoch: 94510 | training loss: 2.3387e-03 | validation loss: 2.1589e-03\n",
      "Epoch: 94520 | training loss: 2.3316e-03 | validation loss: 2.1287e-03\n",
      "Epoch: 94530 | training loss: 2.3316e-03 | validation loss: 2.1281e-03\n",
      "Epoch: 94540 | training loss: 2.3275e-03 | validation loss: 2.1388e-03\n",
      "Epoch: 94550 | training loss: 2.3277e-03 | validation loss: 2.1395e-03\n",
      "Epoch: 94560 | training loss: 2.3273e-03 | validation loss: 2.1345e-03\n",
      "Epoch: 94570 | training loss: 2.3271e-03 | validation loss: 2.1359e-03\n",
      "Epoch: 94580 | training loss: 2.3271e-03 | validation loss: 2.1366e-03\n",
      "Epoch: 94590 | training loss: 2.3270e-03 | validation loss: 2.1353e-03\n",
      "Epoch: 94600 | training loss: 2.3270e-03 | validation loss: 2.1360e-03\n",
      "Epoch: 94610 | training loss: 2.3269e-03 | validation loss: 2.1355e-03\n",
      "Epoch: 94620 | training loss: 2.3269e-03 | validation loss: 2.1357e-03\n",
      "Epoch: 94630 | training loss: 2.3268e-03 | validation loss: 2.1356e-03\n",
      "Epoch: 94640 | training loss: 2.3268e-03 | validation loss: 2.1356e-03\n",
      "Epoch: 94650 | training loss: 2.3270e-03 | validation loss: 2.1375e-03\n",
      "Epoch: 94660 | training loss: 2.3540e-03 | validation loss: 2.1771e-03\n",
      "Epoch: 94670 | training loss: 2.6950e-03 | validation loss: 2.4772e-03\n",
      "Epoch: 94680 | training loss: 2.4368e-03 | validation loss: 2.2628e-03\n",
      "Epoch: 94690 | training loss: 2.3274e-03 | validation loss: 2.1392e-03\n",
      "Epoch: 94700 | training loss: 2.3350e-03 | validation loss: 2.1231e-03\n",
      "Epoch: 94710 | training loss: 2.3469e-03 | validation loss: 2.1181e-03\n",
      "Epoch: 94720 | training loss: 2.4855e-03 | validation loss: 2.1407e-03\n",
      "Epoch: 94730 | training loss: 2.6111e-03 | validation loss: 2.1832e-03\n",
      "Epoch: 94740 | training loss: 2.4495e-03 | validation loss: 2.2457e-03\n",
      "Epoch: 94750 | training loss: 2.3758e-03 | validation loss: 2.1237e-03\n",
      "Epoch: 94760 | training loss: 2.3435e-03 | validation loss: 2.1621e-03\n",
      "Epoch: 94770 | training loss: 2.3281e-03 | validation loss: 2.1278e-03\n",
      "Epoch: 94780 | training loss: 2.3274e-03 | validation loss: 2.1286e-03\n",
      "Epoch: 94790 | training loss: 2.3272e-03 | validation loss: 2.1389e-03\n",
      "Epoch: 94800 | training loss: 2.3277e-03 | validation loss: 2.1404e-03\n",
      "Epoch: 94810 | training loss: 2.3306e-03 | validation loss: 2.1457e-03\n",
      "Epoch: 94820 | training loss: 2.3793e-03 | validation loss: 2.1929e-03\n",
      "Epoch: 94830 | training loss: 2.9896e-03 | validation loss: 2.5620e-03\n",
      "Epoch: 94840 | training loss: 2.5024e-03 | validation loss: 2.1557e-03\n",
      "Epoch: 94850 | training loss: 2.4078e-03 | validation loss: 2.2083e-03\n",
      "Epoch: 94860 | training loss: 2.3594e-03 | validation loss: 2.1231e-03\n",
      "Epoch: 94870 | training loss: 2.3384e-03 | validation loss: 2.1549e-03\n",
      "Epoch: 94880 | training loss: 2.3293e-03 | validation loss: 2.1257e-03\n",
      "Epoch: 94890 | training loss: 2.3258e-03 | validation loss: 2.1343e-03\n",
      "Epoch: 94900 | training loss: 2.3262e-03 | validation loss: 2.1363e-03\n",
      "Epoch: 94910 | training loss: 2.3257e-03 | validation loss: 2.1308e-03\n",
      "Epoch: 94920 | training loss: 2.3260e-03 | validation loss: 2.1297e-03\n",
      "Epoch: 94930 | training loss: 2.3272e-03 | validation loss: 2.1273e-03\n",
      "Epoch: 94940 | training loss: 2.3492e-03 | validation loss: 2.1222e-03\n",
      "Epoch: 94950 | training loss: 2.9084e-03 | validation loss: 2.3008e-03\n",
      "Epoch: 94960 | training loss: 2.4235e-03 | validation loss: 2.2232e-03\n",
      "Epoch: 94970 | training loss: 2.4446e-03 | validation loss: 2.1402e-03\n",
      "Epoch: 94980 | training loss: 2.3745e-03 | validation loss: 2.1819e-03\n",
      "Epoch: 94990 | training loss: 2.3350e-03 | validation loss: 2.1259e-03\n",
      "Epoch: 95000 | training loss: 2.3269e-03 | validation loss: 2.1355e-03\n",
      "Epoch: 95010 | training loss: 2.3258e-03 | validation loss: 2.1304e-03\n",
      "Epoch: 95020 | training loss: 2.3256e-03 | validation loss: 2.1337e-03\n",
      "Epoch: 95030 | training loss: 2.3254e-03 | validation loss: 2.1294e-03\n",
      "Epoch: 95040 | training loss: 2.3250e-03 | validation loss: 2.1326e-03\n",
      "Epoch: 95050 | training loss: 2.3250e-03 | validation loss: 2.1319e-03\n",
      "Epoch: 95060 | training loss: 2.3249e-03 | validation loss: 2.1310e-03\n",
      "Epoch: 95070 | training loss: 2.3248e-03 | validation loss: 2.1306e-03\n",
      "Epoch: 95080 | training loss: 2.3250e-03 | validation loss: 2.1300e-03\n",
      "Epoch: 95090 | training loss: 2.3342e-03 | validation loss: 2.1325e-03\n",
      "Epoch: 95100 | training loss: 2.7398e-03 | validation loss: 2.3775e-03\n",
      "Epoch: 95110 | training loss: 3.2990e-03 | validation loss: 2.8579e-03\n",
      "Epoch: 95120 | training loss: 2.5394e-03 | validation loss: 2.2802e-03\n",
      "Epoch: 95130 | training loss: 2.3794e-03 | validation loss: 2.1861e-03\n",
      "Epoch: 95140 | training loss: 2.3592e-03 | validation loss: 2.1853e-03\n",
      "Epoch: 95150 | training loss: 2.3369e-03 | validation loss: 2.1350e-03\n",
      "Epoch: 95160 | training loss: 2.3274e-03 | validation loss: 2.1418e-03\n",
      "Epoch: 95170 | training loss: 2.3249e-03 | validation loss: 2.1295e-03\n",
      "Epoch: 95180 | training loss: 2.3245e-03 | validation loss: 2.1315e-03\n",
      "Epoch: 95190 | training loss: 2.3245e-03 | validation loss: 2.1271e-03\n",
      "Epoch: 95200 | training loss: 2.3243e-03 | validation loss: 2.1297e-03\n",
      "Epoch: 95210 | training loss: 2.3242e-03 | validation loss: 2.1292e-03\n",
      "Epoch: 95220 | training loss: 2.3242e-03 | validation loss: 2.1289e-03\n",
      "Epoch: 95230 | training loss: 2.3241e-03 | validation loss: 2.1286e-03\n",
      "Epoch: 95240 | training loss: 2.3242e-03 | validation loss: 2.1273e-03\n",
      "Epoch: 95250 | training loss: 2.3275e-03 | validation loss: 2.1220e-03\n",
      "Epoch: 95260 | training loss: 2.4997e-03 | validation loss: 2.1491e-03\n",
      "Epoch: 95270 | training loss: 2.6804e-03 | validation loss: 2.2187e-03\n",
      "Epoch: 95280 | training loss: 2.4056e-03 | validation loss: 2.1250e-03\n",
      "Epoch: 95290 | training loss: 2.4086e-03 | validation loss: 2.2033e-03\n",
      "Epoch: 95300 | training loss: 2.3305e-03 | validation loss: 2.1395e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95310 | training loss: 2.3375e-03 | validation loss: 2.1184e-03\n",
      "Epoch: 95320 | training loss: 2.3240e-03 | validation loss: 2.1315e-03\n",
      "Epoch: 95330 | training loss: 2.3244e-03 | validation loss: 2.1330e-03\n",
      "Epoch: 95340 | training loss: 2.3243e-03 | validation loss: 2.1250e-03\n",
      "Epoch: 95350 | training loss: 2.3238e-03 | validation loss: 2.1303e-03\n",
      "Epoch: 95360 | training loss: 2.3236e-03 | validation loss: 2.1269e-03\n",
      "Epoch: 95370 | training loss: 2.3235e-03 | validation loss: 2.1290e-03\n",
      "Epoch: 95380 | training loss: 2.3234e-03 | validation loss: 2.1274e-03\n",
      "Epoch: 95390 | training loss: 2.3234e-03 | validation loss: 2.1279e-03\n",
      "Epoch: 95400 | training loss: 2.3233e-03 | validation loss: 2.1280e-03\n",
      "Epoch: 95410 | training loss: 2.3233e-03 | validation loss: 2.1278e-03\n",
      "Epoch: 95420 | training loss: 2.3232e-03 | validation loss: 2.1276e-03\n",
      "Epoch: 95430 | training loss: 2.3232e-03 | validation loss: 2.1276e-03\n",
      "Epoch: 95440 | training loss: 2.3232e-03 | validation loss: 2.1278e-03\n",
      "Epoch: 95450 | training loss: 2.3239e-03 | validation loss: 2.1315e-03\n",
      "Epoch: 95460 | training loss: 2.4434e-03 | validation loss: 2.2299e-03\n",
      "Epoch: 95470 | training loss: 2.9317e-03 | validation loss: 2.5147e-03\n",
      "Epoch: 95480 | training loss: 2.8629e-03 | validation loss: 2.4791e-03\n",
      "Epoch: 95490 | training loss: 2.4931e-03 | validation loss: 2.2661e-03\n",
      "Epoch: 95500 | training loss: 2.3686e-03 | validation loss: 2.1804e-03\n",
      "Epoch: 95510 | training loss: 2.3305e-03 | validation loss: 2.1434e-03\n",
      "Epoch: 95520 | training loss: 2.3229e-03 | validation loss: 2.1273e-03\n",
      "Epoch: 95530 | training loss: 2.3237e-03 | validation loss: 2.1224e-03\n",
      "Epoch: 95540 | training loss: 2.3238e-03 | validation loss: 2.1225e-03\n",
      "Epoch: 95550 | training loss: 2.3228e-03 | validation loss: 2.1246e-03\n",
      "Epoch: 95560 | training loss: 2.3227e-03 | validation loss: 2.1273e-03\n",
      "Epoch: 95570 | training loss: 2.3226e-03 | validation loss: 2.1273e-03\n",
      "Epoch: 95580 | training loss: 2.3225e-03 | validation loss: 2.1259e-03\n",
      "Epoch: 95590 | training loss: 2.3225e-03 | validation loss: 2.1258e-03\n",
      "Epoch: 95600 | training loss: 2.3224e-03 | validation loss: 2.1262e-03\n",
      "Epoch: 95610 | training loss: 2.3224e-03 | validation loss: 2.1259e-03\n",
      "Epoch: 95620 | training loss: 2.3223e-03 | validation loss: 2.1258e-03\n",
      "Epoch: 95630 | training loss: 2.3223e-03 | validation loss: 2.1258e-03\n",
      "Epoch: 95640 | training loss: 2.3222e-03 | validation loss: 2.1256e-03\n",
      "Epoch: 95650 | training loss: 2.3222e-03 | validation loss: 2.1256e-03\n",
      "Epoch: 95660 | training loss: 2.3222e-03 | validation loss: 2.1256e-03\n",
      "Epoch: 95670 | training loss: 2.3222e-03 | validation loss: 2.1264e-03\n",
      "Epoch: 95680 | training loss: 2.3303e-03 | validation loss: 2.1437e-03\n",
      "Epoch: 95690 | training loss: 3.0526e-03 | validation loss: 2.7574e-03\n",
      "Epoch: 95700 | training loss: 2.3245e-03 | validation loss: 2.1376e-03\n",
      "Epoch: 95710 | training loss: 2.3372e-03 | validation loss: 2.1126e-03\n",
      "Epoch: 95720 | training loss: 2.3337e-03 | validation loss: 2.1243e-03\n",
      "Epoch: 95730 | training loss: 2.3309e-03 | validation loss: 2.1267e-03\n",
      "Epoch: 95740 | training loss: 2.3263e-03 | validation loss: 2.1256e-03\n",
      "Epoch: 95750 | training loss: 2.3261e-03 | validation loss: 2.1329e-03\n",
      "Epoch: 95760 | training loss: 2.3827e-03 | validation loss: 2.1920e-03\n",
      "Epoch: 95770 | training loss: 3.0814e-03 | validation loss: 2.6152e-03\n",
      "Epoch: 95780 | training loss: 2.5773e-03 | validation loss: 2.1685e-03\n",
      "Epoch: 95790 | training loss: 2.3981e-03 | validation loss: 2.2022e-03\n",
      "Epoch: 95800 | training loss: 2.3355e-03 | validation loss: 2.1125e-03\n",
      "Epoch: 95810 | training loss: 2.3239e-03 | validation loss: 2.1317e-03\n",
      "Epoch: 95820 | training loss: 2.3225e-03 | validation loss: 2.1187e-03\n",
      "Epoch: 95830 | training loss: 2.3225e-03 | validation loss: 2.1285e-03\n",
      "Epoch: 95840 | training loss: 2.3222e-03 | validation loss: 2.1195e-03\n",
      "Epoch: 95850 | training loss: 2.3214e-03 | validation loss: 2.1245e-03\n",
      "Epoch: 95860 | training loss: 2.3214e-03 | validation loss: 2.1252e-03\n",
      "Epoch: 95870 | training loss: 2.3213e-03 | validation loss: 2.1241e-03\n",
      "Epoch: 95880 | training loss: 2.3213e-03 | validation loss: 2.1245e-03\n",
      "Epoch: 95890 | training loss: 2.3226e-03 | validation loss: 2.1293e-03\n",
      "Epoch: 95900 | training loss: 2.3815e-03 | validation loss: 2.1874e-03\n",
      "Epoch: 95910 | training loss: 3.5827e-03 | validation loss: 2.8714e-03\n",
      "Epoch: 95920 | training loss: 2.4985e-03 | validation loss: 2.1634e-03\n",
      "Epoch: 95930 | training loss: 2.3750e-03 | validation loss: 2.1135e-03\n",
      "Epoch: 95940 | training loss: 2.3583e-03 | validation loss: 2.1585e-03\n",
      "Epoch: 95950 | training loss: 2.3222e-03 | validation loss: 2.1262e-03\n",
      "Epoch: 95960 | training loss: 2.3272e-03 | validation loss: 2.1172e-03\n",
      "Epoch: 95970 | training loss: 2.3222e-03 | validation loss: 2.1283e-03\n",
      "Epoch: 95980 | training loss: 2.3208e-03 | validation loss: 2.1202e-03\n",
      "Epoch: 95990 | training loss: 2.3207e-03 | validation loss: 2.1221e-03\n",
      "Epoch: 96000 | training loss: 2.3206e-03 | validation loss: 2.1223e-03\n",
      "Epoch: 96010 | training loss: 2.3206e-03 | validation loss: 2.1218e-03\n",
      "Epoch: 96020 | training loss: 2.3205e-03 | validation loss: 2.1215e-03\n",
      "Epoch: 96030 | training loss: 2.3205e-03 | validation loss: 2.1220e-03\n",
      "Epoch: 96040 | training loss: 2.3204e-03 | validation loss: 2.1217e-03\n",
      "Epoch: 96050 | training loss: 2.3204e-03 | validation loss: 2.1213e-03\n",
      "Epoch: 96060 | training loss: 2.3204e-03 | validation loss: 2.1211e-03\n",
      "Epoch: 96070 | training loss: 2.3204e-03 | validation loss: 2.1202e-03\n",
      "Epoch: 96080 | training loss: 2.3228e-03 | validation loss: 2.1158e-03\n",
      "Epoch: 96090 | training loss: 2.5141e-03 | validation loss: 2.1548e-03\n",
      "Epoch: 96100 | training loss: 2.5358e-03 | validation loss: 2.1621e-03\n",
      "Epoch: 96110 | training loss: 2.6355e-03 | validation loss: 2.1946e-03\n",
      "Epoch: 96120 | training loss: 2.3204e-03 | validation loss: 2.1172e-03\n",
      "Epoch: 96130 | training loss: 2.3564e-03 | validation loss: 2.1622e-03\n",
      "Epoch: 96140 | training loss: 2.3286e-03 | validation loss: 2.1368e-03\n",
      "Epoch: 96150 | training loss: 2.3221e-03 | validation loss: 2.1155e-03\n",
      "Epoch: 96160 | training loss: 2.3212e-03 | validation loss: 2.1163e-03\n",
      "Epoch: 96170 | training loss: 2.3204e-03 | validation loss: 2.1238e-03\n",
      "Epoch: 96180 | training loss: 2.3198e-03 | validation loss: 2.1206e-03\n",
      "Epoch: 96190 | training loss: 2.3198e-03 | validation loss: 2.1192e-03\n",
      "Epoch: 96200 | training loss: 2.3198e-03 | validation loss: 2.1210e-03\n",
      "Epoch: 96210 | training loss: 2.3197e-03 | validation loss: 2.1195e-03\n",
      "Epoch: 96220 | training loss: 2.3196e-03 | validation loss: 2.1203e-03\n",
      "Epoch: 96230 | training loss: 2.3196e-03 | validation loss: 2.1197e-03\n",
      "Epoch: 96240 | training loss: 2.3195e-03 | validation loss: 2.1199e-03\n",
      "Epoch: 96250 | training loss: 2.3195e-03 | validation loss: 2.1198e-03\n",
      "Epoch: 96260 | training loss: 2.3195e-03 | validation loss: 2.1208e-03\n",
      "Epoch: 96270 | training loss: 2.3322e-03 | validation loss: 2.1458e-03\n",
      "Epoch: 96280 | training loss: 3.1036e-03 | validation loss: 2.7945e-03\n",
      "Epoch: 96290 | training loss: 2.4482e-03 | validation loss: 2.2499e-03\n",
      "Epoch: 96300 | training loss: 2.3637e-03 | validation loss: 2.1489e-03\n",
      "Epoch: 96310 | training loss: 2.3363e-03 | validation loss: 2.1509e-03\n",
      "Epoch: 96320 | training loss: 2.3233e-03 | validation loss: 2.1303e-03\n",
      "Epoch: 96330 | training loss: 2.3205e-03 | validation loss: 2.1175e-03\n",
      "Epoch: 96340 | training loss: 2.3219e-03 | validation loss: 2.1115e-03\n",
      "Epoch: 96350 | training loss: 2.3433e-03 | validation loss: 2.1038e-03\n",
      "Epoch: 96360 | training loss: 2.7150e-03 | validation loss: 2.2003e-03\n",
      "Epoch: 96370 | training loss: 2.3206e-03 | validation loss: 2.1250e-03\n",
      "Epoch: 96380 | training loss: 2.3237e-03 | validation loss: 2.1092e-03\n",
      "Epoch: 96390 | training loss: 2.3237e-03 | validation loss: 2.1304e-03\n",
      "Epoch: 96400 | training loss: 2.3201e-03 | validation loss: 2.1130e-03\n",
      "Epoch: 96410 | training loss: 2.3188e-03 | validation loss: 2.1170e-03\n",
      "Epoch: 96420 | training loss: 2.3201e-03 | validation loss: 2.1238e-03\n",
      "Epoch: 96430 | training loss: 2.3197e-03 | validation loss: 2.1132e-03\n",
      "Epoch: 96440 | training loss: 2.3188e-03 | validation loss: 2.1158e-03\n",
      "Epoch: 96450 | training loss: 2.3186e-03 | validation loss: 2.1185e-03\n",
      "Epoch: 96460 | training loss: 2.3190e-03 | validation loss: 2.1206e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96470 | training loss: 2.3248e-03 | validation loss: 2.1321e-03\n",
      "Epoch: 96480 | training loss: 2.5380e-03 | validation loss: 2.2891e-03\n",
      "Epoch: 96490 | training loss: 2.5136e-03 | validation loss: 2.2656e-03\n",
      "Epoch: 96500 | training loss: 2.3355e-03 | validation loss: 2.1450e-03\n",
      "Epoch: 96510 | training loss: 2.3968e-03 | validation loss: 2.1189e-03\n",
      "Epoch: 96520 | training loss: 2.3360e-03 | validation loss: 2.1439e-03\n",
      "Epoch: 96530 | training loss: 2.3184e-03 | validation loss: 2.1147e-03\n",
      "Epoch: 96540 | training loss: 2.3190e-03 | validation loss: 2.1124e-03\n",
      "Epoch: 96550 | training loss: 2.3189e-03 | validation loss: 2.1208e-03\n",
      "Epoch: 96560 | training loss: 2.3184e-03 | validation loss: 2.1147e-03\n",
      "Epoch: 96570 | training loss: 2.3181e-03 | validation loss: 2.1170e-03\n",
      "Epoch: 96580 | training loss: 2.3180e-03 | validation loss: 2.1169e-03\n",
      "Epoch: 96590 | training loss: 2.3180e-03 | validation loss: 2.1154e-03\n",
      "Epoch: 96600 | training loss: 2.3179e-03 | validation loss: 2.1160e-03\n",
      "Epoch: 96610 | training loss: 2.3179e-03 | validation loss: 2.1164e-03\n",
      "Epoch: 96620 | training loss: 2.3179e-03 | validation loss: 2.1168e-03\n",
      "Epoch: 96630 | training loss: 2.3184e-03 | validation loss: 2.1195e-03\n",
      "Epoch: 96640 | training loss: 2.3448e-03 | validation loss: 2.1507e-03\n",
      "Epoch: 96650 | training loss: 3.6198e-03 | validation loss: 2.8793e-03\n",
      "Epoch: 96660 | training loss: 2.8289e-03 | validation loss: 2.2627e-03\n",
      "Epoch: 96670 | training loss: 2.3426e-03 | validation loss: 2.1000e-03\n",
      "Epoch: 96680 | training loss: 2.3657e-03 | validation loss: 2.1656e-03\n",
      "Epoch: 96690 | training loss: 2.3272e-03 | validation loss: 2.1366e-03\n",
      "Epoch: 96700 | training loss: 2.3234e-03 | validation loss: 2.1068e-03\n",
      "Epoch: 96710 | training loss: 2.3178e-03 | validation loss: 2.1115e-03\n",
      "Epoch: 96720 | training loss: 2.3186e-03 | validation loss: 2.1215e-03\n",
      "Epoch: 96730 | training loss: 2.3176e-03 | validation loss: 2.1124e-03\n",
      "Epoch: 96740 | training loss: 2.3173e-03 | validation loss: 2.1158e-03\n",
      "Epoch: 96750 | training loss: 2.3173e-03 | validation loss: 2.1148e-03\n",
      "Epoch: 96760 | training loss: 2.3172e-03 | validation loss: 2.1148e-03\n",
      "Epoch: 96770 | training loss: 2.3172e-03 | validation loss: 2.1146e-03\n",
      "Epoch: 96780 | training loss: 2.3171e-03 | validation loss: 2.1148e-03\n",
      "Epoch: 96790 | training loss: 2.3171e-03 | validation loss: 2.1143e-03\n",
      "Epoch: 96800 | training loss: 2.3170e-03 | validation loss: 2.1142e-03\n",
      "Epoch: 96810 | training loss: 2.3170e-03 | validation loss: 2.1138e-03\n",
      "Epoch: 96820 | training loss: 2.3178e-03 | validation loss: 2.1111e-03\n",
      "Epoch: 96830 | training loss: 2.4110e-03 | validation loss: 2.1426e-03\n",
      "Epoch: 96840 | training loss: 2.3349e-03 | validation loss: 2.1075e-03\n",
      "Epoch: 96850 | training loss: 2.4953e-03 | validation loss: 2.1447e-03\n",
      "Epoch: 96860 | training loss: 2.4898e-03 | validation loss: 2.1247e-03\n",
      "Epoch: 96870 | training loss: 2.3196e-03 | validation loss: 2.1051e-03\n",
      "Epoch: 96880 | training loss: 2.3438e-03 | validation loss: 2.1551e-03\n",
      "Epoch: 96890 | training loss: 2.3417e-03 | validation loss: 2.1542e-03\n",
      "Epoch: 96900 | training loss: 2.3459e-03 | validation loss: 2.1556e-03\n",
      "Epoch: 96910 | training loss: 2.4276e-03 | validation loss: 2.2157e-03\n",
      "Epoch: 96920 | training loss: 2.5346e-03 | validation loss: 2.2840e-03\n",
      "Epoch: 96930 | training loss: 2.3329e-03 | validation loss: 2.1008e-03\n",
      "Epoch: 96940 | training loss: 2.3384e-03 | validation loss: 2.1008e-03\n",
      "Epoch: 96950 | training loss: 2.3218e-03 | validation loss: 2.1262e-03\n",
      "Epoch: 96960 | training loss: 2.3317e-03 | validation loss: 2.1382e-03\n",
      "Epoch: 96970 | training loss: 2.3498e-03 | validation loss: 2.1556e-03\n",
      "Epoch: 96980 | training loss: 2.4827e-03 | validation loss: 2.2505e-03\n",
      "Epoch: 96990 | training loss: 2.4710e-03 | validation loss: 2.2404e-03\n",
      "Epoch: 97000 | training loss: 2.3742e-03 | validation loss: 2.1051e-03\n",
      "Epoch: 97010 | training loss: 2.3170e-03 | validation loss: 2.1080e-03\n",
      "Epoch: 97020 | training loss: 2.3322e-03 | validation loss: 2.1373e-03\n",
      "Epoch: 97030 | training loss: 2.3222e-03 | validation loss: 2.1262e-03\n",
      "Epoch: 97040 | training loss: 2.3198e-03 | validation loss: 2.1224e-03\n",
      "Epoch: 97050 | training loss: 2.3389e-03 | validation loss: 2.1439e-03\n",
      "Epoch: 97060 | training loss: 2.6868e-03 | validation loss: 2.3703e-03\n",
      "Epoch: 97070 | training loss: 2.3213e-03 | validation loss: 2.1197e-03\n",
      "Epoch: 97080 | training loss: 2.3162e-03 | validation loss: 2.1152e-03\n",
      "Epoch: 97090 | training loss: 2.3172e-03 | validation loss: 2.1062e-03\n",
      "Epoch: 97100 | training loss: 2.3161e-03 | validation loss: 2.1132e-03\n",
      "Epoch: 97110 | training loss: 2.3159e-03 | validation loss: 2.1141e-03\n",
      "Epoch: 97120 | training loss: 2.3172e-03 | validation loss: 2.1062e-03\n",
      "Epoch: 97130 | training loss: 2.3167e-03 | validation loss: 2.1164e-03\n",
      "Epoch: 97140 | training loss: 2.3156e-03 | validation loss: 2.1124e-03\n",
      "Epoch: 97150 | training loss: 2.3155e-03 | validation loss: 2.1097e-03\n",
      "Epoch: 97160 | training loss: 2.3160e-03 | validation loss: 2.1080e-03\n",
      "Epoch: 97170 | training loss: 2.3233e-03 | validation loss: 2.1029e-03\n",
      "Epoch: 97180 | training loss: 2.5605e-03 | validation loss: 2.1614e-03\n",
      "Epoch: 97190 | training loss: 2.4642e-03 | validation loss: 2.1311e-03\n",
      "Epoch: 97200 | training loss: 2.3290e-03 | validation loss: 2.1003e-03\n",
      "Epoch: 97210 | training loss: 2.3840e-03 | validation loss: 2.1768e-03\n",
      "Epoch: 97220 | training loss: 2.3381e-03 | validation loss: 2.1007e-03\n",
      "Epoch: 97230 | training loss: 2.3170e-03 | validation loss: 2.1169e-03\n",
      "Epoch: 97240 | training loss: 2.3150e-03 | validation loss: 2.1100e-03\n",
      "Epoch: 97250 | training loss: 2.3150e-03 | validation loss: 2.1095e-03\n",
      "Epoch: 97260 | training loss: 2.3149e-03 | validation loss: 2.1098e-03\n",
      "Epoch: 97270 | training loss: 2.3150e-03 | validation loss: 2.1112e-03\n",
      "Epoch: 97280 | training loss: 2.3150e-03 | validation loss: 2.1085e-03\n",
      "Epoch: 97290 | training loss: 2.3148e-03 | validation loss: 2.1098e-03\n",
      "Epoch: 97300 | training loss: 2.3148e-03 | validation loss: 2.1101e-03\n",
      "Epoch: 97310 | training loss: 2.3150e-03 | validation loss: 2.1085e-03\n",
      "Epoch: 97320 | training loss: 2.3439e-03 | validation loss: 2.1081e-03\n",
      "Epoch: 97330 | training loss: 2.6990e-03 | validation loss: 2.2650e-03\n",
      "Epoch: 97340 | training loss: 2.5411e-03 | validation loss: 2.1588e-03\n",
      "Epoch: 97350 | training loss: 2.3321e-03 | validation loss: 2.1216e-03\n",
      "Epoch: 97360 | training loss: 2.3294e-03 | validation loss: 2.1355e-03\n",
      "Epoch: 97370 | training loss: 2.3170e-03 | validation loss: 2.1146e-03\n",
      "Epoch: 97380 | training loss: 2.3209e-03 | validation loss: 2.1071e-03\n",
      "Epoch: 97390 | training loss: 2.3432e-03 | validation loss: 2.0977e-03\n",
      "Epoch: 97400 | training loss: 2.6508e-03 | validation loss: 2.1726e-03\n",
      "Epoch: 97410 | training loss: 2.3198e-03 | validation loss: 2.0997e-03\n",
      "Epoch: 97420 | training loss: 2.3248e-03 | validation loss: 2.1279e-03\n",
      "Epoch: 97430 | training loss: 2.3252e-03 | validation loss: 2.0970e-03\n",
      "Epoch: 97440 | training loss: 2.3253e-03 | validation loss: 2.1288e-03\n",
      "Epoch: 97450 | training loss: 2.3196e-03 | validation loss: 2.0994e-03\n",
      "Epoch: 97460 | training loss: 2.3141e-03 | validation loss: 2.1077e-03\n",
      "Epoch: 97470 | training loss: 2.3155e-03 | validation loss: 2.1143e-03\n",
      "Epoch: 97480 | training loss: 2.3157e-03 | validation loss: 2.1146e-03\n",
      "Epoch: 97490 | training loss: 2.3211e-03 | validation loss: 2.1236e-03\n",
      "Epoch: 97500 | training loss: 2.4254e-03 | validation loss: 2.2090e-03\n",
      "Epoch: 97510 | training loss: 2.9111e-03 | validation loss: 2.4956e-03\n",
      "Epoch: 97520 | training loss: 2.5044e-03 | validation loss: 2.1397e-03\n",
      "Epoch: 97530 | training loss: 2.3659e-03 | validation loss: 2.1615e-03\n",
      "Epoch: 97540 | training loss: 2.3280e-03 | validation loss: 2.0961e-03\n",
      "Epoch: 97550 | training loss: 2.3197e-03 | validation loss: 2.1210e-03\n",
      "Epoch: 97560 | training loss: 2.3173e-03 | validation loss: 2.1007e-03\n",
      "Epoch: 97570 | training loss: 2.3151e-03 | validation loss: 2.1128e-03\n",
      "Epoch: 97580 | training loss: 2.3135e-03 | validation loss: 2.1068e-03\n",
      "Epoch: 97590 | training loss: 2.3139e-03 | validation loss: 2.1041e-03\n",
      "Epoch: 97600 | training loss: 2.3137e-03 | validation loss: 2.1047e-03\n",
      "Epoch: 97610 | training loss: 2.3140e-03 | validation loss: 2.1034e-03\n",
      "Epoch: 97620 | training loss: 2.3224e-03 | validation loss: 2.0979e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97630 | training loss: 2.6204e-03 | validation loss: 2.1753e-03\n",
      "Epoch: 97640 | training loss: 2.3485e-03 | validation loss: 2.1030e-03\n",
      "Epoch: 97650 | training loss: 2.3838e-03 | validation loss: 2.0997e-03\n",
      "Epoch: 97660 | training loss: 2.3892e-03 | validation loss: 2.1773e-03\n",
      "Epoch: 97670 | training loss: 2.3216e-03 | validation loss: 2.1008e-03\n",
      "Epoch: 97680 | training loss: 2.3137e-03 | validation loss: 2.1013e-03\n",
      "Epoch: 97690 | training loss: 2.3146e-03 | validation loss: 2.1136e-03\n",
      "Epoch: 97700 | training loss: 2.3139e-03 | validation loss: 2.1012e-03\n",
      "Epoch: 97710 | training loss: 2.3133e-03 | validation loss: 2.1092e-03\n",
      "Epoch: 97720 | training loss: 2.3130e-03 | validation loss: 2.1042e-03\n",
      "Epoch: 97730 | training loss: 2.3129e-03 | validation loss: 2.1055e-03\n",
      "Epoch: 97740 | training loss: 2.3128e-03 | validation loss: 2.1067e-03\n",
      "Epoch: 97750 | training loss: 2.3128e-03 | validation loss: 2.1055e-03\n",
      "Epoch: 97760 | training loss: 2.3127e-03 | validation loss: 2.1047e-03\n",
      "Epoch: 97770 | training loss: 2.3128e-03 | validation loss: 2.1035e-03\n",
      "Epoch: 97780 | training loss: 2.3158e-03 | validation loss: 2.0981e-03\n",
      "Epoch: 97790 | training loss: 2.4989e-03 | validation loss: 2.1691e-03\n",
      "Epoch: 97800 | training loss: 2.5086e-03 | validation loss: 2.1888e-03\n",
      "Epoch: 97810 | training loss: 2.6351e-03 | validation loss: 2.1612e-03\n",
      "Epoch: 97820 | training loss: 2.4484e-03 | validation loss: 2.1966e-03\n",
      "Epoch: 97830 | training loss: 2.3409e-03 | validation loss: 2.0867e-03\n",
      "Epoch: 97840 | training loss: 2.3131e-03 | validation loss: 2.1095e-03\n",
      "Epoch: 97850 | training loss: 2.3183e-03 | validation loss: 2.1219e-03\n",
      "Epoch: 97860 | training loss: 2.3136e-03 | validation loss: 2.1022e-03\n",
      "Epoch: 97870 | training loss: 2.3150e-03 | validation loss: 2.0964e-03\n",
      "Epoch: 97880 | training loss: 2.3189e-03 | validation loss: 2.0940e-03\n",
      "Epoch: 97890 | training loss: 2.3775e-03 | validation loss: 2.0971e-03\n",
      "Epoch: 97900 | training loss: 2.9102e-03 | validation loss: 2.2705e-03\n",
      "Epoch: 97910 | training loss: 2.4592e-03 | validation loss: 2.2247e-03\n",
      "Epoch: 97920 | training loss: 2.3672e-03 | validation loss: 2.0972e-03\n",
      "Epoch: 97930 | training loss: 2.3305e-03 | validation loss: 2.1310e-03\n",
      "Epoch: 97940 | training loss: 2.3152e-03 | validation loss: 2.0967e-03\n",
      "Epoch: 97950 | training loss: 2.3119e-03 | validation loss: 2.1020e-03\n",
      "Epoch: 97960 | training loss: 2.3137e-03 | validation loss: 2.1102e-03\n",
      "Epoch: 97970 | training loss: 2.3119e-03 | validation loss: 2.1018e-03\n",
      "Epoch: 97980 | training loss: 2.3125e-03 | validation loss: 2.0996e-03\n",
      "Epoch: 97990 | training loss: 2.3142e-03 | validation loss: 2.0973e-03\n",
      "Epoch: 98000 | training loss: 2.3416e-03 | validation loss: 2.0932e-03\n",
      "Epoch: 98010 | training loss: 2.8911e-03 | validation loss: 2.2704e-03\n",
      "Epoch: 98020 | training loss: 2.3981e-03 | validation loss: 2.1845e-03\n",
      "Epoch: 98030 | training loss: 2.3934e-03 | validation loss: 2.1028e-03\n",
      "Epoch: 98040 | training loss: 2.3562e-03 | validation loss: 2.1494e-03\n",
      "Epoch: 98050 | training loss: 2.3288e-03 | validation loss: 2.0944e-03\n",
      "Epoch: 98060 | training loss: 2.3181e-03 | validation loss: 2.1157e-03\n",
      "Epoch: 98070 | training loss: 2.3138e-03 | validation loss: 2.0972e-03\n",
      "Epoch: 98080 | training loss: 2.3117e-03 | validation loss: 2.1050e-03\n",
      "Epoch: 98090 | training loss: 2.3113e-03 | validation loss: 2.1036e-03\n",
      "Epoch: 98100 | training loss: 2.3114e-03 | validation loss: 2.1003e-03\n",
      "Epoch: 98110 | training loss: 2.3114e-03 | validation loss: 2.1003e-03\n",
      "Epoch: 98120 | training loss: 2.3116e-03 | validation loss: 2.0992e-03\n",
      "Epoch: 98130 | training loss: 2.3180e-03 | validation loss: 2.0943e-03\n",
      "Epoch: 98140 | training loss: 2.5376e-03 | validation loss: 2.1469e-03\n",
      "Epoch: 98150 | training loss: 2.5102e-03 | validation loss: 2.1370e-03\n",
      "Epoch: 98160 | training loss: 2.3250e-03 | validation loss: 2.0930e-03\n",
      "Epoch: 98170 | training loss: 2.3879e-03 | validation loss: 2.1722e-03\n",
      "Epoch: 98180 | training loss: 2.3305e-03 | validation loss: 2.0924e-03\n",
      "Epoch: 98190 | training loss: 2.3111e-03 | validation loss: 2.1036e-03\n",
      "Epoch: 98200 | training loss: 2.3113e-03 | validation loss: 2.1045e-03\n",
      "Epoch: 98210 | training loss: 2.3112e-03 | validation loss: 2.0984e-03\n",
      "Epoch: 98220 | training loss: 2.3108e-03 | validation loss: 2.1029e-03\n",
      "Epoch: 98230 | training loss: 2.3106e-03 | validation loss: 2.1007e-03\n",
      "Epoch: 98240 | training loss: 2.3106e-03 | validation loss: 2.1002e-03\n",
      "Epoch: 98250 | training loss: 2.3106e-03 | validation loss: 2.1018e-03\n",
      "Epoch: 98260 | training loss: 2.3105e-03 | validation loss: 2.1019e-03\n",
      "Epoch: 98270 | training loss: 2.3122e-03 | validation loss: 2.1083e-03\n",
      "Epoch: 98280 | training loss: 2.5037e-03 | validation loss: 2.3048e-03\n",
      "Epoch: 98290 | training loss: 2.5723e-03 | validation loss: 2.2633e-03\n",
      "Epoch: 98300 | training loss: 2.3157e-03 | validation loss: 2.1099e-03\n",
      "Epoch: 98310 | training loss: 2.3501e-03 | validation loss: 2.1209e-03\n",
      "Epoch: 98320 | training loss: 2.3230e-03 | validation loss: 2.1184e-03\n",
      "Epoch: 98330 | training loss: 2.3136e-03 | validation loss: 2.1124e-03\n",
      "Epoch: 98340 | training loss: 2.3246e-03 | validation loss: 2.1225e-03\n",
      "Epoch: 98350 | training loss: 2.4897e-03 | validation loss: 2.2460e-03\n",
      "Epoch: 98360 | training loss: 2.5592e-03 | validation loss: 2.2878e-03\n",
      "Epoch: 98370 | training loss: 2.4159e-03 | validation loss: 2.1013e-03\n",
      "Epoch: 98380 | training loss: 2.3536e-03 | validation loss: 2.1517e-03\n",
      "Epoch: 98390 | training loss: 2.3295e-03 | validation loss: 2.0880e-03\n",
      "Epoch: 98400 | training loss: 2.3161e-03 | validation loss: 2.1132e-03\n",
      "Epoch: 98410 | training loss: 2.3099e-03 | validation loss: 2.0979e-03\n",
      "Epoch: 98420 | training loss: 2.3114e-03 | validation loss: 2.0939e-03\n",
      "Epoch: 98430 | training loss: 2.3099e-03 | validation loss: 2.0973e-03\n",
      "Epoch: 98440 | training loss: 2.3097e-03 | validation loss: 2.0989e-03\n",
      "Epoch: 98450 | training loss: 2.3097e-03 | validation loss: 2.1003e-03\n",
      "Epoch: 98460 | training loss: 2.3127e-03 | validation loss: 2.1081e-03\n",
      "Epoch: 98470 | training loss: 2.5022e-03 | validation loss: 2.2521e-03\n",
      "Epoch: 98480 | training loss: 2.5586e-03 | validation loss: 2.2762e-03\n",
      "Epoch: 98490 | training loss: 2.5030e-03 | validation loss: 2.2532e-03\n",
      "Epoch: 98500 | training loss: 2.3436e-03 | validation loss: 2.1049e-03\n",
      "Epoch: 98510 | training loss: 2.3430e-03 | validation loss: 2.0922e-03\n",
      "Epoch: 98520 | training loss: 2.3122e-03 | validation loss: 2.1016e-03\n",
      "Epoch: 98530 | training loss: 2.3131e-03 | validation loss: 2.1056e-03\n",
      "Epoch: 98540 | training loss: 2.3107e-03 | validation loss: 2.0953e-03\n",
      "Epoch: 98550 | training loss: 2.3092e-03 | validation loss: 2.0986e-03\n",
      "Epoch: 98560 | training loss: 2.3093e-03 | validation loss: 2.0985e-03\n",
      "Epoch: 98570 | training loss: 2.3092e-03 | validation loss: 2.0969e-03\n",
      "Epoch: 98580 | training loss: 2.3091e-03 | validation loss: 2.0984e-03\n",
      "Epoch: 98590 | training loss: 2.3090e-03 | validation loss: 2.0971e-03\n",
      "Epoch: 98600 | training loss: 2.3090e-03 | validation loss: 2.0977e-03\n",
      "Epoch: 98610 | training loss: 2.3089e-03 | validation loss: 2.0975e-03\n",
      "Epoch: 98620 | training loss: 2.3089e-03 | validation loss: 2.0972e-03\n",
      "Epoch: 98630 | training loss: 2.3088e-03 | validation loss: 2.0972e-03\n",
      "Epoch: 98640 | training loss: 2.3088e-03 | validation loss: 2.0970e-03\n",
      "Epoch: 98650 | training loss: 2.3088e-03 | validation loss: 2.0968e-03\n",
      "Epoch: 98660 | training loss: 2.3088e-03 | validation loss: 2.0956e-03\n",
      "Epoch: 98670 | training loss: 2.3177e-03 | validation loss: 2.0895e-03\n",
      "Epoch: 98680 | training loss: 3.2698e-03 | validation loss: 2.4316e-03\n",
      "Epoch: 98690 | training loss: 3.1293e-03 | validation loss: 2.5945e-03\n",
      "Epoch: 98700 | training loss: 2.3721e-03 | validation loss: 2.1406e-03\n",
      "Epoch: 98710 | training loss: 2.3139e-03 | validation loss: 2.0859e-03\n",
      "Epoch: 98720 | training loss: 2.3308e-03 | validation loss: 2.0991e-03\n",
      "Epoch: 98730 | training loss: 2.3212e-03 | validation loss: 2.0961e-03\n",
      "Epoch: 98740 | training loss: 2.3093e-03 | validation loss: 2.0933e-03\n",
      "Epoch: 98750 | training loss: 2.3091e-03 | validation loss: 2.0990e-03\n",
      "Epoch: 98760 | training loss: 2.3088e-03 | validation loss: 2.0976e-03\n",
      "Epoch: 98770 | training loss: 2.3083e-03 | validation loss: 2.0957e-03\n",
      "Epoch: 98780 | training loss: 2.3082e-03 | validation loss: 2.0954e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98790 | training loss: 2.3082e-03 | validation loss: 2.0963e-03\n",
      "Epoch: 98800 | training loss: 2.3081e-03 | validation loss: 2.0956e-03\n",
      "Epoch: 98810 | training loss: 2.3081e-03 | validation loss: 2.0956e-03\n",
      "Epoch: 98820 | training loss: 2.3080e-03 | validation loss: 2.0957e-03\n",
      "Epoch: 98830 | training loss: 2.3080e-03 | validation loss: 2.0955e-03\n",
      "Epoch: 98840 | training loss: 2.3079e-03 | validation loss: 2.0956e-03\n",
      "Epoch: 98850 | training loss: 2.3079e-03 | validation loss: 2.0959e-03\n",
      "Epoch: 98860 | training loss: 2.3084e-03 | validation loss: 2.1001e-03\n",
      "Epoch: 98870 | training loss: 2.3769e-03 | validation loss: 2.1904e-03\n",
      "Epoch: 98880 | training loss: 2.6222e-03 | validation loss: 2.3488e-03\n",
      "Epoch: 98890 | training loss: 2.5127e-03 | validation loss: 2.2281e-03\n",
      "Epoch: 98900 | training loss: 2.3736e-03 | validation loss: 2.1829e-03\n",
      "Epoch: 98910 | training loss: 2.3236e-03 | validation loss: 2.1029e-03\n",
      "Epoch: 98920 | training loss: 2.3111e-03 | validation loss: 2.1064e-03\n",
      "Epoch: 98930 | training loss: 2.3095e-03 | validation loss: 2.0866e-03\n",
      "Epoch: 98940 | training loss: 2.3085e-03 | validation loss: 2.0930e-03\n",
      "Epoch: 98950 | training loss: 2.3078e-03 | validation loss: 2.0950e-03\n",
      "Epoch: 98960 | training loss: 2.3074e-03 | validation loss: 2.0944e-03\n",
      "Epoch: 98970 | training loss: 2.3074e-03 | validation loss: 2.0933e-03\n",
      "Epoch: 98980 | training loss: 2.3080e-03 | validation loss: 2.0899e-03\n",
      "Epoch: 98990 | training loss: 2.3261e-03 | validation loss: 2.0812e-03\n",
      "Epoch: 99000 | training loss: 3.1038e-03 | validation loss: 2.3281e-03\n",
      "Epoch: 99010 | training loss: 2.6725e-03 | validation loss: 2.3509e-03\n",
      "Epoch: 99020 | training loss: 2.4222e-03 | validation loss: 2.1099e-03\n",
      "Epoch: 99030 | training loss: 2.3161e-03 | validation loss: 2.0874e-03\n",
      "Epoch: 99040 | training loss: 2.3301e-03 | validation loss: 2.1236e-03\n",
      "Epoch: 99050 | training loss: 2.3096e-03 | validation loss: 2.0860e-03\n",
      "Epoch: 99060 | training loss: 2.3071e-03 | validation loss: 2.0911e-03\n",
      "Epoch: 99070 | training loss: 2.3075e-03 | validation loss: 2.0968e-03\n",
      "Epoch: 99080 | training loss: 2.3072e-03 | validation loss: 2.0909e-03\n",
      "Epoch: 99090 | training loss: 2.3069e-03 | validation loss: 2.0941e-03\n",
      "Epoch: 99100 | training loss: 2.3068e-03 | validation loss: 2.0923e-03\n",
      "Epoch: 99110 | training loss: 2.3067e-03 | validation loss: 2.0925e-03\n",
      "Epoch: 99120 | training loss: 2.3067e-03 | validation loss: 2.0931e-03\n",
      "Epoch: 99130 | training loss: 2.3067e-03 | validation loss: 2.0926e-03\n",
      "Epoch: 99140 | training loss: 2.3066e-03 | validation loss: 2.0923e-03\n",
      "Epoch: 99150 | training loss: 2.3066e-03 | validation loss: 2.0919e-03\n",
      "Epoch: 99160 | training loss: 2.3067e-03 | validation loss: 2.0906e-03\n",
      "Epoch: 99170 | training loss: 2.3146e-03 | validation loss: 2.0838e-03\n",
      "Epoch: 99180 | training loss: 3.0020e-03 | validation loss: 2.3060e-03\n",
      "Epoch: 99190 | training loss: 2.8244e-03 | validation loss: 2.4348e-03\n",
      "Epoch: 99200 | training loss: 2.3363e-03 | validation loss: 2.0898e-03\n",
      "Epoch: 99210 | training loss: 2.3981e-03 | validation loss: 2.0904e-03\n",
      "Epoch: 99220 | training loss: 2.3232e-03 | validation loss: 2.0769e-03\n",
      "Epoch: 99230 | training loss: 2.3085e-03 | validation loss: 2.1000e-03\n",
      "Epoch: 99240 | training loss: 2.3106e-03 | validation loss: 2.1045e-03\n",
      "Epoch: 99250 | training loss: 2.3063e-03 | validation loss: 2.0895e-03\n",
      "Epoch: 99260 | training loss: 2.3066e-03 | validation loss: 2.0886e-03\n",
      "Epoch: 99270 | training loss: 2.3062e-03 | validation loss: 2.0931e-03\n",
      "Epoch: 99280 | training loss: 2.3060e-03 | validation loss: 2.0914e-03\n",
      "Epoch: 99290 | training loss: 2.3060e-03 | validation loss: 2.0906e-03\n",
      "Epoch: 99300 | training loss: 2.3059e-03 | validation loss: 2.0916e-03\n",
      "Epoch: 99310 | training loss: 2.3059e-03 | validation loss: 2.0908e-03\n",
      "Epoch: 99320 | training loss: 2.3058e-03 | validation loss: 2.0911e-03\n",
      "Epoch: 99330 | training loss: 2.3058e-03 | validation loss: 2.0908e-03\n",
      "Epoch: 99340 | training loss: 2.3057e-03 | validation loss: 2.0906e-03\n",
      "Epoch: 99350 | training loss: 2.3057e-03 | validation loss: 2.0903e-03\n",
      "Epoch: 99360 | training loss: 2.3064e-03 | validation loss: 2.0877e-03\n",
      "Epoch: 99370 | training loss: 2.3950e-03 | validation loss: 2.1163e-03\n",
      "Epoch: 99380 | training loss: 2.3206e-03 | validation loss: 2.0844e-03\n",
      "Epoch: 99390 | training loss: 2.4088e-03 | validation loss: 2.1145e-03\n",
      "Epoch: 99400 | training loss: 2.3429e-03 | validation loss: 2.0881e-03\n",
      "Epoch: 99410 | training loss: 2.3097e-03 | validation loss: 2.0791e-03\n",
      "Epoch: 99420 | training loss: 2.3333e-03 | validation loss: 2.0810e-03\n",
      "Epoch: 99430 | training loss: 2.7564e-03 | validation loss: 2.1958e-03\n",
      "Epoch: 99440 | training loss: 2.3266e-03 | validation loss: 2.1253e-03\n",
      "Epoch: 99450 | training loss: 2.3289e-03 | validation loss: 2.0782e-03\n",
      "Epoch: 99460 | training loss: 2.3222e-03 | validation loss: 2.1159e-03\n",
      "Epoch: 99470 | training loss: 2.3120e-03 | validation loss: 2.0791e-03\n",
      "Epoch: 99480 | training loss: 2.3063e-03 | validation loss: 2.0943e-03\n",
      "Epoch: 99490 | training loss: 2.3052e-03 | validation loss: 2.0905e-03\n",
      "Epoch: 99500 | training loss: 2.3060e-03 | validation loss: 2.0851e-03\n",
      "Epoch: 99510 | training loss: 2.3051e-03 | validation loss: 2.0903e-03\n",
      "Epoch: 99520 | training loss: 2.3054e-03 | validation loss: 2.0920e-03\n",
      "Epoch: 99530 | training loss: 2.3059e-03 | validation loss: 2.0938e-03\n",
      "Epoch: 99540 | training loss: 2.3145e-03 | validation loss: 2.1076e-03\n",
      "Epoch: 99550 | training loss: 2.5524e-03 | validation loss: 2.2770e-03\n",
      "Epoch: 99560 | training loss: 2.4423e-03 | validation loss: 2.1990e-03\n",
      "Epoch: 99570 | training loss: 2.3063e-03 | validation loss: 2.0957e-03\n",
      "Epoch: 99580 | training loss: 2.3339e-03 | validation loss: 2.0808e-03\n",
      "Epoch: 99590 | training loss: 2.3284e-03 | validation loss: 2.1173e-03\n",
      "Epoch: 99600 | training loss: 2.3142e-03 | validation loss: 2.0795e-03\n",
      "Epoch: 99610 | training loss: 2.3082e-03 | validation loss: 2.0987e-03\n",
      "Epoch: 99620 | training loss: 2.3060e-03 | validation loss: 2.0831e-03\n",
      "Epoch: 99630 | training loss: 2.3048e-03 | validation loss: 2.0909e-03\n",
      "Epoch: 99640 | training loss: 2.3044e-03 | validation loss: 2.0881e-03\n",
      "Epoch: 99650 | training loss: 2.3045e-03 | validation loss: 2.0864e-03\n",
      "Epoch: 99660 | training loss: 2.3044e-03 | validation loss: 2.0869e-03\n",
      "Epoch: 99670 | training loss: 2.3043e-03 | validation loss: 2.0870e-03\n",
      "Epoch: 99680 | training loss: 2.3045e-03 | validation loss: 2.0856e-03\n",
      "Epoch: 99690 | training loss: 2.3117e-03 | validation loss: 2.0795e-03\n",
      "Epoch: 99700 | training loss: 2.7598e-03 | validation loss: 2.2122e-03\n",
      "Epoch: 99710 | training loss: 2.3894e-03 | validation loss: 2.1695e-03\n",
      "Epoch: 99720 | training loss: 2.5494e-03 | validation loss: 2.1359e-03\n",
      "Epoch: 99730 | training loss: 2.3171e-03 | validation loss: 2.0739e-03\n",
      "Epoch: 99740 | training loss: 2.3309e-03 | validation loss: 2.1225e-03\n",
      "Epoch: 99750 | training loss: 2.3057e-03 | validation loss: 2.0941e-03\n",
      "Epoch: 99760 | training loss: 2.3083e-03 | validation loss: 2.0791e-03\n",
      "Epoch: 99770 | training loss: 2.3041e-03 | validation loss: 2.0894e-03\n",
      "Epoch: 99780 | training loss: 2.3039e-03 | validation loss: 2.0882e-03\n",
      "Epoch: 99790 | training loss: 2.3039e-03 | validation loss: 2.0850e-03\n",
      "Epoch: 99800 | training loss: 2.3038e-03 | validation loss: 2.0880e-03\n",
      "Epoch: 99810 | training loss: 2.3037e-03 | validation loss: 2.0857e-03\n",
      "Epoch: 99820 | training loss: 2.3036e-03 | validation loss: 2.0868e-03\n",
      "Epoch: 99830 | training loss: 2.3036e-03 | validation loss: 2.0862e-03\n",
      "Epoch: 99840 | training loss: 2.3035e-03 | validation loss: 2.0860e-03\n",
      "Epoch: 99850 | training loss: 2.3035e-03 | validation loss: 2.0857e-03\n",
      "Epoch: 99860 | training loss: 2.3048e-03 | validation loss: 2.0821e-03\n",
      "Epoch: 99870 | training loss: 2.4682e-03 | validation loss: 2.1474e-03\n",
      "Epoch: 99880 | training loss: 2.4804e-03 | validation loss: 2.1988e-03\n",
      "Epoch: 99890 | training loss: 2.3208e-03 | validation loss: 2.0726e-03\n",
      "Epoch: 99900 | training loss: 2.3339e-03 | validation loss: 2.0959e-03\n",
      "Epoch: 99910 | training loss: 2.3253e-03 | validation loss: 2.1009e-03\n",
      "Epoch: 99920 | training loss: 2.3437e-03 | validation loss: 2.1319e-03\n",
      "Epoch: 99930 | training loss: 2.5866e-03 | validation loss: 2.3029e-03\n",
      "Epoch: 99940 | training loss: 2.3183e-03 | validation loss: 2.1130e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99950 | training loss: 2.3518e-03 | validation loss: 2.0748e-03\n",
      "Epoch: 99960 | training loss: 2.3303e-03 | validation loss: 2.1224e-03\n",
      "Epoch: 99970 | training loss: 2.3033e-03 | validation loss: 2.0815e-03\n",
      "Epoch: 99980 | training loss: 2.3096e-03 | validation loss: 2.0751e-03\n",
      "Epoch: 99990 | training loss: 2.3050e-03 | validation loss: 2.0787e-03\n",
      "Epoch: 100000 | training loss: 2.3054e-03 | validation loss: 2.0781e-03\n",
      "Epoch: 100010 | training loss: 2.3285e-03 | validation loss: 2.0720e-03\n",
      "Epoch: 100020 | training loss: 2.8124e-03 | validation loss: 2.2138e-03\n",
      "Epoch: 100030 | training loss: 2.3467e-03 | validation loss: 2.1380e-03\n",
      "Epoch: 100040 | training loss: 2.3589e-03 | validation loss: 2.0790e-03\n",
      "Epoch: 100050 | training loss: 2.3407e-03 | validation loss: 2.1268e-03\n",
      "Epoch: 100060 | training loss: 2.3193e-03 | validation loss: 2.0730e-03\n",
      "Epoch: 100070 | training loss: 2.3089e-03 | validation loss: 2.0983e-03\n",
      "Epoch: 100080 | training loss: 2.3042e-03 | validation loss: 2.0791e-03\n",
      "Epoch: 100090 | training loss: 2.3025e-03 | validation loss: 2.0844e-03\n",
      "Epoch: 100100 | training loss: 2.3028e-03 | validation loss: 2.0866e-03\n",
      "Epoch: 100110 | training loss: 2.3024e-03 | validation loss: 2.0827e-03\n",
      "Epoch: 100120 | training loss: 2.3026e-03 | validation loss: 2.0816e-03\n",
      "Epoch: 100130 | training loss: 2.3033e-03 | validation loss: 2.0796e-03\n",
      "Epoch: 100140 | training loss: 2.3178e-03 | validation loss: 2.0737e-03\n",
      "Epoch: 100150 | training loss: 2.7847e-03 | validation loss: 2.2149e-03\n",
      "Epoch: 100160 | training loss: 2.3384e-03 | validation loss: 2.1300e-03\n",
      "Epoch: 100170 | training loss: 2.4365e-03 | validation loss: 2.0950e-03\n",
      "Epoch: 100180 | training loss: 2.3551e-03 | validation loss: 2.1352e-03\n",
      "Epoch: 100190 | training loss: 2.3056e-03 | validation loss: 2.0811e-03\n",
      "Epoch: 100200 | training loss: 2.3027e-03 | validation loss: 2.0780e-03\n",
      "Epoch: 100210 | training loss: 2.3029e-03 | validation loss: 2.0888e-03\n",
      "Epoch: 100220 | training loss: 2.3023e-03 | validation loss: 2.0793e-03\n",
      "Epoch: 100230 | training loss: 2.3019e-03 | validation loss: 2.0846e-03\n",
      "Epoch: 100240 | training loss: 2.3019e-03 | validation loss: 2.0825e-03\n",
      "Epoch: 100250 | training loss: 2.3018e-03 | validation loss: 2.0815e-03\n",
      "Epoch: 100260 | training loss: 2.3017e-03 | validation loss: 2.0830e-03\n",
      "Epoch: 100270 | training loss: 2.3017e-03 | validation loss: 2.0832e-03\n",
      "Epoch: 100280 | training loss: 2.3017e-03 | validation loss: 2.0835e-03\n",
      "Epoch: 100290 | training loss: 2.3021e-03 | validation loss: 2.0863e-03\n",
      "Epoch: 100300 | training loss: 2.3200e-03 | validation loss: 2.1125e-03\n",
      "Epoch: 100310 | training loss: 3.2179e-03 | validation loss: 2.6539e-03\n",
      "Epoch: 100320 | training loss: 2.8248e-03 | validation loss: 2.2098e-03\n",
      "Epoch: 100330 | training loss: 2.3121e-03 | validation loss: 2.0920e-03\n",
      "Epoch: 100340 | training loss: 2.3751e-03 | validation loss: 2.1768e-03\n",
      "Epoch: 100350 | training loss: 2.3068e-03 | validation loss: 2.0958e-03\n",
      "Epoch: 100360 | training loss: 2.3083e-03 | validation loss: 2.0694e-03\n",
      "Epoch: 100370 | training loss: 2.3021e-03 | validation loss: 2.0761e-03\n",
      "Epoch: 100380 | training loss: 2.3024e-03 | validation loss: 2.0889e-03\n",
      "Epoch: 100390 | training loss: 2.3012e-03 | validation loss: 2.0799e-03\n",
      "Epoch: 100400 | training loss: 2.3012e-03 | validation loss: 2.0799e-03\n",
      "Epoch: 100410 | training loss: 2.3011e-03 | validation loss: 2.0825e-03\n",
      "Epoch: 100420 | training loss: 2.3011e-03 | validation loss: 2.0800e-03\n",
      "Epoch: 100430 | training loss: 2.3010e-03 | validation loss: 2.0814e-03\n",
      "Epoch: 100440 | training loss: 2.3010e-03 | validation loss: 2.0802e-03\n",
      "Epoch: 100450 | training loss: 2.3019e-03 | validation loss: 2.0802e-03\n",
      "Epoch: 100460 | training loss: 2.3671e-03 | validation loss: 2.1137e-03\n",
      "Epoch: 100470 | training loss: 2.5615e-03 | validation loss: 2.2582e-03\n",
      "Epoch: 100480 | training loss: 2.4256e-03 | validation loss: 2.0951e-03\n",
      "Epoch: 100490 | training loss: 2.3033e-03 | validation loss: 2.0889e-03\n",
      "Epoch: 100500 | training loss: 2.3146e-03 | validation loss: 2.1080e-03\n",
      "Epoch: 100510 | training loss: 2.3061e-03 | validation loss: 2.0769e-03\n",
      "Epoch: 100520 | training loss: 2.3020e-03 | validation loss: 2.0744e-03\n",
      "Epoch: 100530 | training loss: 2.3010e-03 | validation loss: 2.0814e-03\n",
      "Epoch: 100540 | training loss: 2.3031e-03 | validation loss: 2.0901e-03\n",
      "Epoch: 100550 | training loss: 2.3365e-03 | validation loss: 2.1250e-03\n",
      "Epoch: 100560 | training loss: 3.0029e-03 | validation loss: 2.5314e-03\n",
      "Epoch: 100570 | training loss: 2.4992e-03 | validation loss: 2.1069e-03\n",
      "Epoch: 100580 | training loss: 2.4143e-03 | validation loss: 2.1749e-03\n",
      "Epoch: 100590 | training loss: 2.3380e-03 | validation loss: 2.0716e-03\n",
      "Epoch: 100600 | training loss: 2.3100e-03 | validation loss: 2.0982e-03\n",
      "Epoch: 100610 | training loss: 2.3035e-03 | validation loss: 2.0720e-03\n",
      "Epoch: 100620 | training loss: 2.3020e-03 | validation loss: 2.0856e-03\n",
      "Epoch: 100630 | training loss: 2.3011e-03 | validation loss: 2.0755e-03\n",
      "Epoch: 100640 | training loss: 2.3002e-03 | validation loss: 2.0800e-03\n",
      "Epoch: 100650 | training loss: 2.3002e-03 | validation loss: 2.0807e-03\n",
      "Epoch: 100660 | training loss: 2.3000e-03 | validation loss: 2.0787e-03\n",
      "Epoch: 100670 | training loss: 2.3000e-03 | validation loss: 2.0781e-03\n",
      "Epoch: 100680 | training loss: 2.3002e-03 | validation loss: 2.0764e-03\n",
      "Epoch: 100690 | training loss: 2.3085e-03 | validation loss: 2.0699e-03\n",
      "Epoch: 100700 | training loss: 2.8092e-03 | validation loss: 2.2205e-03\n",
      "Epoch: 100710 | training loss: 2.4422e-03 | validation loss: 2.2017e-03\n",
      "Epoch: 100720 | training loss: 2.5180e-03 | validation loss: 2.1185e-03\n",
      "Epoch: 100730 | training loss: 2.3179e-03 | validation loss: 2.0616e-03\n",
      "Epoch: 100740 | training loss: 2.3237e-03 | validation loss: 2.1119e-03\n",
      "Epoch: 100750 | training loss: 2.3015e-03 | validation loss: 2.0872e-03\n",
      "Epoch: 100760 | training loss: 2.3038e-03 | validation loss: 2.0687e-03\n",
      "Epoch: 100770 | training loss: 2.2999e-03 | validation loss: 2.0815e-03\n",
      "Epoch: 100780 | training loss: 2.2995e-03 | validation loss: 2.0786e-03\n",
      "Epoch: 100790 | training loss: 2.2996e-03 | validation loss: 2.0763e-03\n",
      "Epoch: 100800 | training loss: 2.2995e-03 | validation loss: 2.0788e-03\n",
      "Epoch: 100810 | training loss: 2.2994e-03 | validation loss: 2.0767e-03\n",
      "Epoch: 100820 | training loss: 2.2993e-03 | validation loss: 2.0779e-03\n",
      "Epoch: 100830 | training loss: 2.2993e-03 | validation loss: 2.0775e-03\n",
      "Epoch: 100840 | training loss: 2.2992e-03 | validation loss: 2.0773e-03\n",
      "Epoch: 100850 | training loss: 2.2992e-03 | validation loss: 2.0780e-03\n",
      "Epoch: 100860 | training loss: 2.3010e-03 | validation loss: 2.0838e-03\n",
      "Epoch: 100870 | training loss: 2.4583e-03 | validation loss: 2.2373e-03\n",
      "Epoch: 100880 | training loss: 2.3757e-03 | validation loss: 2.1125e-03\n",
      "Epoch: 100890 | training loss: 2.3611e-03 | validation loss: 2.1576e-03\n",
      "Epoch: 100900 | training loss: 2.3595e-03 | validation loss: 2.1576e-03\n",
      "Epoch: 100910 | training loss: 2.5124e-03 | validation loss: 2.2470e-03\n",
      "Epoch: 100920 | training loss: 2.3623e-03 | validation loss: 2.1351e-03\n",
      "Epoch: 100930 | training loss: 2.3660e-03 | validation loss: 2.0638e-03\n",
      "Epoch: 100940 | training loss: 2.3036e-03 | validation loss: 2.0897e-03\n",
      "Epoch: 100950 | training loss: 2.3102e-03 | validation loss: 2.0995e-03\n",
      "Epoch: 100960 | training loss: 2.2988e-03 | validation loss: 2.0771e-03\n",
      "Epoch: 100970 | training loss: 2.3002e-03 | validation loss: 2.0699e-03\n",
      "Epoch: 100980 | training loss: 2.3145e-03 | validation loss: 2.0641e-03\n",
      "Epoch: 100990 | training loss: 2.6065e-03 | validation loss: 2.1344e-03\n",
      "Epoch: 101000 | training loss: 2.3361e-03 | validation loss: 2.0716e-03\n",
      "Epoch: 101010 | training loss: 2.2992e-03 | validation loss: 2.0761e-03\n",
      "Epoch: 101020 | training loss: 2.3018e-03 | validation loss: 2.0828e-03\n",
      "Epoch: 101030 | training loss: 2.3013e-03 | validation loss: 2.0700e-03\n",
      "Epoch: 101040 | training loss: 2.2989e-03 | validation loss: 2.0791e-03\n",
      "Epoch: 101050 | training loss: 2.2984e-03 | validation loss: 2.0762e-03\n",
      "Epoch: 101060 | training loss: 2.2992e-03 | validation loss: 2.0716e-03\n",
      "Epoch: 101070 | training loss: 2.2986e-03 | validation loss: 2.0777e-03\n",
      "Epoch: 101080 | training loss: 2.2985e-03 | validation loss: 2.0774e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 101090 | training loss: 2.2982e-03 | validation loss: 2.0764e-03\n",
      "Epoch: 101100 | training loss: 2.2985e-03 | validation loss: 2.0777e-03\n",
      "Epoch: 101110 | training loss: 2.3059e-03 | validation loss: 2.0905e-03\n",
      "Epoch: 101120 | training loss: 2.6420e-03 | validation loss: 2.3153e-03\n",
      "Epoch: 101130 | training loss: 2.3023e-03 | validation loss: 2.0769e-03\n",
      "Epoch: 101140 | training loss: 2.4752e-03 | validation loss: 2.2166e-03\n",
      "Epoch: 101150 | training loss: 2.3391e-03 | validation loss: 2.0710e-03\n",
      "Epoch: 101160 | training loss: 2.3055e-03 | validation loss: 2.0615e-03\n",
      "Epoch: 101170 | training loss: 2.3084e-03 | validation loss: 2.0953e-03\n",
      "Epoch: 101180 | training loss: 2.2994e-03 | validation loss: 2.0700e-03\n",
      "Epoch: 101190 | training loss: 2.2978e-03 | validation loss: 2.0735e-03\n",
      "Epoch: 101200 | training loss: 2.2977e-03 | validation loss: 2.0759e-03\n",
      "Epoch: 101210 | training loss: 2.2976e-03 | validation loss: 2.0730e-03\n",
      "Epoch: 101220 | training loss: 2.2976e-03 | validation loss: 2.0739e-03\n",
      "Epoch: 101230 | training loss: 2.2975e-03 | validation loss: 2.0745e-03\n",
      "Epoch: 101240 | training loss: 2.2975e-03 | validation loss: 2.0731e-03\n",
      "Epoch: 101250 | training loss: 2.2974e-03 | validation loss: 2.0736e-03\n",
      "Epoch: 101260 | training loss: 2.2974e-03 | validation loss: 2.0737e-03\n",
      "Epoch: 101270 | training loss: 2.2974e-03 | validation loss: 2.0736e-03\n",
      "Epoch: 101280 | training loss: 2.2987e-03 | validation loss: 2.0726e-03\n",
      "Epoch: 101290 | training loss: 2.4294e-03 | validation loss: 2.1370e-03\n",
      "Epoch: 101300 | training loss: 2.4229e-03 | validation loss: 2.2148e-03\n",
      "Epoch: 101310 | training loss: 2.6998e-03 | validation loss: 2.3376e-03\n",
      "Epoch: 101320 | training loss: 2.4209e-03 | validation loss: 2.0734e-03\n",
      "Epoch: 101330 | training loss: 2.3289e-03 | validation loss: 2.0962e-03\n",
      "Epoch: 101340 | training loss: 2.3022e-03 | validation loss: 2.0683e-03\n",
      "Epoch: 101350 | training loss: 2.3026e-03 | validation loss: 2.0614e-03\n",
      "Epoch: 101360 | training loss: 2.2985e-03 | validation loss: 2.0797e-03\n",
      "Epoch: 101370 | training loss: 2.2994e-03 | validation loss: 2.0825e-03\n",
      "Epoch: 101380 | training loss: 2.2995e-03 | validation loss: 2.0822e-03\n",
      "Epoch: 101390 | training loss: 2.3134e-03 | validation loss: 2.0999e-03\n",
      "Epoch: 101400 | training loss: 2.5838e-03 | validation loss: 2.2897e-03\n",
      "Epoch: 101410 | training loss: 2.3440e-03 | validation loss: 2.1224e-03\n",
      "Epoch: 101420 | training loss: 2.3027e-03 | validation loss: 2.0632e-03\n",
      "Epoch: 101430 | training loss: 2.2968e-03 | validation loss: 2.0736e-03\n",
      "Epoch: 101440 | training loss: 2.2967e-03 | validation loss: 2.0718e-03\n",
      "Epoch: 101450 | training loss: 2.2972e-03 | validation loss: 2.0752e-03\n",
      "Epoch: 101460 | training loss: 2.2984e-03 | validation loss: 2.0660e-03\n",
      "Epoch: 101470 | training loss: 2.2977e-03 | validation loss: 2.0771e-03\n",
      "Epoch: 101480 | training loss: 2.2965e-03 | validation loss: 2.0727e-03\n",
      "Epoch: 101490 | training loss: 2.2966e-03 | validation loss: 2.0695e-03\n",
      "Epoch: 101500 | training loss: 2.2971e-03 | validation loss: 2.0677e-03\n",
      "Epoch: 101510 | training loss: 2.3042e-03 | validation loss: 2.0620e-03\n",
      "Epoch: 101520 | training loss: 2.5068e-03 | validation loss: 2.1031e-03\n",
      "Epoch: 101530 | training loss: 2.5436e-03 | validation loss: 2.1260e-03\n",
      "Epoch: 101540 | training loss: 2.3014e-03 | validation loss: 2.0713e-03\n",
      "Epoch: 101550 | training loss: 2.3247e-03 | validation loss: 2.1074e-03\n",
      "Epoch: 101560 | training loss: 2.3224e-03 | validation loss: 2.0652e-03\n",
      "Epoch: 101570 | training loss: 2.3062e-03 | validation loss: 2.0861e-03\n",
      "Epoch: 101580 | training loss: 2.2994e-03 | validation loss: 2.0657e-03\n",
      "Epoch: 101590 | training loss: 2.2974e-03 | validation loss: 2.0760e-03\n",
      "Epoch: 101600 | training loss: 2.2965e-03 | validation loss: 2.0676e-03\n",
      "Epoch: 101610 | training loss: 2.2960e-03 | validation loss: 2.0717e-03\n",
      "Epoch: 101620 | training loss: 2.2959e-03 | validation loss: 2.0714e-03\n",
      "Epoch: 101630 | training loss: 2.2958e-03 | validation loss: 2.0696e-03\n",
      "Epoch: 101640 | training loss: 2.2958e-03 | validation loss: 2.0692e-03\n",
      "Epoch: 101650 | training loss: 2.2962e-03 | validation loss: 2.0676e-03\n",
      "Epoch: 101660 | training loss: 2.3055e-03 | validation loss: 2.0621e-03\n",
      "Epoch: 101670 | training loss: 2.7213e-03 | validation loss: 2.1878e-03\n",
      "Epoch: 101680 | training loss: 2.3157e-03 | validation loss: 2.0956e-03\n",
      "Epoch: 101690 | training loss: 2.5016e-03 | validation loss: 2.1105e-03\n",
      "Epoch: 101700 | training loss: 2.3164e-03 | validation loss: 2.0973e-03\n",
      "Epoch: 101710 | training loss: 2.3072e-03 | validation loss: 2.0869e-03\n",
      "Epoch: 101720 | training loss: 2.3058e-03 | validation loss: 2.0629e-03\n",
      "Epoch: 101730 | training loss: 2.2966e-03 | validation loss: 2.0739e-03\n",
      "Epoch: 101740 | training loss: 2.2953e-03 | validation loss: 2.0694e-03\n",
      "Epoch: 101750 | training loss: 2.2953e-03 | validation loss: 2.0687e-03\n",
      "Epoch: 101760 | training loss: 2.2953e-03 | validation loss: 2.0697e-03\n",
      "Epoch: 101770 | training loss: 2.2952e-03 | validation loss: 2.0691e-03\n",
      "Epoch: 101780 | training loss: 2.2952e-03 | validation loss: 2.0684e-03\n",
      "Epoch: 101790 | training loss: 2.2951e-03 | validation loss: 2.0690e-03\n",
      "Epoch: 101800 | training loss: 2.2954e-03 | validation loss: 2.0665e-03\n",
      "Epoch: 101810 | training loss: 2.3211e-03 | validation loss: 2.0615e-03\n",
      "Epoch: 101820 | training loss: 2.7797e-03 | validation loss: 2.2680e-03\n",
      "Epoch: 101830 | training loss: 2.3208e-03 | validation loss: 2.0617e-03\n",
      "Epoch: 101840 | training loss: 2.3286e-03 | validation loss: 2.1230e-03\n",
      "Epoch: 101850 | training loss: 2.3146e-03 | validation loss: 2.0918e-03\n",
      "Epoch: 101860 | training loss: 2.3021e-03 | validation loss: 2.0655e-03\n",
      "Epoch: 101870 | training loss: 2.3016e-03 | validation loss: 2.0562e-03\n",
      "Epoch: 101880 | training loss: 2.3252e-03 | validation loss: 2.0523e-03\n",
      "Epoch: 101890 | training loss: 2.6168e-03 | validation loss: 2.1296e-03\n",
      "Epoch: 101900 | training loss: 2.3074e-03 | validation loss: 2.0613e-03\n",
      "Epoch: 101910 | training loss: 2.3161e-03 | validation loss: 2.0997e-03\n",
      "Epoch: 101920 | training loss: 2.3147e-03 | validation loss: 2.0550e-03\n",
      "Epoch: 101930 | training loss: 2.3089e-03 | validation loss: 2.0908e-03\n",
      "Epoch: 101940 | training loss: 2.2974e-03 | validation loss: 2.0612e-03\n",
      "Epoch: 101950 | training loss: 2.2956e-03 | validation loss: 2.0630e-03\n",
      "Epoch: 101960 | training loss: 2.2947e-03 | validation loss: 2.0699e-03\n",
      "Epoch: 101970 | training loss: 2.2963e-03 | validation loss: 2.0744e-03\n",
      "Epoch: 101980 | training loss: 2.3095e-03 | validation loss: 2.0920e-03\n",
      "Epoch: 101990 | training loss: 2.5788e-03 | validation loss: 2.2762e-03\n",
      "Epoch: 102000 | training loss: 2.3690e-03 | validation loss: 2.1347e-03\n",
      "Epoch: 102010 | training loss: 2.3003e-03 | validation loss: 2.0640e-03\n",
      "Epoch: 102020 | training loss: 2.2950e-03 | validation loss: 2.0628e-03\n",
      "Epoch: 102030 | training loss: 2.2958e-03 | validation loss: 2.0709e-03\n",
      "Epoch: 102040 | training loss: 2.2943e-03 | validation loss: 2.0658e-03\n",
      "Epoch: 102050 | training loss: 2.2942e-03 | validation loss: 2.0643e-03\n",
      "Epoch: 102060 | training loss: 2.2950e-03 | validation loss: 2.0713e-03\n",
      "Epoch: 102070 | training loss: 2.2942e-03 | validation loss: 2.0641e-03\n",
      "Epoch: 102080 | training loss: 2.2942e-03 | validation loss: 2.0643e-03\n",
      "Epoch: 102090 | training loss: 2.2939e-03 | validation loss: 2.0649e-03\n",
      "Epoch: 102100 | training loss: 2.2941e-03 | validation loss: 2.0641e-03\n",
      "Epoch: 102110 | training loss: 2.2990e-03 | validation loss: 2.0590e-03\n",
      "Epoch: 102120 | training loss: 2.5285e-03 | validation loss: 2.1128e-03\n",
      "Epoch: 102130 | training loss: 2.4427e-03 | validation loss: 2.0886e-03\n",
      "Epoch: 102140 | training loss: 2.3990e-03 | validation loss: 2.0695e-03\n",
      "Epoch: 102150 | training loss: 2.3698e-03 | validation loss: 2.1371e-03\n",
      "Epoch: 102160 | training loss: 2.2956e-03 | validation loss: 2.0735e-03\n",
      "Epoch: 102170 | training loss: 2.3060e-03 | validation loss: 2.0552e-03\n",
      "Epoch: 102180 | training loss: 2.2959e-03 | validation loss: 2.0738e-03\n",
      "Epoch: 102190 | training loss: 2.2934e-03 | validation loss: 2.0647e-03\n",
      "Epoch: 102200 | training loss: 2.2935e-03 | validation loss: 2.0643e-03\n",
      "Epoch: 102210 | training loss: 2.2934e-03 | validation loss: 2.0664e-03\n",
      "Epoch: 102220 | training loss: 2.2933e-03 | validation loss: 2.0646e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102230 | training loss: 2.2932e-03 | validation loss: 2.0650e-03\n",
      "Epoch: 102240 | training loss: 2.2932e-03 | validation loss: 2.0656e-03\n",
      "Epoch: 102250 | training loss: 2.2932e-03 | validation loss: 2.0648e-03\n",
      "Epoch: 102260 | training loss: 2.2931e-03 | validation loss: 2.0653e-03\n",
      "Epoch: 102270 | training loss: 2.2946e-03 | validation loss: 2.0711e-03\n",
      "Epoch: 102280 | training loss: 2.4772e-03 | validation loss: 2.2545e-03\n",
      "Epoch: 102290 | training loss: 2.5132e-03 | validation loss: 2.1991e-03\n",
      "Epoch: 102300 | training loss: 2.3228e-03 | validation loss: 2.1120e-03\n",
      "Epoch: 102310 | training loss: 2.3190e-03 | validation loss: 2.0908e-03\n",
      "Epoch: 102320 | training loss: 2.3179e-03 | validation loss: 2.0719e-03\n",
      "Epoch: 102330 | training loss: 2.3345e-03 | validation loss: 2.0555e-03\n",
      "Epoch: 102340 | training loss: 2.5141e-03 | validation loss: 2.0916e-03\n",
      "Epoch: 102350 | training loss: 2.3451e-03 | validation loss: 2.0507e-03\n",
      "Epoch: 102360 | training loss: 2.3579e-03 | validation loss: 2.1309e-03\n",
      "Epoch: 102370 | training loss: 2.2968e-03 | validation loss: 2.0555e-03\n",
      "Epoch: 102380 | training loss: 2.3047e-03 | validation loss: 2.0529e-03\n",
      "Epoch: 102390 | training loss: 2.2930e-03 | validation loss: 2.0608e-03\n",
      "Epoch: 102400 | training loss: 2.2929e-03 | validation loss: 2.0664e-03\n",
      "Epoch: 102410 | training loss: 2.2982e-03 | validation loss: 2.0773e-03\n",
      "Epoch: 102420 | training loss: 2.4393e-03 | validation loss: 2.1892e-03\n",
      "Epoch: 102430 | training loss: 2.7910e-03 | validation loss: 2.3942e-03\n",
      "Epoch: 102440 | training loss: 2.3459e-03 | validation loss: 2.0623e-03\n",
      "Epoch: 102450 | training loss: 2.2974e-03 | validation loss: 2.0571e-03\n",
      "Epoch: 102460 | training loss: 2.3111e-03 | validation loss: 2.0888e-03\n",
      "Epoch: 102470 | training loss: 2.3032e-03 | validation loss: 2.0527e-03\n",
      "Epoch: 102480 | training loss: 2.2965e-03 | validation loss: 2.0741e-03\n",
      "Epoch: 102490 | training loss: 2.2938e-03 | validation loss: 2.0582e-03\n",
      "Epoch: 102500 | training loss: 2.2926e-03 | validation loss: 2.0656e-03\n",
      "Epoch: 102510 | training loss: 2.2921e-03 | validation loss: 2.0623e-03\n",
      "Epoch: 102520 | training loss: 2.2921e-03 | validation loss: 2.0611e-03\n",
      "Epoch: 102530 | training loss: 2.2920e-03 | validation loss: 2.0625e-03\n",
      "Epoch: 102540 | training loss: 2.2920e-03 | validation loss: 2.0632e-03\n",
      "Epoch: 102550 | training loss: 2.2922e-03 | validation loss: 2.0646e-03\n",
      "Epoch: 102560 | training loss: 2.2975e-03 | validation loss: 2.0750e-03\n",
      "Epoch: 102570 | training loss: 2.5598e-03 | validation loss: 2.2574e-03\n",
      "Epoch: 102580 | training loss: 2.3596e-03 | validation loss: 2.1237e-03\n",
      "Epoch: 102590 | training loss: 2.4492e-03 | validation loss: 2.1915e-03\n",
      "Epoch: 102600 | training loss: 2.3440e-03 | validation loss: 2.0593e-03\n",
      "Epoch: 102610 | training loss: 2.3013e-03 | validation loss: 2.0486e-03\n",
      "Epoch: 102620 | training loss: 2.3032e-03 | validation loss: 2.0827e-03\n",
      "Epoch: 102630 | training loss: 2.2923e-03 | validation loss: 2.0601e-03\n",
      "Epoch: 102640 | training loss: 2.2919e-03 | validation loss: 2.0580e-03\n",
      "Epoch: 102650 | training loss: 2.2919e-03 | validation loss: 2.0656e-03\n",
      "Epoch: 102660 | training loss: 2.2916e-03 | validation loss: 2.0589e-03\n",
      "Epoch: 102670 | training loss: 2.2915e-03 | validation loss: 2.0629e-03\n",
      "Epoch: 102680 | training loss: 2.2914e-03 | validation loss: 2.0606e-03\n",
      "Epoch: 102690 | training loss: 2.2913e-03 | validation loss: 2.0610e-03\n",
      "Epoch: 102700 | training loss: 2.2913e-03 | validation loss: 2.0614e-03\n",
      "Epoch: 102710 | training loss: 2.2912e-03 | validation loss: 2.0610e-03\n",
      "Epoch: 102720 | training loss: 2.2912e-03 | validation loss: 2.0606e-03\n",
      "Epoch: 102730 | training loss: 2.2912e-03 | validation loss: 2.0596e-03\n",
      "Epoch: 102740 | training loss: 2.2948e-03 | validation loss: 2.0546e-03\n",
      "Epoch: 102750 | training loss: 2.6635e-03 | validation loss: 2.2431e-03\n",
      "Epoch: 102760 | training loss: 2.5875e-03 | validation loss: 2.2684e-03\n",
      "Epoch: 102770 | training loss: 2.6145e-03 | validation loss: 2.1553e-03\n",
      "Epoch: 102780 | training loss: 2.2935e-03 | validation loss: 2.0706e-03\n",
      "Epoch: 102790 | training loss: 2.3152e-03 | validation loss: 2.0913e-03\n",
      "Epoch: 102800 | training loss: 2.3103e-03 | validation loss: 2.0438e-03\n",
      "Epoch: 102810 | training loss: 2.2935e-03 | validation loss: 2.0518e-03\n",
      "Epoch: 102820 | training loss: 2.2938e-03 | validation loss: 2.0667e-03\n",
      "Epoch: 102830 | training loss: 2.3006e-03 | validation loss: 2.0794e-03\n",
      "Epoch: 102840 | training loss: 2.3684e-03 | validation loss: 2.1390e-03\n",
      "Epoch: 102850 | training loss: 2.7608e-03 | validation loss: 2.3806e-03\n",
      "Epoch: 102860 | training loss: 2.3817e-03 | validation loss: 2.0582e-03\n",
      "Epoch: 102870 | training loss: 2.3043e-03 | validation loss: 2.0822e-03\n",
      "Epoch: 102880 | training loss: 2.2905e-03 | validation loss: 2.0590e-03\n",
      "Epoch: 102890 | training loss: 2.2963e-03 | validation loss: 2.0507e-03\n",
      "Epoch: 102900 | training loss: 2.2939e-03 | validation loss: 2.0694e-03\n",
      "Epoch: 102910 | training loss: 2.2916e-03 | validation loss: 2.0647e-03\n",
      "Epoch: 102920 | training loss: 2.2903e-03 | validation loss: 2.0596e-03\n",
      "Epoch: 102930 | training loss: 2.2903e-03 | validation loss: 2.0581e-03\n",
      "Epoch: 102940 | training loss: 2.2910e-03 | validation loss: 2.0552e-03\n",
      "Epoch: 102950 | training loss: 2.3246e-03 | validation loss: 2.0481e-03\n",
      "Epoch: 102960 | training loss: 3.6682e-03 | validation loss: 2.5371e-03\n",
      "Epoch: 102970 | training loss: 2.6814e-03 | validation loss: 2.3203e-03\n",
      "Epoch: 102980 | training loss: 2.3346e-03 | validation loss: 2.1111e-03\n",
      "Epoch: 102990 | training loss: 2.3305e-03 | validation loss: 2.0585e-03\n",
      "Epoch: 103000 | training loss: 2.2981e-03 | validation loss: 2.0516e-03\n",
      "Epoch: 103010 | training loss: 2.2964e-03 | validation loss: 2.0697e-03\n",
      "Epoch: 103020 | training loss: 2.2900e-03 | validation loss: 2.0581e-03\n",
      "Epoch: 103030 | training loss: 2.2907e-03 | validation loss: 2.0556e-03\n",
      "Epoch: 103040 | training loss: 2.2902e-03 | validation loss: 2.0610e-03\n",
      "Epoch: 103050 | training loss: 2.2899e-03 | validation loss: 2.0566e-03\n",
      "Epoch: 103060 | training loss: 2.2898e-03 | validation loss: 2.0588e-03\n",
      "Epoch: 103070 | training loss: 2.2897e-03 | validation loss: 2.0574e-03\n",
      "Epoch: 103080 | training loss: 2.2897e-03 | validation loss: 2.0582e-03\n",
      "Epoch: 103090 | training loss: 2.2896e-03 | validation loss: 2.0575e-03\n",
      "Epoch: 103100 | training loss: 2.2896e-03 | validation loss: 2.0576e-03\n",
      "Epoch: 103110 | training loss: 2.2895e-03 | validation loss: 2.0577e-03\n",
      "Epoch: 103120 | training loss: 2.2895e-03 | validation loss: 2.0577e-03\n",
      "Epoch: 103130 | training loss: 2.2895e-03 | validation loss: 2.0579e-03\n",
      "Epoch: 103140 | training loss: 2.2897e-03 | validation loss: 2.0596e-03\n",
      "Epoch: 103150 | training loss: 2.3067e-03 | validation loss: 2.0821e-03\n",
      "Epoch: 103160 | training loss: 3.6145e-03 | validation loss: 2.8289e-03\n",
      "Epoch: 103170 | training loss: 2.9428e-03 | validation loss: 2.2625e-03\n",
      "Epoch: 103180 | training loss: 2.3898e-03 | validation loss: 2.0665e-03\n",
      "Epoch: 103190 | training loss: 2.2942e-03 | validation loss: 2.0674e-03\n",
      "Epoch: 103200 | training loss: 2.3168e-03 | validation loss: 2.0888e-03\n",
      "Epoch: 103210 | training loss: 2.2937e-03 | validation loss: 2.0672e-03\n",
      "Epoch: 103220 | training loss: 2.2905e-03 | validation loss: 2.0528e-03\n",
      "Epoch: 103230 | training loss: 2.2900e-03 | validation loss: 2.0534e-03\n",
      "Epoch: 103240 | training loss: 2.2893e-03 | validation loss: 2.0587e-03\n",
      "Epoch: 103250 | training loss: 2.2890e-03 | validation loss: 2.0571e-03\n",
      "Epoch: 103260 | training loss: 2.2890e-03 | validation loss: 2.0554e-03\n",
      "Epoch: 103270 | training loss: 2.2889e-03 | validation loss: 2.0569e-03\n",
      "Epoch: 103280 | training loss: 2.2888e-03 | validation loss: 2.0560e-03\n",
      "Epoch: 103290 | training loss: 2.2888e-03 | validation loss: 2.0563e-03\n",
      "Epoch: 103300 | training loss: 2.2887e-03 | validation loss: 2.0560e-03\n",
      "Epoch: 103310 | training loss: 2.2887e-03 | validation loss: 2.0561e-03\n",
      "Epoch: 103320 | training loss: 2.2886e-03 | validation loss: 2.0562e-03\n",
      "Epoch: 103330 | training loss: 2.2890e-03 | validation loss: 2.0592e-03\n",
      "Epoch: 103340 | training loss: 2.3672e-03 | validation loss: 2.1559e-03\n",
      "Epoch: 103350 | training loss: 2.4241e-03 | validation loss: 2.1659e-03\n",
      "Epoch: 103360 | training loss: 2.3543e-03 | validation loss: 2.0837e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 103370 | training loss: 2.3135e-03 | validation loss: 2.0978e-03\n",
      "Epoch: 103380 | training loss: 2.2977e-03 | validation loss: 2.0717e-03\n",
      "Epoch: 103390 | training loss: 2.2953e-03 | validation loss: 2.0582e-03\n",
      "Epoch: 103400 | training loss: 2.2907e-03 | validation loss: 2.0619e-03\n",
      "Epoch: 103410 | training loss: 2.2899e-03 | validation loss: 2.0627e-03\n",
      "Epoch: 103420 | training loss: 2.2916e-03 | validation loss: 2.0663e-03\n",
      "Epoch: 103430 | training loss: 2.3335e-03 | validation loss: 2.1087e-03\n",
      "Epoch: 103440 | training loss: 2.9683e-03 | validation loss: 2.4969e-03\n",
      "Epoch: 103450 | training loss: 2.4852e-03 | validation loss: 2.0801e-03\n",
      "Epoch: 103460 | training loss: 2.3836e-03 | validation loss: 2.1435e-03\n",
      "Epoch: 103470 | training loss: 2.3238e-03 | validation loss: 2.0423e-03\n",
      "Epoch: 103480 | training loss: 2.3008e-03 | validation loss: 2.0761e-03\n",
      "Epoch: 103490 | training loss: 2.2932e-03 | validation loss: 2.0458e-03\n",
      "Epoch: 103500 | training loss: 2.2898e-03 | validation loss: 2.0614e-03\n",
      "Epoch: 103510 | training loss: 2.2880e-03 | validation loss: 2.0526e-03\n",
      "Epoch: 103520 | training loss: 2.2881e-03 | validation loss: 2.0515e-03\n",
      "Epoch: 103530 | training loss: 2.2878e-03 | validation loss: 2.0542e-03\n",
      "Epoch: 103540 | training loss: 2.2879e-03 | validation loss: 2.0556e-03\n",
      "Epoch: 103550 | training loss: 2.2889e-03 | validation loss: 2.0592e-03\n",
      "Epoch: 103560 | training loss: 2.3139e-03 | validation loss: 2.0893e-03\n",
      "Epoch: 103570 | training loss: 3.1076e-03 | validation loss: 2.5656e-03\n",
      "Epoch: 103580 | training loss: 2.6136e-03 | validation loss: 2.1262e-03\n",
      "Epoch: 103590 | training loss: 2.4124e-03 | validation loss: 2.1529e-03\n",
      "Epoch: 103600 | training loss: 2.2878e-03 | validation loss: 2.0511e-03\n",
      "Epoch: 103610 | training loss: 2.2988e-03 | validation loss: 2.0469e-03\n",
      "Epoch: 103620 | training loss: 2.2949e-03 | validation loss: 2.0681e-03\n",
      "Epoch: 103630 | training loss: 2.2898e-03 | validation loss: 2.0469e-03\n",
      "Epoch: 103640 | training loss: 2.2881e-03 | validation loss: 2.0574e-03\n",
      "Epoch: 103650 | training loss: 2.2876e-03 | validation loss: 2.0507e-03\n",
      "Epoch: 103660 | training loss: 2.2874e-03 | validation loss: 2.0545e-03\n",
      "Epoch: 103670 | training loss: 2.2872e-03 | validation loss: 2.0523e-03\n",
      "Epoch: 103680 | training loss: 2.2872e-03 | validation loss: 2.0521e-03\n",
      "Epoch: 103690 | training loss: 2.2871e-03 | validation loss: 2.0529e-03\n",
      "Epoch: 103700 | training loss: 2.2871e-03 | validation loss: 2.0531e-03\n",
      "Epoch: 103710 | training loss: 2.2872e-03 | validation loss: 2.0541e-03\n",
      "Epoch: 103720 | training loss: 2.2906e-03 | validation loss: 2.0620e-03\n",
      "Epoch: 103730 | training loss: 2.4894e-03 | validation loss: 2.2064e-03\n",
      "Epoch: 103740 | training loss: 2.5232e-03 | validation loss: 2.2251e-03\n",
      "Epoch: 103750 | training loss: 2.4534e-03 | validation loss: 2.1858e-03\n",
      "Epoch: 103760 | training loss: 2.3312e-03 | validation loss: 2.0454e-03\n",
      "Epoch: 103770 | training loss: 2.3134e-03 | validation loss: 2.0399e-03\n",
      "Epoch: 103780 | training loss: 2.2931e-03 | validation loss: 2.0662e-03\n",
      "Epoch: 103790 | training loss: 2.2883e-03 | validation loss: 2.0583e-03\n",
      "Epoch: 103800 | training loss: 2.2886e-03 | validation loss: 2.0464e-03\n",
      "Epoch: 103810 | training loss: 2.2869e-03 | validation loss: 2.0542e-03\n",
      "Epoch: 103820 | training loss: 2.2866e-03 | validation loss: 2.0515e-03\n",
      "Epoch: 103830 | training loss: 2.2865e-03 | validation loss: 2.0512e-03\n",
      "Epoch: 103840 | training loss: 2.2865e-03 | validation loss: 2.0517e-03\n",
      "Epoch: 103850 | training loss: 2.2864e-03 | validation loss: 2.0515e-03\n",
      "Epoch: 103860 | training loss: 2.2864e-03 | validation loss: 2.0511e-03\n",
      "Epoch: 103870 | training loss: 2.2863e-03 | validation loss: 2.0516e-03\n",
      "Epoch: 103880 | training loss: 2.2863e-03 | validation loss: 2.0522e-03\n",
      "Epoch: 103890 | training loss: 2.2894e-03 | validation loss: 2.0611e-03\n",
      "Epoch: 103900 | training loss: 2.6012e-03 | validation loss: 2.3518e-03\n",
      "Epoch: 103910 | training loss: 2.5720e-03 | validation loss: 2.2162e-03\n",
      "Epoch: 103920 | training loss: 2.2988e-03 | validation loss: 2.0558e-03\n",
      "Epoch: 103930 | training loss: 2.2975e-03 | validation loss: 2.0553e-03\n",
      "Epoch: 103940 | training loss: 2.3083e-03 | validation loss: 2.0556e-03\n",
      "Epoch: 103950 | training loss: 2.3453e-03 | validation loss: 2.0437e-03\n",
      "Epoch: 103960 | training loss: 2.6262e-03 | validation loss: 2.1160e-03\n",
      "Epoch: 103970 | training loss: 2.2903e-03 | validation loss: 2.0584e-03\n",
      "Epoch: 103980 | training loss: 2.3035e-03 | validation loss: 2.0769e-03\n",
      "Epoch: 103990 | training loss: 2.3102e-03 | validation loss: 2.0382e-03\n",
      "Epoch: 104000 | training loss: 2.2871e-03 | validation loss: 2.0564e-03\n",
      "Epoch: 104010 | training loss: 2.2921e-03 | validation loss: 2.0645e-03\n",
      "Epoch: 104020 | training loss: 2.2894e-03 | validation loss: 2.0602e-03\n",
      "Epoch: 104030 | training loss: 2.2961e-03 | validation loss: 2.0698e-03\n",
      "Epoch: 104040 | training loss: 2.4159e-03 | validation loss: 2.1640e-03\n",
      "Epoch: 104050 | training loss: 2.7441e-03 | validation loss: 2.3590e-03\n",
      "Epoch: 104060 | training loss: 2.4501e-03 | validation loss: 2.0730e-03\n",
      "Epoch: 104070 | training loss: 2.3456e-03 | validation loss: 2.1099e-03\n",
      "Epoch: 104080 | training loss: 2.3094e-03 | validation loss: 2.0382e-03\n",
      "Epoch: 104090 | training loss: 2.2942e-03 | validation loss: 2.0665e-03\n",
      "Epoch: 104100 | training loss: 2.2863e-03 | validation loss: 2.0452e-03\n",
      "Epoch: 104110 | training loss: 2.2860e-03 | validation loss: 2.0456e-03\n",
      "Epoch: 104120 | training loss: 2.2857e-03 | validation loss: 2.0521e-03\n",
      "Epoch: 104130 | training loss: 2.2862e-03 | validation loss: 2.0535e-03\n",
      "Epoch: 104140 | training loss: 2.2890e-03 | validation loss: 2.0590e-03\n",
      "Epoch: 104150 | training loss: 2.3405e-03 | validation loss: 2.1065e-03\n",
      "Epoch: 104160 | training loss: 3.0630e-03 | validation loss: 2.5316e-03\n",
      "Epoch: 104170 | training loss: 2.5367e-03 | validation loss: 2.1001e-03\n",
      "Epoch: 104180 | training loss: 2.3868e-03 | validation loss: 2.1359e-03\n",
      "Epoch: 104190 | training loss: 2.3137e-03 | validation loss: 2.0425e-03\n",
      "Epoch: 104200 | training loss: 2.2930e-03 | validation loss: 2.0612e-03\n",
      "Epoch: 104210 | training loss: 2.2883e-03 | validation loss: 2.0443e-03\n",
      "Epoch: 104220 | training loss: 2.2869e-03 | validation loss: 2.0538e-03\n",
      "Epoch: 104230 | training loss: 2.2856e-03 | validation loss: 2.0455e-03\n",
      "Epoch: 104240 | training loss: 2.2848e-03 | validation loss: 2.0481e-03\n",
      "Epoch: 104250 | training loss: 2.2850e-03 | validation loss: 2.0495e-03\n",
      "Epoch: 104260 | training loss: 2.2849e-03 | validation loss: 2.0495e-03\n",
      "Epoch: 104270 | training loss: 2.2854e-03 | validation loss: 2.0511e-03\n",
      "Epoch: 104280 | training loss: 2.2966e-03 | validation loss: 2.0657e-03\n",
      "Epoch: 104290 | training loss: 2.6971e-03 | validation loss: 2.3180e-03\n",
      "Epoch: 104300 | training loss: 2.3125e-03 | validation loss: 2.0846e-03\n",
      "Epoch: 104310 | training loss: 2.4290e-03 | validation loss: 2.1420e-03\n",
      "Epoch: 104320 | training loss: 2.3229e-03 | validation loss: 2.0534e-03\n",
      "Epoch: 104330 | training loss: 2.2870e-03 | validation loss: 2.0527e-03\n",
      "Epoch: 104340 | training loss: 2.2928e-03 | validation loss: 2.0516e-03\n",
      "Epoch: 104350 | training loss: 2.2870e-03 | validation loss: 2.0480e-03\n",
      "Epoch: 104360 | training loss: 2.2847e-03 | validation loss: 2.0487e-03\n",
      "Epoch: 104370 | training loss: 2.2843e-03 | validation loss: 2.0459e-03\n",
      "Epoch: 104380 | training loss: 2.2842e-03 | validation loss: 2.0464e-03\n",
      "Epoch: 104390 | training loss: 2.2842e-03 | validation loss: 2.0465e-03\n",
      "Epoch: 104400 | training loss: 2.2842e-03 | validation loss: 2.0464e-03\n",
      "Epoch: 104410 | training loss: 2.2842e-03 | validation loss: 2.0445e-03\n",
      "Epoch: 104420 | training loss: 2.2912e-03 | validation loss: 2.0343e-03\n",
      "Epoch: 104430 | training loss: 2.8478e-03 | validation loss: 2.2323e-03\n",
      "Epoch: 104440 | training loss: 2.5951e-03 | validation loss: 2.3463e-03\n",
      "Epoch: 104450 | training loss: 2.3416e-03 | validation loss: 2.1057e-03\n",
      "Epoch: 104460 | training loss: 2.2904e-03 | validation loss: 2.0622e-03\n",
      "Epoch: 104470 | training loss: 2.2897e-03 | validation loss: 2.0339e-03\n",
      "Epoch: 104480 | training loss: 2.2892e-03 | validation loss: 2.0439e-03\n",
      "Epoch: 104490 | training loss: 2.2853e-03 | validation loss: 2.0408e-03\n",
      "Epoch: 104500 | training loss: 2.2839e-03 | validation loss: 2.0438e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 104510 | training loss: 2.2840e-03 | validation loss: 2.0491e-03\n",
      "Epoch: 104520 | training loss: 2.2837e-03 | validation loss: 2.0472e-03\n",
      "Epoch: 104530 | training loss: 2.2837e-03 | validation loss: 2.0455e-03\n",
      "Epoch: 104540 | training loss: 2.2837e-03 | validation loss: 2.0471e-03\n",
      "Epoch: 104550 | training loss: 2.2880e-03 | validation loss: 2.0564e-03\n",
      "Epoch: 104560 | training loss: 2.5234e-03 | validation loss: 2.2223e-03\n",
      "Epoch: 104570 | training loss: 2.3916e-03 | validation loss: 2.1378e-03\n",
      "Epoch: 104580 | training loss: 2.4754e-03 | validation loss: 2.1915e-03\n",
      "Epoch: 104590 | training loss: 2.3174e-03 | validation loss: 2.0369e-03\n",
      "Epoch: 104600 | training loss: 2.3079e-03 | validation loss: 2.0362e-03\n",
      "Epoch: 104610 | training loss: 2.2903e-03 | validation loss: 2.0593e-03\n",
      "Epoch: 104620 | training loss: 2.2840e-03 | validation loss: 2.0487e-03\n",
      "Epoch: 104630 | training loss: 2.2850e-03 | validation loss: 2.0399e-03\n",
      "Epoch: 104640 | training loss: 2.2837e-03 | validation loss: 2.0478e-03\n",
      "Epoch: 104650 | training loss: 2.2832e-03 | validation loss: 2.0436e-03\n",
      "Epoch: 104660 | training loss: 2.2831e-03 | validation loss: 2.0450e-03\n",
      "Epoch: 104670 | training loss: 2.2830e-03 | validation loss: 2.0440e-03\n",
      "Epoch: 104680 | training loss: 2.2830e-03 | validation loss: 2.0448e-03\n",
      "Epoch: 104690 | training loss: 2.2829e-03 | validation loss: 2.0439e-03\n",
      "Epoch: 104700 | training loss: 2.2829e-03 | validation loss: 2.0442e-03\n",
      "Epoch: 104710 | training loss: 2.2829e-03 | validation loss: 2.0443e-03\n",
      "Epoch: 104720 | training loss: 2.2828e-03 | validation loss: 2.0443e-03\n",
      "Epoch: 104730 | training loss: 2.2828e-03 | validation loss: 2.0446e-03\n",
      "Epoch: 104740 | training loss: 2.2832e-03 | validation loss: 2.0469e-03\n",
      "Epoch: 104750 | training loss: 2.3105e-03 | validation loss: 2.0783e-03\n",
      "Epoch: 104760 | training loss: 3.8381e-03 | validation loss: 2.9361e-03\n",
      "Epoch: 104770 | training loss: 2.6793e-03 | validation loss: 2.1499e-03\n",
      "Epoch: 104780 | training loss: 2.4254e-03 | validation loss: 2.0613e-03\n",
      "Epoch: 104790 | training loss: 2.2833e-03 | validation loss: 2.0462e-03\n",
      "Epoch: 104800 | training loss: 2.3065e-03 | validation loss: 2.0743e-03\n",
      "Epoch: 104810 | training loss: 2.2849e-03 | validation loss: 2.0509e-03\n",
      "Epoch: 104820 | training loss: 2.2848e-03 | validation loss: 2.0381e-03\n",
      "Epoch: 104830 | training loss: 2.2826e-03 | validation loss: 2.0414e-03\n",
      "Epoch: 104840 | training loss: 2.2828e-03 | validation loss: 2.0462e-03\n",
      "Epoch: 104850 | training loss: 2.2823e-03 | validation loss: 2.0423e-03\n",
      "Epoch: 104860 | training loss: 2.2822e-03 | validation loss: 2.0429e-03\n",
      "Epoch: 104870 | training loss: 2.2822e-03 | validation loss: 2.0433e-03\n",
      "Epoch: 104880 | training loss: 2.2822e-03 | validation loss: 2.0426e-03\n",
      "Epoch: 104890 | training loss: 2.2821e-03 | validation loss: 2.0429e-03\n",
      "Epoch: 104900 | training loss: 2.2821e-03 | validation loss: 2.0427e-03\n",
      "Epoch: 104910 | training loss: 2.2820e-03 | validation loss: 2.0426e-03\n",
      "Epoch: 104920 | training loss: 2.2820e-03 | validation loss: 2.0426e-03\n",
      "Epoch: 104930 | training loss: 2.2819e-03 | validation loss: 2.0425e-03\n",
      "Epoch: 104940 | training loss: 2.2819e-03 | validation loss: 2.0426e-03\n",
      "Epoch: 104950 | training loss: 2.2822e-03 | validation loss: 2.0456e-03\n",
      "Epoch: 104960 | training loss: 2.4152e-03 | validation loss: 2.1915e-03\n",
      "Epoch: 104970 | training loss: 2.7689e-03 | validation loss: 2.3419e-03\n",
      "Epoch: 104980 | training loss: 2.4101e-03 | validation loss: 2.0811e-03\n",
      "Epoch: 104990 | training loss: 2.3503e-03 | validation loss: 2.0522e-03\n",
      "Epoch: 105000 | training loss: 2.3072e-03 | validation loss: 2.0562e-03\n",
      "Epoch: 105010 | training loss: 2.2840e-03 | validation loss: 2.0350e-03\n",
      "Epoch: 105020 | training loss: 2.2841e-03 | validation loss: 2.0341e-03\n",
      "Epoch: 105030 | training loss: 2.2848e-03 | validation loss: 2.0345e-03\n",
      "Epoch: 105040 | training loss: 2.2998e-03 | validation loss: 2.0292e-03\n",
      "Epoch: 105050 | training loss: 2.5574e-03 | validation loss: 2.0877e-03\n",
      "Epoch: 105060 | training loss: 2.3354e-03 | validation loss: 2.0323e-03\n",
      "Epoch: 105070 | training loss: 2.2982e-03 | validation loss: 2.0684e-03\n",
      "Epoch: 105080 | training loss: 2.2860e-03 | validation loss: 2.0330e-03\n",
      "Epoch: 105090 | training loss: 2.2849e-03 | validation loss: 2.0514e-03\n",
      "Epoch: 105100 | training loss: 2.2856e-03 | validation loss: 2.0330e-03\n",
      "Epoch: 105110 | training loss: 2.2841e-03 | validation loss: 2.0501e-03\n",
      "Epoch: 105120 | training loss: 2.2812e-03 | validation loss: 2.0396e-03\n",
      "Epoch: 105130 | training loss: 2.2819e-03 | validation loss: 2.0367e-03\n",
      "Epoch: 105140 | training loss: 2.2817e-03 | validation loss: 2.0370e-03\n",
      "Epoch: 105150 | training loss: 2.2835e-03 | validation loss: 2.0341e-03\n",
      "Epoch: 105160 | training loss: 2.3217e-03 | validation loss: 2.0292e-03\n",
      "Epoch: 105170 | training loss: 3.0747e-03 | validation loss: 2.2786e-03\n",
      "Epoch: 105180 | training loss: 2.5545e-03 | validation loss: 2.2399e-03\n",
      "Epoch: 105190 | training loss: 2.3927e-03 | validation loss: 2.0452e-03\n",
      "Epoch: 105200 | training loss: 2.2990e-03 | validation loss: 2.0670e-03\n",
      "Epoch: 105210 | training loss: 2.2817e-03 | validation loss: 2.0365e-03\n",
      "Epoch: 105220 | training loss: 2.2808e-03 | validation loss: 2.0403e-03\n",
      "Epoch: 105230 | training loss: 2.2807e-03 | validation loss: 2.0393e-03\n",
      "Epoch: 105240 | training loss: 2.2809e-03 | validation loss: 2.0418e-03\n",
      "Epoch: 105250 | training loss: 2.2810e-03 | validation loss: 2.0372e-03\n",
      "Epoch: 105260 | training loss: 2.2807e-03 | validation loss: 2.0407e-03\n",
      "Epoch: 105270 | training loss: 2.2807e-03 | validation loss: 2.0409e-03\n",
      "Epoch: 105280 | training loss: 2.2806e-03 | validation loss: 2.0403e-03\n",
      "Epoch: 105290 | training loss: 2.2806e-03 | validation loss: 2.0410e-03\n",
      "Epoch: 105300 | training loss: 2.2829e-03 | validation loss: 2.0473e-03\n",
      "Epoch: 105310 | training loss: 2.3942e-03 | validation loss: 2.1389e-03\n",
      "Epoch: 105320 | training loss: 3.1328e-03 | validation loss: 2.5620e-03\n",
      "Epoch: 105330 | training loss: 2.2821e-03 | validation loss: 2.0474e-03\n",
      "Epoch: 105340 | training loss: 2.4015e-03 | validation loss: 2.0532e-03\n",
      "Epoch: 105350 | training loss: 2.2811e-03 | validation loss: 2.0405e-03\n",
      "Epoch: 105360 | training loss: 2.2944e-03 | validation loss: 2.0592e-03\n",
      "Epoch: 105370 | training loss: 2.2830e-03 | validation loss: 2.0336e-03\n",
      "Epoch: 105380 | training loss: 2.2802e-03 | validation loss: 2.0384e-03\n",
      "Epoch: 105390 | training loss: 2.2805e-03 | validation loss: 2.0414e-03\n",
      "Epoch: 105400 | training loss: 2.2803e-03 | validation loss: 2.0366e-03\n",
      "Epoch: 105410 | training loss: 2.2801e-03 | validation loss: 2.0401e-03\n",
      "Epoch: 105420 | training loss: 2.2800e-03 | validation loss: 2.0376e-03\n",
      "Epoch: 105430 | training loss: 2.2799e-03 | validation loss: 2.0388e-03\n",
      "Epoch: 105440 | training loss: 2.2799e-03 | validation loss: 2.0384e-03\n",
      "Epoch: 105450 | training loss: 2.2798e-03 | validation loss: 2.0380e-03\n",
      "Epoch: 105460 | training loss: 2.2798e-03 | validation loss: 2.0380e-03\n",
      "Epoch: 105470 | training loss: 2.2797e-03 | validation loss: 2.0380e-03\n",
      "Epoch: 105480 | training loss: 2.2797e-03 | validation loss: 2.0377e-03\n",
      "Epoch: 105490 | training loss: 2.2799e-03 | validation loss: 2.0361e-03\n",
      "Epoch: 105500 | training loss: 2.2959e-03 | validation loss: 2.0293e-03\n",
      "Epoch: 105510 | training loss: 3.6792e-03 | validation loss: 2.5476e-03\n",
      "Epoch: 105520 | training loss: 2.9282e-03 | validation loss: 2.4471e-03\n",
      "Epoch: 105530 | training loss: 2.4425e-03 | validation loss: 2.1637e-03\n",
      "Epoch: 105540 | training loss: 2.2809e-03 | validation loss: 2.0412e-03\n",
      "Epoch: 105550 | training loss: 2.2941e-03 | validation loss: 2.0292e-03\n",
      "Epoch: 105560 | training loss: 2.2904e-03 | validation loss: 2.0297e-03\n",
      "Epoch: 105570 | training loss: 2.2795e-03 | validation loss: 2.0358e-03\n",
      "Epoch: 105580 | training loss: 2.2806e-03 | validation loss: 2.0424e-03\n",
      "Epoch: 105590 | training loss: 2.2793e-03 | validation loss: 2.0383e-03\n",
      "Epoch: 105600 | training loss: 2.2794e-03 | validation loss: 2.0355e-03\n",
      "Epoch: 105610 | training loss: 2.2792e-03 | validation loss: 2.0374e-03\n",
      "Epoch: 105620 | training loss: 2.2791e-03 | validation loss: 2.0373e-03\n",
      "Epoch: 105630 | training loss: 2.2791e-03 | validation loss: 2.0365e-03\n",
      "Epoch: 105640 | training loss: 2.2791e-03 | validation loss: 2.0371e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 105650 | training loss: 2.2790e-03 | validation loss: 2.0366e-03\n",
      "Epoch: 105660 | training loss: 2.2790e-03 | validation loss: 2.0368e-03\n",
      "Epoch: 105670 | training loss: 2.2789e-03 | validation loss: 2.0367e-03\n",
      "Epoch: 105680 | training loss: 2.2790e-03 | validation loss: 2.0382e-03\n",
      "Epoch: 105690 | training loss: 2.2964e-03 | validation loss: 2.0695e-03\n",
      "Epoch: 105700 | training loss: 2.9299e-03 | validation loss: 2.6053e-03\n",
      "Epoch: 105710 | training loss: 2.4090e-03 | validation loss: 2.1403e-03\n",
      "Epoch: 105720 | training loss: 2.2969e-03 | validation loss: 2.0623e-03\n",
      "Epoch: 105730 | training loss: 2.2830e-03 | validation loss: 2.0494e-03\n",
      "Epoch: 105740 | training loss: 2.2809e-03 | validation loss: 2.0283e-03\n",
      "Epoch: 105750 | training loss: 2.2796e-03 | validation loss: 2.0313e-03\n",
      "Epoch: 105760 | training loss: 2.2797e-03 | validation loss: 2.0353e-03\n",
      "Epoch: 105770 | training loss: 2.2801e-03 | validation loss: 2.0398e-03\n",
      "Epoch: 105780 | training loss: 2.2948e-03 | validation loss: 2.0627e-03\n",
      "Epoch: 105790 | training loss: 2.6954e-03 | validation loss: 2.3311e-03\n",
      "Epoch: 105800 | training loss: 2.2872e-03 | validation loss: 2.0227e-03\n",
      "Epoch: 105810 | training loss: 2.3379e-03 | validation loss: 2.0964e-03\n",
      "Epoch: 105820 | training loss: 2.3286e-03 | validation loss: 2.0248e-03\n",
      "Epoch: 105830 | training loss: 2.2977e-03 | validation loss: 2.0638e-03\n",
      "Epoch: 105840 | training loss: 2.2845e-03 | validation loss: 2.0266e-03\n",
      "Epoch: 105850 | training loss: 2.2807e-03 | validation loss: 2.0434e-03\n",
      "Epoch: 105860 | training loss: 2.2793e-03 | validation loss: 2.0303e-03\n",
      "Epoch: 105870 | training loss: 2.2784e-03 | validation loss: 2.0370e-03\n",
      "Epoch: 105880 | training loss: 2.2781e-03 | validation loss: 2.0355e-03\n",
      "Epoch: 105890 | training loss: 2.2781e-03 | validation loss: 2.0332e-03\n",
      "Epoch: 105900 | training loss: 2.2781e-03 | validation loss: 2.0328e-03\n",
      "Epoch: 105910 | training loss: 2.2785e-03 | validation loss: 2.0311e-03\n",
      "Epoch: 105920 | training loss: 2.2877e-03 | validation loss: 2.0246e-03\n",
      "Epoch: 105930 | training loss: 2.6408e-03 | validation loss: 2.1172e-03\n",
      "Epoch: 105940 | training loss: 2.2821e-03 | validation loss: 2.0426e-03\n",
      "Epoch: 105950 | training loss: 2.4010e-03 | validation loss: 2.0425e-03\n",
      "Epoch: 105960 | training loss: 2.3419e-03 | validation loss: 2.0911e-03\n",
      "Epoch: 105970 | training loss: 2.2788e-03 | validation loss: 2.0320e-03\n",
      "Epoch: 105980 | training loss: 2.2810e-03 | validation loss: 2.0299e-03\n",
      "Epoch: 105990 | training loss: 2.2809e-03 | validation loss: 2.0413e-03\n",
      "Epoch: 106000 | training loss: 2.2790e-03 | validation loss: 2.0298e-03\n",
      "Epoch: 106010 | training loss: 2.2781e-03 | validation loss: 2.0371e-03\n",
      "Epoch: 106020 | training loss: 2.2777e-03 | validation loss: 2.0318e-03\n",
      "Epoch: 106030 | training loss: 2.2775e-03 | validation loss: 2.0341e-03\n",
      "Epoch: 106040 | training loss: 2.2774e-03 | validation loss: 2.0339e-03\n",
      "Epoch: 106050 | training loss: 2.2774e-03 | validation loss: 2.0330e-03\n",
      "Epoch: 106060 | training loss: 2.2774e-03 | validation loss: 2.0327e-03\n",
      "Epoch: 106070 | training loss: 2.2774e-03 | validation loss: 2.0320e-03\n",
      "Epoch: 106080 | training loss: 2.2785e-03 | validation loss: 2.0291e-03\n",
      "Epoch: 106090 | training loss: 2.3317e-03 | validation loss: 2.0289e-03\n",
      "Epoch: 106100 | training loss: 3.6383e-03 | validation loss: 2.5260e-03\n",
      "Epoch: 106110 | training loss: 2.4840e-03 | validation loss: 2.1887e-03\n",
      "Epoch: 106120 | training loss: 2.3478e-03 | validation loss: 2.0983e-03\n",
      "Epoch: 106130 | training loss: 2.3067e-03 | validation loss: 2.0243e-03\n",
      "Epoch: 106140 | training loss: 2.2832e-03 | validation loss: 2.0259e-03\n",
      "Epoch: 106150 | training loss: 2.2837e-03 | validation loss: 2.0458e-03\n",
      "Epoch: 106160 | training loss: 2.2771e-03 | validation loss: 2.0309e-03\n",
      "Epoch: 106170 | training loss: 2.2771e-03 | validation loss: 2.0307e-03\n",
      "Epoch: 106180 | training loss: 2.2771e-03 | validation loss: 2.0346e-03\n",
      "Epoch: 106190 | training loss: 2.2769e-03 | validation loss: 2.0310e-03\n",
      "Epoch: 106200 | training loss: 2.2768e-03 | validation loss: 2.0331e-03\n",
      "Epoch: 106210 | training loss: 2.2767e-03 | validation loss: 2.0317e-03\n",
      "Epoch: 106220 | training loss: 2.2767e-03 | validation loss: 2.0321e-03\n",
      "Epoch: 106230 | training loss: 2.2766e-03 | validation loss: 2.0323e-03\n",
      "Epoch: 106240 | training loss: 2.2766e-03 | validation loss: 2.0325e-03\n",
      "Epoch: 106250 | training loss: 2.2784e-03 | validation loss: 2.0395e-03\n",
      "Epoch: 106260 | training loss: 2.6224e-03 | validation loss: 2.3652e-03\n",
      "Epoch: 106270 | training loss: 2.7018e-03 | validation loss: 2.2836e-03\n",
      "Epoch: 106280 | training loss: 2.4058e-03 | validation loss: 2.0589e-03\n",
      "Epoch: 106290 | training loss: 2.3111e-03 | validation loss: 2.0359e-03\n",
      "Epoch: 106300 | training loss: 2.2924e-03 | validation loss: 2.0404e-03\n",
      "Epoch: 106310 | training loss: 2.2790e-03 | validation loss: 2.0282e-03\n",
      "Epoch: 106320 | training loss: 2.2775e-03 | validation loss: 2.0251e-03\n",
      "Epoch: 106330 | training loss: 2.2826e-03 | validation loss: 2.0215e-03\n",
      "Epoch: 106340 | training loss: 2.3875e-03 | validation loss: 2.0315e-03\n",
      "Epoch: 106350 | training loss: 2.8420e-03 | validation loss: 2.1814e-03\n",
      "Epoch: 106360 | training loss: 2.4463e-03 | validation loss: 2.1701e-03\n",
      "Epoch: 106370 | training loss: 2.3135e-03 | validation loss: 2.0190e-03\n",
      "Epoch: 106380 | training loss: 2.2833e-03 | validation loss: 2.0456e-03\n",
      "Epoch: 106390 | training loss: 2.2785e-03 | validation loss: 2.0239e-03\n",
      "Epoch: 106400 | training loss: 2.2780e-03 | validation loss: 2.0378e-03\n",
      "Epoch: 106410 | training loss: 2.2776e-03 | validation loss: 2.0251e-03\n",
      "Epoch: 106420 | training loss: 2.2762e-03 | validation loss: 2.0328e-03\n",
      "Epoch: 106430 | training loss: 2.2761e-03 | validation loss: 2.0325e-03\n",
      "Epoch: 106440 | training loss: 2.2758e-03 | validation loss: 2.0300e-03\n",
      "Epoch: 106450 | training loss: 2.2758e-03 | validation loss: 2.0289e-03\n",
      "Epoch: 106460 | training loss: 2.2764e-03 | validation loss: 2.0264e-03\n",
      "Epoch: 106470 | training loss: 2.2988e-03 | validation loss: 2.0184e-03\n",
      "Epoch: 106480 | training loss: 3.2795e-03 | validation loss: 2.3537e-03\n",
      "Epoch: 106490 | training loss: 2.7666e-03 | validation loss: 2.3539e-03\n",
      "Epoch: 106500 | training loss: 2.3054e-03 | validation loss: 2.0294e-03\n",
      "Epoch: 106510 | training loss: 2.3239e-03 | validation loss: 2.0273e-03\n",
      "Epoch: 106520 | training loss: 2.2859e-03 | validation loss: 2.0473e-03\n",
      "Epoch: 106530 | training loss: 2.2771e-03 | validation loss: 2.0336e-03\n",
      "Epoch: 106540 | training loss: 2.2784e-03 | validation loss: 2.0230e-03\n",
      "Epoch: 106550 | training loss: 2.2763e-03 | validation loss: 2.0342e-03\n",
      "Epoch: 106560 | training loss: 2.2755e-03 | validation loss: 2.0279e-03\n",
      "Epoch: 106570 | training loss: 2.2753e-03 | validation loss: 2.0297e-03\n",
      "Epoch: 106580 | training loss: 2.2753e-03 | validation loss: 2.0286e-03\n",
      "Epoch: 106590 | training loss: 2.2752e-03 | validation loss: 2.0296e-03\n",
      "Epoch: 106600 | training loss: 2.2752e-03 | validation loss: 2.0286e-03\n",
      "Epoch: 106610 | training loss: 2.2751e-03 | validation loss: 2.0288e-03\n",
      "Epoch: 106620 | training loss: 2.2751e-03 | validation loss: 2.0290e-03\n",
      "Epoch: 106630 | training loss: 2.2750e-03 | validation loss: 2.0290e-03\n",
      "Epoch: 106640 | training loss: 2.2750e-03 | validation loss: 2.0295e-03\n",
      "Epoch: 106650 | training loss: 2.2761e-03 | validation loss: 2.0335e-03\n",
      "Epoch: 106660 | training loss: 2.3452e-03 | validation loss: 2.0963e-03\n",
      "Epoch: 106670 | training loss: 3.7079e-03 | validation loss: 2.8588e-03\n",
      "Epoch: 106680 | training loss: 2.2763e-03 | validation loss: 2.0304e-03\n",
      "Epoch: 106690 | training loss: 2.4206e-03 | validation loss: 2.0446e-03\n",
      "Epoch: 106700 | training loss: 2.2976e-03 | validation loss: 2.0149e-03\n",
      "Epoch: 106710 | training loss: 2.2817e-03 | validation loss: 2.0433e-03\n",
      "Epoch: 106720 | training loss: 2.2804e-03 | validation loss: 2.0417e-03\n",
      "Epoch: 106730 | training loss: 2.2755e-03 | validation loss: 2.0239e-03\n",
      "Epoch: 106740 | training loss: 2.2750e-03 | validation loss: 2.0256e-03\n",
      "Epoch: 106750 | training loss: 2.2749e-03 | validation loss: 2.0307e-03\n",
      "Epoch: 106760 | training loss: 2.2745e-03 | validation loss: 2.0269e-03\n",
      "Epoch: 106770 | training loss: 2.2745e-03 | validation loss: 2.0279e-03\n",
      "Epoch: 106780 | training loss: 2.2744e-03 | validation loss: 2.0277e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 106790 | training loss: 2.2744e-03 | validation loss: 2.0275e-03\n",
      "Epoch: 106800 | training loss: 2.2743e-03 | validation loss: 2.0274e-03\n",
      "Epoch: 106810 | training loss: 2.2743e-03 | validation loss: 2.0276e-03\n",
      "Epoch: 106820 | training loss: 2.2742e-03 | validation loss: 2.0274e-03\n",
      "Epoch: 106830 | training loss: 2.2742e-03 | validation loss: 2.0281e-03\n",
      "Epoch: 106840 | training loss: 2.2795e-03 | validation loss: 2.0410e-03\n",
      "Epoch: 106850 | training loss: 2.8931e-03 | validation loss: 2.5698e-03\n",
      "Epoch: 106860 | training loss: 2.3722e-03 | validation loss: 2.0868e-03\n",
      "Epoch: 106870 | training loss: 2.3445e-03 | validation loss: 2.0432e-03\n",
      "Epoch: 106880 | training loss: 2.3105e-03 | validation loss: 2.0206e-03\n",
      "Epoch: 106890 | training loss: 2.2869e-03 | validation loss: 2.0149e-03\n",
      "Epoch: 106900 | training loss: 2.2855e-03 | validation loss: 2.0126e-03\n",
      "Epoch: 106910 | training loss: 2.3923e-03 | validation loss: 2.0275e-03\n",
      "Epoch: 106920 | training loss: 2.7020e-03 | validation loss: 2.1270e-03\n",
      "Epoch: 106930 | training loss: 2.4292e-03 | validation loss: 2.1585e-03\n",
      "Epoch: 106940 | training loss: 2.3313e-03 | validation loss: 2.0172e-03\n",
      "Epoch: 106950 | training loss: 2.2937e-03 | validation loss: 2.0562e-03\n",
      "Epoch: 106960 | training loss: 2.2772e-03 | validation loss: 2.0184e-03\n",
      "Epoch: 106970 | training loss: 2.2739e-03 | validation loss: 2.0234e-03\n",
      "Epoch: 106980 | training loss: 2.2754e-03 | validation loss: 2.0329e-03\n",
      "Epoch: 106990 | training loss: 2.2739e-03 | validation loss: 2.0286e-03\n",
      "Epoch: 107000 | training loss: 2.2735e-03 | validation loss: 2.0264e-03\n",
      "Epoch: 107010 | training loss: 2.2735e-03 | validation loss: 2.0265e-03\n",
      "Epoch: 107020 | training loss: 2.2745e-03 | validation loss: 2.0309e-03\n",
      "Epoch: 107030 | training loss: 2.3387e-03 | validation loss: 2.0937e-03\n",
      "Epoch: 107040 | training loss: 3.6687e-03 | validation loss: 2.8450e-03\n",
      "Epoch: 107050 | training loss: 2.2991e-03 | validation loss: 2.0393e-03\n",
      "Epoch: 107060 | training loss: 2.4241e-03 | validation loss: 2.0543e-03\n",
      "Epoch: 107070 | training loss: 2.2775e-03 | validation loss: 2.0175e-03\n",
      "Epoch: 107080 | training loss: 2.2896e-03 | validation loss: 2.0460e-03\n",
      "Epoch: 107090 | training loss: 2.2745e-03 | validation loss: 2.0273e-03\n",
      "Epoch: 107100 | training loss: 2.2756e-03 | validation loss: 2.0193e-03\n",
      "Epoch: 107110 | training loss: 2.2732e-03 | validation loss: 2.0271e-03\n",
      "Epoch: 107120 | training loss: 2.2732e-03 | validation loss: 2.0265e-03\n",
      "Epoch: 107130 | training loss: 2.2731e-03 | validation loss: 2.0231e-03\n",
      "Epoch: 107140 | training loss: 2.2730e-03 | validation loss: 2.0256e-03\n",
      "Epoch: 107150 | training loss: 2.2729e-03 | validation loss: 2.0242e-03\n",
      "Epoch: 107160 | training loss: 2.2729e-03 | validation loss: 2.0248e-03\n",
      "Epoch: 107170 | training loss: 2.2728e-03 | validation loss: 2.0244e-03\n",
      "Epoch: 107180 | training loss: 2.2728e-03 | validation loss: 2.0243e-03\n",
      "Epoch: 107190 | training loss: 2.2728e-03 | validation loss: 2.0245e-03\n",
      "Epoch: 107200 | training loss: 2.2727e-03 | validation loss: 2.0243e-03\n",
      "Epoch: 107210 | training loss: 2.2727e-03 | validation loss: 2.0243e-03\n",
      "Epoch: 107220 | training loss: 2.2727e-03 | validation loss: 2.0246e-03\n",
      "Epoch: 107230 | training loss: 2.2731e-03 | validation loss: 2.0272e-03\n",
      "Epoch: 107240 | training loss: 2.3146e-03 | validation loss: 2.0704e-03\n",
      "Epoch: 107250 | training loss: 4.1869e-03 | validation loss: 3.1052e-03\n",
      "Epoch: 107260 | training loss: 2.2848e-03 | validation loss: 2.0199e-03\n",
      "Epoch: 107270 | training loss: 2.4197e-03 | validation loss: 2.0410e-03\n",
      "Epoch: 107280 | training loss: 2.3409e-03 | validation loss: 2.0171e-03\n",
      "Epoch: 107290 | training loss: 2.2779e-03 | validation loss: 2.0155e-03\n",
      "Epoch: 107300 | training loss: 2.2748e-03 | validation loss: 2.0319e-03\n",
      "Epoch: 107310 | training loss: 2.2756e-03 | validation loss: 2.0327e-03\n",
      "Epoch: 107320 | training loss: 2.2723e-03 | validation loss: 2.0238e-03\n",
      "Epoch: 107330 | training loss: 2.2727e-03 | validation loss: 2.0206e-03\n",
      "Epoch: 107340 | training loss: 2.2722e-03 | validation loss: 2.0237e-03\n",
      "Epoch: 107350 | training loss: 2.2722e-03 | validation loss: 2.0240e-03\n",
      "Epoch: 107360 | training loss: 2.2721e-03 | validation loss: 2.0225e-03\n",
      "Epoch: 107370 | training loss: 2.2720e-03 | validation loss: 2.0233e-03\n",
      "Epoch: 107380 | training loss: 2.2720e-03 | validation loss: 2.0229e-03\n",
      "Epoch: 107390 | training loss: 2.2720e-03 | validation loss: 2.0229e-03\n",
      "Epoch: 107400 | training loss: 2.2719e-03 | validation loss: 2.0227e-03\n",
      "Epoch: 107410 | training loss: 2.2719e-03 | validation loss: 2.0226e-03\n",
      "Epoch: 107420 | training loss: 2.2720e-03 | validation loss: 2.0210e-03\n",
      "Epoch: 107430 | training loss: 2.2902e-03 | validation loss: 2.0167e-03\n",
      "Epoch: 107440 | training loss: 2.8205e-03 | validation loss: 2.2935e-03\n",
      "Epoch: 107450 | training loss: 2.3277e-03 | validation loss: 2.0401e-03\n",
      "Epoch: 107460 | training loss: 2.2757e-03 | validation loss: 2.0359e-03\n",
      "Epoch: 107470 | training loss: 2.2816e-03 | validation loss: 2.0302e-03\n",
      "Epoch: 107480 | training loss: 2.2801e-03 | validation loss: 2.0301e-03\n",
      "Epoch: 107490 | training loss: 2.2738e-03 | validation loss: 2.0246e-03\n",
      "Epoch: 107500 | training loss: 2.2723e-03 | validation loss: 2.0174e-03\n",
      "Epoch: 107510 | training loss: 2.2859e-03 | validation loss: 2.0078e-03\n",
      "Epoch: 107520 | training loss: 2.7638e-03 | validation loss: 2.1398e-03\n",
      "Epoch: 107530 | training loss: 2.3414e-03 | validation loss: 2.0985e-03\n",
      "Epoch: 107540 | training loss: 2.4235e-03 | validation loss: 2.0404e-03\n",
      "Epoch: 107550 | training loss: 2.3108e-03 | validation loss: 2.0693e-03\n",
      "Epoch: 107560 | training loss: 2.2715e-03 | validation loss: 2.0191e-03\n",
      "Epoch: 107570 | training loss: 2.2735e-03 | validation loss: 2.0147e-03\n",
      "Epoch: 107580 | training loss: 2.2732e-03 | validation loss: 2.0279e-03\n",
      "Epoch: 107590 | training loss: 2.2720e-03 | validation loss: 2.0173e-03\n",
      "Epoch: 107600 | training loss: 2.2714e-03 | validation loss: 2.0235e-03\n",
      "Epoch: 107610 | training loss: 2.2711e-03 | validation loss: 2.0205e-03\n",
      "Epoch: 107620 | training loss: 2.2711e-03 | validation loss: 2.0198e-03\n",
      "Epoch: 107630 | training loss: 2.2710e-03 | validation loss: 2.0215e-03\n",
      "Epoch: 107640 | training loss: 2.2710e-03 | validation loss: 2.0214e-03\n",
      "Epoch: 107650 | training loss: 2.2710e-03 | validation loss: 2.0216e-03\n",
      "Epoch: 107660 | training loss: 2.2713e-03 | validation loss: 2.0238e-03\n",
      "Epoch: 107670 | training loss: 2.2852e-03 | validation loss: 2.0441e-03\n",
      "Epoch: 107680 | training loss: 3.0009e-03 | validation loss: 2.4814e-03\n",
      "Epoch: 107690 | training loss: 2.6215e-03 | validation loss: 2.0987e-03\n",
      "Epoch: 107700 | training loss: 2.3988e-03 | validation loss: 2.1189e-03\n",
      "Epoch: 107710 | training loss: 2.2957e-03 | validation loss: 2.0590e-03\n",
      "Epoch: 107720 | training loss: 2.2913e-03 | validation loss: 2.0164e-03\n",
      "Epoch: 107730 | training loss: 2.2722e-03 | validation loss: 2.0144e-03\n",
      "Epoch: 107740 | training loss: 2.2729e-03 | validation loss: 2.0279e-03\n",
      "Epoch: 107750 | training loss: 2.2715e-03 | validation loss: 2.0184e-03\n",
      "Epoch: 107760 | training loss: 2.2708e-03 | validation loss: 2.0199e-03\n",
      "Epoch: 107770 | training loss: 2.2706e-03 | validation loss: 2.0205e-03\n",
      "Epoch: 107780 | training loss: 2.2705e-03 | validation loss: 2.0193e-03\n",
      "Epoch: 107790 | training loss: 2.2704e-03 | validation loss: 2.0198e-03\n",
      "Epoch: 107800 | training loss: 2.2703e-03 | validation loss: 2.0196e-03\n",
      "Epoch: 107810 | training loss: 2.2703e-03 | validation loss: 2.0194e-03\n",
      "Epoch: 107820 | training loss: 2.2702e-03 | validation loss: 2.0195e-03\n",
      "Epoch: 107830 | training loss: 2.2702e-03 | validation loss: 2.0194e-03\n",
      "Epoch: 107840 | training loss: 2.2702e-03 | validation loss: 2.0192e-03\n",
      "Epoch: 107850 | training loss: 2.2707e-03 | validation loss: 2.0189e-03\n",
      "Epoch: 107860 | training loss: 2.3014e-03 | validation loss: 2.0315e-03\n",
      "Epoch: 107870 | training loss: 3.3452e-03 | validation loss: 2.4643e-03\n",
      "Epoch: 107880 | training loss: 2.5721e-03 | validation loss: 2.2851e-03\n",
      "Epoch: 107890 | training loss: 2.3223e-03 | validation loss: 2.0165e-03\n",
      "Epoch: 107900 | training loss: 2.2764e-03 | validation loss: 2.0105e-03\n",
      "Epoch: 107910 | training loss: 2.2807e-03 | validation loss: 2.0377e-03\n",
      "Epoch: 107920 | training loss: 2.2712e-03 | validation loss: 2.0121e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 107930 | training loss: 2.2706e-03 | validation loss: 2.0142e-03\n",
      "Epoch: 107940 | training loss: 2.2706e-03 | validation loss: 2.0182e-03\n",
      "Epoch: 107950 | training loss: 2.2701e-03 | validation loss: 2.0149e-03\n",
      "Epoch: 107960 | training loss: 2.2697e-03 | validation loss: 2.0187e-03\n",
      "Epoch: 107970 | training loss: 2.2697e-03 | validation loss: 2.0181e-03\n",
      "Epoch: 107980 | training loss: 2.2696e-03 | validation loss: 2.0181e-03\n",
      "Epoch: 107990 | training loss: 2.2696e-03 | validation loss: 2.0182e-03\n",
      "Epoch: 108000 | training loss: 2.2695e-03 | validation loss: 2.0179e-03\n",
      "Epoch: 108010 | training loss: 2.2695e-03 | validation loss: 2.0178e-03\n",
      "Epoch: 108020 | training loss: 2.2694e-03 | validation loss: 2.0177e-03\n",
      "Epoch: 108030 | training loss: 2.2694e-03 | validation loss: 2.0175e-03\n",
      "Epoch: 108040 | training loss: 2.2694e-03 | validation loss: 2.0168e-03\n",
      "Epoch: 108050 | training loss: 2.2732e-03 | validation loss: 2.0102e-03\n",
      "Epoch: 108060 | training loss: 3.0300e-03 | validation loss: 2.2448e-03\n",
      "Epoch: 108070 | training loss: 3.3197e-03 | validation loss: 2.6571e-03\n",
      "Epoch: 108080 | training loss: 2.4863e-03 | validation loss: 2.2018e-03\n",
      "Epoch: 108090 | training loss: 2.3343e-03 | validation loss: 2.0997e-03\n",
      "Epoch: 108100 | training loss: 2.2916e-03 | validation loss: 2.0590e-03\n",
      "Epoch: 108110 | training loss: 2.2782e-03 | validation loss: 2.0397e-03\n",
      "Epoch: 108120 | training loss: 2.2733e-03 | validation loss: 2.0285e-03\n",
      "Epoch: 108130 | training loss: 2.2705e-03 | validation loss: 2.0213e-03\n",
      "Epoch: 108140 | training loss: 2.2691e-03 | validation loss: 2.0172e-03\n",
      "Epoch: 108150 | training loss: 2.2690e-03 | validation loss: 2.0158e-03\n",
      "Epoch: 108160 | training loss: 2.2690e-03 | validation loss: 2.0156e-03\n",
      "Epoch: 108170 | training loss: 2.2689e-03 | validation loss: 2.0163e-03\n",
      "Epoch: 108180 | training loss: 2.2688e-03 | validation loss: 2.0169e-03\n",
      "Epoch: 108190 | training loss: 2.2688e-03 | validation loss: 2.0169e-03\n",
      "Epoch: 108200 | training loss: 2.2687e-03 | validation loss: 2.0165e-03\n",
      "Epoch: 108210 | training loss: 2.2687e-03 | validation loss: 2.0164e-03\n",
      "Epoch: 108220 | training loss: 2.2687e-03 | validation loss: 2.0165e-03\n",
      "Epoch: 108230 | training loss: 2.2686e-03 | validation loss: 2.0163e-03\n",
      "Epoch: 108240 | training loss: 2.2686e-03 | validation loss: 2.0163e-03\n",
      "Epoch: 108250 | training loss: 2.2685e-03 | validation loss: 2.0162e-03\n",
      "Epoch: 108260 | training loss: 2.2685e-03 | validation loss: 2.0161e-03\n",
      "Epoch: 108270 | training loss: 2.2685e-03 | validation loss: 2.0160e-03\n",
      "Epoch: 108280 | training loss: 2.2684e-03 | validation loss: 2.0160e-03\n",
      "Epoch: 108290 | training loss: 2.2684e-03 | validation loss: 2.0159e-03\n",
      "Epoch: 108300 | training loss: 2.2683e-03 | validation loss: 2.0158e-03\n",
      "Epoch: 108310 | training loss: 2.2683e-03 | validation loss: 2.0157e-03\n",
      "Epoch: 108320 | training loss: 2.2683e-03 | validation loss: 2.0156e-03\n",
      "Epoch: 108330 | training loss: 2.2687e-03 | validation loss: 2.0154e-03\n",
      "Epoch: 108340 | training loss: 2.3625e-03 | validation loss: 2.0629e-03\n",
      "Epoch: 108350 | training loss: 3.5302e-03 | validation loss: 2.4427e-03\n",
      "Epoch: 108360 | training loss: 2.3382e-03 | validation loss: 2.0349e-03\n",
      "Epoch: 108370 | training loss: 2.4462e-03 | validation loss: 2.1220e-03\n",
      "Epoch: 108380 | training loss: 2.2915e-03 | validation loss: 2.0372e-03\n",
      "Epoch: 108390 | training loss: 2.2764e-03 | validation loss: 2.0249e-03\n",
      "Epoch: 108400 | training loss: 2.2759e-03 | validation loss: 2.0173e-03\n",
      "Epoch: 108410 | training loss: 2.2680e-03 | validation loss: 2.0162e-03\n",
      "Epoch: 108420 | training loss: 2.2685e-03 | validation loss: 2.0162e-03\n",
      "Epoch: 108430 | training loss: 2.2682e-03 | validation loss: 2.0116e-03\n",
      "Epoch: 108440 | training loss: 2.2679e-03 | validation loss: 2.0136e-03\n",
      "Epoch: 108450 | training loss: 2.2678e-03 | validation loss: 2.0144e-03\n",
      "Epoch: 108460 | training loss: 2.2677e-03 | validation loss: 2.0137e-03\n",
      "Epoch: 108470 | training loss: 2.2677e-03 | validation loss: 2.0144e-03\n",
      "Epoch: 108480 | training loss: 2.2676e-03 | validation loss: 2.0140e-03\n",
      "Epoch: 108490 | training loss: 2.2676e-03 | validation loss: 2.0141e-03\n",
      "Epoch: 108500 | training loss: 2.2676e-03 | validation loss: 2.0141e-03\n",
      "Epoch: 108510 | training loss: 2.2675e-03 | validation loss: 2.0139e-03\n",
      "Epoch: 108520 | training loss: 2.2675e-03 | validation loss: 2.0138e-03\n",
      "Epoch: 108530 | training loss: 2.2674e-03 | validation loss: 2.0138e-03\n",
      "Epoch: 108540 | training loss: 2.2674e-03 | validation loss: 2.0135e-03\n",
      "Epoch: 108550 | training loss: 2.2675e-03 | validation loss: 2.0121e-03\n",
      "Epoch: 108560 | training loss: 2.2773e-03 | validation loss: 2.0033e-03\n",
      "Epoch: 108570 | training loss: 3.4189e-03 | validation loss: 2.3893e-03\n",
      "Epoch: 108580 | training loss: 3.0732e-03 | validation loss: 2.5100e-03\n",
      "Epoch: 108590 | training loss: 2.4102e-03 | validation loss: 2.1294e-03\n",
      "Epoch: 108600 | training loss: 2.2694e-03 | validation loss: 2.0175e-03\n",
      "Epoch: 108610 | training loss: 2.2774e-03 | validation loss: 2.0021e-03\n",
      "Epoch: 108620 | training loss: 2.2792e-03 | validation loss: 2.0029e-03\n",
      "Epoch: 108630 | training loss: 2.2690e-03 | validation loss: 2.0077e-03\n",
      "Epoch: 108640 | training loss: 2.2674e-03 | validation loss: 2.0157e-03\n",
      "Epoch: 108650 | training loss: 2.2675e-03 | validation loss: 2.0167e-03\n",
      "Epoch: 108660 | training loss: 2.2670e-03 | validation loss: 2.0128e-03\n",
      "Epoch: 108670 | training loss: 2.2670e-03 | validation loss: 2.0122e-03\n",
      "Epoch: 108680 | training loss: 2.2669e-03 | validation loss: 2.0136e-03\n",
      "Epoch: 108690 | training loss: 2.2668e-03 | validation loss: 2.0128e-03\n",
      "Epoch: 108700 | training loss: 2.2668e-03 | validation loss: 2.0127e-03\n",
      "Epoch: 108710 | training loss: 2.2668e-03 | validation loss: 2.0129e-03\n",
      "Epoch: 108720 | training loss: 2.2667e-03 | validation loss: 2.0126e-03\n",
      "Epoch: 108730 | training loss: 2.2667e-03 | validation loss: 2.0126e-03\n",
      "Epoch: 108740 | training loss: 2.2666e-03 | validation loss: 2.0125e-03\n",
      "Epoch: 108750 | training loss: 2.2666e-03 | validation loss: 2.0124e-03\n",
      "Epoch: 108760 | training loss: 2.2666e-03 | validation loss: 2.0124e-03\n",
      "Epoch: 108770 | training loss: 2.2665e-03 | validation loss: 2.0123e-03\n",
      "Epoch: 108780 | training loss: 2.2665e-03 | validation loss: 2.0123e-03\n",
      "Epoch: 108790 | training loss: 2.2664e-03 | validation loss: 2.0123e-03\n",
      "Epoch: 108800 | training loss: 2.2664e-03 | validation loss: 2.0128e-03\n",
      "Epoch: 108810 | training loss: 2.2686e-03 | validation loss: 2.0193e-03\n",
      "Epoch: 108820 | training loss: 2.6201e-03 | validation loss: 2.2576e-03\n",
      "Epoch: 108830 | training loss: 2.4663e-03 | validation loss: 2.0393e-03\n",
      "Epoch: 108840 | training loss: 2.3083e-03 | validation loss: 2.0484e-03\n",
      "Epoch: 108850 | training loss: 2.3144e-03 | validation loss: 2.0609e-03\n",
      "Epoch: 108860 | training loss: 2.2974e-03 | validation loss: 2.0532e-03\n",
      "Epoch: 108870 | training loss: 2.2836e-03 | validation loss: 2.0423e-03\n",
      "Epoch: 108880 | training loss: 2.2727e-03 | validation loss: 2.0275e-03\n",
      "Epoch: 108890 | training loss: 2.2671e-03 | validation loss: 2.0162e-03\n",
      "Epoch: 108900 | training loss: 2.2660e-03 | validation loss: 2.0109e-03\n",
      "Epoch: 108910 | training loss: 2.2663e-03 | validation loss: 2.0091e-03\n",
      "Epoch: 108920 | training loss: 2.2660e-03 | validation loss: 2.0101e-03\n",
      "Epoch: 108930 | training loss: 2.2659e-03 | validation loss: 2.0120e-03\n",
      "Epoch: 108940 | training loss: 2.2659e-03 | validation loss: 2.0115e-03\n",
      "Epoch: 108950 | training loss: 2.2658e-03 | validation loss: 2.0108e-03\n",
      "Epoch: 108960 | training loss: 2.2658e-03 | validation loss: 2.0110e-03\n",
      "Epoch: 108970 | training loss: 2.2657e-03 | validation loss: 2.0111e-03\n",
      "Epoch: 108980 | training loss: 2.2657e-03 | validation loss: 2.0108e-03\n",
      "Epoch: 108990 | training loss: 2.2657e-03 | validation loss: 2.0109e-03\n",
      "Epoch: 109000 | training loss: 2.2656e-03 | validation loss: 2.0107e-03\n",
      "Epoch: 109010 | training loss: 2.2656e-03 | validation loss: 2.0107e-03\n",
      "Epoch: 109020 | training loss: 2.2655e-03 | validation loss: 2.0105e-03\n",
      "Epoch: 109030 | training loss: 2.2655e-03 | validation loss: 2.0098e-03\n",
      "Epoch: 109040 | training loss: 2.2700e-03 | validation loss: 2.0055e-03\n",
      "Epoch: 109050 | training loss: 2.8720e-03 | validation loss: 2.3443e-03\n",
      "Epoch: 109060 | training loss: 2.3415e-03 | validation loss: 2.0849e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 109070 | training loss: 2.3342e-03 | validation loss: 2.0859e-03\n",
      "Epoch: 109080 | training loss: 2.2958e-03 | validation loss: 2.0484e-03\n",
      "Epoch: 109090 | training loss: 2.2756e-03 | validation loss: 2.0254e-03\n",
      "Epoch: 109100 | training loss: 2.2673e-03 | validation loss: 2.0134e-03\n",
      "Epoch: 109110 | training loss: 2.2668e-03 | validation loss: 2.0042e-03\n",
      "Epoch: 109120 | training loss: 2.3211e-03 | validation loss: 1.9968e-03\n",
      "Epoch: 109130 | training loss: 3.3649e-03 | validation loss: 2.3583e-03\n",
      "Epoch: 109140 | training loss: 2.4985e-03 | validation loss: 2.1820e-03\n",
      "Epoch: 109150 | training loss: 2.2742e-03 | validation loss: 2.0232e-03\n",
      "Epoch: 109160 | training loss: 2.3093e-03 | validation loss: 1.9972e-03\n",
      "Epoch: 109170 | training loss: 2.2703e-03 | validation loss: 2.0210e-03\n",
      "Epoch: 109180 | training loss: 2.2653e-03 | validation loss: 2.0120e-03\n",
      "Epoch: 109190 | training loss: 2.2662e-03 | validation loss: 2.0047e-03\n",
      "Epoch: 109200 | training loss: 2.2656e-03 | validation loss: 2.0134e-03\n",
      "Epoch: 109210 | training loss: 2.2651e-03 | validation loss: 2.0066e-03\n",
      "Epoch: 109220 | training loss: 2.2648e-03 | validation loss: 2.0101e-03\n",
      "Epoch: 109230 | training loss: 2.2647e-03 | validation loss: 2.0083e-03\n",
      "Epoch: 109240 | training loss: 2.2647e-03 | validation loss: 2.0082e-03\n",
      "Epoch: 109250 | training loss: 2.2646e-03 | validation loss: 2.0089e-03\n",
      "Epoch: 109260 | training loss: 2.2646e-03 | validation loss: 2.0089e-03\n",
      "Epoch: 109270 | training loss: 2.2646e-03 | validation loss: 2.0092e-03\n",
      "Epoch: 109280 | training loss: 2.2648e-03 | validation loss: 2.0111e-03\n",
      "Epoch: 109290 | training loss: 2.2763e-03 | validation loss: 2.0293e-03\n",
      "Epoch: 109300 | training loss: 2.9834e-03 | validation loss: 2.4643e-03\n",
      "Epoch: 109310 | training loss: 2.6577e-03 | validation loss: 2.1017e-03\n",
      "Epoch: 109320 | training loss: 2.3701e-03 | validation loss: 2.0894e-03\n",
      "Epoch: 109330 | training loss: 2.3153e-03 | validation loss: 2.0631e-03\n",
      "Epoch: 109340 | training loss: 2.2750e-03 | validation loss: 2.0090e-03\n",
      "Epoch: 109350 | training loss: 2.2697e-03 | validation loss: 2.0009e-03\n",
      "Epoch: 109360 | training loss: 2.2673e-03 | validation loss: 2.0128e-03\n",
      "Epoch: 109370 | training loss: 2.2642e-03 | validation loss: 2.0086e-03\n",
      "Epoch: 109380 | training loss: 2.2643e-03 | validation loss: 2.0066e-03\n",
      "Epoch: 109390 | training loss: 2.2643e-03 | validation loss: 2.0086e-03\n",
      "Epoch: 109400 | training loss: 2.2641e-03 | validation loss: 2.0071e-03\n",
      "Epoch: 109410 | training loss: 2.2640e-03 | validation loss: 2.0079e-03\n",
      "Epoch: 109420 | training loss: 2.2640e-03 | validation loss: 2.0073e-03\n",
      "Epoch: 109430 | training loss: 2.2639e-03 | validation loss: 2.0073e-03\n",
      "Epoch: 109440 | training loss: 2.2639e-03 | validation loss: 2.0075e-03\n",
      "Epoch: 109450 | training loss: 2.2638e-03 | validation loss: 2.0072e-03\n",
      "Epoch: 109460 | training loss: 2.2638e-03 | validation loss: 2.0070e-03\n",
      "Epoch: 109470 | training loss: 2.2638e-03 | validation loss: 2.0068e-03\n",
      "Epoch: 109480 | training loss: 2.2638e-03 | validation loss: 2.0060e-03\n",
      "Epoch: 109490 | training loss: 2.2683e-03 | validation loss: 2.0019e-03\n",
      "Epoch: 109500 | training loss: 2.7142e-03 | validation loss: 2.1524e-03\n",
      "Epoch: 109510 | training loss: 2.5017e-03 | validation loss: 2.1534e-03\n",
      "Epoch: 109520 | training loss: 2.3727e-03 | validation loss: 2.0047e-03\n",
      "Epoch: 109530 | training loss: 2.3241e-03 | validation loss: 2.0444e-03\n",
      "Epoch: 109540 | training loss: 2.3018e-03 | validation loss: 2.0523e-03\n",
      "Epoch: 109550 | training loss: 2.2754e-03 | validation loss: 2.0159e-03\n",
      "Epoch: 109560 | training loss: 2.2659e-03 | validation loss: 2.0053e-03\n",
      "Epoch: 109570 | training loss: 2.2636e-03 | validation loss: 2.0093e-03\n",
      "Epoch: 109580 | training loss: 2.2637e-03 | validation loss: 2.0047e-03\n",
      "Epoch: 109590 | training loss: 2.2635e-03 | validation loss: 2.0062e-03\n",
      "Epoch: 109600 | training loss: 2.2633e-03 | validation loss: 2.0060e-03\n",
      "Epoch: 109610 | training loss: 2.2632e-03 | validation loss: 2.0064e-03\n",
      "Epoch: 109620 | training loss: 2.2632e-03 | validation loss: 2.0057e-03\n",
      "Epoch: 109630 | training loss: 2.2631e-03 | validation loss: 2.0060e-03\n",
      "Epoch: 109640 | training loss: 2.2631e-03 | validation loss: 2.0059e-03\n",
      "Epoch: 109650 | training loss: 2.2631e-03 | validation loss: 2.0057e-03\n",
      "Epoch: 109660 | training loss: 2.2630e-03 | validation loss: 2.0056e-03\n",
      "Epoch: 109670 | training loss: 2.2630e-03 | validation loss: 2.0052e-03\n",
      "Epoch: 109680 | training loss: 2.2631e-03 | validation loss: 2.0035e-03\n",
      "Epoch: 109690 | training loss: 2.2729e-03 | validation loss: 1.9919e-03\n",
      "Epoch: 109700 | training loss: 3.2376e-03 | validation loss: 2.3327e-03\n",
      "Epoch: 109710 | training loss: 2.6649e-03 | validation loss: 2.3252e-03\n",
      "Epoch: 109720 | training loss: 2.4279e-03 | validation loss: 2.1626e-03\n",
      "Epoch: 109730 | training loss: 2.3146e-03 | validation loss: 2.0680e-03\n",
      "Epoch: 109740 | training loss: 2.2820e-03 | validation loss: 2.0417e-03\n",
      "Epoch: 109750 | training loss: 2.2658e-03 | validation loss: 2.0117e-03\n",
      "Epoch: 109760 | training loss: 2.2627e-03 | validation loss: 2.0053e-03\n",
      "Epoch: 109770 | training loss: 2.2629e-03 | validation loss: 2.0037e-03\n",
      "Epoch: 109780 | training loss: 2.2629e-03 | validation loss: 2.0013e-03\n",
      "Epoch: 109790 | training loss: 2.2626e-03 | validation loss: 2.0038e-03\n",
      "Epoch: 109800 | training loss: 2.2625e-03 | validation loss: 2.0047e-03\n",
      "Epoch: 109810 | training loss: 2.2624e-03 | validation loss: 2.0046e-03\n",
      "Epoch: 109820 | training loss: 2.2624e-03 | validation loss: 2.0043e-03\n",
      "Epoch: 109830 | training loss: 2.2624e-03 | validation loss: 2.0041e-03\n",
      "Epoch: 109840 | training loss: 2.2623e-03 | validation loss: 2.0043e-03\n",
      "Epoch: 109850 | training loss: 2.2623e-03 | validation loss: 2.0042e-03\n",
      "Epoch: 109860 | training loss: 2.2623e-03 | validation loss: 2.0052e-03\n",
      "Epoch: 109870 | training loss: 2.2689e-03 | validation loss: 2.0164e-03\n",
      "Epoch: 109880 | training loss: 2.9533e-03 | validation loss: 2.4307e-03\n",
      "Epoch: 109890 | training loss: 2.8520e-03 | validation loss: 2.1948e-03\n",
      "Epoch: 109900 | training loss: 2.2752e-03 | validation loss: 2.0324e-03\n",
      "Epoch: 109910 | training loss: 2.3374e-03 | validation loss: 2.0814e-03\n",
      "Epoch: 109920 | training loss: 2.2957e-03 | validation loss: 2.0457e-03\n",
      "Epoch: 109930 | training loss: 2.2627e-03 | validation loss: 2.0072e-03\n",
      "Epoch: 109940 | training loss: 2.2652e-03 | validation loss: 1.9961e-03\n",
      "Epoch: 109950 | training loss: 2.2629e-03 | validation loss: 1.9988e-03\n",
      "Epoch: 109960 | training loss: 2.2622e-03 | validation loss: 2.0058e-03\n",
      "Epoch: 109970 | training loss: 2.2619e-03 | validation loss: 2.0049e-03\n",
      "Epoch: 109980 | training loss: 2.2619e-03 | validation loss: 2.0021e-03\n",
      "Epoch: 109990 | training loss: 2.2618e-03 | validation loss: 2.0034e-03\n",
      "Epoch: 110000 | training loss: 2.2617e-03 | validation loss: 2.0032e-03\n",
      "Epoch: 110010 | training loss: 2.2617e-03 | validation loss: 2.0029e-03\n",
      "Epoch: 110020 | training loss: 2.2616e-03 | validation loss: 2.0030e-03\n",
      "Epoch: 110030 | training loss: 2.2616e-03 | validation loss: 2.0029e-03\n",
      "Epoch: 110040 | training loss: 2.2615e-03 | validation loss: 2.0028e-03\n",
      "Epoch: 110050 | training loss: 2.2615e-03 | validation loss: 2.0028e-03\n",
      "Epoch: 110060 | training loss: 2.2615e-03 | validation loss: 2.0027e-03\n",
      "Epoch: 110070 | training loss: 2.2614e-03 | validation loss: 2.0026e-03\n",
      "Epoch: 110080 | training loss: 2.2614e-03 | validation loss: 2.0025e-03\n",
      "Epoch: 110090 | training loss: 2.2613e-03 | validation loss: 2.0023e-03\n",
      "Epoch: 110100 | training loss: 2.2613e-03 | validation loss: 2.0015e-03\n",
      "Epoch: 110110 | training loss: 2.2655e-03 | validation loss: 1.9962e-03\n",
      "Epoch: 110120 | training loss: 2.8721e-03 | validation loss: 2.1931e-03\n",
      "Epoch: 110130 | training loss: 2.9633e-03 | validation loss: 2.4384e-03\n",
      "Epoch: 110140 | training loss: 2.2755e-03 | validation loss: 2.0181e-03\n",
      "Epoch: 110150 | training loss: 2.2680e-03 | validation loss: 1.9938e-03\n",
      "Epoch: 110160 | training loss: 2.2787e-03 | validation loss: 1.9955e-03\n",
      "Epoch: 110170 | training loss: 2.2755e-03 | validation loss: 1.9953e-03\n",
      "Epoch: 110180 | training loss: 2.2663e-03 | validation loss: 1.9960e-03\n",
      "Epoch: 110190 | training loss: 2.2612e-03 | validation loss: 2.0002e-03\n",
      "Epoch: 110200 | training loss: 2.2612e-03 | validation loss: 2.0038e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 110210 | training loss: 2.2611e-03 | validation loss: 2.0036e-03\n",
      "Epoch: 110220 | training loss: 2.2608e-03 | validation loss: 2.0013e-03\n",
      "Epoch: 110230 | training loss: 2.2608e-03 | validation loss: 2.0009e-03\n",
      "Epoch: 110240 | training loss: 2.2608e-03 | validation loss: 2.0017e-03\n",
      "Epoch: 110250 | training loss: 2.2607e-03 | validation loss: 2.0015e-03\n",
      "Epoch: 110260 | training loss: 2.2607e-03 | validation loss: 2.0012e-03\n",
      "Epoch: 110270 | training loss: 2.2606e-03 | validation loss: 2.0014e-03\n",
      "Epoch: 110280 | training loss: 2.2606e-03 | validation loss: 2.0012e-03\n",
      "Epoch: 110290 | training loss: 2.2606e-03 | validation loss: 2.0013e-03\n",
      "Epoch: 110300 | training loss: 2.2605e-03 | validation loss: 2.0020e-03\n",
      "Epoch: 110310 | training loss: 2.2642e-03 | validation loss: 2.0136e-03\n",
      "Epoch: 110320 | training loss: 2.7974e-03 | validation loss: 2.4939e-03\n",
      "Epoch: 110330 | training loss: 2.4126e-03 | validation loss: 2.0570e-03\n",
      "Epoch: 110340 | training loss: 2.3472e-03 | validation loss: 2.0358e-03\n",
      "Epoch: 110350 | training loss: 2.2895e-03 | validation loss: 2.0023e-03\n",
      "Epoch: 110360 | training loss: 2.2676e-03 | validation loss: 1.9933e-03\n",
      "Epoch: 110370 | training loss: 2.2620e-03 | validation loss: 2.0043e-03\n",
      "Epoch: 110380 | training loss: 2.2607e-03 | validation loss: 1.9986e-03\n",
      "Epoch: 110390 | training loss: 2.2609e-03 | validation loss: 1.9997e-03\n",
      "Epoch: 110400 | training loss: 2.2603e-03 | validation loss: 1.9997e-03\n",
      "Epoch: 110410 | training loss: 2.2602e-03 | validation loss: 1.9979e-03\n",
      "Epoch: 110420 | training loss: 2.2619e-03 | validation loss: 1.9934e-03\n",
      "Epoch: 110430 | training loss: 2.3411e-03 | validation loss: 1.9927e-03\n",
      "Epoch: 110440 | training loss: 3.2820e-03 | validation loss: 2.3255e-03\n",
      "Epoch: 110450 | training loss: 2.3110e-03 | validation loss: 2.0431e-03\n",
      "Epoch: 110460 | training loss: 2.3465e-03 | validation loss: 2.0756e-03\n",
      "Epoch: 110470 | training loss: 2.2805e-03 | validation loss: 1.9887e-03\n",
      "Epoch: 110480 | training loss: 2.2631e-03 | validation loss: 1.9951e-03\n",
      "Epoch: 110490 | training loss: 2.2657e-03 | validation loss: 2.0140e-03\n",
      "Epoch: 110500 | training loss: 2.2610e-03 | validation loss: 1.9950e-03\n",
      "Epoch: 110510 | training loss: 2.2598e-03 | validation loss: 2.0000e-03\n",
      "Epoch: 110520 | training loss: 2.2597e-03 | validation loss: 1.9988e-03\n",
      "Epoch: 110530 | training loss: 2.2596e-03 | validation loss: 1.9995e-03\n",
      "Epoch: 110540 | training loss: 2.2596e-03 | validation loss: 1.9984e-03\n",
      "Epoch: 110550 | training loss: 2.2596e-03 | validation loss: 1.9995e-03\n",
      "Epoch: 110560 | training loss: 2.2595e-03 | validation loss: 1.9986e-03\n",
      "Epoch: 110570 | training loss: 2.2595e-03 | validation loss: 1.9984e-03\n",
      "Epoch: 110580 | training loss: 2.2594e-03 | validation loss: 1.9985e-03\n",
      "Epoch: 110590 | training loss: 2.2594e-03 | validation loss: 1.9983e-03\n",
      "Epoch: 110600 | training loss: 2.2595e-03 | validation loss: 1.9971e-03\n",
      "Epoch: 110610 | training loss: 2.2649e-03 | validation loss: 1.9909e-03\n",
      "Epoch: 110620 | training loss: 2.7628e-03 | validation loss: 2.1381e-03\n",
      "Epoch: 110630 | training loss: 2.5189e-03 | validation loss: 2.1980e-03\n",
      "Epoch: 110640 | training loss: 2.3862e-03 | validation loss: 2.0188e-03\n",
      "Epoch: 110650 | training loss: 2.3622e-03 | validation loss: 1.9952e-03\n",
      "Epoch: 110660 | training loss: 2.2698e-03 | validation loss: 1.9834e-03\n",
      "Epoch: 110670 | training loss: 2.2636e-03 | validation loss: 2.0117e-03\n",
      "Epoch: 110680 | training loss: 2.2635e-03 | validation loss: 2.0124e-03\n",
      "Epoch: 110690 | training loss: 2.2592e-03 | validation loss: 1.9954e-03\n",
      "Epoch: 110700 | training loss: 2.2595e-03 | validation loss: 1.9947e-03\n",
      "Epoch: 110710 | training loss: 2.2590e-03 | validation loss: 1.9995e-03\n",
      "Epoch: 110720 | training loss: 2.2589e-03 | validation loss: 1.9983e-03\n",
      "Epoch: 110730 | training loss: 2.2589e-03 | validation loss: 1.9968e-03\n",
      "Epoch: 110740 | training loss: 2.2588e-03 | validation loss: 1.9983e-03\n",
      "Epoch: 110750 | training loss: 2.2588e-03 | validation loss: 1.9972e-03\n",
      "Epoch: 110760 | training loss: 2.2587e-03 | validation loss: 1.9977e-03\n",
      "Epoch: 110770 | training loss: 2.2587e-03 | validation loss: 1.9973e-03\n",
      "Epoch: 110780 | training loss: 2.2586e-03 | validation loss: 1.9974e-03\n",
      "Epoch: 110790 | training loss: 2.2586e-03 | validation loss: 1.9973e-03\n",
      "Epoch: 110800 | training loss: 2.2586e-03 | validation loss: 1.9968e-03\n",
      "Epoch: 110810 | training loss: 2.2603e-03 | validation loss: 1.9940e-03\n",
      "Epoch: 110820 | training loss: 2.5227e-03 | validation loss: 2.1284e-03\n",
      "Epoch: 110830 | training loss: 2.5495e-03 | validation loss: 2.2620e-03\n",
      "Epoch: 110840 | training loss: 2.2924e-03 | validation loss: 2.0487e-03\n",
      "Epoch: 110850 | training loss: 2.2714e-03 | validation loss: 2.0210e-03\n",
      "Epoch: 110860 | training loss: 2.3300e-03 | validation loss: 2.0653e-03\n",
      "Epoch: 110870 | training loss: 2.5947e-03 | validation loss: 2.2360e-03\n",
      "Epoch: 110880 | training loss: 2.2739e-03 | validation loss: 1.9812e-03\n",
      "Epoch: 110890 | training loss: 2.2691e-03 | validation loss: 1.9831e-03\n",
      "Epoch: 110900 | training loss: 2.2798e-03 | validation loss: 2.0276e-03\n",
      "Epoch: 110910 | training loss: 2.2587e-03 | validation loss: 1.9928e-03\n",
      "Epoch: 110920 | training loss: 2.2644e-03 | validation loss: 1.9874e-03\n",
      "Epoch: 110930 | training loss: 2.2662e-03 | validation loss: 1.9862e-03\n",
      "Epoch: 110940 | training loss: 2.2972e-03 | validation loss: 1.9838e-03\n",
      "Epoch: 110950 | training loss: 2.6136e-03 | validation loss: 2.0712e-03\n",
      "Epoch: 110960 | training loss: 2.2584e-03 | validation loss: 1.9945e-03\n",
      "Epoch: 110970 | training loss: 2.2733e-03 | validation loss: 2.0211e-03\n",
      "Epoch: 110980 | training loss: 2.2794e-03 | validation loss: 1.9842e-03\n",
      "Epoch: 110990 | training loss: 2.2706e-03 | validation loss: 2.0173e-03\n",
      "Epoch: 111000 | training loss: 2.2580e-03 | validation loss: 1.9938e-03\n",
      "Epoch: 111010 | training loss: 2.2610e-03 | validation loss: 1.9889e-03\n",
      "Epoch: 111020 | training loss: 2.2595e-03 | validation loss: 1.9902e-03\n",
      "Epoch: 111030 | training loss: 2.2619e-03 | validation loss: 1.9880e-03\n",
      "Epoch: 111040 | training loss: 2.3136e-03 | validation loss: 1.9876e-03\n",
      "Epoch: 111050 | training loss: 2.9888e-03 | validation loss: 2.2180e-03\n",
      "Epoch: 111060 | training loss: 2.4814e-03 | validation loss: 2.1636e-03\n",
      "Epoch: 111070 | training loss: 2.3550e-03 | validation loss: 2.0028e-03\n",
      "Epoch: 111080 | training loss: 2.2920e-03 | validation loss: 2.0336e-03\n",
      "Epoch: 111090 | training loss: 2.2698e-03 | validation loss: 1.9859e-03\n",
      "Epoch: 111100 | training loss: 2.2627e-03 | validation loss: 2.0076e-03\n",
      "Epoch: 111110 | training loss: 2.2593e-03 | validation loss: 1.9896e-03\n",
      "Epoch: 111120 | training loss: 2.2574e-03 | validation loss: 1.9961e-03\n",
      "Epoch: 111130 | training loss: 2.2577e-03 | validation loss: 1.9973e-03\n",
      "Epoch: 111140 | training loss: 2.2573e-03 | validation loss: 1.9949e-03\n",
      "Epoch: 111150 | training loss: 2.2573e-03 | validation loss: 1.9935e-03\n",
      "Epoch: 111160 | training loss: 2.2578e-03 | validation loss: 1.9915e-03\n",
      "Epoch: 111170 | training loss: 2.2737e-03 | validation loss: 1.9855e-03\n",
      "Epoch: 111180 | training loss: 2.9666e-03 | validation loss: 2.2196e-03\n",
      "Epoch: 111190 | training loss: 2.5482e-03 | validation loss: 2.2035e-03\n",
      "Epoch: 111200 | training loss: 2.4133e-03 | validation loss: 2.0165e-03\n",
      "Epoch: 111210 | training loss: 2.2608e-03 | validation loss: 1.9849e-03\n",
      "Epoch: 111220 | training loss: 2.2820e-03 | validation loss: 2.0280e-03\n",
      "Epoch: 111230 | training loss: 2.2600e-03 | validation loss: 1.9879e-03\n",
      "Epoch: 111240 | training loss: 2.2570e-03 | validation loss: 1.9922e-03\n",
      "Epoch: 111250 | training loss: 2.2575e-03 | validation loss: 1.9978e-03\n",
      "Epoch: 111260 | training loss: 2.2572e-03 | validation loss: 1.9915e-03\n",
      "Epoch: 111270 | training loss: 2.2569e-03 | validation loss: 1.9955e-03\n",
      "Epoch: 111280 | training loss: 2.2567e-03 | validation loss: 1.9928e-03\n",
      "Epoch: 111290 | training loss: 2.2567e-03 | validation loss: 1.9938e-03\n",
      "Epoch: 111300 | training loss: 2.2566e-03 | validation loss: 1.9941e-03\n",
      "Epoch: 111310 | training loss: 2.2566e-03 | validation loss: 1.9937e-03\n",
      "Epoch: 111320 | training loss: 2.2567e-03 | validation loss: 1.9950e-03\n",
      "Epoch: 111330 | training loss: 2.2705e-03 | validation loss: 2.0187e-03\n",
      "Epoch: 111340 | training loss: 2.8178e-03 | validation loss: 2.4865e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 111350 | training loss: 2.3366e-03 | validation loss: 2.0656e-03\n",
      "Epoch: 111360 | training loss: 2.3266e-03 | validation loss: 2.0315e-03\n",
      "Epoch: 111370 | training loss: 2.2750e-03 | validation loss: 1.9858e-03\n",
      "Epoch: 111380 | training loss: 2.2689e-03 | validation loss: 1.9797e-03\n",
      "Epoch: 111390 | training loss: 2.3384e-03 | validation loss: 1.9912e-03\n",
      "Epoch: 111400 | training loss: 2.6219e-03 | validation loss: 2.0737e-03\n",
      "Epoch: 111410 | training loss: 2.2954e-03 | validation loss: 2.0401e-03\n",
      "Epoch: 111420 | training loss: 2.2572e-03 | validation loss: 1.9959e-03\n",
      "Epoch: 111430 | training loss: 2.2727e-03 | validation loss: 1.9802e-03\n",
      "Epoch: 111440 | training loss: 2.2609e-03 | validation loss: 2.0051e-03\n",
      "Epoch: 111450 | training loss: 2.2606e-03 | validation loss: 2.0046e-03\n",
      "Epoch: 111460 | training loss: 2.2573e-03 | validation loss: 1.9978e-03\n",
      "Epoch: 111470 | training loss: 2.2589e-03 | validation loss: 2.0014e-03\n",
      "Epoch: 111480 | training loss: 2.3017e-03 | validation loss: 2.0450e-03\n",
      "Epoch: 111490 | training loss: 3.0477e-03 | validation loss: 2.4905e-03\n",
      "Epoch: 111500 | training loss: 2.5187e-03 | validation loss: 2.0450e-03\n",
      "Epoch: 111510 | training loss: 2.3648e-03 | validation loss: 2.0831e-03\n",
      "Epoch: 111520 | training loss: 2.2795e-03 | validation loss: 1.9824e-03\n",
      "Epoch: 111530 | training loss: 2.2595e-03 | validation loss: 2.0028e-03\n",
      "Epoch: 111540 | training loss: 2.2566e-03 | validation loss: 1.9876e-03\n",
      "Epoch: 111550 | training loss: 2.2564e-03 | validation loss: 1.9953e-03\n",
      "Epoch: 111560 | training loss: 2.2564e-03 | validation loss: 1.9885e-03\n",
      "Epoch: 111570 | training loss: 2.2559e-03 | validation loss: 1.9937e-03\n",
      "Epoch: 111580 | training loss: 2.2556e-03 | validation loss: 1.9924e-03\n",
      "Epoch: 111590 | training loss: 2.2555e-03 | validation loss: 1.9905e-03\n",
      "Epoch: 111600 | training loss: 2.2556e-03 | validation loss: 1.9897e-03\n",
      "Epoch: 111610 | training loss: 2.2570e-03 | validation loss: 1.9866e-03\n",
      "Epoch: 111620 | training loss: 2.3002e-03 | validation loss: 1.9842e-03\n",
      "Epoch: 111630 | training loss: 3.3626e-03 | validation loss: 2.3753e-03\n",
      "Epoch: 111640 | training loss: 2.6441e-03 | validation loss: 2.2546e-03\n",
      "Epoch: 111650 | training loss: 2.2670e-03 | validation loss: 1.9871e-03\n",
      "Epoch: 111660 | training loss: 2.2851e-03 | validation loss: 1.9782e-03\n",
      "Epoch: 111670 | training loss: 2.2715e-03 | validation loss: 2.0158e-03\n",
      "Epoch: 111680 | training loss: 2.2563e-03 | validation loss: 1.9878e-03\n",
      "Epoch: 111690 | training loss: 2.2552e-03 | validation loss: 1.9887e-03\n",
      "Epoch: 111700 | training loss: 2.2553e-03 | validation loss: 1.9932e-03\n",
      "Epoch: 111710 | training loss: 2.2551e-03 | validation loss: 1.9893e-03\n",
      "Epoch: 111720 | training loss: 2.2550e-03 | validation loss: 1.9906e-03\n",
      "Epoch: 111730 | training loss: 2.2550e-03 | validation loss: 1.9912e-03\n",
      "Epoch: 111740 | training loss: 2.2549e-03 | validation loss: 1.9897e-03\n",
      "Epoch: 111750 | training loss: 2.2549e-03 | validation loss: 1.9903e-03\n",
      "Epoch: 111760 | training loss: 2.2549e-03 | validation loss: 1.9911e-03\n",
      "Epoch: 111770 | training loss: 2.2556e-03 | validation loss: 1.9949e-03\n",
      "Epoch: 111780 | training loss: 2.3144e-03 | validation loss: 2.0644e-03\n",
      "Epoch: 111790 | training loss: 2.3506e-03 | validation loss: 2.1043e-03\n",
      "Epoch: 111800 | training loss: 2.4605e-03 | validation loss: 2.1930e-03\n",
      "Epoch: 111810 | training loss: 2.3854e-03 | validation loss: 2.1022e-03\n",
      "Epoch: 111820 | training loss: 2.2904e-03 | validation loss: 1.9736e-03\n",
      "Epoch: 111830 | training loss: 2.2739e-03 | validation loss: 1.9738e-03\n",
      "Epoch: 111840 | training loss: 2.2564e-03 | validation loss: 1.9972e-03\n",
      "Epoch: 111850 | training loss: 2.2674e-03 | validation loss: 2.0154e-03\n",
      "Epoch: 111860 | training loss: 2.3178e-03 | validation loss: 2.0571e-03\n",
      "Epoch: 111870 | training loss: 2.6044e-03 | validation loss: 2.2390e-03\n",
      "Epoch: 111880 | training loss: 2.2601e-03 | validation loss: 1.9800e-03\n",
      "Epoch: 111890 | training loss: 2.2709e-03 | validation loss: 1.9781e-03\n",
      "Epoch: 111900 | training loss: 2.2788e-03 | validation loss: 2.0228e-03\n",
      "Epoch: 111910 | training loss: 2.2551e-03 | validation loss: 1.9850e-03\n",
      "Epoch: 111920 | training loss: 2.2609e-03 | validation loss: 1.9802e-03\n",
      "Epoch: 111930 | training loss: 2.2597e-03 | validation loss: 1.9808e-03\n",
      "Epoch: 111940 | training loss: 2.2740e-03 | validation loss: 1.9779e-03\n",
      "Epoch: 111950 | training loss: 2.4827e-03 | validation loss: 2.0278e-03\n",
      "Epoch: 111960 | training loss: 2.4081e-03 | validation loss: 2.0107e-03\n",
      "Epoch: 111970 | training loss: 2.3360e-03 | validation loss: 2.0624e-03\n",
      "Epoch: 111980 | training loss: 2.2932e-03 | validation loss: 1.9814e-03\n",
      "Epoch: 111990 | training loss: 2.2725e-03 | validation loss: 2.0151e-03\n",
      "Epoch: 112000 | training loss: 2.2581e-03 | validation loss: 1.9814e-03\n",
      "Epoch: 112010 | training loss: 2.2542e-03 | validation loss: 1.9862e-03\n",
      "Epoch: 112020 | training loss: 2.2553e-03 | validation loss: 1.9937e-03\n",
      "Epoch: 112030 | training loss: 2.2554e-03 | validation loss: 1.9943e-03\n",
      "Epoch: 112040 | training loss: 2.2584e-03 | validation loss: 1.9992e-03\n",
      "Epoch: 112050 | training loss: 2.3129e-03 | validation loss: 2.0477e-03\n",
      "Epoch: 112060 | training loss: 2.9854e-03 | validation loss: 2.4439e-03\n",
      "Epoch: 112070 | training loss: 2.4768e-03 | validation loss: 2.0314e-03\n",
      "Epoch: 112080 | training loss: 2.3498e-03 | validation loss: 2.0727e-03\n",
      "Epoch: 112090 | training loss: 2.2881e-03 | validation loss: 1.9801e-03\n",
      "Epoch: 112100 | training loss: 2.2662e-03 | validation loss: 2.0078e-03\n",
      "Epoch: 112110 | training loss: 2.2588e-03 | validation loss: 1.9807e-03\n",
      "Epoch: 112120 | training loss: 2.2551e-03 | validation loss: 1.9943e-03\n",
      "Epoch: 112130 | training loss: 2.2534e-03 | validation loss: 1.9863e-03\n",
      "Epoch: 112140 | training loss: 2.2538e-03 | validation loss: 1.9849e-03\n",
      "Epoch: 112150 | training loss: 2.2534e-03 | validation loss: 1.9873e-03\n",
      "Epoch: 112160 | training loss: 2.2534e-03 | validation loss: 1.9889e-03\n",
      "Epoch: 112170 | training loss: 2.2548e-03 | validation loss: 1.9943e-03\n",
      "Epoch: 112180 | training loss: 2.3115e-03 | validation loss: 2.0631e-03\n",
      "Epoch: 112190 | training loss: 2.4210e-03 | validation loss: 2.1658e-03\n",
      "Epoch: 112200 | training loss: 2.6919e-03 | validation loss: 2.3044e-03\n",
      "Epoch: 112210 | training loss: 2.3282e-03 | validation loss: 1.9783e-03\n",
      "Epoch: 112220 | training loss: 2.2696e-03 | validation loss: 1.9897e-03\n",
      "Epoch: 112230 | training loss: 2.2542e-03 | validation loss: 1.9935e-03\n",
      "Epoch: 112240 | training loss: 2.2614e-03 | validation loss: 1.9871e-03\n",
      "Epoch: 112250 | training loss: 2.2565e-03 | validation loss: 1.9970e-03\n",
      "Epoch: 112260 | training loss: 2.2536e-03 | validation loss: 1.9869e-03\n",
      "Epoch: 112270 | training loss: 2.2533e-03 | validation loss: 1.9842e-03\n",
      "Epoch: 112280 | training loss: 2.2549e-03 | validation loss: 1.9813e-03\n",
      "Epoch: 112290 | training loss: 2.2762e-03 | validation loss: 1.9749e-03\n",
      "Epoch: 112300 | training loss: 2.7386e-03 | validation loss: 2.1154e-03\n",
      "Epoch: 112310 | training loss: 2.2786e-03 | validation loss: 2.0237e-03\n",
      "Epoch: 112320 | training loss: 2.3046e-03 | validation loss: 1.9798e-03\n",
      "Epoch: 112330 | training loss: 2.2931e-03 | validation loss: 2.0288e-03\n",
      "Epoch: 112340 | training loss: 2.2708e-03 | validation loss: 1.9772e-03\n",
      "Epoch: 112350 | training loss: 2.2596e-03 | validation loss: 2.0006e-03\n",
      "Epoch: 112360 | training loss: 2.2549e-03 | validation loss: 1.9801e-03\n",
      "Epoch: 112370 | training loss: 2.2527e-03 | validation loss: 1.9882e-03\n",
      "Epoch: 112380 | training loss: 2.2526e-03 | validation loss: 1.9872e-03\n",
      "Epoch: 112390 | training loss: 2.2526e-03 | validation loss: 1.9840e-03\n",
      "Epoch: 112400 | training loss: 2.2526e-03 | validation loss: 1.9837e-03\n",
      "Epoch: 112410 | training loss: 2.2528e-03 | validation loss: 1.9829e-03\n",
      "Epoch: 112420 | training loss: 2.2574e-03 | validation loss: 1.9784e-03\n",
      "Epoch: 112430 | training loss: 2.4147e-03 | validation loss: 2.0096e-03\n",
      "Epoch: 112440 | training loss: 2.7334e-03 | validation loss: 2.1248e-03\n",
      "Epoch: 112450 | training loss: 2.2565e-03 | validation loss: 1.9927e-03\n",
      "Epoch: 112460 | training loss: 2.3100e-03 | validation loss: 2.0445e-03\n",
      "Epoch: 112470 | training loss: 2.2829e-03 | validation loss: 1.9761e-03\n",
      "Epoch: 112480 | training loss: 2.2546e-03 | validation loss: 1.9922e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 112490 | training loss: 2.2521e-03 | validation loss: 1.9865e-03\n",
      "Epoch: 112500 | training loss: 2.2522e-03 | validation loss: 1.9824e-03\n",
      "Epoch: 112510 | training loss: 2.2520e-03 | validation loss: 1.9863e-03\n",
      "Epoch: 112520 | training loss: 2.2519e-03 | validation loss: 1.9848e-03\n",
      "Epoch: 112530 | training loss: 2.2519e-03 | validation loss: 1.9837e-03\n",
      "Epoch: 112540 | training loss: 2.2519e-03 | validation loss: 1.9855e-03\n",
      "Epoch: 112550 | training loss: 2.2518e-03 | validation loss: 1.9847e-03\n",
      "Epoch: 112560 | training loss: 2.2517e-03 | validation loss: 1.9838e-03\n",
      "Epoch: 112570 | training loss: 2.2520e-03 | validation loss: 1.9816e-03\n",
      "Epoch: 112580 | training loss: 2.2759e-03 | validation loss: 1.9779e-03\n",
      "Epoch: 112590 | training loss: 2.7319e-03 | validation loss: 2.2019e-03\n",
      "Epoch: 112600 | training loss: 2.4216e-03 | validation loss: 1.9959e-03\n",
      "Epoch: 112610 | training loss: 2.2764e-03 | validation loss: 2.0079e-03\n",
      "Epoch: 112620 | training loss: 2.3015e-03 | validation loss: 2.0533e-03\n",
      "Epoch: 112630 | training loss: 2.2591e-03 | validation loss: 2.0033e-03\n",
      "Epoch: 112640 | training loss: 2.2527e-03 | validation loss: 1.9785e-03\n",
      "Epoch: 112650 | training loss: 2.2631e-03 | validation loss: 1.9703e-03\n",
      "Epoch: 112660 | training loss: 2.3906e-03 | validation loss: 1.9909e-03\n",
      "Epoch: 112670 | training loss: 2.6548e-03 | validation loss: 2.0809e-03\n",
      "Epoch: 112680 | training loss: 2.4022e-03 | validation loss: 2.1087e-03\n",
      "Epoch: 112690 | training loss: 2.3079e-03 | validation loss: 1.9756e-03\n",
      "Epoch: 112700 | training loss: 2.2740e-03 | validation loss: 2.0147e-03\n",
      "Epoch: 112710 | training loss: 2.2584e-03 | validation loss: 1.9745e-03\n",
      "Epoch: 112720 | training loss: 2.2514e-03 | validation loss: 1.9855e-03\n",
      "Epoch: 112730 | training loss: 2.2524e-03 | validation loss: 1.9887e-03\n",
      "Epoch: 112740 | training loss: 2.2511e-03 | validation loss: 1.9820e-03\n",
      "Epoch: 112750 | training loss: 2.2517e-03 | validation loss: 1.9796e-03\n",
      "Epoch: 112760 | training loss: 2.2552e-03 | validation loss: 1.9757e-03\n",
      "Epoch: 112770 | training loss: 2.3303e-03 | validation loss: 1.9804e-03\n",
      "Epoch: 112780 | training loss: 3.0735e-03 | validation loss: 2.2452e-03\n",
      "Epoch: 112790 | training loss: 2.5090e-03 | validation loss: 2.1684e-03\n",
      "Epoch: 112800 | training loss: 2.2991e-03 | validation loss: 1.9797e-03\n",
      "Epoch: 112810 | training loss: 2.2530e-03 | validation loss: 1.9894e-03\n",
      "Epoch: 112820 | training loss: 2.2509e-03 | validation loss: 1.9826e-03\n",
      "Epoch: 112830 | training loss: 2.2508e-03 | validation loss: 1.9816e-03\n",
      "Epoch: 112840 | training loss: 2.2507e-03 | validation loss: 1.9819e-03\n",
      "Epoch: 112850 | training loss: 2.2510e-03 | validation loss: 1.9846e-03\n",
      "Epoch: 112860 | training loss: 2.2509e-03 | validation loss: 1.9801e-03\n",
      "Epoch: 112870 | training loss: 2.2506e-03 | validation loss: 1.9819e-03\n",
      "Epoch: 112880 | training loss: 2.2506e-03 | validation loss: 1.9833e-03\n",
      "Epoch: 112890 | training loss: 2.2507e-03 | validation loss: 1.9842e-03\n",
      "Epoch: 112900 | training loss: 2.2532e-03 | validation loss: 1.9900e-03\n",
      "Epoch: 112910 | training loss: 2.3323e-03 | validation loss: 2.0574e-03\n",
      "Epoch: 112920 | training loss: 3.2784e-03 | validation loss: 2.5972e-03\n",
      "Epoch: 112930 | training loss: 2.4223e-03 | validation loss: 2.0111e-03\n",
      "Epoch: 112940 | training loss: 2.2604e-03 | validation loss: 1.9711e-03\n",
      "Epoch: 112950 | training loss: 2.2910e-03 | validation loss: 2.0273e-03\n",
      "Epoch: 112960 | training loss: 2.2588e-03 | validation loss: 1.9734e-03\n",
      "Epoch: 112970 | training loss: 2.2503e-03 | validation loss: 1.9827e-03\n",
      "Epoch: 112980 | training loss: 2.2504e-03 | validation loss: 1.9836e-03\n",
      "Epoch: 112990 | training loss: 2.2503e-03 | validation loss: 1.9797e-03\n",
      "Epoch: 113000 | training loss: 2.2501e-03 | validation loss: 1.9821e-03\n",
      "Epoch: 113010 | training loss: 2.2500e-03 | validation loss: 1.9815e-03\n",
      "Epoch: 113020 | training loss: 2.2500e-03 | validation loss: 1.9804e-03\n",
      "Epoch: 113030 | training loss: 2.2499e-03 | validation loss: 1.9817e-03\n",
      "Epoch: 113040 | training loss: 2.2499e-03 | validation loss: 1.9820e-03\n",
      "Epoch: 113050 | training loss: 2.2501e-03 | validation loss: 1.9840e-03\n",
      "Epoch: 113060 | training loss: 2.2722e-03 | validation loss: 2.0187e-03\n",
      "Epoch: 113070 | training loss: 2.7484e-03 | validation loss: 2.4269e-03\n",
      "Epoch: 113080 | training loss: 2.3550e-03 | validation loss: 2.0933e-03\n",
      "Epoch: 113090 | training loss: 2.2733e-03 | validation loss: 1.9874e-03\n",
      "Epoch: 113100 | training loss: 2.2841e-03 | validation loss: 1.9678e-03\n",
      "Epoch: 113110 | training loss: 2.2730e-03 | validation loss: 1.9640e-03\n",
      "Epoch: 113120 | training loss: 2.2954e-03 | validation loss: 1.9700e-03\n",
      "Epoch: 113130 | training loss: 2.4468e-03 | validation loss: 2.0081e-03\n",
      "Epoch: 113140 | training loss: 2.3134e-03 | validation loss: 1.9744e-03\n",
      "Epoch: 113150 | training loss: 2.3015e-03 | validation loss: 2.0373e-03\n",
      "Epoch: 113160 | training loss: 2.2512e-03 | validation loss: 1.9856e-03\n",
      "Epoch: 113170 | training loss: 2.2602e-03 | validation loss: 1.9694e-03\n",
      "Epoch: 113180 | training loss: 2.2689e-03 | validation loss: 1.9688e-03\n",
      "Epoch: 113190 | training loss: 2.3162e-03 | validation loss: 1.9731e-03\n",
      "Epoch: 113200 | training loss: 2.5260e-03 | validation loss: 2.0329e-03\n",
      "Epoch: 113210 | training loss: 2.2505e-03 | validation loss: 1.9756e-03\n",
      "Epoch: 113220 | training loss: 2.2954e-03 | validation loss: 2.0313e-03\n",
      "Epoch: 113230 | training loss: 2.2559e-03 | validation loss: 1.9708e-03\n",
      "Epoch: 113240 | training loss: 2.2619e-03 | validation loss: 1.9696e-03\n",
      "Epoch: 113250 | training loss: 2.2540e-03 | validation loss: 1.9720e-03\n",
      "Epoch: 113260 | training loss: 2.2592e-03 | validation loss: 1.9700e-03\n",
      "Epoch: 113270 | training loss: 2.3652e-03 | validation loss: 1.9865e-03\n",
      "Epoch: 113280 | training loss: 2.7719e-03 | validation loss: 2.1284e-03\n",
      "Epoch: 113290 | training loss: 2.4302e-03 | validation loss: 2.1186e-03\n",
      "Epoch: 113300 | training loss: 2.3158e-03 | validation loss: 1.9786e-03\n",
      "Epoch: 113310 | training loss: 2.2743e-03 | validation loss: 2.0104e-03\n",
      "Epoch: 113320 | training loss: 2.2566e-03 | validation loss: 1.9713e-03\n",
      "Epoch: 113330 | training loss: 2.2492e-03 | validation loss: 1.9818e-03\n",
      "Epoch: 113340 | training loss: 2.2499e-03 | validation loss: 1.9835e-03\n",
      "Epoch: 113350 | training loss: 2.2490e-03 | validation loss: 1.9768e-03\n",
      "Epoch: 113360 | training loss: 2.2496e-03 | validation loss: 1.9752e-03\n",
      "Epoch: 113370 | training loss: 2.2525e-03 | validation loss: 1.9723e-03\n",
      "Epoch: 113380 | training loss: 2.3047e-03 | validation loss: 1.9745e-03\n",
      "Epoch: 113390 | training loss: 3.0292e-03 | validation loss: 2.2337e-03\n",
      "Epoch: 113400 | training loss: 2.5029e-03 | validation loss: 2.1633e-03\n",
      "Epoch: 113410 | training loss: 2.3505e-03 | validation loss: 1.9850e-03\n",
      "Epoch: 113420 | training loss: 2.2769e-03 | validation loss: 2.0124e-03\n",
      "Epoch: 113430 | training loss: 2.2560e-03 | validation loss: 1.9708e-03\n",
      "Epoch: 113440 | training loss: 2.2516e-03 | validation loss: 1.9870e-03\n",
      "Epoch: 113450 | training loss: 2.2503e-03 | validation loss: 1.9730e-03\n",
      "Epoch: 113460 | training loss: 2.2491e-03 | validation loss: 1.9824e-03\n",
      "Epoch: 113470 | training loss: 2.2482e-03 | validation loss: 1.9781e-03\n",
      "Epoch: 113480 | training loss: 2.2484e-03 | validation loss: 1.9759e-03\n",
      "Epoch: 113490 | training loss: 2.2484e-03 | validation loss: 1.9759e-03\n",
      "Epoch: 113500 | training loss: 2.2488e-03 | validation loss: 1.9739e-03\n",
      "Epoch: 113510 | training loss: 2.2598e-03 | validation loss: 1.9645e-03\n",
      "Epoch: 113520 | training loss: 2.6958e-03 | validation loss: 2.0866e-03\n",
      "Epoch: 113530 | training loss: 2.4390e-03 | validation loss: 2.1804e-03\n",
      "Epoch: 113540 | training loss: 2.3694e-03 | validation loss: 1.9797e-03\n",
      "Epoch: 113550 | training loss: 2.2632e-03 | validation loss: 1.9678e-03\n",
      "Epoch: 113560 | training loss: 2.2632e-03 | validation loss: 2.0090e-03\n",
      "Epoch: 113570 | training loss: 2.2487e-03 | validation loss: 1.9826e-03\n",
      "Epoch: 113580 | training loss: 2.2507e-03 | validation loss: 1.9683e-03\n",
      "Epoch: 113590 | training loss: 2.2524e-03 | validation loss: 1.9806e-03\n",
      "Epoch: 113600 | training loss: 2.3836e-03 | validation loss: 2.0390e-03\n",
      "Epoch: 113610 | training loss: 2.4910e-03 | validation loss: 2.0262e-03\n",
      "Epoch: 113620 | training loss: 2.3093e-03 | validation loss: 2.0431e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 113630 | training loss: 2.2597e-03 | validation loss: 1.9818e-03\n",
      "Epoch: 113640 | training loss: 2.2498e-03 | validation loss: 1.9803e-03\n",
      "Epoch: 113650 | training loss: 2.2499e-03 | validation loss: 1.9693e-03\n",
      "Epoch: 113660 | training loss: 2.2498e-03 | validation loss: 1.9836e-03\n",
      "Epoch: 113670 | training loss: 2.2487e-03 | validation loss: 1.9740e-03\n",
      "Epoch: 113680 | training loss: 2.2475e-03 | validation loss: 1.9751e-03\n",
      "Epoch: 113690 | training loss: 2.2477e-03 | validation loss: 1.9786e-03\n",
      "Epoch: 113700 | training loss: 2.2480e-03 | validation loss: 1.9790e-03\n",
      "Epoch: 113710 | training loss: 2.2515e-03 | validation loss: 1.9851e-03\n",
      "Epoch: 113720 | training loss: 2.3369e-03 | validation loss: 2.0520e-03\n",
      "Epoch: 113730 | training loss: 3.0749e-03 | validation loss: 2.4780e-03\n",
      "Epoch: 113740 | training loss: 2.4670e-03 | validation loss: 2.0133e-03\n",
      "Epoch: 113750 | training loss: 2.2649e-03 | validation loss: 2.0057e-03\n",
      "Epoch: 113760 | training loss: 2.2488e-03 | validation loss: 1.9811e-03\n",
      "Epoch: 113770 | training loss: 2.2524e-03 | validation loss: 1.9671e-03\n",
      "Epoch: 113780 | training loss: 2.2501e-03 | validation loss: 1.9853e-03\n",
      "Epoch: 113790 | training loss: 2.2480e-03 | validation loss: 1.9716e-03\n",
      "Epoch: 113800 | training loss: 2.2471e-03 | validation loss: 1.9769e-03\n",
      "Epoch: 113810 | training loss: 2.2470e-03 | validation loss: 1.9765e-03\n",
      "Epoch: 113820 | training loss: 2.2471e-03 | validation loss: 1.9742e-03\n",
      "Epoch: 113830 | training loss: 2.2469e-03 | validation loss: 1.9745e-03\n",
      "Epoch: 113840 | training loss: 2.2468e-03 | validation loss: 1.9753e-03\n",
      "Epoch: 113850 | training loss: 2.2468e-03 | validation loss: 1.9754e-03\n",
      "Epoch: 113860 | training loss: 2.2468e-03 | validation loss: 1.9759e-03\n",
      "Epoch: 113870 | training loss: 2.2484e-03 | validation loss: 1.9811e-03\n",
      "Epoch: 113880 | training loss: 2.4362e-03 | validation loss: 2.1185e-03\n",
      "Epoch: 113890 | training loss: 2.3989e-03 | validation loss: 2.1003e-03\n",
      "Epoch: 113900 | training loss: 2.6792e-03 | validation loss: 2.2581e-03\n",
      "Epoch: 113910 | training loss: 2.3508e-03 | validation loss: 2.0583e-03\n",
      "Epoch: 113920 | training loss: 2.2513e-03 | validation loss: 1.9831e-03\n",
      "Epoch: 113930 | training loss: 2.2507e-03 | validation loss: 1.9705e-03\n",
      "Epoch: 113940 | training loss: 2.2533e-03 | validation loss: 1.9688e-03\n",
      "Epoch: 113950 | training loss: 2.2473e-03 | validation loss: 1.9717e-03\n",
      "Epoch: 113960 | training loss: 2.2468e-03 | validation loss: 1.9770e-03\n",
      "Epoch: 113970 | training loss: 2.2466e-03 | validation loss: 1.9762e-03\n",
      "Epoch: 113980 | training loss: 2.2464e-03 | validation loss: 1.9737e-03\n",
      "Epoch: 113990 | training loss: 2.2463e-03 | validation loss: 1.9740e-03\n",
      "Epoch: 114000 | training loss: 2.2463e-03 | validation loss: 1.9747e-03\n",
      "Epoch: 114010 | training loss: 2.2462e-03 | validation loss: 1.9740e-03\n",
      "Epoch: 114020 | training loss: 2.2462e-03 | validation loss: 1.9743e-03\n",
      "Epoch: 114030 | training loss: 2.2461e-03 | validation loss: 1.9741e-03\n",
      "Epoch: 114040 | training loss: 2.2461e-03 | validation loss: 1.9741e-03\n",
      "Epoch: 114050 | training loss: 2.2460e-03 | validation loss: 1.9741e-03\n",
      "Epoch: 114060 | training loss: 2.2460e-03 | validation loss: 1.9751e-03\n",
      "Epoch: 114070 | training loss: 2.2526e-03 | validation loss: 1.9916e-03\n",
      "Epoch: 114080 | training loss: 2.9927e-03 | validation loss: 2.6263e-03\n",
      "Epoch: 114090 | training loss: 2.2639e-03 | validation loss: 1.9645e-03\n",
      "Epoch: 114100 | training loss: 2.2775e-03 | validation loss: 1.9783e-03\n",
      "Epoch: 114110 | training loss: 2.2658e-03 | validation loss: 1.9770e-03\n",
      "Epoch: 114120 | training loss: 2.2580e-03 | validation loss: 1.9627e-03\n",
      "Epoch: 114130 | training loss: 2.2508e-03 | validation loss: 1.9752e-03\n",
      "Epoch: 114140 | training loss: 2.2468e-03 | validation loss: 1.9729e-03\n",
      "Epoch: 114150 | training loss: 2.2458e-03 | validation loss: 1.9712e-03\n",
      "Epoch: 114160 | training loss: 2.2463e-03 | validation loss: 1.9704e-03\n",
      "Epoch: 114170 | training loss: 2.2526e-03 | validation loss: 1.9638e-03\n",
      "Epoch: 114180 | training loss: 2.4481e-03 | validation loss: 1.9974e-03\n",
      "Epoch: 114190 | training loss: 2.4678e-03 | validation loss: 2.0111e-03\n",
      "Epoch: 114200 | training loss: 2.2457e-03 | validation loss: 1.9713e-03\n",
      "Epoch: 114210 | training loss: 2.2846e-03 | validation loss: 2.0167e-03\n",
      "Epoch: 114220 | training loss: 2.2731e-03 | validation loss: 1.9604e-03\n",
      "Epoch: 114230 | training loss: 2.2536e-03 | validation loss: 1.9887e-03\n",
      "Epoch: 114240 | training loss: 2.2475e-03 | validation loss: 1.9671e-03\n",
      "Epoch: 114250 | training loss: 2.2462e-03 | validation loss: 1.9771e-03\n",
      "Epoch: 114260 | training loss: 2.2458e-03 | validation loss: 1.9691e-03\n",
      "Epoch: 114270 | training loss: 2.2454e-03 | validation loss: 1.9741e-03\n",
      "Epoch: 114280 | training loss: 2.2452e-03 | validation loss: 1.9722e-03\n",
      "Epoch: 114290 | training loss: 2.2452e-03 | validation loss: 1.9709e-03\n",
      "Epoch: 114300 | training loss: 2.2452e-03 | validation loss: 1.9709e-03\n",
      "Epoch: 114310 | training loss: 2.2453e-03 | validation loss: 1.9699e-03\n",
      "Epoch: 114320 | training loss: 2.2484e-03 | validation loss: 1.9652e-03\n",
      "Epoch: 114330 | training loss: 2.3886e-03 | validation loss: 1.9860e-03\n",
      "Epoch: 114340 | training loss: 2.8407e-03 | validation loss: 2.1547e-03\n",
      "Epoch: 114350 | training loss: 2.2530e-03 | validation loss: 1.9587e-03\n",
      "Epoch: 114360 | training loss: 2.3477e-03 | validation loss: 2.0600e-03\n",
      "Epoch: 114370 | training loss: 2.2529e-03 | validation loss: 1.9725e-03\n",
      "Epoch: 114380 | training loss: 2.2509e-03 | validation loss: 1.9630e-03\n",
      "Epoch: 114390 | training loss: 2.2498e-03 | validation loss: 1.9805e-03\n",
      "Epoch: 114400 | training loss: 2.2458e-03 | validation loss: 1.9702e-03\n",
      "Epoch: 114410 | training loss: 2.2448e-03 | validation loss: 1.9714e-03\n",
      "Epoch: 114420 | training loss: 2.2447e-03 | validation loss: 1.9713e-03\n",
      "Epoch: 114430 | training loss: 2.2446e-03 | validation loss: 1.9713e-03\n",
      "Epoch: 114440 | training loss: 2.2446e-03 | validation loss: 1.9707e-03\n",
      "Epoch: 114450 | training loss: 2.2446e-03 | validation loss: 1.9712e-03\n",
      "Epoch: 114460 | training loss: 2.2445e-03 | validation loss: 1.9711e-03\n",
      "Epoch: 114470 | training loss: 2.2445e-03 | validation loss: 1.9707e-03\n",
      "Epoch: 114480 | training loss: 2.2444e-03 | validation loss: 1.9704e-03\n",
      "Epoch: 114490 | training loss: 2.2445e-03 | validation loss: 1.9698e-03\n",
      "Epoch: 114500 | training loss: 2.2467e-03 | validation loss: 1.9667e-03\n",
      "Epoch: 114510 | training loss: 2.3927e-03 | validation loss: 2.0028e-03\n",
      "Epoch: 114520 | training loss: 2.8224e-03 | validation loss: 2.1273e-03\n",
      "Epoch: 114530 | training loss: 2.4283e-03 | validation loss: 2.0944e-03\n",
      "Epoch: 114540 | training loss: 2.2674e-03 | validation loss: 2.0113e-03\n",
      "Epoch: 114550 | training loss: 2.2700e-03 | validation loss: 1.9706e-03\n",
      "Epoch: 114560 | training loss: 2.2573e-03 | validation loss: 1.9729e-03\n",
      "Epoch: 114570 | training loss: 2.2453e-03 | validation loss: 1.9738e-03\n",
      "Epoch: 114580 | training loss: 2.2452e-03 | validation loss: 1.9684e-03\n",
      "Epoch: 114590 | training loss: 2.2446e-03 | validation loss: 1.9734e-03\n",
      "Epoch: 114600 | training loss: 2.2441e-03 | validation loss: 1.9684e-03\n",
      "Epoch: 114610 | training loss: 2.2440e-03 | validation loss: 1.9703e-03\n",
      "Epoch: 114620 | training loss: 2.2439e-03 | validation loss: 1.9700e-03\n",
      "Epoch: 114630 | training loss: 2.2439e-03 | validation loss: 1.9693e-03\n",
      "Epoch: 114640 | training loss: 2.2438e-03 | validation loss: 1.9696e-03\n",
      "Epoch: 114650 | training loss: 2.2438e-03 | validation loss: 1.9697e-03\n",
      "Epoch: 114660 | training loss: 2.2437e-03 | validation loss: 1.9698e-03\n",
      "Epoch: 114670 | training loss: 2.2438e-03 | validation loss: 1.9711e-03\n",
      "Epoch: 114680 | training loss: 2.2476e-03 | validation loss: 1.9833e-03\n",
      "Epoch: 114690 | training loss: 2.6599e-03 | validation loss: 2.3083e-03\n",
      "Epoch: 114700 | training loss: 2.5515e-03 | validation loss: 2.0643e-03\n",
      "Epoch: 114710 | training loss: 2.3045e-03 | validation loss: 2.0129e-03\n",
      "Epoch: 114720 | training loss: 2.2562e-03 | validation loss: 1.9795e-03\n",
      "Epoch: 114730 | training loss: 2.2525e-03 | validation loss: 1.9867e-03\n",
      "Epoch: 114740 | training loss: 2.2519e-03 | validation loss: 1.9905e-03\n",
      "Epoch: 114750 | training loss: 2.2472e-03 | validation loss: 1.9767e-03\n",
      "Epoch: 114760 | training loss: 2.2440e-03 | validation loss: 1.9737e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 114770 | training loss: 2.2434e-03 | validation loss: 1.9671e-03\n",
      "Epoch: 114780 | training loss: 2.2435e-03 | validation loss: 1.9674e-03\n",
      "Epoch: 114790 | training loss: 2.2433e-03 | validation loss: 1.9674e-03\n",
      "Epoch: 114800 | training loss: 2.2432e-03 | validation loss: 1.9690e-03\n",
      "Epoch: 114810 | training loss: 2.2432e-03 | validation loss: 1.9686e-03\n",
      "Epoch: 114820 | training loss: 2.2432e-03 | validation loss: 1.9679e-03\n",
      "Epoch: 114830 | training loss: 2.2431e-03 | validation loss: 1.9681e-03\n",
      "Epoch: 114840 | training loss: 2.2431e-03 | validation loss: 1.9677e-03\n",
      "Epoch: 114850 | training loss: 2.2433e-03 | validation loss: 1.9662e-03\n",
      "Epoch: 114860 | training loss: 2.2637e-03 | validation loss: 1.9623e-03\n",
      "Epoch: 114870 | training loss: 3.8036e-03 | validation loss: 2.5744e-03\n",
      "Epoch: 114880 | training loss: 2.7105e-03 | validation loss: 2.2869e-03\n",
      "Epoch: 114890 | training loss: 2.4141e-03 | validation loss: 2.1018e-03\n",
      "Epoch: 114900 | training loss: 2.2465e-03 | validation loss: 1.9702e-03\n",
      "Epoch: 114910 | training loss: 2.2583e-03 | validation loss: 1.9541e-03\n",
      "Epoch: 114920 | training loss: 2.2520e-03 | validation loss: 1.9566e-03\n",
      "Epoch: 114930 | training loss: 2.2428e-03 | validation loss: 1.9672e-03\n",
      "Epoch: 114940 | training loss: 2.2440e-03 | validation loss: 1.9734e-03\n",
      "Epoch: 114950 | training loss: 2.2427e-03 | validation loss: 1.9679e-03\n",
      "Epoch: 114960 | training loss: 2.2428e-03 | validation loss: 1.9659e-03\n",
      "Epoch: 114970 | training loss: 2.2427e-03 | validation loss: 1.9679e-03\n",
      "Epoch: 114980 | training loss: 2.2426e-03 | validation loss: 1.9671e-03\n",
      "Epoch: 114990 | training loss: 2.2425e-03 | validation loss: 1.9672e-03\n",
      "Epoch: 115000 | training loss: 2.2425e-03 | validation loss: 1.9671e-03\n",
      "Epoch: 115010 | training loss: 2.2425e-03 | validation loss: 1.9671e-03\n",
      "Epoch: 115020 | training loss: 2.2424e-03 | validation loss: 1.9669e-03\n",
      "Epoch: 115030 | training loss: 2.2424e-03 | validation loss: 1.9670e-03\n",
      "Epoch: 115040 | training loss: 2.2423e-03 | validation loss: 1.9668e-03\n",
      "Epoch: 115050 | training loss: 2.2423e-03 | validation loss: 1.9668e-03\n",
      "Epoch: 115060 | training loss: 2.2423e-03 | validation loss: 1.9667e-03\n",
      "Epoch: 115070 | training loss: 2.2422e-03 | validation loss: 1.9666e-03\n",
      "Epoch: 115080 | training loss: 2.2422e-03 | validation loss: 1.9661e-03\n",
      "Epoch: 115090 | training loss: 2.2431e-03 | validation loss: 1.9632e-03\n",
      "Epoch: 115100 | training loss: 2.3570e-03 | validation loss: 1.9792e-03\n",
      "Epoch: 115110 | training loss: 2.9970e-03 | validation loss: 2.2080e-03\n",
      "Epoch: 115120 | training loss: 2.7390e-03 | validation loss: 2.1227e-03\n",
      "Epoch: 115130 | training loss: 2.3458e-03 | validation loss: 1.9859e-03\n",
      "Epoch: 115140 | training loss: 2.2524e-03 | validation loss: 1.9618e-03\n",
      "Epoch: 115150 | training loss: 2.2423e-03 | validation loss: 1.9671e-03\n",
      "Epoch: 115160 | training loss: 2.2461e-03 | validation loss: 1.9745e-03\n",
      "Epoch: 115170 | training loss: 2.2449e-03 | validation loss: 1.9732e-03\n",
      "Epoch: 115180 | training loss: 2.2421e-03 | validation loss: 1.9674e-03\n",
      "Epoch: 115190 | training loss: 2.2420e-03 | validation loss: 1.9649e-03\n",
      "Epoch: 115200 | training loss: 2.2419e-03 | validation loss: 1.9649e-03\n",
      "Epoch: 115210 | training loss: 2.2417e-03 | validation loss: 1.9662e-03\n",
      "Epoch: 115220 | training loss: 2.2417e-03 | validation loss: 1.9660e-03\n",
      "Epoch: 115230 | training loss: 2.2416e-03 | validation loss: 1.9654e-03\n",
      "Epoch: 115240 | training loss: 2.2416e-03 | validation loss: 1.9657e-03\n",
      "Epoch: 115250 | training loss: 2.2416e-03 | validation loss: 1.9656e-03\n",
      "Epoch: 115260 | training loss: 2.2415e-03 | validation loss: 1.9655e-03\n",
      "Epoch: 115270 | training loss: 2.2415e-03 | validation loss: 1.9654e-03\n",
      "Epoch: 115280 | training loss: 2.2414e-03 | validation loss: 1.9651e-03\n",
      "Epoch: 115290 | training loss: 2.2415e-03 | validation loss: 1.9636e-03\n",
      "Epoch: 115300 | training loss: 2.2550e-03 | validation loss: 1.9551e-03\n",
      "Epoch: 115310 | training loss: 3.0324e-03 | validation loss: 2.3318e-03\n",
      "Epoch: 115320 | training loss: 2.3116e-03 | validation loss: 2.0182e-03\n",
      "Epoch: 115330 | training loss: 2.2578e-03 | validation loss: 1.9578e-03\n",
      "Epoch: 115340 | training loss: 2.2516e-03 | validation loss: 1.9890e-03\n",
      "Epoch: 115350 | training loss: 2.2476e-03 | validation loss: 1.9741e-03\n",
      "Epoch: 115360 | training loss: 2.2448e-03 | validation loss: 1.9691e-03\n",
      "Epoch: 115370 | training loss: 2.2424e-03 | validation loss: 1.9716e-03\n",
      "Epoch: 115380 | training loss: 2.2412e-03 | validation loss: 1.9657e-03\n",
      "Epoch: 115390 | training loss: 2.2412e-03 | validation loss: 1.9630e-03\n",
      "Epoch: 115400 | training loss: 2.2411e-03 | validation loss: 1.9626e-03\n",
      "Epoch: 115410 | training loss: 2.2420e-03 | validation loss: 1.9599e-03\n",
      "Epoch: 115420 | training loss: 2.2927e-03 | validation loss: 1.9530e-03\n",
      "Epoch: 115430 | training loss: 3.5836e-03 | validation loss: 2.4177e-03\n",
      "Epoch: 115440 | training loss: 2.4005e-03 | validation loss: 2.0821e-03\n",
      "Epoch: 115450 | training loss: 2.3299e-03 | validation loss: 2.0421e-03\n",
      "Epoch: 115460 | training loss: 2.2598e-03 | validation loss: 1.9550e-03\n",
      "Epoch: 115470 | training loss: 2.2509e-03 | validation loss: 1.9580e-03\n",
      "Epoch: 115480 | training loss: 2.2462e-03 | validation loss: 1.9780e-03\n",
      "Epoch: 115490 | training loss: 2.2407e-03 | validation loss: 1.9638e-03\n",
      "Epoch: 115500 | training loss: 2.2412e-03 | validation loss: 1.9602e-03\n",
      "Epoch: 115510 | training loss: 2.2409e-03 | validation loss: 1.9658e-03\n",
      "Epoch: 115520 | training loss: 2.2407e-03 | validation loss: 1.9622e-03\n",
      "Epoch: 115530 | training loss: 2.2406e-03 | validation loss: 1.9644e-03\n",
      "Epoch: 115540 | training loss: 2.2405e-03 | validation loss: 1.9626e-03\n",
      "Epoch: 115550 | training loss: 2.2404e-03 | validation loss: 1.9635e-03\n",
      "Epoch: 115560 | training loss: 2.2404e-03 | validation loss: 1.9633e-03\n",
      "Epoch: 115570 | training loss: 2.2404e-03 | validation loss: 1.9630e-03\n",
      "Epoch: 115580 | training loss: 2.2403e-03 | validation loss: 1.9628e-03\n",
      "Epoch: 115590 | training loss: 2.2403e-03 | validation loss: 1.9626e-03\n",
      "Epoch: 115600 | training loss: 2.2404e-03 | validation loss: 1.9614e-03\n",
      "Epoch: 115610 | training loss: 2.2462e-03 | validation loss: 1.9553e-03\n",
      "Epoch: 115620 | training loss: 2.7348e-03 | validation loss: 2.1014e-03\n",
      "Epoch: 115630 | training loss: 2.4533e-03 | validation loss: 2.1316e-03\n",
      "Epoch: 115640 | training loss: 2.4025e-03 | validation loss: 1.9904e-03\n",
      "Epoch: 115650 | training loss: 2.3318e-03 | validation loss: 1.9575e-03\n",
      "Epoch: 115660 | training loss: 2.2417e-03 | validation loss: 1.9553e-03\n",
      "Epoch: 115670 | training loss: 2.2508e-03 | validation loss: 1.9853e-03\n",
      "Epoch: 115680 | training loss: 2.2416e-03 | validation loss: 1.9698e-03\n",
      "Epoch: 115690 | training loss: 2.2412e-03 | validation loss: 1.9568e-03\n",
      "Epoch: 115700 | training loss: 2.2400e-03 | validation loss: 1.9617e-03\n",
      "Epoch: 115710 | training loss: 2.2401e-03 | validation loss: 1.9645e-03\n",
      "Epoch: 115720 | training loss: 2.2399e-03 | validation loss: 1.9612e-03\n",
      "Epoch: 115730 | training loss: 2.2398e-03 | validation loss: 1.9624e-03\n",
      "Epoch: 115740 | training loss: 2.2397e-03 | validation loss: 1.9620e-03\n",
      "Epoch: 115750 | training loss: 2.2397e-03 | validation loss: 1.9621e-03\n",
      "Epoch: 115760 | training loss: 2.2397e-03 | validation loss: 1.9619e-03\n",
      "Epoch: 115770 | training loss: 2.2396e-03 | validation loss: 1.9620e-03\n",
      "Epoch: 115780 | training loss: 2.2396e-03 | validation loss: 1.9618e-03\n",
      "Epoch: 115790 | training loss: 2.2396e-03 | validation loss: 1.9621e-03\n",
      "Epoch: 115800 | training loss: 2.2402e-03 | validation loss: 1.9651e-03\n",
      "Epoch: 115810 | training loss: 2.3319e-03 | validation loss: 2.0609e-03\n",
      "Epoch: 115820 | training loss: 2.2506e-03 | validation loss: 1.9668e-03\n",
      "Epoch: 115830 | training loss: 2.3140e-03 | validation loss: 2.0449e-03\n",
      "Epoch: 115840 | training loss: 2.2850e-03 | validation loss: 2.0189e-03\n",
      "Epoch: 115850 | training loss: 2.2575e-03 | validation loss: 1.9954e-03\n",
      "Epoch: 115860 | training loss: 2.3660e-03 | validation loss: 2.0773e-03\n",
      "Epoch: 115870 | training loss: 2.6867e-03 | validation loss: 2.2677e-03\n",
      "Epoch: 115880 | training loss: 2.3887e-03 | validation loss: 1.9702e-03\n",
      "Epoch: 115890 | training loss: 2.2855e-03 | validation loss: 2.0124e-03\n",
      "Epoch: 115900 | training loss: 2.2560e-03 | validation loss: 1.9483e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 115910 | training loss: 2.2473e-03 | validation loss: 1.9779e-03\n",
      "Epoch: 115920 | training loss: 2.2427e-03 | validation loss: 1.9537e-03\n",
      "Epoch: 115930 | training loss: 2.2393e-03 | validation loss: 1.9632e-03\n",
      "Epoch: 115940 | training loss: 2.2396e-03 | validation loss: 1.9643e-03\n",
      "Epoch: 115950 | training loss: 2.2390e-03 | validation loss: 1.9603e-03\n",
      "Epoch: 115960 | training loss: 2.2391e-03 | validation loss: 1.9586e-03\n",
      "Epoch: 115970 | training loss: 2.2404e-03 | validation loss: 1.9553e-03\n",
      "Epoch: 115980 | training loss: 2.2788e-03 | validation loss: 1.9493e-03\n",
      "Epoch: 115990 | training loss: 3.2674e-03 | validation loss: 2.2945e-03\n",
      "Epoch: 116000 | training loss: 2.6258e-03 | validation loss: 2.2234e-03\n",
      "Epoch: 116010 | training loss: 2.2748e-03 | validation loss: 1.9594e-03\n",
      "Epoch: 116020 | training loss: 2.2520e-03 | validation loss: 1.9537e-03\n",
      "Epoch: 116030 | training loss: 2.2566e-03 | validation loss: 1.9844e-03\n",
      "Epoch: 116040 | training loss: 2.2433e-03 | validation loss: 1.9519e-03\n",
      "Epoch: 116050 | training loss: 2.2392e-03 | validation loss: 1.9640e-03\n",
      "Epoch: 116060 | training loss: 2.2387e-03 | validation loss: 1.9588e-03\n",
      "Epoch: 116070 | training loss: 2.2386e-03 | validation loss: 1.9606e-03\n",
      "Epoch: 116080 | training loss: 2.2386e-03 | validation loss: 1.9586e-03\n",
      "Epoch: 116090 | training loss: 2.2385e-03 | validation loss: 1.9606e-03\n",
      "Epoch: 116100 | training loss: 2.2384e-03 | validation loss: 1.9596e-03\n",
      "Epoch: 116110 | training loss: 2.2384e-03 | validation loss: 1.9590e-03\n",
      "Epoch: 116120 | training loss: 2.2384e-03 | validation loss: 1.9588e-03\n",
      "Epoch: 116130 | training loss: 2.2385e-03 | validation loss: 1.9576e-03\n",
      "Epoch: 116140 | training loss: 2.2432e-03 | validation loss: 1.9524e-03\n",
      "Epoch: 116150 | training loss: 2.5002e-03 | validation loss: 2.0159e-03\n",
      "Epoch: 116160 | training loss: 2.3066e-03 | validation loss: 1.9628e-03\n",
      "Epoch: 116170 | training loss: 2.4407e-03 | validation loss: 1.9919e-03\n",
      "Epoch: 116180 | training loss: 2.2682e-03 | validation loss: 1.9930e-03\n",
      "Epoch: 116190 | training loss: 2.2626e-03 | validation loss: 1.9942e-03\n",
      "Epoch: 116200 | training loss: 2.2451e-03 | validation loss: 1.9501e-03\n",
      "Epoch: 116210 | training loss: 2.2386e-03 | validation loss: 1.9555e-03\n",
      "Epoch: 116220 | training loss: 2.2397e-03 | validation loss: 1.9657e-03\n",
      "Epoch: 116230 | training loss: 2.2385e-03 | validation loss: 1.9556e-03\n",
      "Epoch: 116240 | training loss: 2.2380e-03 | validation loss: 1.9603e-03\n",
      "Epoch: 116250 | training loss: 2.2379e-03 | validation loss: 1.9581e-03\n",
      "Epoch: 116260 | training loss: 2.2378e-03 | validation loss: 1.9592e-03\n",
      "Epoch: 116270 | training loss: 2.2378e-03 | validation loss: 1.9579e-03\n",
      "Epoch: 116280 | training loss: 2.2377e-03 | validation loss: 1.9587e-03\n",
      "Epoch: 116290 | training loss: 2.2377e-03 | validation loss: 1.9582e-03\n",
      "Epoch: 116300 | training loss: 2.2378e-03 | validation loss: 1.9566e-03\n",
      "Epoch: 116310 | training loss: 2.2505e-03 | validation loss: 1.9526e-03\n",
      "Epoch: 116320 | training loss: 2.7712e-03 | validation loss: 2.2346e-03\n",
      "Epoch: 116330 | training loss: 2.3013e-03 | validation loss: 1.9831e-03\n",
      "Epoch: 116340 | training loss: 2.3137e-03 | validation loss: 2.0093e-03\n",
      "Epoch: 116350 | training loss: 2.2495e-03 | validation loss: 1.9574e-03\n",
      "Epoch: 116360 | training loss: 2.2577e-03 | validation loss: 1.9420e-03\n",
      "Epoch: 116370 | training loss: 2.4075e-03 | validation loss: 1.9699e-03\n",
      "Epoch: 116380 | training loss: 2.4789e-03 | validation loss: 1.9970e-03\n",
      "Epoch: 116390 | training loss: 2.3532e-03 | validation loss: 2.0633e-03\n",
      "Epoch: 116400 | training loss: 2.2797e-03 | validation loss: 1.9478e-03\n",
      "Epoch: 116410 | training loss: 2.2447e-03 | validation loss: 1.9731e-03\n",
      "Epoch: 116420 | training loss: 2.2379e-03 | validation loss: 1.9608e-03\n",
      "Epoch: 116430 | training loss: 2.2408e-03 | validation loss: 1.9502e-03\n",
      "Epoch: 116440 | training loss: 2.2381e-03 | validation loss: 1.9532e-03\n",
      "Epoch: 116450 | training loss: 2.2374e-03 | validation loss: 1.9548e-03\n",
      "Epoch: 116460 | training loss: 2.2384e-03 | validation loss: 1.9524e-03\n",
      "Epoch: 116470 | training loss: 2.2703e-03 | validation loss: 1.9459e-03\n",
      "Epoch: 116480 | training loss: 3.1999e-03 | validation loss: 2.2670e-03\n",
      "Epoch: 116490 | training loss: 2.6290e-03 | validation loss: 2.2251e-03\n",
      "Epoch: 116500 | training loss: 2.3067e-03 | validation loss: 1.9665e-03\n",
      "Epoch: 116510 | training loss: 2.2420e-03 | validation loss: 1.9484e-03\n",
      "Epoch: 116520 | training loss: 2.2544e-03 | validation loss: 1.9784e-03\n",
      "Epoch: 116530 | training loss: 2.2427e-03 | validation loss: 1.9506e-03\n",
      "Epoch: 116540 | training loss: 2.2379e-03 | validation loss: 1.9623e-03\n",
      "Epoch: 116550 | training loss: 2.2370e-03 | validation loss: 1.9540e-03\n",
      "Epoch: 116560 | training loss: 2.2368e-03 | validation loss: 1.9584e-03\n",
      "Epoch: 116570 | training loss: 2.2368e-03 | validation loss: 1.9550e-03\n",
      "Epoch: 116580 | training loss: 2.2367e-03 | validation loss: 1.9574e-03\n",
      "Epoch: 116590 | training loss: 2.2366e-03 | validation loss: 1.9564e-03\n",
      "Epoch: 116600 | training loss: 2.2366e-03 | validation loss: 1.9559e-03\n",
      "Epoch: 116610 | training loss: 2.2365e-03 | validation loss: 1.9555e-03\n",
      "Epoch: 116620 | training loss: 2.2366e-03 | validation loss: 1.9545e-03\n",
      "Epoch: 116630 | training loss: 2.2404e-03 | validation loss: 1.9500e-03\n",
      "Epoch: 116640 | training loss: 2.4409e-03 | validation loss: 1.9950e-03\n",
      "Epoch: 116650 | training loss: 2.4771e-03 | validation loss: 2.0082e-03\n",
      "Epoch: 116660 | training loss: 2.3701e-03 | validation loss: 1.9709e-03\n",
      "Epoch: 116670 | training loss: 2.2984e-03 | validation loss: 2.0163e-03\n",
      "Epoch: 116680 | training loss: 2.2516e-03 | validation loss: 1.9789e-03\n",
      "Epoch: 116690 | training loss: 2.2474e-03 | validation loss: 1.9472e-03\n",
      "Epoch: 116700 | training loss: 2.2362e-03 | validation loss: 1.9549e-03\n",
      "Epoch: 116710 | training loss: 2.2376e-03 | validation loss: 1.9613e-03\n",
      "Epoch: 116720 | training loss: 2.2368e-03 | validation loss: 1.9526e-03\n",
      "Epoch: 116730 | training loss: 2.2363e-03 | validation loss: 1.9574e-03\n",
      "Epoch: 116740 | training loss: 2.2361e-03 | validation loss: 1.9546e-03\n",
      "Epoch: 116750 | training loss: 2.2360e-03 | validation loss: 1.9561e-03\n",
      "Epoch: 116760 | training loss: 2.2360e-03 | validation loss: 1.9549e-03\n",
      "Epoch: 116770 | training loss: 2.2359e-03 | validation loss: 1.9556e-03\n",
      "Epoch: 116780 | training loss: 2.2359e-03 | validation loss: 1.9555e-03\n",
      "Epoch: 116790 | training loss: 2.2359e-03 | validation loss: 1.9563e-03\n",
      "Epoch: 116800 | training loss: 2.2443e-03 | validation loss: 1.9743e-03\n",
      "Epoch: 116810 | training loss: 2.9872e-03 | validation loss: 2.6017e-03\n",
      "Epoch: 116820 | training loss: 2.2684e-03 | validation loss: 2.0005e-03\n",
      "Epoch: 116830 | training loss: 2.2644e-03 | validation loss: 1.9412e-03\n",
      "Epoch: 116840 | training loss: 2.2486e-03 | validation loss: 1.9469e-03\n",
      "Epoch: 116850 | training loss: 2.2461e-03 | validation loss: 1.9600e-03\n",
      "Epoch: 116860 | training loss: 2.2456e-03 | validation loss: 1.9658e-03\n",
      "Epoch: 116870 | training loss: 2.2676e-03 | validation loss: 1.9932e-03\n",
      "Epoch: 116880 | training loss: 2.5448e-03 | validation loss: 2.1829e-03\n",
      "Epoch: 116890 | training loss: 2.2495e-03 | validation loss: 1.9777e-03\n",
      "Epoch: 116900 | training loss: 2.2644e-03 | validation loss: 1.9423e-03\n",
      "Epoch: 116910 | training loss: 2.2613e-03 | validation loss: 1.9901e-03\n",
      "Epoch: 116920 | training loss: 2.2489e-03 | validation loss: 1.9433e-03\n",
      "Epoch: 116930 | training loss: 2.2358e-03 | validation loss: 1.9576e-03\n",
      "Epoch: 116940 | training loss: 2.2383e-03 | validation loss: 1.9632e-03\n",
      "Epoch: 116950 | training loss: 2.2357e-03 | validation loss: 1.9570e-03\n",
      "Epoch: 116960 | training loss: 2.2354e-03 | validation loss: 1.9556e-03\n",
      "Epoch: 116970 | training loss: 2.2363e-03 | validation loss: 1.9591e-03\n",
      "Epoch: 116980 | training loss: 2.2703e-03 | validation loss: 1.9977e-03\n",
      "Epoch: 116990 | training loss: 3.3751e-03 | validation loss: 2.6410e-03\n",
      "Epoch: 117000 | training loss: 2.6638e-03 | validation loss: 2.0714e-03\n",
      "Epoch: 117010 | training loss: 2.2397e-03 | validation loss: 1.9596e-03\n",
      "Epoch: 117020 | training loss: 2.2818e-03 | validation loss: 1.9998e-03\n",
      "Epoch: 117030 | training loss: 2.2449e-03 | validation loss: 1.9435e-03\n",
      "Epoch: 117040 | training loss: 2.2353e-03 | validation loss: 1.9519e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 117050 | training loss: 2.2369e-03 | validation loss: 1.9608e-03\n",
      "Epoch: 117060 | training loss: 2.2359e-03 | validation loss: 1.9494e-03\n",
      "Epoch: 117070 | training loss: 2.2352e-03 | validation loss: 1.9557e-03\n",
      "Epoch: 117080 | training loss: 2.2349e-03 | validation loss: 1.9518e-03\n",
      "Epoch: 117090 | training loss: 2.2348e-03 | validation loss: 1.9541e-03\n",
      "Epoch: 117100 | training loss: 2.2347e-03 | validation loss: 1.9527e-03\n",
      "Epoch: 117110 | training loss: 2.2347e-03 | validation loss: 1.9527e-03\n",
      "Epoch: 117120 | training loss: 2.2346e-03 | validation loss: 1.9530e-03\n",
      "Epoch: 117130 | training loss: 2.2346e-03 | validation loss: 1.9532e-03\n",
      "Epoch: 117140 | training loss: 2.2346e-03 | validation loss: 1.9536e-03\n",
      "Epoch: 117150 | training loss: 2.2352e-03 | validation loss: 1.9566e-03\n",
      "Epoch: 117160 | training loss: 2.2699e-03 | validation loss: 1.9940e-03\n",
      "Epoch: 117170 | training loss: 3.7353e-03 | validation loss: 2.8203e-03\n",
      "Epoch: 117180 | training loss: 2.5725e-03 | validation loss: 2.0411e-03\n",
      "Epoch: 117190 | training loss: 2.3352e-03 | validation loss: 1.9524e-03\n",
      "Epoch: 117200 | training loss: 2.2472e-03 | validation loss: 1.9701e-03\n",
      "Epoch: 117210 | training loss: 2.2562e-03 | validation loss: 1.9857e-03\n",
      "Epoch: 117220 | training loss: 2.2348e-03 | validation loss: 1.9496e-03\n",
      "Epoch: 117230 | training loss: 2.2368e-03 | validation loss: 1.9455e-03\n",
      "Epoch: 117240 | training loss: 2.2349e-03 | validation loss: 1.9568e-03\n",
      "Epoch: 117250 | training loss: 2.2342e-03 | validation loss: 1.9523e-03\n",
      "Epoch: 117260 | training loss: 2.2342e-03 | validation loss: 1.9511e-03\n",
      "Epoch: 117270 | training loss: 2.2342e-03 | validation loss: 1.9531e-03\n",
      "Epoch: 117280 | training loss: 2.2341e-03 | validation loss: 1.9513e-03\n",
      "Epoch: 117290 | training loss: 2.2340e-03 | validation loss: 1.9524e-03\n",
      "Epoch: 117300 | training loss: 2.2340e-03 | validation loss: 1.9518e-03\n",
      "Epoch: 117310 | training loss: 2.2339e-03 | validation loss: 1.9518e-03\n",
      "Epoch: 117320 | training loss: 2.2339e-03 | validation loss: 1.9521e-03\n",
      "Epoch: 117330 | training loss: 2.2340e-03 | validation loss: 1.9532e-03\n",
      "Epoch: 117340 | training loss: 2.2426e-03 | validation loss: 1.9697e-03\n",
      "Epoch: 117350 | training loss: 2.7906e-03 | validation loss: 2.4338e-03\n",
      "Epoch: 117360 | training loss: 2.3273e-03 | validation loss: 2.0083e-03\n",
      "Epoch: 117370 | training loss: 2.3214e-03 | validation loss: 2.0012e-03\n",
      "Epoch: 117380 | training loss: 2.2575e-03 | validation loss: 1.9735e-03\n",
      "Epoch: 117390 | training loss: 2.2758e-03 | validation loss: 2.0053e-03\n",
      "Epoch: 117400 | training loss: 2.4674e-03 | validation loss: 2.1421e-03\n",
      "Epoch: 117410 | training loss: 2.2769e-03 | validation loss: 2.0053e-03\n",
      "Epoch: 117420 | training loss: 2.2977e-03 | validation loss: 1.9421e-03\n",
      "Epoch: 117430 | training loss: 2.2402e-03 | validation loss: 1.9644e-03\n",
      "Epoch: 117440 | training loss: 2.2438e-03 | validation loss: 1.9702e-03\n",
      "Epoch: 117450 | training loss: 2.2335e-03 | validation loss: 1.9515e-03\n",
      "Epoch: 117460 | training loss: 2.2353e-03 | validation loss: 1.9451e-03\n",
      "Epoch: 117470 | training loss: 2.2534e-03 | validation loss: 1.9386e-03\n",
      "Epoch: 117480 | training loss: 2.6015e-03 | validation loss: 2.0325e-03\n",
      "Epoch: 117490 | training loss: 2.2376e-03 | validation loss: 1.9498e-03\n",
      "Epoch: 117500 | training loss: 2.2355e-03 | validation loss: 1.9441e-03\n",
      "Epoch: 117510 | training loss: 2.2403e-03 | validation loss: 1.9629e-03\n",
      "Epoch: 117520 | training loss: 2.2368e-03 | validation loss: 1.9436e-03\n",
      "Epoch: 117530 | training loss: 2.2337e-03 | validation loss: 1.9541e-03\n",
      "Epoch: 117540 | training loss: 2.2333e-03 | validation loss: 1.9520e-03\n",
      "Epoch: 117550 | training loss: 2.2342e-03 | validation loss: 1.9461e-03\n",
      "Epoch: 117560 | training loss: 2.2332e-03 | validation loss: 1.9519e-03\n",
      "Epoch: 117570 | training loss: 2.2334e-03 | validation loss: 1.9528e-03\n",
      "Epoch: 117580 | training loss: 2.2335e-03 | validation loss: 1.9535e-03\n",
      "Epoch: 117590 | training loss: 2.2370e-03 | validation loss: 1.9604e-03\n",
      "Epoch: 117600 | training loss: 2.3297e-03 | validation loss: 2.0372e-03\n",
      "Epoch: 117610 | training loss: 3.0734e-03 | validation loss: 2.4660e-03\n",
      "Epoch: 117620 | training loss: 2.4137e-03 | validation loss: 1.9849e-03\n",
      "Epoch: 117630 | training loss: 2.2358e-03 | validation loss: 1.9502e-03\n",
      "Epoch: 117640 | training loss: 2.2459e-03 | validation loss: 1.9728e-03\n",
      "Epoch: 117650 | training loss: 2.2442e-03 | validation loss: 1.9408e-03\n",
      "Epoch: 117660 | training loss: 2.2375e-03 | validation loss: 1.9602e-03\n",
      "Epoch: 117670 | training loss: 2.2345e-03 | validation loss: 1.9454e-03\n",
      "Epoch: 117680 | training loss: 2.2332e-03 | validation loss: 1.9529e-03\n",
      "Epoch: 117690 | training loss: 2.2327e-03 | validation loss: 1.9477e-03\n",
      "Epoch: 117700 | training loss: 2.2326e-03 | validation loss: 1.9495e-03\n",
      "Epoch: 117710 | training loss: 2.2325e-03 | validation loss: 1.9500e-03\n",
      "Epoch: 117720 | training loss: 2.2325e-03 | validation loss: 1.9495e-03\n",
      "Epoch: 117730 | training loss: 2.2326e-03 | validation loss: 1.9495e-03\n",
      "Epoch: 117740 | training loss: 2.2345e-03 | validation loss: 1.9515e-03\n",
      "Epoch: 117750 | training loss: 2.3060e-03 | validation loss: 1.9992e-03\n",
      "Epoch: 117760 | training loss: 3.0760e-03 | validation loss: 2.4637e-03\n",
      "Epoch: 117770 | training loss: 2.5358e-03 | validation loss: 2.0127e-03\n",
      "Epoch: 117780 | training loss: 2.2777e-03 | validation loss: 2.0042e-03\n",
      "Epoch: 117790 | training loss: 2.2516e-03 | validation loss: 1.9756e-03\n",
      "Epoch: 117800 | training loss: 2.2435e-03 | validation loss: 1.9614e-03\n",
      "Epoch: 117810 | training loss: 2.2344e-03 | validation loss: 1.9579e-03\n",
      "Epoch: 117820 | training loss: 2.2338e-03 | validation loss: 1.9416e-03\n",
      "Epoch: 117830 | training loss: 2.2326e-03 | validation loss: 1.9489e-03\n",
      "Epoch: 117840 | training loss: 2.2321e-03 | validation loss: 1.9486e-03\n",
      "Epoch: 117850 | training loss: 2.2320e-03 | validation loss: 1.9482e-03\n",
      "Epoch: 117860 | training loss: 2.2320e-03 | validation loss: 1.9484e-03\n",
      "Epoch: 117870 | training loss: 2.2319e-03 | validation loss: 1.9488e-03\n",
      "Epoch: 117880 | training loss: 2.2319e-03 | validation loss: 1.9485e-03\n",
      "Epoch: 117890 | training loss: 2.2319e-03 | validation loss: 1.9490e-03\n",
      "Epoch: 117900 | training loss: 2.2335e-03 | validation loss: 1.9545e-03\n",
      "Epoch: 117910 | training loss: 2.3349e-03 | validation loss: 2.0416e-03\n",
      "Epoch: 117920 | training loss: 3.2157e-03 | validation loss: 2.5418e-03\n",
      "Epoch: 117930 | training loss: 2.2744e-03 | validation loss: 2.0088e-03\n",
      "Epoch: 117940 | training loss: 2.3330e-03 | validation loss: 1.9633e-03\n",
      "Epoch: 117950 | training loss: 2.2552e-03 | validation loss: 1.9314e-03\n",
      "Epoch: 117960 | training loss: 2.2404e-03 | validation loss: 1.9550e-03\n",
      "Epoch: 117970 | training loss: 2.2347e-03 | validation loss: 1.9595e-03\n",
      "Epoch: 117980 | training loss: 2.2330e-03 | validation loss: 1.9469e-03\n",
      "Epoch: 117990 | training loss: 2.2319e-03 | validation loss: 1.9448e-03\n",
      "Epoch: 118000 | training loss: 2.2315e-03 | validation loss: 1.9495e-03\n",
      "Epoch: 118010 | training loss: 2.2314e-03 | validation loss: 1.9468e-03\n",
      "Epoch: 118020 | training loss: 2.2314e-03 | validation loss: 1.9472e-03\n",
      "Epoch: 118030 | training loss: 2.2313e-03 | validation loss: 1.9473e-03\n",
      "Epoch: 118040 | training loss: 2.2313e-03 | validation loss: 1.9471e-03\n",
      "Epoch: 118050 | training loss: 2.2313e-03 | validation loss: 1.9471e-03\n",
      "Epoch: 118060 | training loss: 2.2312e-03 | validation loss: 1.9471e-03\n",
      "Epoch: 118070 | training loss: 2.2312e-03 | validation loss: 1.9469e-03\n",
      "Epoch: 118080 | training loss: 2.2311e-03 | validation loss: 1.9468e-03\n",
      "Epoch: 118090 | training loss: 2.2311e-03 | validation loss: 1.9468e-03\n",
      "Epoch: 118100 | training loss: 2.2311e-03 | validation loss: 1.9466e-03\n",
      "Epoch: 118110 | training loss: 2.2317e-03 | validation loss: 1.9460e-03\n",
      "Epoch: 118120 | training loss: 2.3124e-03 | validation loss: 1.9819e-03\n",
      "Epoch: 118130 | training loss: 3.3859e-03 | validation loss: 2.3201e-03\n",
      "Epoch: 118140 | training loss: 2.3847e-03 | validation loss: 2.1119e-03\n",
      "Epoch: 118150 | training loss: 2.2890e-03 | validation loss: 2.0098e-03\n",
      "Epoch: 118160 | training loss: 2.2479e-03 | validation loss: 1.9343e-03\n",
      "Epoch: 118170 | training loss: 2.2512e-03 | validation loss: 1.9327e-03\n",
      "Epoch: 118180 | training loss: 2.2361e-03 | validation loss: 1.9472e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 118190 | training loss: 2.2320e-03 | validation loss: 1.9465e-03\n",
      "Epoch: 118200 | training loss: 2.2313e-03 | validation loss: 1.9418e-03\n",
      "Epoch: 118210 | training loss: 2.2308e-03 | validation loss: 1.9470e-03\n",
      "Epoch: 118220 | training loss: 2.2307e-03 | validation loss: 1.9448e-03\n",
      "Epoch: 118230 | training loss: 2.2306e-03 | validation loss: 1.9458e-03\n",
      "Epoch: 118240 | training loss: 2.2306e-03 | validation loss: 1.9452e-03\n",
      "Epoch: 118250 | training loss: 2.2305e-03 | validation loss: 1.9457e-03\n",
      "Epoch: 118260 | training loss: 2.2305e-03 | validation loss: 1.9453e-03\n",
      "Epoch: 118270 | training loss: 2.2305e-03 | validation loss: 1.9454e-03\n",
      "Epoch: 118280 | training loss: 2.2304e-03 | validation loss: 1.9455e-03\n",
      "Epoch: 118290 | training loss: 2.2304e-03 | validation loss: 1.9456e-03\n",
      "Epoch: 118300 | training loss: 2.2304e-03 | validation loss: 1.9457e-03\n",
      "Epoch: 118310 | training loss: 2.2305e-03 | validation loss: 1.9475e-03\n",
      "Epoch: 118320 | training loss: 2.2438e-03 | validation loss: 1.9690e-03\n",
      "Epoch: 118330 | training loss: 3.4661e-03 | validation loss: 2.6881e-03\n",
      "Epoch: 118340 | training loss: 2.9226e-03 | validation loss: 2.1632e-03\n",
      "Epoch: 118350 | training loss: 2.3232e-03 | validation loss: 1.9435e-03\n",
      "Epoch: 118360 | training loss: 2.2359e-03 | validation loss: 1.9507e-03\n",
      "Epoch: 118370 | training loss: 2.2569e-03 | validation loss: 1.9744e-03\n",
      "Epoch: 118380 | training loss: 2.2367e-03 | validation loss: 1.9552e-03\n",
      "Epoch: 118390 | training loss: 2.2310e-03 | validation loss: 1.9398e-03\n",
      "Epoch: 118400 | training loss: 2.2313e-03 | validation loss: 1.9401e-03\n",
      "Epoch: 118410 | training loss: 2.2301e-03 | validation loss: 1.9465e-03\n",
      "Epoch: 118420 | training loss: 2.2300e-03 | validation loss: 1.9466e-03\n",
      "Epoch: 118430 | training loss: 2.2300e-03 | validation loss: 1.9438e-03\n",
      "Epoch: 118440 | training loss: 2.2299e-03 | validation loss: 1.9449e-03\n",
      "Epoch: 118450 | training loss: 2.2298e-03 | validation loss: 1.9445e-03\n",
      "Epoch: 118460 | training loss: 2.2298e-03 | validation loss: 1.9446e-03\n",
      "Epoch: 118470 | training loss: 2.2298e-03 | validation loss: 1.9445e-03\n",
      "Epoch: 118480 | training loss: 2.2297e-03 | validation loss: 1.9445e-03\n",
      "Epoch: 118490 | training loss: 2.2297e-03 | validation loss: 1.9443e-03\n",
      "Epoch: 118500 | training loss: 2.2296e-03 | validation loss: 1.9443e-03\n",
      "Epoch: 118510 | training loss: 2.2296e-03 | validation loss: 1.9443e-03\n",
      "Epoch: 118520 | training loss: 2.2296e-03 | validation loss: 1.9442e-03\n",
      "Epoch: 118530 | training loss: 2.2295e-03 | validation loss: 1.9442e-03\n",
      "Epoch: 118540 | training loss: 2.2295e-03 | validation loss: 1.9444e-03\n",
      "Epoch: 118550 | training loss: 2.2297e-03 | validation loss: 1.9463e-03\n",
      "Epoch: 118560 | training loss: 2.2608e-03 | validation loss: 1.9820e-03\n",
      "Epoch: 118570 | training loss: 4.5028e-03 | validation loss: 3.2135e-03\n",
      "Epoch: 118580 | training loss: 2.2337e-03 | validation loss: 1.9595e-03\n",
      "Epoch: 118590 | training loss: 2.2763e-03 | validation loss: 1.9396e-03\n",
      "Epoch: 118600 | training loss: 2.2733e-03 | validation loss: 1.9320e-03\n",
      "Epoch: 118610 | training loss: 2.2562e-03 | validation loss: 1.9305e-03\n",
      "Epoch: 118620 | training loss: 2.2399e-03 | validation loss: 1.9338e-03\n",
      "Epoch: 118630 | training loss: 2.2308e-03 | validation loss: 1.9388e-03\n",
      "Epoch: 118640 | training loss: 2.2292e-03 | validation loss: 1.9442e-03\n",
      "Epoch: 118650 | training loss: 2.2296e-03 | validation loss: 1.9470e-03\n",
      "Epoch: 118660 | training loss: 2.2292e-03 | validation loss: 1.9447e-03\n",
      "Epoch: 118670 | training loss: 2.2291e-03 | validation loss: 1.9425e-03\n",
      "Epoch: 118680 | training loss: 2.2290e-03 | validation loss: 1.9428e-03\n",
      "Epoch: 118690 | training loss: 2.2290e-03 | validation loss: 1.9437e-03\n",
      "Epoch: 118700 | training loss: 2.2289e-03 | validation loss: 1.9432e-03\n",
      "Epoch: 118710 | training loss: 2.2289e-03 | validation loss: 1.9430e-03\n",
      "Epoch: 118720 | training loss: 2.2289e-03 | validation loss: 1.9432e-03\n",
      "Epoch: 118730 | training loss: 2.2288e-03 | validation loss: 1.9430e-03\n",
      "Epoch: 118740 | training loss: 2.2288e-03 | validation loss: 1.9431e-03\n",
      "Epoch: 118750 | training loss: 2.2287e-03 | validation loss: 1.9431e-03\n",
      "Epoch: 118760 | training loss: 2.2289e-03 | validation loss: 1.9448e-03\n",
      "Epoch: 118770 | training loss: 2.2570e-03 | validation loss: 1.9842e-03\n",
      "Epoch: 118780 | training loss: 2.5503e-03 | validation loss: 2.2389e-03\n",
      "Epoch: 118790 | training loss: 2.3596e-03 | validation loss: 2.0588e-03\n",
      "Epoch: 118800 | training loss: 2.2406e-03 | validation loss: 1.9594e-03\n",
      "Epoch: 118810 | training loss: 2.2296e-03 | validation loss: 1.9451e-03\n",
      "Epoch: 118820 | training loss: 2.2321e-03 | validation loss: 1.9390e-03\n",
      "Epoch: 118830 | training loss: 2.2312e-03 | validation loss: 1.9373e-03\n",
      "Epoch: 118840 | training loss: 2.2295e-03 | validation loss: 1.9366e-03\n",
      "Epoch: 118850 | training loss: 2.2413e-03 | validation loss: 1.9311e-03\n",
      "Epoch: 118860 | training loss: 2.6341e-03 | validation loss: 2.0326e-03\n",
      "Epoch: 118870 | training loss: 2.2401e-03 | validation loss: 1.9679e-03\n",
      "Epoch: 118880 | training loss: 2.3265e-03 | validation loss: 1.9445e-03\n",
      "Epoch: 118890 | training loss: 2.2879e-03 | validation loss: 2.0036e-03\n",
      "Epoch: 118900 | training loss: 2.2380e-03 | validation loss: 1.9308e-03\n",
      "Epoch: 118910 | training loss: 2.2286e-03 | validation loss: 1.9438e-03\n",
      "Epoch: 118920 | training loss: 2.2281e-03 | validation loss: 1.9417e-03\n",
      "Epoch: 118930 | training loss: 2.2281e-03 | validation loss: 1.9420e-03\n",
      "Epoch: 118940 | training loss: 2.2282e-03 | validation loss: 1.9401e-03\n",
      "Epoch: 118950 | training loss: 2.2282e-03 | validation loss: 1.9433e-03\n",
      "Epoch: 118960 | training loss: 2.2280e-03 | validation loss: 1.9405e-03\n",
      "Epoch: 118970 | training loss: 2.2280e-03 | validation loss: 1.9403e-03\n",
      "Epoch: 118980 | training loss: 2.2279e-03 | validation loss: 1.9406e-03\n",
      "Epoch: 118990 | training loss: 2.2279e-03 | validation loss: 1.9401e-03\n",
      "Epoch: 119000 | training loss: 2.2289e-03 | validation loss: 1.9371e-03\n",
      "Epoch: 119010 | training loss: 2.2759e-03 | validation loss: 1.9326e-03\n",
      "Epoch: 119020 | training loss: 3.6756e-03 | validation loss: 2.4574e-03\n",
      "Epoch: 119030 | training loss: 2.4356e-03 | validation loss: 2.0895e-03\n",
      "Epoch: 119040 | training loss: 2.3251e-03 | validation loss: 2.0356e-03\n",
      "Epoch: 119050 | training loss: 2.2470e-03 | validation loss: 1.9458e-03\n",
      "Epoch: 119060 | training loss: 2.2409e-03 | validation loss: 1.9277e-03\n",
      "Epoch: 119070 | training loss: 2.2313e-03 | validation loss: 1.9445e-03\n",
      "Epoch: 119080 | training loss: 2.2288e-03 | validation loss: 1.9480e-03\n",
      "Epoch: 119090 | training loss: 2.2280e-03 | validation loss: 1.9373e-03\n",
      "Epoch: 119100 | training loss: 2.2277e-03 | validation loss: 1.9410e-03\n",
      "Epoch: 119110 | training loss: 2.2275e-03 | validation loss: 1.9413e-03\n",
      "Epoch: 119120 | training loss: 2.2275e-03 | validation loss: 1.9398e-03\n",
      "Epoch: 119130 | training loss: 2.2274e-03 | validation loss: 1.9408e-03\n",
      "Epoch: 119140 | training loss: 2.2274e-03 | validation loss: 1.9402e-03\n",
      "Epoch: 119150 | training loss: 2.2273e-03 | validation loss: 1.9401e-03\n",
      "Epoch: 119160 | training loss: 2.2273e-03 | validation loss: 1.9404e-03\n",
      "Epoch: 119170 | training loss: 2.2272e-03 | validation loss: 1.9402e-03\n",
      "Epoch: 119180 | training loss: 2.2272e-03 | validation loss: 1.9400e-03\n",
      "Epoch: 119190 | training loss: 2.2272e-03 | validation loss: 1.9400e-03\n",
      "Epoch: 119200 | training loss: 2.2304e-03 | validation loss: 1.9417e-03\n",
      "Epoch: 119210 | training loss: 2.4898e-03 | validation loss: 2.0915e-03\n",
      "Epoch: 119220 | training loss: 3.1842e-03 | validation loss: 2.3129e-03\n",
      "Epoch: 119230 | training loss: 2.3378e-03 | validation loss: 2.0137e-03\n",
      "Epoch: 119240 | training loss: 2.2451e-03 | validation loss: 1.9464e-03\n",
      "Epoch: 119250 | training loss: 2.2494e-03 | validation loss: 1.9237e-03\n",
      "Epoch: 119260 | training loss: 2.2357e-03 | validation loss: 1.9496e-03\n",
      "Epoch: 119270 | training loss: 2.2288e-03 | validation loss: 1.9321e-03\n",
      "Epoch: 119280 | training loss: 2.2272e-03 | validation loss: 1.9411e-03\n",
      "Epoch: 119290 | training loss: 2.2270e-03 | validation loss: 1.9377e-03\n",
      "Epoch: 119300 | training loss: 2.2270e-03 | validation loss: 1.9417e-03\n",
      "Epoch: 119310 | training loss: 2.2268e-03 | validation loss: 1.9388e-03\n",
      "Epoch: 119320 | training loss: 2.2267e-03 | validation loss: 1.9391e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 119330 | training loss: 2.2267e-03 | validation loss: 1.9394e-03\n",
      "Epoch: 119340 | training loss: 2.2267e-03 | validation loss: 1.9395e-03\n",
      "Epoch: 119350 | training loss: 2.2268e-03 | validation loss: 1.9408e-03\n",
      "Epoch: 119360 | training loss: 2.2300e-03 | validation loss: 1.9491e-03\n",
      "Epoch: 119370 | training loss: 2.4015e-03 | validation loss: 2.0821e-03\n",
      "Epoch: 119380 | training loss: 2.5771e-03 | validation loss: 2.1801e-03\n",
      "Epoch: 119390 | training loss: 2.3023e-03 | validation loss: 2.0146e-03\n",
      "Epoch: 119400 | training loss: 2.3145e-03 | validation loss: 1.9467e-03\n",
      "Epoch: 119410 | training loss: 2.2311e-03 | validation loss: 1.9350e-03\n",
      "Epoch: 119420 | training loss: 2.2403e-03 | validation loss: 1.9612e-03\n",
      "Epoch: 119430 | training loss: 2.2271e-03 | validation loss: 1.9343e-03\n",
      "Epoch: 119440 | training loss: 2.2267e-03 | validation loss: 1.9352e-03\n",
      "Epoch: 119450 | training loss: 2.2268e-03 | validation loss: 1.9420e-03\n",
      "Epoch: 119460 | training loss: 2.2265e-03 | validation loss: 1.9366e-03\n",
      "Epoch: 119470 | training loss: 2.2263e-03 | validation loss: 1.9395e-03\n",
      "Epoch: 119480 | training loss: 2.2262e-03 | validation loss: 1.9374e-03\n",
      "Epoch: 119490 | training loss: 2.2261e-03 | validation loss: 1.9385e-03\n",
      "Epoch: 119500 | training loss: 2.2261e-03 | validation loss: 1.9382e-03\n",
      "Epoch: 119510 | training loss: 2.2260e-03 | validation loss: 1.9378e-03\n",
      "Epoch: 119520 | training loss: 2.2260e-03 | validation loss: 1.9377e-03\n",
      "Epoch: 119530 | training loss: 2.2260e-03 | validation loss: 1.9376e-03\n",
      "Epoch: 119540 | training loss: 2.2260e-03 | validation loss: 1.9369e-03\n",
      "Epoch: 119550 | training loss: 2.2279e-03 | validation loss: 1.9328e-03\n",
      "Epoch: 119560 | training loss: 2.3814e-03 | validation loss: 1.9592e-03\n",
      "Epoch: 119570 | training loss: 2.6807e-03 | validation loss: 2.0707e-03\n",
      "Epoch: 119580 | training loss: 2.4963e-03 | validation loss: 1.9919e-03\n",
      "Epoch: 119590 | training loss: 2.2293e-03 | validation loss: 1.9391e-03\n",
      "Epoch: 119600 | training loss: 2.2681e-03 | validation loss: 1.9866e-03\n",
      "Epoch: 119610 | training loss: 2.2341e-03 | validation loss: 1.9568e-03\n",
      "Epoch: 119620 | training loss: 2.2279e-03 | validation loss: 1.9309e-03\n",
      "Epoch: 119630 | training loss: 2.2272e-03 | validation loss: 1.9323e-03\n",
      "Epoch: 119640 | training loss: 2.2260e-03 | validation loss: 1.9409e-03\n",
      "Epoch: 119650 | training loss: 2.2256e-03 | validation loss: 1.9381e-03\n",
      "Epoch: 119660 | training loss: 2.2256e-03 | validation loss: 1.9358e-03\n",
      "Epoch: 119670 | training loss: 2.2255e-03 | validation loss: 1.9381e-03\n",
      "Epoch: 119680 | training loss: 2.2254e-03 | validation loss: 1.9366e-03\n",
      "Epoch: 119690 | training loss: 2.2254e-03 | validation loss: 1.9373e-03\n",
      "Epoch: 119700 | training loss: 2.2254e-03 | validation loss: 1.9368e-03\n",
      "Epoch: 119710 | training loss: 2.2253e-03 | validation loss: 1.9371e-03\n",
      "Epoch: 119720 | training loss: 2.2253e-03 | validation loss: 1.9369e-03\n",
      "Epoch: 119730 | training loss: 2.2252e-03 | validation loss: 1.9369e-03\n",
      "Epoch: 119740 | training loss: 2.2253e-03 | validation loss: 1.9383e-03\n",
      "Epoch: 119750 | training loss: 2.2430e-03 | validation loss: 1.9658e-03\n",
      "Epoch: 119760 | training loss: 2.7856e-03 | validation loss: 2.4163e-03\n",
      "Epoch: 119770 | training loss: 2.2939e-03 | validation loss: 2.0251e-03\n",
      "Epoch: 119780 | training loss: 2.2292e-03 | validation loss: 1.9453e-03\n",
      "Epoch: 119790 | training loss: 2.2339e-03 | validation loss: 1.9362e-03\n",
      "Epoch: 119800 | training loss: 2.2339e-03 | validation loss: 1.9387e-03\n",
      "Epoch: 119810 | training loss: 2.2491e-03 | validation loss: 1.9659e-03\n",
      "Epoch: 119820 | training loss: 2.8472e-03 | validation loss: 2.3483e-03\n",
      "Epoch: 119830 | training loss: 2.3914e-03 | validation loss: 1.9530e-03\n",
      "Epoch: 119840 | training loss: 2.3650e-03 | validation loss: 2.0550e-03\n",
      "Epoch: 119850 | training loss: 2.2548e-03 | validation loss: 1.9244e-03\n",
      "Epoch: 119860 | training loss: 2.2255e-03 | validation loss: 1.9398e-03\n",
      "Epoch: 119870 | training loss: 2.2252e-03 | validation loss: 1.9389e-03\n",
      "Epoch: 119880 | training loss: 2.2252e-03 | validation loss: 1.9328e-03\n",
      "Epoch: 119890 | training loss: 2.2248e-03 | validation loss: 1.9369e-03\n",
      "Epoch: 119900 | training loss: 2.2247e-03 | validation loss: 1.9361e-03\n",
      "Epoch: 119910 | training loss: 2.2247e-03 | validation loss: 1.9338e-03\n",
      "Epoch: 119920 | training loss: 2.2246e-03 | validation loss: 1.9362e-03\n",
      "Epoch: 119930 | training loss: 2.2246e-03 | validation loss: 1.9364e-03\n",
      "Epoch: 119940 | training loss: 2.2245e-03 | validation loss: 1.9362e-03\n",
      "Epoch: 119950 | training loss: 2.2247e-03 | validation loss: 1.9375e-03\n",
      "Epoch: 119960 | training loss: 2.2300e-03 | validation loss: 1.9484e-03\n",
      "Epoch: 119970 | training loss: 2.5068e-03 | validation loss: 2.1432e-03\n",
      "Epoch: 119980 | training loss: 2.2652e-03 | validation loss: 1.9717e-03\n",
      "Epoch: 119990 | training loss: 2.4103e-03 | validation loss: 2.0814e-03\n",
      "Epoch: 120000 | training loss: 2.2642e-03 | validation loss: 1.9387e-03\n",
      "Epoch: 120010 | training loss: 2.2388e-03 | validation loss: 1.9267e-03\n",
      "Epoch: 120020 | training loss: 2.2350e-03 | validation loss: 1.9497e-03\n",
      "Epoch: 120030 | training loss: 2.2243e-03 | validation loss: 1.9335e-03\n",
      "Epoch: 120040 | training loss: 2.2249e-03 | validation loss: 1.9332e-03\n",
      "Epoch: 120050 | training loss: 2.2247e-03 | validation loss: 1.9375e-03\n",
      "Epoch: 120060 | training loss: 2.2243e-03 | validation loss: 1.9328e-03\n",
      "Epoch: 120070 | training loss: 2.2241e-03 | validation loss: 1.9361e-03\n",
      "Epoch: 120080 | training loss: 2.2240e-03 | validation loss: 1.9337e-03\n",
      "Epoch: 120090 | training loss: 2.2240e-03 | validation loss: 1.9347e-03\n",
      "Epoch: 120100 | training loss: 2.2239e-03 | validation loss: 1.9346e-03\n",
      "Epoch: 120110 | training loss: 2.2239e-03 | validation loss: 1.9343e-03\n",
      "Epoch: 120120 | training loss: 2.2239e-03 | validation loss: 1.9341e-03\n",
      "Epoch: 120130 | training loss: 2.2238e-03 | validation loss: 1.9337e-03\n",
      "Epoch: 120140 | training loss: 2.2241e-03 | validation loss: 1.9322e-03\n",
      "Epoch: 120150 | training loss: 2.2366e-03 | validation loss: 1.9261e-03\n",
      "Epoch: 120160 | training loss: 3.1355e-03 | validation loss: 2.2477e-03\n",
      "Epoch: 120170 | training loss: 2.8457e-03 | validation loss: 2.3261e-03\n",
      "Epoch: 120180 | training loss: 2.2287e-03 | validation loss: 1.9236e-03\n",
      "Epoch: 120190 | training loss: 2.3038e-03 | validation loss: 1.9404e-03\n",
      "Epoch: 120200 | training loss: 2.2328e-03 | validation loss: 1.9283e-03\n",
      "Epoch: 120210 | training loss: 2.2296e-03 | validation loss: 1.9446e-03\n",
      "Epoch: 120220 | training loss: 2.2253e-03 | validation loss: 1.9397e-03\n",
      "Epoch: 120230 | training loss: 2.2246e-03 | validation loss: 1.9304e-03\n",
      "Epoch: 120240 | training loss: 2.2234e-03 | validation loss: 1.9337e-03\n",
      "Epoch: 120250 | training loss: 2.2235e-03 | validation loss: 1.9349e-03\n",
      "Epoch: 120260 | training loss: 2.2234e-03 | validation loss: 1.9326e-03\n",
      "Epoch: 120270 | training loss: 2.2234e-03 | validation loss: 1.9341e-03\n",
      "Epoch: 120280 | training loss: 2.2233e-03 | validation loss: 1.9331e-03\n",
      "Epoch: 120290 | training loss: 2.2233e-03 | validation loss: 1.9337e-03\n",
      "Epoch: 120300 | training loss: 2.2232e-03 | validation loss: 1.9334e-03\n",
      "Epoch: 120310 | training loss: 2.2232e-03 | validation loss: 1.9344e-03\n",
      "Epoch: 120320 | training loss: 2.2269e-03 | validation loss: 1.9456e-03\n",
      "Epoch: 120330 | training loss: 2.6421e-03 | validation loss: 2.3275e-03\n",
      "Epoch: 120340 | training loss: 2.4891e-03 | validation loss: 2.0781e-03\n",
      "Epoch: 120350 | training loss: 2.3034e-03 | validation loss: 1.9371e-03\n",
      "Epoch: 120360 | training loss: 2.2367e-03 | validation loss: 1.9491e-03\n",
      "Epoch: 120370 | training loss: 2.2246e-03 | validation loss: 1.9333e-03\n",
      "Epoch: 120380 | training loss: 2.2282e-03 | validation loss: 1.9347e-03\n",
      "Epoch: 120390 | training loss: 2.2241e-03 | validation loss: 1.9355e-03\n",
      "Epoch: 120400 | training loss: 2.2230e-03 | validation loss: 1.9337e-03\n",
      "Epoch: 120410 | training loss: 2.2242e-03 | validation loss: 1.9370e-03\n",
      "Epoch: 120420 | training loss: 2.2580e-03 | validation loss: 1.9774e-03\n",
      "Epoch: 120430 | training loss: 3.2385e-03 | validation loss: 2.5606e-03\n",
      "Epoch: 120440 | training loss: 2.6192e-03 | validation loss: 2.0331e-03\n",
      "Epoch: 120450 | training loss: 2.2556e-03 | validation loss: 1.9691e-03\n",
      "Epoch: 120460 | training loss: 2.2402e-03 | validation loss: 1.9547e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 120470 | training loss: 2.2412e-03 | validation loss: 1.9204e-03\n",
      "Epoch: 120480 | training loss: 2.2259e-03 | validation loss: 1.9416e-03\n",
      "Epoch: 120490 | training loss: 2.2227e-03 | validation loss: 1.9310e-03\n",
      "Epoch: 120500 | training loss: 2.2225e-03 | validation loss: 1.9319e-03\n",
      "Epoch: 120510 | training loss: 2.2225e-03 | validation loss: 1.9314e-03\n",
      "Epoch: 120520 | training loss: 2.2225e-03 | validation loss: 1.9325e-03\n",
      "Epoch: 120530 | training loss: 2.2225e-03 | validation loss: 1.9308e-03\n",
      "Epoch: 120540 | training loss: 2.2224e-03 | validation loss: 1.9320e-03\n",
      "Epoch: 120550 | training loss: 2.2223e-03 | validation loss: 1.9321e-03\n",
      "Epoch: 120560 | training loss: 2.2223e-03 | validation loss: 1.9318e-03\n",
      "Epoch: 120570 | training loss: 2.2223e-03 | validation loss: 1.9319e-03\n",
      "Epoch: 120580 | training loss: 2.2224e-03 | validation loss: 1.9335e-03\n",
      "Epoch: 120590 | training loss: 2.2325e-03 | validation loss: 1.9499e-03\n",
      "Epoch: 120600 | training loss: 3.0273e-03 | validation loss: 2.4310e-03\n",
      "Epoch: 120610 | training loss: 2.8036e-03 | validation loss: 2.0996e-03\n",
      "Epoch: 120620 | training loss: 2.2423e-03 | validation loss: 1.9451e-03\n",
      "Epoch: 120630 | training loss: 2.3030e-03 | validation loss: 2.0080e-03\n",
      "Epoch: 120640 | training loss: 2.2369e-03 | validation loss: 1.9615e-03\n",
      "Epoch: 120650 | training loss: 2.2253e-03 | validation loss: 1.9251e-03\n",
      "Epoch: 120660 | training loss: 2.2256e-03 | validation loss: 1.9217e-03\n",
      "Epoch: 120670 | training loss: 2.2223e-03 | validation loss: 1.9350e-03\n",
      "Epoch: 120680 | training loss: 2.2221e-03 | validation loss: 1.9337e-03\n",
      "Epoch: 120690 | training loss: 2.2220e-03 | validation loss: 1.9285e-03\n",
      "Epoch: 120700 | training loss: 2.2218e-03 | validation loss: 1.9317e-03\n",
      "Epoch: 120710 | training loss: 2.2218e-03 | validation loss: 1.9306e-03\n",
      "Epoch: 120720 | training loss: 2.2217e-03 | validation loss: 1.9305e-03\n",
      "Epoch: 120730 | training loss: 2.2217e-03 | validation loss: 1.9307e-03\n",
      "Epoch: 120740 | training loss: 2.2217e-03 | validation loss: 1.9306e-03\n",
      "Epoch: 120750 | training loss: 2.2216e-03 | validation loss: 1.9303e-03\n",
      "Epoch: 120760 | training loss: 2.2216e-03 | validation loss: 1.9304e-03\n",
      "Epoch: 120770 | training loss: 2.2216e-03 | validation loss: 1.9302e-03\n",
      "Epoch: 120780 | training loss: 2.2217e-03 | validation loss: 1.9292e-03\n",
      "Epoch: 120790 | training loss: 2.2353e-03 | validation loss: 1.9287e-03\n",
      "Epoch: 120800 | training loss: 2.7779e-03 | validation loss: 2.2457e-03\n",
      "Epoch: 120810 | training loss: 2.2674e-03 | validation loss: 1.9943e-03\n",
      "Epoch: 120820 | training loss: 2.4035e-03 | validation loss: 2.1111e-03\n",
      "Epoch: 120830 | training loss: 2.2995e-03 | validation loss: 2.0235e-03\n",
      "Epoch: 120840 | training loss: 2.2298e-03 | validation loss: 1.9233e-03\n",
      "Epoch: 120850 | training loss: 2.2500e-03 | validation loss: 1.9147e-03\n",
      "Epoch: 120860 | training loss: 2.2604e-03 | validation loss: 1.9148e-03\n",
      "Epoch: 120870 | training loss: 2.3140e-03 | validation loss: 1.9254e-03\n",
      "Epoch: 120880 | training loss: 2.3710e-03 | validation loss: 1.9418e-03\n",
      "Epoch: 120890 | training loss: 2.2231e-03 | validation loss: 1.9244e-03\n",
      "Epoch: 120900 | training loss: 2.2481e-03 | validation loss: 1.9664e-03\n",
      "Epoch: 120910 | training loss: 2.2556e-03 | validation loss: 1.9728e-03\n",
      "Epoch: 120920 | training loss: 2.2740e-03 | validation loss: 1.9883e-03\n",
      "Epoch: 120930 | training loss: 2.3500e-03 | validation loss: 2.0425e-03\n",
      "Epoch: 120940 | training loss: 2.3110e-03 | validation loss: 2.0142e-03\n",
      "Epoch: 120950 | training loss: 2.2232e-03 | validation loss: 1.9232e-03\n",
      "Epoch: 120960 | training loss: 2.2511e-03 | validation loss: 1.9181e-03\n",
      "Epoch: 120970 | training loss: 2.2770e-03 | validation loss: 1.9214e-03\n",
      "Epoch: 120980 | training loss: 2.3441e-03 | validation loss: 1.9372e-03\n",
      "Epoch: 120990 | training loss: 2.3044e-03 | validation loss: 1.9282e-03\n",
      "Epoch: 121000 | training loss: 2.2209e-03 | validation loss: 1.9305e-03\n",
      "Epoch: 121010 | training loss: 2.2432e-03 | validation loss: 1.9599e-03\n",
      "Epoch: 121020 | training loss: 2.2981e-03 | validation loss: 2.0030e-03\n",
      "Epoch: 121030 | training loss: 2.4395e-03 | validation loss: 2.0945e-03\n",
      "Epoch: 121040 | training loss: 2.2270e-03 | validation loss: 1.9414e-03\n",
      "Epoch: 121050 | training loss: 2.2642e-03 | validation loss: 1.9219e-03\n",
      "Epoch: 121060 | training loss: 2.2265e-03 | validation loss: 1.9208e-03\n",
      "Epoch: 121070 | training loss: 2.2213e-03 | validation loss: 1.9328e-03\n",
      "Epoch: 121080 | training loss: 2.2309e-03 | validation loss: 1.9466e-03\n",
      "Epoch: 121090 | training loss: 2.3607e-03 | validation loss: 2.0432e-03\n",
      "Epoch: 121100 | training loss: 2.6791e-03 | validation loss: 2.2317e-03\n",
      "Epoch: 121110 | training loss: 2.3837e-03 | validation loss: 1.9545e-03\n",
      "Epoch: 121120 | training loss: 2.2785e-03 | validation loss: 1.9854e-03\n",
      "Epoch: 121130 | training loss: 2.2436e-03 | validation loss: 1.9198e-03\n",
      "Epoch: 121140 | training loss: 2.2297e-03 | validation loss: 1.9452e-03\n",
      "Epoch: 121150 | training loss: 2.2219e-03 | validation loss: 1.9226e-03\n",
      "Epoch: 121160 | training loss: 2.2205e-03 | validation loss: 1.9268e-03\n",
      "Epoch: 121170 | training loss: 2.2208e-03 | validation loss: 1.9322e-03\n",
      "Epoch: 121180 | training loss: 2.2209e-03 | validation loss: 1.9317e-03\n",
      "Epoch: 121190 | training loss: 2.2223e-03 | validation loss: 1.9340e-03\n",
      "Epoch: 121200 | training loss: 2.2512e-03 | validation loss: 1.9598e-03\n",
      "Epoch: 121210 | training loss: 2.8575e-03 | validation loss: 2.3169e-03\n",
      "Epoch: 121220 | training loss: 2.4213e-03 | validation loss: 2.0748e-03\n",
      "Epoch: 121230 | training loss: 2.2805e-03 | validation loss: 1.9553e-03\n",
      "Epoch: 121240 | training loss: 2.2251e-03 | validation loss: 1.9261e-03\n",
      "Epoch: 121250 | training loss: 2.2329e-03 | validation loss: 1.9388e-03\n",
      "Epoch: 121260 | training loss: 2.2213e-03 | validation loss: 1.9331e-03\n",
      "Epoch: 121270 | training loss: 2.2211e-03 | validation loss: 1.9220e-03\n",
      "Epoch: 121280 | training loss: 2.2204e-03 | validation loss: 1.9277e-03\n",
      "Epoch: 121290 | training loss: 2.2200e-03 | validation loss: 1.9277e-03\n",
      "Epoch: 121300 | training loss: 2.2201e-03 | validation loss: 1.9311e-03\n",
      "Epoch: 121310 | training loss: 2.2235e-03 | validation loss: 1.9395e-03\n",
      "Epoch: 121320 | training loss: 2.3344e-03 | validation loss: 2.0448e-03\n",
      "Epoch: 121330 | training loss: 2.7283e-03 | validation loss: 2.2818e-03\n",
      "Epoch: 121340 | training loss: 2.3063e-03 | validation loss: 1.9579e-03\n",
      "Epoch: 121350 | training loss: 2.2736e-03 | validation loss: 1.9389e-03\n",
      "Epoch: 121360 | training loss: 2.2259e-03 | validation loss: 1.9395e-03\n",
      "Epoch: 121370 | training loss: 2.2214e-03 | validation loss: 1.9361e-03\n",
      "Epoch: 121380 | training loss: 2.2223e-03 | validation loss: 1.9182e-03\n",
      "Epoch: 121390 | training loss: 2.2206e-03 | validation loss: 1.9336e-03\n",
      "Epoch: 121400 | training loss: 2.2196e-03 | validation loss: 1.9234e-03\n",
      "Epoch: 121410 | training loss: 2.2194e-03 | validation loss: 1.9266e-03\n",
      "Epoch: 121420 | training loss: 2.2193e-03 | validation loss: 1.9258e-03\n",
      "Epoch: 121430 | training loss: 2.2193e-03 | validation loss: 1.9278e-03\n",
      "Epoch: 121440 | training loss: 2.2201e-03 | validation loss: 1.9292e-03\n",
      "Epoch: 121450 | training loss: 2.2477e-03 | validation loss: 1.9567e-03\n",
      "Epoch: 121460 | training loss: 3.2722e-03 | validation loss: 2.5450e-03\n",
      "Epoch: 121470 | training loss: 2.6468e-03 | validation loss: 2.0524e-03\n",
      "Epoch: 121480 | training loss: 2.2799e-03 | validation loss: 2.0077e-03\n",
      "Epoch: 121490 | training loss: 2.2421e-03 | validation loss: 1.9427e-03\n",
      "Epoch: 121500 | training loss: 2.2339e-03 | validation loss: 1.9108e-03\n",
      "Epoch: 121510 | training loss: 2.2247e-03 | validation loss: 1.9431e-03\n",
      "Epoch: 121520 | training loss: 2.2191e-03 | validation loss: 1.9233e-03\n",
      "Epoch: 121530 | training loss: 2.2191e-03 | validation loss: 1.9246e-03\n",
      "Epoch: 121540 | training loss: 2.2190e-03 | validation loss: 1.9268e-03\n",
      "Epoch: 121550 | training loss: 2.2188e-03 | validation loss: 1.9255e-03\n",
      "Epoch: 121560 | training loss: 2.2188e-03 | validation loss: 1.9249e-03\n",
      "Epoch: 121570 | training loss: 2.2187e-03 | validation loss: 1.9259e-03\n",
      "Epoch: 121580 | training loss: 2.2187e-03 | validation loss: 1.9258e-03\n",
      "Epoch: 121590 | training loss: 2.2186e-03 | validation loss: 1.9251e-03\n",
      "Epoch: 121600 | training loss: 2.2186e-03 | validation loss: 1.9248e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 121610 | training loss: 2.2186e-03 | validation loss: 1.9237e-03\n",
      "Epoch: 121620 | training loss: 2.2214e-03 | validation loss: 1.9172e-03\n",
      "Epoch: 121630 | training loss: 2.4496e-03 | validation loss: 1.9555e-03\n",
      "Epoch: 121640 | training loss: 2.4312e-03 | validation loss: 2.0666e-03\n",
      "Epoch: 121650 | training loss: 2.4351e-03 | validation loss: 1.9788e-03\n",
      "Epoch: 121660 | training loss: 2.3205e-03 | validation loss: 1.9415e-03\n",
      "Epoch: 121670 | training loss: 2.2437e-03 | validation loss: 1.9098e-03\n",
      "Epoch: 121680 | training loss: 2.2203e-03 | validation loss: 1.9199e-03\n",
      "Epoch: 121690 | training loss: 2.2188e-03 | validation loss: 1.9285e-03\n",
      "Epoch: 121700 | training loss: 2.2197e-03 | validation loss: 1.9318e-03\n",
      "Epoch: 121710 | training loss: 2.2186e-03 | validation loss: 1.9277e-03\n",
      "Epoch: 121720 | training loss: 2.2182e-03 | validation loss: 1.9234e-03\n",
      "Epoch: 121730 | training loss: 2.2182e-03 | validation loss: 1.9227e-03\n",
      "Epoch: 121740 | training loss: 2.2181e-03 | validation loss: 1.9246e-03\n",
      "Epoch: 121750 | training loss: 2.2181e-03 | validation loss: 1.9246e-03\n",
      "Epoch: 121760 | training loss: 2.2180e-03 | validation loss: 1.9238e-03\n",
      "Epoch: 121770 | training loss: 2.2180e-03 | validation loss: 1.9241e-03\n",
      "Epoch: 121780 | training loss: 2.2180e-03 | validation loss: 1.9241e-03\n",
      "Epoch: 121790 | training loss: 2.2181e-03 | validation loss: 1.9248e-03\n",
      "Epoch: 121800 | training loss: 2.2325e-03 | validation loss: 1.9403e-03\n",
      "Epoch: 121810 | training loss: 3.7351e-03 | validation loss: 2.7911e-03\n",
      "Epoch: 121820 | training loss: 2.7500e-03 | validation loss: 2.0582e-03\n",
      "Epoch: 121830 | training loss: 2.3458e-03 | validation loss: 1.9236e-03\n",
      "Epoch: 121840 | training loss: 2.2228e-03 | validation loss: 1.9175e-03\n",
      "Epoch: 121850 | training loss: 2.2364e-03 | validation loss: 1.9444e-03\n",
      "Epoch: 121860 | training loss: 2.2226e-03 | validation loss: 1.9334e-03\n",
      "Epoch: 121870 | training loss: 2.2183e-03 | validation loss: 1.9214e-03\n",
      "Epoch: 121880 | training loss: 2.2186e-03 | validation loss: 1.9220e-03\n",
      "Epoch: 121890 | training loss: 2.2179e-03 | validation loss: 1.9267e-03\n",
      "Epoch: 121900 | training loss: 2.2176e-03 | validation loss: 1.9249e-03\n",
      "Epoch: 121910 | training loss: 2.2176e-03 | validation loss: 1.9223e-03\n",
      "Epoch: 121920 | training loss: 2.2175e-03 | validation loss: 1.9234e-03\n",
      "Epoch: 121930 | training loss: 2.2175e-03 | validation loss: 1.9229e-03\n",
      "Epoch: 121940 | training loss: 2.2174e-03 | validation loss: 1.9234e-03\n",
      "Epoch: 121950 | training loss: 2.2174e-03 | validation loss: 1.9230e-03\n",
      "Epoch: 121960 | training loss: 2.2174e-03 | validation loss: 1.9231e-03\n",
      "Epoch: 121970 | training loss: 2.2173e-03 | validation loss: 1.9230e-03\n",
      "Epoch: 121980 | training loss: 2.2173e-03 | validation loss: 1.9229e-03\n",
      "Epoch: 121990 | training loss: 2.2172e-03 | validation loss: 1.9229e-03\n",
      "Epoch: 122000 | training loss: 2.2172e-03 | validation loss: 1.9229e-03\n",
      "Epoch: 122010 | training loss: 2.2172e-03 | validation loss: 1.9229e-03\n",
      "Epoch: 122020 | training loss: 2.2171e-03 | validation loss: 1.9231e-03\n",
      "Epoch: 122030 | training loss: 2.2179e-03 | validation loss: 1.9266e-03\n",
      "Epoch: 122040 | training loss: 2.3363e-03 | validation loss: 2.0221e-03\n",
      "Epoch: 122050 | training loss: 2.8376e-03 | validation loss: 2.3178e-03\n",
      "Epoch: 122060 | training loss: 2.7672e-03 | validation loss: 2.2749e-03\n",
      "Epoch: 122070 | training loss: 2.3973e-03 | validation loss: 2.0583e-03\n",
      "Epoch: 122080 | training loss: 2.2695e-03 | validation loss: 1.9722e-03\n",
      "Epoch: 122090 | training loss: 2.2280e-03 | validation loss: 1.9386e-03\n",
      "Epoch: 122100 | training loss: 2.2175e-03 | validation loss: 1.9253e-03\n",
      "Epoch: 122110 | training loss: 2.2173e-03 | validation loss: 1.9206e-03\n",
      "Epoch: 122120 | training loss: 2.2178e-03 | validation loss: 1.9194e-03\n",
      "Epoch: 122130 | training loss: 2.2171e-03 | validation loss: 1.9204e-03\n",
      "Epoch: 122140 | training loss: 2.2167e-03 | validation loss: 1.9225e-03\n",
      "Epoch: 122150 | training loss: 2.2168e-03 | validation loss: 1.9230e-03\n",
      "Epoch: 122160 | training loss: 2.2167e-03 | validation loss: 1.9220e-03\n",
      "Epoch: 122170 | training loss: 2.2166e-03 | validation loss: 1.9217e-03\n",
      "Epoch: 122180 | training loss: 2.2166e-03 | validation loss: 1.9221e-03\n",
      "Epoch: 122190 | training loss: 2.2166e-03 | validation loss: 1.9220e-03\n",
      "Epoch: 122200 | training loss: 2.2165e-03 | validation loss: 1.9218e-03\n",
      "Epoch: 122210 | training loss: 2.2165e-03 | validation loss: 1.9218e-03\n",
      "Epoch: 122220 | training loss: 2.2165e-03 | validation loss: 1.9217e-03\n",
      "Epoch: 122230 | training loss: 2.2164e-03 | validation loss: 1.9217e-03\n",
      "Epoch: 122240 | training loss: 2.2164e-03 | validation loss: 1.9216e-03\n",
      "Epoch: 122250 | training loss: 2.2163e-03 | validation loss: 1.9216e-03\n",
      "Epoch: 122260 | training loss: 2.2163e-03 | validation loss: 1.9216e-03\n",
      "Epoch: 122270 | training loss: 2.2163e-03 | validation loss: 1.9227e-03\n",
      "Epoch: 122280 | training loss: 2.2276e-03 | validation loss: 1.9457e-03\n",
      "Epoch: 122290 | training loss: 3.1279e-03 | validation loss: 2.6832e-03\n",
      "Epoch: 122300 | training loss: 2.3350e-03 | validation loss: 2.0452e-03\n",
      "Epoch: 122310 | training loss: 2.2705e-03 | validation loss: 1.9575e-03\n",
      "Epoch: 122320 | training loss: 2.2466e-03 | validation loss: 1.9681e-03\n",
      "Epoch: 122330 | training loss: 2.2258e-03 | validation loss: 1.9294e-03\n",
      "Epoch: 122340 | training loss: 2.2206e-03 | validation loss: 1.9226e-03\n",
      "Epoch: 122350 | training loss: 2.2169e-03 | validation loss: 1.9238e-03\n",
      "Epoch: 122360 | training loss: 2.2164e-03 | validation loss: 1.9245e-03\n",
      "Epoch: 122370 | training loss: 2.2195e-03 | validation loss: 1.9313e-03\n",
      "Epoch: 122380 | training loss: 2.3201e-03 | validation loss: 2.0175e-03\n",
      "Epoch: 122390 | training loss: 2.9622e-03 | validation loss: 2.3948e-03\n",
      "Epoch: 122400 | training loss: 2.3076e-03 | validation loss: 1.9204e-03\n",
      "Epoch: 122410 | training loss: 2.2252e-03 | validation loss: 1.9108e-03\n",
      "Epoch: 122420 | training loss: 2.2464e-03 | validation loss: 1.9592e-03\n",
      "Epoch: 122430 | training loss: 2.2272e-03 | validation loss: 1.9105e-03\n",
      "Epoch: 122440 | training loss: 2.2180e-03 | validation loss: 1.9281e-03\n",
      "Epoch: 122450 | training loss: 2.2162e-03 | validation loss: 1.9172e-03\n",
      "Epoch: 122460 | training loss: 2.2159e-03 | validation loss: 1.9224e-03\n",
      "Epoch: 122470 | training loss: 2.2158e-03 | validation loss: 1.9179e-03\n",
      "Epoch: 122480 | training loss: 2.2157e-03 | validation loss: 1.9213e-03\n",
      "Epoch: 122490 | training loss: 2.2155e-03 | validation loss: 1.9199e-03\n",
      "Epoch: 122500 | training loss: 2.2155e-03 | validation loss: 1.9190e-03\n",
      "Epoch: 122510 | training loss: 2.2155e-03 | validation loss: 1.9187e-03\n",
      "Epoch: 122520 | training loss: 2.2157e-03 | validation loss: 1.9174e-03\n",
      "Epoch: 122530 | training loss: 2.2212e-03 | validation loss: 1.9115e-03\n",
      "Epoch: 122540 | training loss: 2.4937e-03 | validation loss: 1.9758e-03\n",
      "Epoch: 122550 | training loss: 2.2640e-03 | validation loss: 1.9217e-03\n",
      "Epoch: 122560 | training loss: 2.3758e-03 | validation loss: 1.9415e-03\n",
      "Epoch: 122570 | training loss: 2.2667e-03 | validation loss: 1.9683e-03\n",
      "Epoch: 122580 | training loss: 2.2233e-03 | validation loss: 1.9323e-03\n",
      "Epoch: 122590 | training loss: 2.2272e-03 | validation loss: 1.9116e-03\n",
      "Epoch: 122600 | training loss: 2.2162e-03 | validation loss: 1.9250e-03\n",
      "Epoch: 122610 | training loss: 2.2152e-03 | validation loss: 1.9203e-03\n",
      "Epoch: 122620 | training loss: 2.2153e-03 | validation loss: 1.9169e-03\n",
      "Epoch: 122630 | training loss: 2.2152e-03 | validation loss: 1.9210e-03\n",
      "Epoch: 122640 | training loss: 2.2150e-03 | validation loss: 1.9182e-03\n",
      "Epoch: 122650 | training loss: 2.2150e-03 | validation loss: 1.9192e-03\n",
      "Epoch: 122660 | training loss: 2.2149e-03 | validation loss: 1.9193e-03\n",
      "Epoch: 122670 | training loss: 2.2149e-03 | validation loss: 1.9186e-03\n",
      "Epoch: 122680 | training loss: 2.2149e-03 | validation loss: 1.9186e-03\n",
      "Epoch: 122690 | training loss: 2.2148e-03 | validation loss: 1.9186e-03\n",
      "Epoch: 122700 | training loss: 2.2148e-03 | validation loss: 1.9184e-03\n",
      "Epoch: 122710 | training loss: 2.2149e-03 | validation loss: 1.9171e-03\n",
      "Epoch: 122720 | training loss: 2.2251e-03 | validation loss: 1.9104e-03\n",
      "Epoch: 122730 | training loss: 3.1767e-03 | validation loss: 2.2476e-03\n",
      "Epoch: 122740 | training loss: 2.9777e-03 | validation loss: 2.3928e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 122750 | training loss: 2.2338e-03 | validation loss: 1.9478e-03\n",
      "Epoch: 122760 | training loss: 2.2467e-03 | validation loss: 1.9094e-03\n",
      "Epoch: 122770 | training loss: 2.2492e-03 | validation loss: 1.9082e-03\n",
      "Epoch: 122780 | training loss: 2.2177e-03 | validation loss: 1.9124e-03\n",
      "Epoch: 122790 | training loss: 2.2164e-03 | validation loss: 1.9253e-03\n",
      "Epoch: 122800 | training loss: 2.2156e-03 | validation loss: 1.9233e-03\n",
      "Epoch: 122810 | training loss: 2.2146e-03 | validation loss: 1.9165e-03\n",
      "Epoch: 122820 | training loss: 2.2145e-03 | validation loss: 1.9169e-03\n",
      "Epoch: 122830 | training loss: 2.2144e-03 | validation loss: 1.9194e-03\n",
      "Epoch: 122840 | training loss: 2.2143e-03 | validation loss: 1.9176e-03\n",
      "Epoch: 122850 | training loss: 2.2143e-03 | validation loss: 1.9180e-03\n",
      "Epoch: 122860 | training loss: 2.2142e-03 | validation loss: 1.9180e-03\n",
      "Epoch: 122870 | training loss: 2.2142e-03 | validation loss: 1.9178e-03\n",
      "Epoch: 122880 | training loss: 2.2142e-03 | validation loss: 1.9178e-03\n",
      "Epoch: 122890 | training loss: 2.2141e-03 | validation loss: 1.9179e-03\n",
      "Epoch: 122900 | training loss: 2.2141e-03 | validation loss: 1.9184e-03\n",
      "Epoch: 122910 | training loss: 2.2173e-03 | validation loss: 1.9277e-03\n",
      "Epoch: 122920 | training loss: 2.6769e-03 | validation loss: 2.3366e-03\n",
      "Epoch: 122930 | training loss: 2.4484e-03 | validation loss: 2.0451e-03\n",
      "Epoch: 122940 | training loss: 2.3147e-03 | validation loss: 1.9405e-03\n",
      "Epoch: 122950 | training loss: 2.2367e-03 | validation loss: 1.9174e-03\n",
      "Epoch: 122960 | training loss: 2.2190e-03 | validation loss: 1.9221e-03\n",
      "Epoch: 122970 | training loss: 2.2162e-03 | validation loss: 1.9261e-03\n",
      "Epoch: 122980 | training loss: 2.2222e-03 | validation loss: 1.9374e-03\n",
      "Epoch: 122990 | training loss: 2.3122e-03 | validation loss: 2.0141e-03\n",
      "Epoch: 123000 | training loss: 2.7655e-03 | validation loss: 2.2881e-03\n",
      "Epoch: 123010 | training loss: 2.3999e-03 | validation loss: 1.9411e-03\n",
      "Epoch: 123020 | training loss: 2.2807e-03 | validation loss: 1.9859e-03\n",
      "Epoch: 123030 | training loss: 2.2391e-03 | validation loss: 1.9046e-03\n",
      "Epoch: 123040 | training loss: 2.2236e-03 | validation loss: 1.9354e-03\n",
      "Epoch: 123050 | training loss: 2.2159e-03 | validation loss: 1.9105e-03\n",
      "Epoch: 123060 | training loss: 2.2136e-03 | validation loss: 1.9156e-03\n",
      "Epoch: 123070 | training loss: 2.2144e-03 | validation loss: 1.9211e-03\n",
      "Epoch: 123080 | training loss: 2.2137e-03 | validation loss: 1.9187e-03\n",
      "Epoch: 123090 | training loss: 2.2136e-03 | validation loss: 1.9180e-03\n",
      "Epoch: 123100 | training loss: 2.2145e-03 | validation loss: 1.9215e-03\n",
      "Epoch: 123110 | training loss: 2.2443e-03 | validation loss: 1.9562e-03\n",
      "Epoch: 123120 | training loss: 3.2452e-03 | validation loss: 2.5452e-03\n",
      "Epoch: 123130 | training loss: 2.6491e-03 | validation loss: 2.0341e-03\n",
      "Epoch: 123140 | training loss: 2.2523e-03 | validation loss: 1.9508e-03\n",
      "Epoch: 123150 | training loss: 2.2383e-03 | validation loss: 1.9457e-03\n",
      "Epoch: 123160 | training loss: 2.2321e-03 | validation loss: 1.9103e-03\n",
      "Epoch: 123170 | training loss: 2.2143e-03 | validation loss: 1.9217e-03\n",
      "Epoch: 123180 | training loss: 2.2134e-03 | validation loss: 1.9169e-03\n",
      "Epoch: 123190 | training loss: 2.2135e-03 | validation loss: 1.9133e-03\n",
      "Epoch: 123200 | training loss: 2.2133e-03 | validation loss: 1.9182e-03\n",
      "Epoch: 123210 | training loss: 2.2131e-03 | validation loss: 1.9145e-03\n",
      "Epoch: 123220 | training loss: 2.2130e-03 | validation loss: 1.9159e-03\n",
      "Epoch: 123230 | training loss: 2.2130e-03 | validation loss: 1.9160e-03\n",
      "Epoch: 123240 | training loss: 2.2129e-03 | validation loss: 1.9152e-03\n",
      "Epoch: 123250 | training loss: 2.2129e-03 | validation loss: 1.9150e-03\n",
      "Epoch: 123260 | training loss: 2.2129e-03 | validation loss: 1.9150e-03\n",
      "Epoch: 123270 | training loss: 2.2129e-03 | validation loss: 1.9141e-03\n",
      "Epoch: 123280 | training loss: 2.2158e-03 | validation loss: 1.9097e-03\n",
      "Epoch: 123290 | training loss: 2.4042e-03 | validation loss: 1.9497e-03\n",
      "Epoch: 123300 | training loss: 2.4815e-03 | validation loss: 1.9787e-03\n",
      "Epoch: 123310 | training loss: 2.4333e-03 | validation loss: 1.9564e-03\n",
      "Epoch: 123320 | training loss: 2.2314e-03 | validation loss: 1.9399e-03\n",
      "Epoch: 123330 | training loss: 2.2550e-03 | validation loss: 1.9628e-03\n",
      "Epoch: 123340 | training loss: 2.2127e-03 | validation loss: 1.9135e-03\n",
      "Epoch: 123350 | training loss: 2.2181e-03 | validation loss: 1.9076e-03\n",
      "Epoch: 123360 | training loss: 2.2130e-03 | validation loss: 1.9180e-03\n",
      "Epoch: 123370 | training loss: 2.2127e-03 | validation loss: 1.9170e-03\n",
      "Epoch: 123380 | training loss: 2.2127e-03 | validation loss: 1.9127e-03\n",
      "Epoch: 123390 | training loss: 2.2125e-03 | validation loss: 1.9162e-03\n",
      "Epoch: 123400 | training loss: 2.2124e-03 | validation loss: 1.9140e-03\n",
      "Epoch: 123410 | training loss: 2.2123e-03 | validation loss: 1.9151e-03\n",
      "Epoch: 123420 | training loss: 2.2123e-03 | validation loss: 1.9142e-03\n",
      "Epoch: 123430 | training loss: 2.2123e-03 | validation loss: 1.9146e-03\n",
      "Epoch: 123440 | training loss: 2.2122e-03 | validation loss: 1.9139e-03\n",
      "Epoch: 123450 | training loss: 2.2134e-03 | validation loss: 1.9103e-03\n",
      "Epoch: 123460 | training loss: 2.3623e-03 | validation loss: 1.9668e-03\n",
      "Epoch: 123470 | training loss: 2.3621e-03 | validation loss: 2.0017e-03\n",
      "Epoch: 123480 | training loss: 2.2337e-03 | validation loss: 1.9208e-03\n",
      "Epoch: 123490 | training loss: 2.2515e-03 | validation loss: 1.9330e-03\n",
      "Epoch: 123500 | training loss: 2.2240e-03 | validation loss: 1.9116e-03\n",
      "Epoch: 123510 | training loss: 2.2134e-03 | validation loss: 1.9076e-03\n",
      "Epoch: 123520 | training loss: 2.2209e-03 | validation loss: 1.9067e-03\n",
      "Epoch: 123530 | training loss: 2.3644e-03 | validation loss: 1.9267e-03\n",
      "Epoch: 123540 | training loss: 2.5999e-03 | validation loss: 2.0045e-03\n",
      "Epoch: 123550 | training loss: 2.3075e-03 | validation loss: 2.0001e-03\n",
      "Epoch: 123560 | training loss: 2.2245e-03 | validation loss: 1.9011e-03\n",
      "Epoch: 123570 | training loss: 2.2130e-03 | validation loss: 1.9177e-03\n",
      "Epoch: 123580 | training loss: 2.2122e-03 | validation loss: 1.9108e-03\n",
      "Epoch: 123590 | training loss: 2.2127e-03 | validation loss: 1.9188e-03\n",
      "Epoch: 123600 | training loss: 2.2130e-03 | validation loss: 1.9086e-03\n",
      "Epoch: 123610 | training loss: 2.2120e-03 | validation loss: 1.9159e-03\n",
      "Epoch: 123620 | training loss: 2.2118e-03 | validation loss: 1.9153e-03\n",
      "Epoch: 123630 | training loss: 2.2116e-03 | validation loss: 1.9131e-03\n",
      "Epoch: 123640 | training loss: 2.2116e-03 | validation loss: 1.9121e-03\n",
      "Epoch: 123650 | training loss: 2.2121e-03 | validation loss: 1.9097e-03\n",
      "Epoch: 123660 | training loss: 2.2352e-03 | validation loss: 1.9021e-03\n",
      "Epoch: 123670 | training loss: 3.3084e-03 | validation loss: 2.2806e-03\n",
      "Epoch: 123680 | training loss: 2.7352e-03 | validation loss: 2.2553e-03\n",
      "Epoch: 123690 | training loss: 2.2252e-03 | validation loss: 1.9254e-03\n",
      "Epoch: 123700 | training loss: 2.2707e-03 | validation loss: 1.9115e-03\n",
      "Epoch: 123710 | training loss: 2.2156e-03 | validation loss: 1.9140e-03\n",
      "Epoch: 123720 | training loss: 2.2165e-03 | validation loss: 1.9234e-03\n",
      "Epoch: 123730 | training loss: 2.2140e-03 | validation loss: 1.9110e-03\n",
      "Epoch: 123740 | training loss: 2.2114e-03 | validation loss: 1.9129e-03\n",
      "Epoch: 123750 | training loss: 2.2112e-03 | validation loss: 1.9129e-03\n",
      "Epoch: 123760 | training loss: 2.2112e-03 | validation loss: 1.9123e-03\n",
      "Epoch: 123770 | training loss: 2.2111e-03 | validation loss: 1.9125e-03\n",
      "Epoch: 123780 | training loss: 2.2111e-03 | validation loss: 1.9125e-03\n",
      "Epoch: 123790 | training loss: 2.2110e-03 | validation loss: 1.9120e-03\n",
      "Epoch: 123800 | training loss: 2.2110e-03 | validation loss: 1.9125e-03\n",
      "Epoch: 123810 | training loss: 2.2109e-03 | validation loss: 1.9123e-03\n",
      "Epoch: 123820 | training loss: 2.2109e-03 | validation loss: 1.9120e-03\n",
      "Epoch: 123830 | training loss: 2.2109e-03 | validation loss: 1.9119e-03\n",
      "Epoch: 123840 | training loss: 2.2109e-03 | validation loss: 1.9113e-03\n",
      "Epoch: 123850 | training loss: 2.2124e-03 | validation loss: 1.9084e-03\n",
      "Epoch: 123860 | training loss: 2.3617e-03 | validation loss: 1.9429e-03\n",
      "Epoch: 123870 | training loss: 2.6904e-03 | validation loss: 2.0324e-03\n",
      "Epoch: 123880 | training loss: 2.5654e-03 | validation loss: 2.0661e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 123890 | training loss: 2.2911e-03 | validation loss: 1.9909e-03\n",
      "Epoch: 123900 | training loss: 2.2180e-03 | validation loss: 1.9266e-03\n",
      "Epoch: 123910 | training loss: 2.2125e-03 | validation loss: 1.9051e-03\n",
      "Epoch: 123920 | training loss: 2.2133e-03 | validation loss: 1.9119e-03\n",
      "Epoch: 123930 | training loss: 2.2129e-03 | validation loss: 1.9143e-03\n",
      "Epoch: 123940 | training loss: 2.2110e-03 | validation loss: 1.9096e-03\n",
      "Epoch: 123950 | training loss: 2.2105e-03 | validation loss: 1.9126e-03\n",
      "Epoch: 123960 | training loss: 2.2105e-03 | validation loss: 1.9112e-03\n",
      "Epoch: 123970 | training loss: 2.2104e-03 | validation loss: 1.9116e-03\n",
      "Epoch: 123980 | training loss: 2.2104e-03 | validation loss: 1.9109e-03\n",
      "Epoch: 123990 | training loss: 2.2103e-03 | validation loss: 1.9113e-03\n",
      "Epoch: 124000 | training loss: 2.2103e-03 | validation loss: 1.9112e-03\n",
      "Epoch: 124010 | training loss: 2.2103e-03 | validation loss: 1.9110e-03\n",
      "Epoch: 124020 | training loss: 2.2102e-03 | validation loss: 1.9109e-03\n",
      "Epoch: 124030 | training loss: 2.2102e-03 | validation loss: 1.9107e-03\n",
      "Epoch: 124040 | training loss: 2.2102e-03 | validation loss: 1.9095e-03\n",
      "Epoch: 124050 | training loss: 2.2138e-03 | validation loss: 1.9015e-03\n",
      "Epoch: 124060 | training loss: 2.6076e-03 | validation loss: 2.0101e-03\n",
      "Epoch: 124070 | training loss: 2.5026e-03 | validation loss: 2.1946e-03\n",
      "Epoch: 124080 | training loss: 2.2825e-03 | validation loss: 1.9369e-03\n",
      "Epoch: 124090 | training loss: 2.2216e-03 | validation loss: 1.9212e-03\n",
      "Epoch: 124100 | training loss: 2.2162e-03 | validation loss: 1.9059e-03\n",
      "Epoch: 124110 | training loss: 2.2173e-03 | validation loss: 1.8985e-03\n",
      "Epoch: 124120 | training loss: 2.2136e-03 | validation loss: 1.9088e-03\n",
      "Epoch: 124130 | training loss: 2.2107e-03 | validation loss: 1.9052e-03\n",
      "Epoch: 124140 | training loss: 2.2099e-03 | validation loss: 1.9114e-03\n",
      "Epoch: 124150 | training loss: 2.2099e-03 | validation loss: 1.9107e-03\n",
      "Epoch: 124160 | training loss: 2.2098e-03 | validation loss: 1.9111e-03\n",
      "Epoch: 124170 | training loss: 2.2097e-03 | validation loss: 1.9094e-03\n",
      "Epoch: 124180 | training loss: 2.2097e-03 | validation loss: 1.9093e-03\n",
      "Epoch: 124190 | training loss: 2.2097e-03 | validation loss: 1.9098e-03\n",
      "Epoch: 124200 | training loss: 2.2096e-03 | validation loss: 1.9094e-03\n",
      "Epoch: 124210 | training loss: 2.2096e-03 | validation loss: 1.9090e-03\n",
      "Epoch: 124220 | training loss: 2.2106e-03 | validation loss: 1.9063e-03\n",
      "Epoch: 124230 | training loss: 2.2932e-03 | validation loss: 1.9150e-03\n",
      "Epoch: 124240 | training loss: 3.4938e-03 | validation loss: 2.3726e-03\n",
      "Epoch: 124250 | training loss: 2.3072e-03 | validation loss: 1.9249e-03\n",
      "Epoch: 124260 | training loss: 2.2530e-03 | validation loss: 1.9624e-03\n",
      "Epoch: 124270 | training loss: 2.2677e-03 | validation loss: 1.9700e-03\n",
      "Epoch: 124280 | training loss: 2.2168e-03 | validation loss: 1.9239e-03\n",
      "Epoch: 124290 | training loss: 2.2116e-03 | validation loss: 1.9031e-03\n",
      "Epoch: 124300 | training loss: 2.2117e-03 | validation loss: 1.9032e-03\n",
      "Epoch: 124310 | training loss: 2.2094e-03 | validation loss: 1.9110e-03\n",
      "Epoch: 124320 | training loss: 2.2095e-03 | validation loss: 1.9117e-03\n",
      "Epoch: 124330 | training loss: 2.2093e-03 | validation loss: 1.9079e-03\n",
      "Epoch: 124340 | training loss: 2.2092e-03 | validation loss: 1.9091e-03\n",
      "Epoch: 124350 | training loss: 2.2091e-03 | validation loss: 1.9093e-03\n",
      "Epoch: 124360 | training loss: 2.2091e-03 | validation loss: 1.9087e-03\n",
      "Epoch: 124370 | training loss: 2.2090e-03 | validation loss: 1.9090e-03\n",
      "Epoch: 124380 | training loss: 2.2090e-03 | validation loss: 1.9088e-03\n",
      "Epoch: 124390 | training loss: 2.2090e-03 | validation loss: 1.9088e-03\n",
      "Epoch: 124400 | training loss: 2.2089e-03 | validation loss: 1.9088e-03\n",
      "Epoch: 124410 | training loss: 2.2089e-03 | validation loss: 1.9087e-03\n",
      "Epoch: 124420 | training loss: 2.2089e-03 | validation loss: 1.9086e-03\n",
      "Epoch: 124430 | training loss: 2.2088e-03 | validation loss: 1.9085e-03\n",
      "Epoch: 124440 | training loss: 2.2088e-03 | validation loss: 1.9083e-03\n",
      "Epoch: 124450 | training loss: 2.2089e-03 | validation loss: 1.9072e-03\n",
      "Epoch: 124460 | training loss: 2.2176e-03 | validation loss: 1.9008e-03\n",
      "Epoch: 124470 | training loss: 3.3108e-03 | validation loss: 2.2990e-03\n",
      "Epoch: 124480 | training loss: 3.1198e-03 | validation loss: 2.4598e-03\n",
      "Epoch: 124490 | training loss: 2.3814e-03 | validation loss: 2.0390e-03\n",
      "Epoch: 124500 | training loss: 2.2195e-03 | validation loss: 1.9244e-03\n",
      "Epoch: 124510 | training loss: 2.2107e-03 | validation loss: 1.9036e-03\n",
      "Epoch: 124520 | training loss: 2.2173e-03 | validation loss: 1.9012e-03\n",
      "Epoch: 124530 | training loss: 2.2133e-03 | validation loss: 1.9021e-03\n",
      "Epoch: 124540 | training loss: 2.2087e-03 | validation loss: 1.9065e-03\n",
      "Epoch: 124550 | training loss: 2.2088e-03 | validation loss: 1.9105e-03\n",
      "Epoch: 124560 | training loss: 2.2085e-03 | validation loss: 1.9095e-03\n",
      "Epoch: 124570 | training loss: 2.2084e-03 | validation loss: 1.9072e-03\n",
      "Epoch: 124580 | training loss: 2.2083e-03 | validation loss: 1.9074e-03\n",
      "Epoch: 124590 | training loss: 2.2083e-03 | validation loss: 1.9082e-03\n",
      "Epoch: 124600 | training loss: 2.2083e-03 | validation loss: 1.9076e-03\n",
      "Epoch: 124610 | training loss: 2.2082e-03 | validation loss: 1.9077e-03\n",
      "Epoch: 124620 | training loss: 2.2082e-03 | validation loss: 1.9076e-03\n",
      "Epoch: 124630 | training loss: 2.2081e-03 | validation loss: 1.9075e-03\n",
      "Epoch: 124640 | training loss: 2.2081e-03 | validation loss: 1.9075e-03\n",
      "Epoch: 124650 | training loss: 2.2081e-03 | validation loss: 1.9072e-03\n",
      "Epoch: 124660 | training loss: 2.2088e-03 | validation loss: 1.9037e-03\n",
      "Epoch: 124670 | training loss: 2.4969e-03 | validation loss: 2.0287e-03\n",
      "Epoch: 124680 | training loss: 2.6618e-03 | validation loss: 2.2592e-03\n",
      "Epoch: 124690 | training loss: 2.3517e-03 | validation loss: 2.0496e-03\n",
      "Epoch: 124700 | training loss: 2.2373e-03 | validation loss: 1.9373e-03\n",
      "Epoch: 124710 | training loss: 2.2108e-03 | validation loss: 1.9170e-03\n",
      "Epoch: 124720 | training loss: 2.2092e-03 | validation loss: 1.9009e-03\n",
      "Epoch: 124730 | training loss: 2.2087e-03 | validation loss: 1.9090e-03\n",
      "Epoch: 124740 | training loss: 2.2080e-03 | validation loss: 1.9060e-03\n",
      "Epoch: 124750 | training loss: 2.2079e-03 | validation loss: 1.9040e-03\n",
      "Epoch: 124760 | training loss: 2.2081e-03 | validation loss: 1.9034e-03\n",
      "Epoch: 124770 | training loss: 2.2104e-03 | validation loss: 1.8995e-03\n",
      "Epoch: 124780 | training loss: 2.2721e-03 | validation loss: 1.8973e-03\n",
      "Epoch: 124790 | training loss: 3.1066e-03 | validation loss: 2.1831e-03\n",
      "Epoch: 124800 | training loss: 2.4692e-03 | validation loss: 2.1010e-03\n",
      "Epoch: 124810 | training loss: 2.2239e-03 | validation loss: 1.8939e-03\n",
      "Epoch: 124820 | training loss: 2.2131e-03 | validation loss: 1.8968e-03\n",
      "Epoch: 124830 | training loss: 2.2177e-03 | validation loss: 1.9248e-03\n",
      "Epoch: 124840 | training loss: 2.2126e-03 | validation loss: 1.8979e-03\n",
      "Epoch: 124850 | training loss: 2.2094e-03 | validation loss: 1.9133e-03\n",
      "Epoch: 124860 | training loss: 2.2081e-03 | validation loss: 1.9023e-03\n",
      "Epoch: 124870 | training loss: 2.2075e-03 | validation loss: 1.9076e-03\n",
      "Epoch: 124880 | training loss: 2.2073e-03 | validation loss: 1.9063e-03\n",
      "Epoch: 124890 | training loss: 2.2073e-03 | validation loss: 1.9045e-03\n",
      "Epoch: 124900 | training loss: 2.2073e-03 | validation loss: 1.9049e-03\n",
      "Epoch: 124910 | training loss: 2.2072e-03 | validation loss: 1.9050e-03\n",
      "Epoch: 124920 | training loss: 2.2073e-03 | validation loss: 1.9038e-03\n",
      "Epoch: 124930 | training loss: 2.2116e-03 | validation loss: 1.8980e-03\n",
      "Epoch: 124940 | training loss: 2.4866e-03 | validation loss: 1.9604e-03\n",
      "Epoch: 124950 | training loss: 2.2337e-03 | validation loss: 1.9100e-03\n",
      "Epoch: 124960 | training loss: 2.4715e-03 | validation loss: 1.9641e-03\n",
      "Epoch: 124970 | training loss: 2.2116e-03 | validation loss: 1.9086e-03\n",
      "Epoch: 124980 | training loss: 2.2447e-03 | validation loss: 1.9432e-03\n",
      "Epoch: 124990 | training loss: 2.2072e-03 | validation loss: 1.9024e-03\n",
      "Epoch: 125000 | training loss: 2.2112e-03 | validation loss: 1.9001e-03\n",
      "Epoch: 125010 | training loss: 2.2079e-03 | validation loss: 1.9106e-03\n",
      "Epoch: 125020 | training loss: 2.2068e-03 | validation loss: 1.9050e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 125030 | training loss: 2.2069e-03 | validation loss: 1.9036e-03\n",
      "Epoch: 125040 | training loss: 2.2068e-03 | validation loss: 1.9063e-03\n",
      "Epoch: 125050 | training loss: 2.2068e-03 | validation loss: 1.9043e-03\n",
      "Epoch: 125060 | training loss: 2.2067e-03 | validation loss: 1.9052e-03\n",
      "Epoch: 125070 | training loss: 2.2067e-03 | validation loss: 1.9048e-03\n",
      "Epoch: 125080 | training loss: 2.2066e-03 | validation loss: 1.9046e-03\n",
      "Epoch: 125090 | training loss: 2.2066e-03 | validation loss: 1.9048e-03\n",
      "Epoch: 125100 | training loss: 2.2066e-03 | validation loss: 1.9048e-03\n",
      "Epoch: 125110 | training loss: 2.2065e-03 | validation loss: 1.9048e-03\n",
      "Epoch: 125120 | training loss: 2.2065e-03 | validation loss: 1.9054e-03\n",
      "Epoch: 125130 | training loss: 2.2079e-03 | validation loss: 1.9101e-03\n",
      "Epoch: 125140 | training loss: 2.3229e-03 | validation loss: 2.0033e-03\n",
      "Epoch: 125150 | training loss: 3.0533e-03 | validation loss: 2.4222e-03\n",
      "Epoch: 125160 | training loss: 2.4149e-03 | validation loss: 2.0645e-03\n",
      "Epoch: 125170 | training loss: 2.2163e-03 | validation loss: 1.8969e-03\n",
      "Epoch: 125180 | training loss: 2.2554e-03 | validation loss: 1.8961e-03\n",
      "Epoch: 125190 | training loss: 2.2165e-03 | validation loss: 1.8952e-03\n",
      "Epoch: 125200 | training loss: 2.2076e-03 | validation loss: 1.9103e-03\n",
      "Epoch: 125210 | training loss: 2.2085e-03 | validation loss: 1.9116e-03\n",
      "Epoch: 125220 | training loss: 2.2063e-03 | validation loss: 1.9027e-03\n",
      "Epoch: 125230 | training loss: 2.2063e-03 | validation loss: 1.9022e-03\n",
      "Epoch: 125240 | training loss: 2.2062e-03 | validation loss: 1.9056e-03\n",
      "Epoch: 125250 | training loss: 2.2061e-03 | validation loss: 1.9035e-03\n",
      "Epoch: 125260 | training loss: 2.2060e-03 | validation loss: 1.9038e-03\n",
      "Epoch: 125270 | training loss: 2.2060e-03 | validation loss: 1.9039e-03\n",
      "Epoch: 125280 | training loss: 2.2059e-03 | validation loss: 1.9037e-03\n",
      "Epoch: 125290 | training loss: 2.2059e-03 | validation loss: 1.9037e-03\n",
      "Epoch: 125300 | training loss: 2.2059e-03 | validation loss: 1.9039e-03\n",
      "Epoch: 125310 | training loss: 2.2059e-03 | validation loss: 1.9049e-03\n",
      "Epoch: 125320 | training loss: 2.2172e-03 | validation loss: 1.9261e-03\n",
      "Epoch: 125330 | training loss: 2.8954e-03 | validation loss: 2.4961e-03\n",
      "Epoch: 125340 | training loss: 2.2114e-03 | validation loss: 1.9211e-03\n",
      "Epoch: 125350 | training loss: 2.2281e-03 | validation loss: 1.8920e-03\n",
      "Epoch: 125360 | training loss: 2.2236e-03 | validation loss: 1.9057e-03\n",
      "Epoch: 125370 | training loss: 2.2169e-03 | validation loss: 1.9080e-03\n",
      "Epoch: 125380 | training loss: 2.2093e-03 | validation loss: 1.9076e-03\n",
      "Epoch: 125390 | training loss: 2.2137e-03 | validation loss: 1.9205e-03\n",
      "Epoch: 125400 | training loss: 2.3362e-03 | validation loss: 2.0221e-03\n",
      "Epoch: 125410 | training loss: 2.6661e-03 | validation loss: 2.2210e-03\n",
      "Epoch: 125420 | training loss: 2.3477e-03 | validation loss: 1.9159e-03\n",
      "Epoch: 125430 | training loss: 2.2415e-03 | validation loss: 1.9476e-03\n",
      "Epoch: 125440 | training loss: 2.2158e-03 | validation loss: 1.8924e-03\n",
      "Epoch: 125450 | training loss: 2.2106e-03 | validation loss: 1.9149e-03\n",
      "Epoch: 125460 | training loss: 2.2088e-03 | validation loss: 1.8954e-03\n",
      "Epoch: 125470 | training loss: 2.2065e-03 | validation loss: 1.9080e-03\n",
      "Epoch: 125480 | training loss: 2.2053e-03 | validation loss: 1.9033e-03\n",
      "Epoch: 125490 | training loss: 2.2056e-03 | validation loss: 1.8998e-03\n",
      "Epoch: 125500 | training loss: 2.2057e-03 | validation loss: 1.8991e-03\n",
      "Epoch: 125510 | training loss: 2.2079e-03 | validation loss: 1.8959e-03\n",
      "Epoch: 125520 | training loss: 2.2558e-03 | validation loss: 1.8934e-03\n",
      "Epoch: 125530 | training loss: 3.0741e-03 | validation loss: 2.1790e-03\n",
      "Epoch: 125540 | training loss: 2.5099e-03 | validation loss: 2.1183e-03\n",
      "Epoch: 125550 | training loss: 2.2948e-03 | validation loss: 1.9111e-03\n",
      "Epoch: 125560 | training loss: 2.2127e-03 | validation loss: 1.9152e-03\n",
      "Epoch: 125570 | training loss: 2.2051e-03 | validation loss: 1.9008e-03\n",
      "Epoch: 125580 | training loss: 2.2053e-03 | validation loss: 1.9007e-03\n",
      "Epoch: 125590 | training loss: 2.2050e-03 | validation loss: 1.9028e-03\n",
      "Epoch: 125600 | training loss: 2.2049e-03 | validation loss: 1.9025e-03\n",
      "Epoch: 125610 | training loss: 2.2051e-03 | validation loss: 1.8999e-03\n",
      "Epoch: 125620 | training loss: 2.2049e-03 | validation loss: 1.9029e-03\n",
      "Epoch: 125630 | training loss: 2.2048e-03 | validation loss: 1.9028e-03\n",
      "Epoch: 125640 | training loss: 2.2047e-03 | validation loss: 1.9020e-03\n",
      "Epoch: 125650 | training loss: 2.2047e-03 | validation loss: 1.9022e-03\n",
      "Epoch: 125660 | training loss: 2.2053e-03 | validation loss: 1.9051e-03\n",
      "Epoch: 125670 | training loss: 2.2367e-03 | validation loss: 1.9396e-03\n",
      "Epoch: 125680 | training loss: 3.6603e-03 | validation loss: 2.7446e-03\n",
      "Epoch: 125690 | training loss: 2.6117e-03 | validation loss: 2.0143e-03\n",
      "Epoch: 125700 | training loss: 2.2823e-03 | validation loss: 1.8976e-03\n",
      "Epoch: 125710 | training loss: 2.2261e-03 | validation loss: 1.9295e-03\n",
      "Epoch: 125720 | training loss: 2.2244e-03 | validation loss: 1.9308e-03\n",
      "Epoch: 125730 | training loss: 2.2060e-03 | validation loss: 1.8961e-03\n",
      "Epoch: 125740 | training loss: 2.2065e-03 | validation loss: 1.8960e-03\n",
      "Epoch: 125750 | training loss: 2.2052e-03 | validation loss: 1.9055e-03\n",
      "Epoch: 125760 | training loss: 2.2043e-03 | validation loss: 1.9008e-03\n",
      "Epoch: 125770 | training loss: 2.2043e-03 | validation loss: 1.9000e-03\n",
      "Epoch: 125780 | training loss: 2.2043e-03 | validation loss: 1.9018e-03\n",
      "Epoch: 125790 | training loss: 2.2042e-03 | validation loss: 1.9003e-03\n",
      "Epoch: 125800 | training loss: 2.2042e-03 | validation loss: 1.9011e-03\n",
      "Epoch: 125810 | training loss: 2.2041e-03 | validation loss: 1.9006e-03\n",
      "Epoch: 125820 | training loss: 2.2041e-03 | validation loss: 1.9006e-03\n",
      "Epoch: 125830 | training loss: 2.2041e-03 | validation loss: 1.9010e-03\n",
      "Epoch: 125840 | training loss: 2.2045e-03 | validation loss: 1.9038e-03\n",
      "Epoch: 125850 | training loss: 2.2780e-03 | validation loss: 1.9891e-03\n",
      "Epoch: 125860 | training loss: 2.2606e-03 | validation loss: 1.9633e-03\n",
      "Epoch: 125870 | training loss: 2.2826e-03 | validation loss: 1.9874e-03\n",
      "Epoch: 125880 | training loss: 2.2569e-03 | validation loss: 1.9399e-03\n",
      "Epoch: 125890 | training loss: 2.2246e-03 | validation loss: 1.9183e-03\n",
      "Epoch: 125900 | training loss: 2.2096e-03 | validation loss: 1.9075e-03\n",
      "Epoch: 125910 | training loss: 2.2052e-03 | validation loss: 1.8984e-03\n",
      "Epoch: 125920 | training loss: 2.2195e-03 | validation loss: 1.8871e-03\n",
      "Epoch: 125930 | training loss: 2.7682e-03 | validation loss: 2.0476e-03\n",
      "Epoch: 125940 | training loss: 2.3390e-03 | validation loss: 2.0174e-03\n",
      "Epoch: 125950 | training loss: 2.3699e-03 | validation loss: 1.9196e-03\n",
      "Epoch: 125960 | training loss: 2.2238e-03 | validation loss: 1.9296e-03\n",
      "Epoch: 125970 | training loss: 2.2055e-03 | validation loss: 1.9070e-03\n",
      "Epoch: 125980 | training loss: 2.2094e-03 | validation loss: 1.8918e-03\n",
      "Epoch: 125990 | training loss: 2.2066e-03 | validation loss: 1.9088e-03\n",
      "Epoch: 126000 | training loss: 2.2047e-03 | validation loss: 1.8950e-03\n",
      "Epoch: 126010 | training loss: 2.2039e-03 | validation loss: 1.9025e-03\n",
      "Epoch: 126020 | training loss: 2.2036e-03 | validation loss: 1.8977e-03\n",
      "Epoch: 126030 | training loss: 2.2034e-03 | validation loss: 1.8994e-03\n",
      "Epoch: 126040 | training loss: 2.2034e-03 | validation loss: 1.9000e-03\n",
      "Epoch: 126050 | training loss: 2.2033e-03 | validation loss: 1.8991e-03\n",
      "Epoch: 126060 | training loss: 2.2033e-03 | validation loss: 1.8987e-03\n",
      "Epoch: 126070 | training loss: 2.2033e-03 | validation loss: 1.8980e-03\n",
      "Epoch: 126080 | training loss: 2.2045e-03 | validation loss: 1.8945e-03\n",
      "Epoch: 126090 | training loss: 2.2735e-03 | validation loss: 1.8945e-03\n",
      "Epoch: 126100 | training loss: 3.5645e-03 | validation loss: 2.3838e-03\n",
      "Epoch: 126110 | training loss: 2.2254e-03 | validation loss: 1.9126e-03\n",
      "Epoch: 126120 | training loss: 2.3491e-03 | validation loss: 2.0134e-03\n",
      "Epoch: 126130 | training loss: 2.2083e-03 | validation loss: 1.9146e-03\n",
      "Epoch: 126140 | training loss: 2.2203e-03 | validation loss: 1.8962e-03\n",
      "Epoch: 126150 | training loss: 2.2036e-03 | validation loss: 1.8947e-03\n",
      "Epoch: 126160 | training loss: 2.2055e-03 | validation loss: 1.9039e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 126170 | training loss: 2.2032e-03 | validation loss: 1.8980e-03\n",
      "Epoch: 126180 | training loss: 2.2029e-03 | validation loss: 1.8982e-03\n",
      "Epoch: 126190 | training loss: 2.2029e-03 | validation loss: 1.8990e-03\n",
      "Epoch: 126200 | training loss: 2.2029e-03 | validation loss: 1.8980e-03\n",
      "Epoch: 126210 | training loss: 2.2028e-03 | validation loss: 1.8986e-03\n",
      "Epoch: 126220 | training loss: 2.2028e-03 | validation loss: 1.8982e-03\n",
      "Epoch: 126230 | training loss: 2.2027e-03 | validation loss: 1.8982e-03\n",
      "Epoch: 126240 | training loss: 2.2027e-03 | validation loss: 1.8983e-03\n",
      "Epoch: 126250 | training loss: 2.2027e-03 | validation loss: 1.8981e-03\n",
      "Epoch: 126260 | training loss: 2.2026e-03 | validation loss: 1.8980e-03\n",
      "Epoch: 126270 | training loss: 2.2026e-03 | validation loss: 1.8978e-03\n",
      "Epoch: 126280 | training loss: 2.2026e-03 | validation loss: 1.8973e-03\n",
      "Epoch: 126290 | training loss: 2.2042e-03 | validation loss: 1.8943e-03\n",
      "Epoch: 126300 | training loss: 2.3595e-03 | validation loss: 1.9313e-03\n",
      "Epoch: 126310 | training loss: 2.6400e-03 | validation loss: 2.0025e-03\n",
      "Epoch: 126320 | training loss: 2.5569e-03 | validation loss: 2.0508e-03\n",
      "Epoch: 126330 | training loss: 2.2857e-03 | validation loss: 1.9787e-03\n",
      "Epoch: 126340 | training loss: 2.2106e-03 | validation loss: 1.9134e-03\n",
      "Epoch: 126350 | training loss: 2.2041e-03 | validation loss: 1.8911e-03\n",
      "Epoch: 126360 | training loss: 2.2049e-03 | validation loss: 1.8979e-03\n",
      "Epoch: 126370 | training loss: 2.2046e-03 | validation loss: 1.9002e-03\n",
      "Epoch: 126380 | training loss: 2.2027e-03 | validation loss: 1.8956e-03\n",
      "Epoch: 126390 | training loss: 2.2023e-03 | validation loss: 1.8986e-03\n",
      "Epoch: 126400 | training loss: 2.2023e-03 | validation loss: 1.8971e-03\n",
      "Epoch: 126410 | training loss: 2.2021e-03 | validation loss: 1.8976e-03\n",
      "Epoch: 126420 | training loss: 2.2021e-03 | validation loss: 1.8969e-03\n",
      "Epoch: 126430 | training loss: 2.2021e-03 | validation loss: 1.8973e-03\n",
      "Epoch: 126440 | training loss: 2.2020e-03 | validation loss: 1.8972e-03\n",
      "Epoch: 126450 | training loss: 2.2020e-03 | validation loss: 1.8970e-03\n",
      "Epoch: 126460 | training loss: 2.2020e-03 | validation loss: 1.8969e-03\n",
      "Epoch: 126470 | training loss: 2.2019e-03 | validation loss: 1.8967e-03\n",
      "Epoch: 126480 | training loss: 2.2019e-03 | validation loss: 1.8955e-03\n",
      "Epoch: 126490 | training loss: 2.2055e-03 | validation loss: 1.8876e-03\n",
      "Epoch: 126500 | training loss: 2.6058e-03 | validation loss: 1.9990e-03\n",
      "Epoch: 126510 | training loss: 2.5099e-03 | validation loss: 2.1925e-03\n",
      "Epoch: 126520 | training loss: 2.2795e-03 | validation loss: 1.9301e-03\n",
      "Epoch: 126530 | training loss: 2.2104e-03 | validation loss: 1.9074e-03\n",
      "Epoch: 126540 | training loss: 2.2061e-03 | validation loss: 1.8938e-03\n",
      "Epoch: 126550 | training loss: 2.2080e-03 | validation loss: 1.8849e-03\n",
      "Epoch: 126560 | training loss: 2.2052e-03 | validation loss: 1.8947e-03\n",
      "Epoch: 126570 | training loss: 2.2026e-03 | validation loss: 1.8910e-03\n",
      "Epoch: 126580 | training loss: 2.2016e-03 | validation loss: 1.8964e-03\n",
      "Epoch: 126590 | training loss: 2.2016e-03 | validation loss: 1.8965e-03\n",
      "Epoch: 126600 | training loss: 2.2015e-03 | validation loss: 1.8973e-03\n",
      "Epoch: 126610 | training loss: 2.2015e-03 | validation loss: 1.8954e-03\n",
      "Epoch: 126620 | training loss: 2.2014e-03 | validation loss: 1.8952e-03\n",
      "Epoch: 126630 | training loss: 2.2014e-03 | validation loss: 1.8958e-03\n",
      "Epoch: 126640 | training loss: 2.2014e-03 | validation loss: 1.8958e-03\n",
      "Epoch: 126650 | training loss: 2.2013e-03 | validation loss: 1.8958e-03\n",
      "Epoch: 126660 | training loss: 2.2014e-03 | validation loss: 1.8970e-03\n",
      "Epoch: 126670 | training loss: 2.2108e-03 | validation loss: 1.9115e-03\n",
      "Epoch: 126680 | training loss: 3.2754e-03 | validation loss: 2.5309e-03\n",
      "Epoch: 126690 | training loss: 3.0522e-03 | validation loss: 2.1815e-03\n",
      "Epoch: 126700 | training loss: 2.3128e-03 | validation loss: 1.9118e-03\n",
      "Epoch: 126710 | training loss: 2.2034e-03 | validation loss: 1.9026e-03\n",
      "Epoch: 126720 | training loss: 2.2171e-03 | validation loss: 1.9232e-03\n",
      "Epoch: 126730 | training loss: 2.2141e-03 | validation loss: 1.9183e-03\n",
      "Epoch: 126740 | training loss: 2.2025e-03 | validation loss: 1.9009e-03\n",
      "Epoch: 126750 | training loss: 2.2015e-03 | validation loss: 1.8917e-03\n",
      "Epoch: 126760 | training loss: 2.2015e-03 | validation loss: 1.8920e-03\n",
      "Epoch: 126770 | training loss: 2.2010e-03 | validation loss: 1.8962e-03\n",
      "Epoch: 126780 | training loss: 2.2010e-03 | validation loss: 1.8963e-03\n",
      "Epoch: 126790 | training loss: 2.2009e-03 | validation loss: 1.8943e-03\n",
      "Epoch: 126800 | training loss: 2.2008e-03 | validation loss: 1.8950e-03\n",
      "Epoch: 126810 | training loss: 2.2008e-03 | validation loss: 1.8951e-03\n",
      "Epoch: 126820 | training loss: 2.2008e-03 | validation loss: 1.8947e-03\n",
      "Epoch: 126830 | training loss: 2.2007e-03 | validation loss: 1.8949e-03\n",
      "Epoch: 126840 | training loss: 2.2007e-03 | validation loss: 1.8948e-03\n",
      "Epoch: 126850 | training loss: 2.2007e-03 | validation loss: 1.8948e-03\n",
      "Epoch: 126860 | training loss: 2.2006e-03 | validation loss: 1.8947e-03\n",
      "Epoch: 126870 | training loss: 2.2006e-03 | validation loss: 1.8946e-03\n",
      "Epoch: 126880 | training loss: 2.2006e-03 | validation loss: 1.8946e-03\n",
      "Epoch: 126890 | training loss: 2.2005e-03 | validation loss: 1.8945e-03\n",
      "Epoch: 126900 | training loss: 2.2005e-03 | validation loss: 1.8944e-03\n",
      "Epoch: 126910 | training loss: 2.2005e-03 | validation loss: 1.8938e-03\n",
      "Epoch: 126920 | training loss: 2.2020e-03 | validation loss: 1.8898e-03\n",
      "Epoch: 126930 | training loss: 2.4521e-03 | validation loss: 1.9470e-03\n",
      "Epoch: 126940 | training loss: 2.2142e-03 | validation loss: 1.9261e-03\n",
      "Epoch: 126950 | training loss: 2.3875e-03 | validation loss: 1.9226e-03\n",
      "Epoch: 126960 | training loss: 2.3139e-03 | validation loss: 1.8940e-03\n",
      "Epoch: 126970 | training loss: 2.2523e-03 | validation loss: 1.8842e-03\n",
      "Epoch: 126980 | training loss: 2.2210e-03 | validation loss: 1.8828e-03\n",
      "Epoch: 126990 | training loss: 2.2059e-03 | validation loss: 1.8851e-03\n",
      "Epoch: 127000 | training loss: 2.2006e-03 | validation loss: 1.8915e-03\n",
      "Epoch: 127010 | training loss: 2.2003e-03 | validation loss: 1.8958e-03\n",
      "Epoch: 127020 | training loss: 2.2004e-03 | validation loss: 1.8971e-03\n",
      "Epoch: 127030 | training loss: 2.2001e-03 | validation loss: 1.8947e-03\n",
      "Epoch: 127040 | training loss: 2.2001e-03 | validation loss: 1.8930e-03\n",
      "Epoch: 127050 | training loss: 2.2000e-03 | validation loss: 1.8934e-03\n",
      "Epoch: 127060 | training loss: 2.2000e-03 | validation loss: 1.8940e-03\n",
      "Epoch: 127070 | training loss: 2.1999e-03 | validation loss: 1.8936e-03\n",
      "Epoch: 127080 | training loss: 2.1999e-03 | validation loss: 1.8935e-03\n",
      "Epoch: 127090 | training loss: 2.1999e-03 | validation loss: 1.8939e-03\n",
      "Epoch: 127100 | training loss: 2.2003e-03 | validation loss: 1.8966e-03\n",
      "Epoch: 127110 | training loss: 2.2749e-03 | validation loss: 1.9802e-03\n",
      "Epoch: 127120 | training loss: 2.2027e-03 | validation loss: 1.9024e-03\n",
      "Epoch: 127130 | training loss: 2.2862e-03 | validation loss: 1.9782e-03\n",
      "Epoch: 127140 | training loss: 2.2439e-03 | validation loss: 1.9442e-03\n",
      "Epoch: 127150 | training loss: 2.2095e-03 | validation loss: 1.9080e-03\n",
      "Epoch: 127160 | training loss: 2.1998e-03 | validation loss: 1.8951e-03\n",
      "Epoch: 127170 | training loss: 2.2005e-03 | validation loss: 1.8896e-03\n",
      "Epoch: 127180 | training loss: 2.2003e-03 | validation loss: 1.8911e-03\n",
      "Epoch: 127190 | training loss: 2.1995e-03 | validation loss: 1.8931e-03\n",
      "Epoch: 127200 | training loss: 2.1996e-03 | validation loss: 1.8939e-03\n",
      "Epoch: 127210 | training loss: 2.1995e-03 | validation loss: 1.8931e-03\n",
      "Epoch: 127220 | training loss: 2.1996e-03 | validation loss: 1.8944e-03\n",
      "Epoch: 127230 | training loss: 2.2078e-03 | validation loss: 1.9110e-03\n",
      "Epoch: 127240 | training loss: 2.8582e-03 | validation loss: 2.3301e-03\n",
      "Epoch: 127250 | training loss: 2.6077e-03 | validation loss: 1.9840e-03\n",
      "Epoch: 127260 | training loss: 2.3077e-03 | validation loss: 1.9661e-03\n",
      "Epoch: 127270 | training loss: 2.2580e-03 | validation loss: 1.9459e-03\n",
      "Epoch: 127280 | training loss: 2.2034e-03 | validation loss: 1.8932e-03\n",
      "Epoch: 127290 | training loss: 2.2095e-03 | validation loss: 1.8924e-03\n",
      "Epoch: 127300 | training loss: 2.2004e-03 | validation loss: 1.8988e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 127310 | training loss: 2.1996e-03 | validation loss: 1.8936e-03\n",
      "Epoch: 127320 | training loss: 2.1996e-03 | validation loss: 1.8883e-03\n",
      "Epoch: 127330 | training loss: 2.1992e-03 | validation loss: 1.8937e-03\n",
      "Epoch: 127340 | training loss: 2.1991e-03 | validation loss: 1.8917e-03\n",
      "Epoch: 127350 | training loss: 2.1990e-03 | validation loss: 1.8918e-03\n",
      "Epoch: 127360 | training loss: 2.1990e-03 | validation loss: 1.8916e-03\n",
      "Epoch: 127370 | training loss: 2.1989e-03 | validation loss: 1.8920e-03\n",
      "Epoch: 127380 | training loss: 2.1989e-03 | validation loss: 1.8915e-03\n",
      "Epoch: 127390 | training loss: 2.1989e-03 | validation loss: 1.8916e-03\n",
      "Epoch: 127400 | training loss: 2.1988e-03 | validation loss: 1.8916e-03\n",
      "Epoch: 127410 | training loss: 2.1988e-03 | validation loss: 1.8916e-03\n",
      "Epoch: 127420 | training loss: 2.1988e-03 | validation loss: 1.8917e-03\n",
      "Epoch: 127430 | training loss: 2.1989e-03 | validation loss: 1.8931e-03\n",
      "Epoch: 127440 | training loss: 2.2115e-03 | validation loss: 1.9115e-03\n",
      "Epoch: 127450 | training loss: 3.4721e-03 | validation loss: 2.6348e-03\n",
      "Epoch: 127460 | training loss: 2.9650e-03 | validation loss: 2.1453e-03\n",
      "Epoch: 127470 | training loss: 2.3537e-03 | validation loss: 1.9196e-03\n",
      "Epoch: 127480 | training loss: 2.2007e-03 | validation loss: 1.8887e-03\n",
      "Epoch: 127490 | training loss: 2.2103e-03 | validation loss: 1.9088e-03\n",
      "Epoch: 127500 | training loss: 2.2108e-03 | validation loss: 1.9090e-03\n",
      "Epoch: 127510 | training loss: 2.1997e-03 | validation loss: 1.8959e-03\n",
      "Epoch: 127520 | training loss: 2.1991e-03 | validation loss: 1.8885e-03\n",
      "Epoch: 127530 | training loss: 2.1988e-03 | validation loss: 1.8889e-03\n",
      "Epoch: 127540 | training loss: 2.1985e-03 | validation loss: 1.8921e-03\n",
      "Epoch: 127550 | training loss: 2.1984e-03 | validation loss: 1.8914e-03\n",
      "Epoch: 127560 | training loss: 2.1983e-03 | validation loss: 1.8902e-03\n",
      "Epoch: 127570 | training loss: 2.1983e-03 | validation loss: 1.8910e-03\n",
      "Epoch: 127580 | training loss: 2.1982e-03 | validation loss: 1.8906e-03\n",
      "Epoch: 127590 | training loss: 2.1982e-03 | validation loss: 1.8907e-03\n",
      "Epoch: 127600 | training loss: 2.1982e-03 | validation loss: 1.8906e-03\n",
      "Epoch: 127610 | training loss: 2.1981e-03 | validation loss: 1.8906e-03\n",
      "Epoch: 127620 | training loss: 2.1981e-03 | validation loss: 1.8905e-03\n",
      "Epoch: 127630 | training loss: 2.1981e-03 | validation loss: 1.8909e-03\n",
      "Epoch: 127640 | training loss: 2.1990e-03 | validation loss: 1.8958e-03\n",
      "Epoch: 127650 | training loss: 2.3616e-03 | validation loss: 2.0670e-03\n",
      "Epoch: 127660 | training loss: 2.4904e-03 | validation loss: 2.0705e-03\n",
      "Epoch: 127670 | training loss: 2.2362e-03 | validation loss: 1.8762e-03\n",
      "Epoch: 127680 | training loss: 2.2051e-03 | validation loss: 1.9082e-03\n",
      "Epoch: 127690 | training loss: 2.2006e-03 | validation loss: 1.8980e-03\n",
      "Epoch: 127700 | training loss: 2.2033e-03 | validation loss: 1.8899e-03\n",
      "Epoch: 127710 | training loss: 2.1999e-03 | validation loss: 1.8934e-03\n",
      "Epoch: 127720 | training loss: 2.1985e-03 | validation loss: 1.8942e-03\n",
      "Epoch: 127730 | training loss: 2.1990e-03 | validation loss: 1.8960e-03\n",
      "Epoch: 127740 | training loss: 2.2234e-03 | validation loss: 1.9253e-03\n",
      "Epoch: 127750 | training loss: 2.9412e-03 | validation loss: 2.3667e-03\n",
      "Epoch: 127760 | training loss: 2.4647e-03 | validation loss: 1.9399e-03\n",
      "Epoch: 127770 | training loss: 2.3320e-03 | validation loss: 2.0016e-03\n",
      "Epoch: 127780 | training loss: 2.2064e-03 | validation loss: 1.8787e-03\n",
      "Epoch: 127790 | training loss: 2.1997e-03 | validation loss: 1.8829e-03\n",
      "Epoch: 127800 | training loss: 2.2017e-03 | validation loss: 1.9001e-03\n",
      "Epoch: 127810 | training loss: 2.1997e-03 | validation loss: 1.8838e-03\n",
      "Epoch: 127820 | training loss: 2.1983e-03 | validation loss: 1.8937e-03\n",
      "Epoch: 127830 | training loss: 2.1976e-03 | validation loss: 1.8871e-03\n",
      "Epoch: 127840 | training loss: 2.1974e-03 | validation loss: 1.8890e-03\n",
      "Epoch: 127850 | training loss: 2.1974e-03 | validation loss: 1.8901e-03\n",
      "Epoch: 127860 | training loss: 2.1973e-03 | validation loss: 1.8886e-03\n",
      "Epoch: 127870 | training loss: 2.1973e-03 | validation loss: 1.8880e-03\n",
      "Epoch: 127880 | training loss: 2.1974e-03 | validation loss: 1.8872e-03\n",
      "Epoch: 127890 | training loss: 2.1994e-03 | validation loss: 1.8832e-03\n",
      "Epoch: 127900 | training loss: 2.2875e-03 | validation loss: 1.8895e-03\n",
      "Epoch: 127910 | training loss: 3.2501e-03 | validation loss: 2.2503e-03\n",
      "Epoch: 127920 | training loss: 2.2427e-03 | validation loss: 1.9283e-03\n",
      "Epoch: 127930 | training loss: 2.2886e-03 | validation loss: 1.9715e-03\n",
      "Epoch: 127940 | training loss: 2.2174e-03 | validation loss: 1.8867e-03\n",
      "Epoch: 127950 | training loss: 2.2011e-03 | validation loss: 1.8814e-03\n",
      "Epoch: 127960 | training loss: 2.2030e-03 | validation loss: 1.8990e-03\n",
      "Epoch: 127970 | training loss: 2.1981e-03 | validation loss: 1.8863e-03\n",
      "Epoch: 127980 | training loss: 2.1969e-03 | validation loss: 1.8889e-03\n",
      "Epoch: 127990 | training loss: 2.1969e-03 | validation loss: 1.8883e-03\n",
      "Epoch: 128000 | training loss: 2.1968e-03 | validation loss: 1.8884e-03\n",
      "Epoch: 128010 | training loss: 2.1968e-03 | validation loss: 1.8878e-03\n",
      "Epoch: 128020 | training loss: 2.1968e-03 | validation loss: 1.8887e-03\n",
      "Epoch: 128030 | training loss: 2.1967e-03 | validation loss: 1.8878e-03\n",
      "Epoch: 128040 | training loss: 2.1967e-03 | validation loss: 1.8878e-03\n",
      "Epoch: 128050 | training loss: 2.1967e-03 | validation loss: 1.8879e-03\n",
      "Epoch: 128060 | training loss: 2.1966e-03 | validation loss: 1.8879e-03\n",
      "Epoch: 128070 | training loss: 2.1966e-03 | validation loss: 1.8879e-03\n",
      "Epoch: 128080 | training loss: 2.1966e-03 | validation loss: 1.8876e-03\n",
      "Epoch: 128090 | training loss: 2.1971e-03 | validation loss: 1.8859e-03\n",
      "Epoch: 128100 | training loss: 2.3071e-03 | validation loss: 1.9089e-03\n",
      "Epoch: 128110 | training loss: 2.9379e-03 | validation loss: 2.1038e-03\n",
      "Epoch: 128120 | training loss: 2.5992e-03 | validation loss: 2.0144e-03\n",
      "Epoch: 128130 | training loss: 2.2433e-03 | validation loss: 1.8932e-03\n",
      "Epoch: 128140 | training loss: 2.1999e-03 | validation loss: 1.8871e-03\n",
      "Epoch: 128150 | training loss: 2.2137e-03 | validation loss: 1.9002e-03\n",
      "Epoch: 128160 | training loss: 2.2055e-03 | validation loss: 1.8952e-03\n",
      "Epoch: 128170 | training loss: 2.1982e-03 | validation loss: 1.8876e-03\n",
      "Epoch: 128180 | training loss: 2.1967e-03 | validation loss: 1.8848e-03\n",
      "Epoch: 128190 | training loss: 2.1963e-03 | validation loss: 1.8863e-03\n",
      "Epoch: 128200 | training loss: 2.1962e-03 | validation loss: 1.8880e-03\n",
      "Epoch: 128210 | training loss: 2.1962e-03 | validation loss: 1.8871e-03\n",
      "Epoch: 128220 | training loss: 2.1961e-03 | validation loss: 1.8867e-03\n",
      "Epoch: 128230 | training loss: 2.1961e-03 | validation loss: 1.8871e-03\n",
      "Epoch: 128240 | training loss: 2.1961e-03 | validation loss: 1.8868e-03\n",
      "Epoch: 128250 | training loss: 2.1960e-03 | validation loss: 1.8869e-03\n",
      "Epoch: 128260 | training loss: 2.1960e-03 | validation loss: 1.8868e-03\n",
      "Epoch: 128270 | training loss: 2.1960e-03 | validation loss: 1.8869e-03\n",
      "Epoch: 128280 | training loss: 2.1959e-03 | validation loss: 1.8868e-03\n",
      "Epoch: 128290 | training loss: 2.1959e-03 | validation loss: 1.8867e-03\n",
      "Epoch: 128300 | training loss: 2.1958e-03 | validation loss: 1.8867e-03\n",
      "Epoch: 128310 | training loss: 2.1958e-03 | validation loss: 1.8866e-03\n",
      "Epoch: 128320 | training loss: 2.1958e-03 | validation loss: 1.8866e-03\n",
      "Epoch: 128330 | training loss: 2.1957e-03 | validation loss: 1.8867e-03\n",
      "Epoch: 128340 | training loss: 2.1958e-03 | validation loss: 1.8878e-03\n",
      "Epoch: 128350 | training loss: 2.2020e-03 | validation loss: 1.9040e-03\n",
      "Epoch: 128360 | training loss: 3.2830e-03 | validation loss: 2.6328e-03\n",
      "Epoch: 128370 | training loss: 2.7494e-03 | validation loss: 2.0299e-03\n",
      "Epoch: 128380 | training loss: 2.2917e-03 | validation loss: 1.8914e-03\n",
      "Epoch: 128390 | training loss: 2.2261e-03 | validation loss: 1.8987e-03\n",
      "Epoch: 128400 | training loss: 2.2030e-03 | validation loss: 1.8945e-03\n",
      "Epoch: 128410 | training loss: 2.1957e-03 | validation loss: 1.8857e-03\n",
      "Epoch: 128420 | training loss: 2.1967e-03 | validation loss: 1.8847e-03\n",
      "Epoch: 128430 | training loss: 2.1955e-03 | validation loss: 1.8875e-03\n",
      "Epoch: 128440 | training loss: 2.1955e-03 | validation loss: 1.8870e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 128450 | training loss: 2.1954e-03 | validation loss: 1.8844e-03\n",
      "Epoch: 128460 | training loss: 2.1954e-03 | validation loss: 1.8851e-03\n",
      "Epoch: 128470 | training loss: 2.1953e-03 | validation loss: 1.8848e-03\n",
      "Epoch: 128480 | training loss: 2.1953e-03 | validation loss: 1.8850e-03\n",
      "Epoch: 128490 | training loss: 2.1952e-03 | validation loss: 1.8853e-03\n",
      "Epoch: 128500 | training loss: 2.1952e-03 | validation loss: 1.8854e-03\n",
      "Epoch: 128510 | training loss: 2.1952e-03 | validation loss: 1.8852e-03\n",
      "Epoch: 128520 | training loss: 2.1951e-03 | validation loss: 1.8851e-03\n",
      "Epoch: 128530 | training loss: 2.1951e-03 | validation loss: 1.8851e-03\n",
      "Epoch: 128540 | training loss: 2.1951e-03 | validation loss: 1.8851e-03\n",
      "Epoch: 128550 | training loss: 2.1950e-03 | validation loss: 1.8851e-03\n",
      "Epoch: 128560 | training loss: 2.1950e-03 | validation loss: 1.8854e-03\n",
      "Epoch: 128570 | training loss: 2.1960e-03 | validation loss: 1.8889e-03\n",
      "Epoch: 128580 | training loss: 2.3221e-03 | validation loss: 1.9864e-03\n",
      "Epoch: 128590 | training loss: 2.7677e-03 | validation loss: 2.2593e-03\n",
      "Epoch: 128600 | training loss: 2.6687e-03 | validation loss: 2.1969e-03\n",
      "Epoch: 128610 | training loss: 2.2878e-03 | validation loss: 1.9637e-03\n",
      "Epoch: 128620 | training loss: 2.2018e-03 | validation loss: 1.8916e-03\n",
      "Epoch: 128630 | training loss: 2.1982e-03 | validation loss: 1.8757e-03\n",
      "Epoch: 128640 | training loss: 2.2012e-03 | validation loss: 1.8746e-03\n",
      "Epoch: 128650 | training loss: 2.1969e-03 | validation loss: 1.8784e-03\n",
      "Epoch: 128660 | training loss: 2.1947e-03 | validation loss: 1.8850e-03\n",
      "Epoch: 128670 | training loss: 2.1950e-03 | validation loss: 1.8879e-03\n",
      "Epoch: 128680 | training loss: 2.1946e-03 | validation loss: 1.8852e-03\n",
      "Epoch: 128690 | training loss: 2.1946e-03 | validation loss: 1.8834e-03\n",
      "Epoch: 128700 | training loss: 2.1946e-03 | validation loss: 1.8844e-03\n",
      "Epoch: 128710 | training loss: 2.1945e-03 | validation loss: 1.8846e-03\n",
      "Epoch: 128720 | training loss: 2.1945e-03 | validation loss: 1.8841e-03\n",
      "Epoch: 128730 | training loss: 2.1945e-03 | validation loss: 1.8844e-03\n",
      "Epoch: 128740 | training loss: 2.1944e-03 | validation loss: 1.8842e-03\n",
      "Epoch: 128750 | training loss: 2.1944e-03 | validation loss: 1.8842e-03\n",
      "Epoch: 128760 | training loss: 2.1944e-03 | validation loss: 1.8841e-03\n",
      "Epoch: 128770 | training loss: 2.1943e-03 | validation loss: 1.8841e-03\n",
      "Epoch: 128780 | training loss: 2.1943e-03 | validation loss: 1.8840e-03\n",
      "Epoch: 128790 | training loss: 2.1942e-03 | validation loss: 1.8839e-03\n",
      "Epoch: 128800 | training loss: 2.1942e-03 | validation loss: 1.8839e-03\n",
      "Epoch: 128810 | training loss: 2.1942e-03 | validation loss: 1.8838e-03\n",
      "Epoch: 128820 | training loss: 2.1941e-03 | validation loss: 1.8837e-03\n",
      "Epoch: 128830 | training loss: 2.1941e-03 | validation loss: 1.8829e-03\n",
      "Epoch: 128840 | training loss: 2.1992e-03 | validation loss: 1.8764e-03\n",
      "Epoch: 128850 | training loss: 3.2590e-03 | validation loss: 2.2481e-03\n",
      "Epoch: 128860 | training loss: 3.3276e-03 | validation loss: 2.5688e-03\n",
      "Epoch: 128870 | training loss: 2.5830e-03 | validation loss: 2.1745e-03\n",
      "Epoch: 128880 | training loss: 2.3303e-03 | validation loss: 2.0088e-03\n",
      "Epoch: 128890 | training loss: 2.2415e-03 | validation loss: 1.9386e-03\n",
      "Epoch: 128900 | training loss: 2.2107e-03 | validation loss: 1.9127e-03\n",
      "Epoch: 128910 | training loss: 2.2001e-03 | validation loss: 1.8994e-03\n",
      "Epoch: 128920 | training loss: 2.1959e-03 | validation loss: 1.8912e-03\n",
      "Epoch: 128930 | training loss: 2.1943e-03 | validation loss: 1.8874e-03\n",
      "Epoch: 128940 | training loss: 2.1938e-03 | validation loss: 1.8839e-03\n",
      "Epoch: 128950 | training loss: 2.1938e-03 | validation loss: 1.8826e-03\n",
      "Epoch: 128960 | training loss: 2.1937e-03 | validation loss: 1.8821e-03\n",
      "Epoch: 128970 | training loss: 2.1937e-03 | validation loss: 1.8828e-03\n",
      "Epoch: 128980 | training loss: 2.1936e-03 | validation loss: 1.8833e-03\n",
      "Epoch: 128990 | training loss: 2.1936e-03 | validation loss: 1.8831e-03\n",
      "Epoch: 129000 | training loss: 2.1936e-03 | validation loss: 1.8828e-03\n",
      "Epoch: 129010 | training loss: 2.1935e-03 | validation loss: 1.8829e-03\n",
      "Epoch: 129020 | training loss: 2.1935e-03 | validation loss: 1.8829e-03\n",
      "Epoch: 129030 | training loss: 2.1935e-03 | validation loss: 1.8829e-03\n",
      "Epoch: 129040 | training loss: 2.1937e-03 | validation loss: 1.8849e-03\n",
      "Epoch: 129050 | training loss: 2.2790e-03 | validation loss: 1.9749e-03\n",
      "Epoch: 129060 | training loss: 2.2654e-03 | validation loss: 1.9060e-03\n",
      "Epoch: 129070 | training loss: 2.1935e-03 | validation loss: 1.8839e-03\n",
      "Epoch: 129080 | training loss: 2.1937e-03 | validation loss: 1.8806e-03\n",
      "Epoch: 129090 | training loss: 2.1938e-03 | validation loss: 1.8798e-03\n",
      "Epoch: 129100 | training loss: 2.1934e-03 | validation loss: 1.8810e-03\n",
      "Epoch: 129110 | training loss: 2.1932e-03 | validation loss: 1.8821e-03\n",
      "Epoch: 129120 | training loss: 2.1932e-03 | validation loss: 1.8826e-03\n",
      "Epoch: 129130 | training loss: 2.1933e-03 | validation loss: 1.8831e-03\n",
      "Epoch: 129140 | training loss: 2.1932e-03 | validation loss: 1.8829e-03\n",
      "Epoch: 129150 | training loss: 2.1931e-03 | validation loss: 1.8823e-03\n",
      "Epoch: 129160 | training loss: 2.1930e-03 | validation loss: 1.8817e-03\n",
      "Epoch: 129170 | training loss: 2.1930e-03 | validation loss: 1.8818e-03\n",
      "Epoch: 129180 | training loss: 2.1933e-03 | validation loss: 1.8845e-03\n",
      "Epoch: 129190 | training loss: 2.2286e-03 | validation loss: 1.9275e-03\n",
      "Epoch: 129200 | training loss: 4.1523e-03 | validation loss: 3.0130e-03\n",
      "Epoch: 129210 | training loss: 2.2250e-03 | validation loss: 1.9247e-03\n",
      "Epoch: 129220 | training loss: 2.3125e-03 | validation loss: 1.9198e-03\n",
      "Epoch: 129230 | training loss: 2.2663e-03 | validation loss: 1.8954e-03\n",
      "Epoch: 129240 | training loss: 2.2086e-03 | validation loss: 1.8810e-03\n",
      "Epoch: 129250 | training loss: 2.1937e-03 | validation loss: 1.8870e-03\n",
      "Epoch: 129260 | training loss: 2.1956e-03 | validation loss: 1.8915e-03\n",
      "Epoch: 129270 | training loss: 2.1932e-03 | validation loss: 1.8849e-03\n",
      "Epoch: 129280 | training loss: 2.1929e-03 | validation loss: 1.8793e-03\n",
      "Epoch: 129290 | training loss: 2.1927e-03 | validation loss: 1.8798e-03\n",
      "Epoch: 129300 | training loss: 2.1927e-03 | validation loss: 1.8817e-03\n",
      "Epoch: 129310 | training loss: 2.1926e-03 | validation loss: 1.8808e-03\n",
      "Epoch: 129320 | training loss: 2.1925e-03 | validation loss: 1.8809e-03\n",
      "Epoch: 129330 | training loss: 2.1925e-03 | validation loss: 1.8811e-03\n",
      "Epoch: 129340 | training loss: 2.1925e-03 | validation loss: 1.8807e-03\n",
      "Epoch: 129350 | training loss: 2.1924e-03 | validation loss: 1.8809e-03\n",
      "Epoch: 129360 | training loss: 2.1924e-03 | validation loss: 1.8807e-03\n",
      "Epoch: 129370 | training loss: 2.1924e-03 | validation loss: 1.8807e-03\n",
      "Epoch: 129380 | training loss: 2.1923e-03 | validation loss: 1.8807e-03\n",
      "Epoch: 129390 | training loss: 2.1923e-03 | validation loss: 1.8806e-03\n",
      "Epoch: 129400 | training loss: 2.1923e-03 | validation loss: 1.8805e-03\n",
      "Epoch: 129410 | training loss: 2.1922e-03 | validation loss: 1.8805e-03\n",
      "Epoch: 129420 | training loss: 2.1922e-03 | validation loss: 1.8804e-03\n",
      "Epoch: 129430 | training loss: 2.1922e-03 | validation loss: 1.8801e-03\n",
      "Epoch: 129440 | training loss: 2.1925e-03 | validation loss: 1.8780e-03\n",
      "Epoch: 129450 | training loss: 2.2764e-03 | validation loss: 1.8819e-03\n",
      "Epoch: 129460 | training loss: 3.2520e-03 | validation loss: 2.2578e-03\n",
      "Epoch: 129470 | training loss: 2.7974e-03 | validation loss: 2.0634e-03\n",
      "Epoch: 129480 | training loss: 2.3578e-03 | validation loss: 1.8979e-03\n",
      "Epoch: 129490 | training loss: 2.2238e-03 | validation loss: 1.8658e-03\n",
      "Epoch: 129500 | training loss: 2.1965e-03 | validation loss: 1.8707e-03\n",
      "Epoch: 129510 | training loss: 2.1925e-03 | validation loss: 1.8766e-03\n",
      "Epoch: 129520 | training loss: 2.1921e-03 | validation loss: 1.8786e-03\n",
      "Epoch: 129530 | training loss: 2.1920e-03 | validation loss: 1.8783e-03\n",
      "Epoch: 129540 | training loss: 2.1920e-03 | validation loss: 1.8779e-03\n",
      "Epoch: 129550 | training loss: 2.1919e-03 | validation loss: 1.8782e-03\n",
      "Epoch: 129560 | training loss: 2.1918e-03 | validation loss: 1.8788e-03\n",
      "Epoch: 129570 | training loss: 2.1917e-03 | validation loss: 1.8794e-03\n",
      "Epoch: 129580 | training loss: 2.1917e-03 | validation loss: 1.8800e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 129590 | training loss: 2.1917e-03 | validation loss: 1.8800e-03\n",
      "Epoch: 129600 | training loss: 2.1916e-03 | validation loss: 1.8797e-03\n",
      "Epoch: 129610 | training loss: 2.1916e-03 | validation loss: 1.8795e-03\n",
      "Epoch: 129620 | training loss: 2.1916e-03 | validation loss: 1.8796e-03\n",
      "Epoch: 129630 | training loss: 2.1915e-03 | validation loss: 1.8796e-03\n",
      "Epoch: 129640 | training loss: 2.1915e-03 | validation loss: 1.8794e-03\n",
      "Epoch: 129650 | training loss: 2.1915e-03 | validation loss: 1.8794e-03\n",
      "Epoch: 129660 | training loss: 2.1914e-03 | validation loss: 1.8794e-03\n",
      "Epoch: 129670 | training loss: 2.1914e-03 | validation loss: 1.8794e-03\n",
      "Epoch: 129680 | training loss: 2.1914e-03 | validation loss: 1.8801e-03\n",
      "Epoch: 129690 | training loss: 2.1997e-03 | validation loss: 1.8960e-03\n",
      "Epoch: 129700 | training loss: 2.9625e-03 | validation loss: 2.5167e-03\n",
      "Epoch: 129710 | training loss: 2.2153e-03 | validation loss: 1.9051e-03\n",
      "Epoch: 129720 | training loss: 2.1915e-03 | validation loss: 1.8818e-03\n",
      "Epoch: 129730 | training loss: 2.1919e-03 | validation loss: 1.8763e-03\n",
      "Epoch: 129740 | training loss: 2.1928e-03 | validation loss: 1.8742e-03\n",
      "Epoch: 129750 | training loss: 2.1929e-03 | validation loss: 1.8769e-03\n",
      "Epoch: 129760 | training loss: 2.1923e-03 | validation loss: 1.8771e-03\n",
      "Epoch: 129770 | training loss: 2.1915e-03 | validation loss: 1.8778e-03\n",
      "Epoch: 129780 | training loss: 2.1913e-03 | validation loss: 1.8806e-03\n",
      "Epoch: 129790 | training loss: 2.1971e-03 | validation loss: 1.8937e-03\n",
      "Epoch: 129800 | training loss: 2.5022e-03 | validation loss: 2.1117e-03\n",
      "Epoch: 129810 | training loss: 2.1979e-03 | validation loss: 1.8808e-03\n",
      "Epoch: 129820 | training loss: 2.3821e-03 | validation loss: 2.0258e-03\n",
      "Epoch: 129830 | training loss: 2.2210e-03 | validation loss: 1.8668e-03\n",
      "Epoch: 129840 | training loss: 2.2027e-03 | validation loss: 1.8686e-03\n",
      "Epoch: 129850 | training loss: 2.2015e-03 | validation loss: 1.8981e-03\n",
      "Epoch: 129860 | training loss: 2.1914e-03 | validation loss: 1.8755e-03\n",
      "Epoch: 129870 | training loss: 2.1909e-03 | validation loss: 1.8766e-03\n",
      "Epoch: 129880 | training loss: 2.1910e-03 | validation loss: 1.8800e-03\n",
      "Epoch: 129890 | training loss: 2.1908e-03 | validation loss: 1.8762e-03\n",
      "Epoch: 129900 | training loss: 2.1907e-03 | validation loss: 1.8786e-03\n",
      "Epoch: 129910 | training loss: 2.1906e-03 | validation loss: 1.8776e-03\n",
      "Epoch: 129920 | training loss: 2.1906e-03 | validation loss: 1.8772e-03\n",
      "Epoch: 129930 | training loss: 2.1905e-03 | validation loss: 1.8778e-03\n",
      "Epoch: 129940 | training loss: 2.1905e-03 | validation loss: 1.8778e-03\n",
      "Epoch: 129950 | training loss: 2.1905e-03 | validation loss: 1.8778e-03\n",
      "Epoch: 129960 | training loss: 2.1905e-03 | validation loss: 1.8784e-03\n",
      "Epoch: 129970 | training loss: 2.1918e-03 | validation loss: 1.8832e-03\n",
      "Epoch: 129980 | training loss: 2.2817e-03 | validation loss: 1.9619e-03\n",
      "Epoch: 129990 | training loss: 3.3442e-03 | validation loss: 2.5627e-03\n",
      "Epoch: 130000 | training loss: 2.2270e-03 | validation loss: 1.9326e-03\n",
      "Epoch: 130010 | training loss: 2.2905e-03 | validation loss: 1.8897e-03\n",
      "Epoch: 130020 | training loss: 2.2246e-03 | validation loss: 1.8612e-03\n",
      "Epoch: 130030 | training loss: 2.1948e-03 | validation loss: 1.8793e-03\n",
      "Epoch: 130040 | training loss: 2.1958e-03 | validation loss: 1.8937e-03\n",
      "Epoch: 130050 | training loss: 2.1906e-03 | validation loss: 1.8777e-03\n",
      "Epoch: 130060 | training loss: 2.1910e-03 | validation loss: 1.8722e-03\n",
      "Epoch: 130070 | training loss: 2.1903e-03 | validation loss: 1.8797e-03\n",
      "Epoch: 130080 | training loss: 2.1901e-03 | validation loss: 1.8767e-03\n",
      "Epoch: 130090 | training loss: 2.1900e-03 | validation loss: 1.8763e-03\n",
      "Epoch: 130100 | training loss: 2.1900e-03 | validation loss: 1.8772e-03\n",
      "Epoch: 130110 | training loss: 2.1900e-03 | validation loss: 1.8765e-03\n",
      "Epoch: 130120 | training loss: 2.1899e-03 | validation loss: 1.8767e-03\n",
      "Epoch: 130130 | training loss: 2.1899e-03 | validation loss: 1.8767e-03\n",
      "Epoch: 130140 | training loss: 2.1899e-03 | validation loss: 1.8765e-03\n",
      "Epoch: 130150 | training loss: 2.1898e-03 | validation loss: 1.8765e-03\n",
      "Epoch: 130160 | training loss: 2.1898e-03 | validation loss: 1.8766e-03\n",
      "Epoch: 130170 | training loss: 2.1899e-03 | validation loss: 1.8774e-03\n",
      "Epoch: 130180 | training loss: 2.2017e-03 | validation loss: 1.8911e-03\n",
      "Epoch: 130190 | training loss: 2.8112e-03 | validation loss: 2.3058e-03\n",
      "Epoch: 130200 | training loss: 2.6243e-03 | validation loss: 1.9765e-03\n",
      "Epoch: 130210 | training loss: 2.3401e-03 | validation loss: 1.9824e-03\n",
      "Epoch: 130220 | training loss: 2.2434e-03 | validation loss: 1.8648e-03\n",
      "Epoch: 130230 | training loss: 2.2064e-03 | validation loss: 1.8875e-03\n",
      "Epoch: 130240 | training loss: 2.1938e-03 | validation loss: 1.8720e-03\n",
      "Epoch: 130250 | training loss: 2.1928e-03 | validation loss: 1.8672e-03\n",
      "Epoch: 130260 | training loss: 2.1902e-03 | validation loss: 1.8788e-03\n",
      "Epoch: 130270 | training loss: 2.1905e-03 | validation loss: 1.8817e-03\n",
      "Epoch: 130280 | training loss: 2.1911e-03 | validation loss: 1.8833e-03\n",
      "Epoch: 130290 | training loss: 2.2021e-03 | validation loss: 1.8990e-03\n",
      "Epoch: 130300 | training loss: 2.4585e-03 | validation loss: 2.0805e-03\n",
      "Epoch: 130310 | training loss: 2.2655e-03 | validation loss: 1.9469e-03\n",
      "Epoch: 130320 | training loss: 2.1905e-03 | validation loss: 1.8707e-03\n",
      "Epoch: 130330 | training loss: 2.1955e-03 | validation loss: 1.8679e-03\n",
      "Epoch: 130340 | training loss: 2.1963e-03 | validation loss: 1.8910e-03\n",
      "Epoch: 130350 | training loss: 2.1921e-03 | validation loss: 1.8691e-03\n",
      "Epoch: 130360 | training loss: 2.1895e-03 | validation loss: 1.8779e-03\n",
      "Epoch: 130370 | training loss: 2.1892e-03 | validation loss: 1.8766e-03\n",
      "Epoch: 130380 | training loss: 2.1896e-03 | validation loss: 1.8722e-03\n",
      "Epoch: 130390 | training loss: 2.1891e-03 | validation loss: 1.8755e-03\n",
      "Epoch: 130400 | training loss: 2.1892e-03 | validation loss: 1.8770e-03\n",
      "Epoch: 130410 | training loss: 2.1896e-03 | validation loss: 1.8787e-03\n",
      "Epoch: 130420 | training loss: 2.1961e-03 | validation loss: 1.8900e-03\n",
      "Epoch: 130430 | training loss: 2.4002e-03 | validation loss: 2.0383e-03\n",
      "Epoch: 130440 | training loss: 2.4399e-03 | validation loss: 2.0556e-03\n",
      "Epoch: 130450 | training loss: 2.1920e-03 | validation loss: 1.8859e-03\n",
      "Epoch: 130460 | training loss: 2.2389e-03 | validation loss: 1.8691e-03\n",
      "Epoch: 130470 | training loss: 2.2180e-03 | validation loss: 1.9065e-03\n",
      "Epoch: 130480 | training loss: 2.1948e-03 | validation loss: 1.8700e-03\n",
      "Epoch: 130490 | training loss: 2.1896e-03 | validation loss: 1.8785e-03\n",
      "Epoch: 130500 | training loss: 2.1889e-03 | validation loss: 1.8724e-03\n",
      "Epoch: 130510 | training loss: 2.1889e-03 | validation loss: 1.8770e-03\n",
      "Epoch: 130520 | training loss: 2.1889e-03 | validation loss: 1.8725e-03\n",
      "Epoch: 130530 | training loss: 2.1887e-03 | validation loss: 1.8756e-03\n",
      "Epoch: 130540 | training loss: 2.1886e-03 | validation loss: 1.8750e-03\n",
      "Epoch: 130550 | training loss: 2.1885e-03 | validation loss: 1.8741e-03\n",
      "Epoch: 130560 | training loss: 2.1885e-03 | validation loss: 1.8737e-03\n",
      "Epoch: 130570 | training loss: 2.1887e-03 | validation loss: 1.8724e-03\n",
      "Epoch: 130580 | training loss: 2.1973e-03 | validation loss: 1.8668e-03\n",
      "Epoch: 130590 | training loss: 2.7416e-03 | validation loss: 2.0453e-03\n",
      "Epoch: 130600 | training loss: 2.3936e-03 | validation loss: 2.0255e-03\n",
      "Epoch: 130610 | training loss: 2.3814e-03 | validation loss: 1.9116e-03\n",
      "Epoch: 130620 | training loss: 2.2228e-03 | validation loss: 1.8716e-03\n",
      "Epoch: 130630 | training loss: 2.2051e-03 | validation loss: 1.8956e-03\n",
      "Epoch: 130640 | training loss: 2.1932e-03 | validation loss: 1.8844e-03\n",
      "Epoch: 130650 | training loss: 2.1917e-03 | validation loss: 1.8691e-03\n",
      "Epoch: 130660 | training loss: 2.1882e-03 | validation loss: 1.8741e-03\n",
      "Epoch: 130670 | training loss: 2.1885e-03 | validation loss: 1.8759e-03\n",
      "Epoch: 130680 | training loss: 2.1883e-03 | validation loss: 1.8724e-03\n",
      "Epoch: 130690 | training loss: 2.1881e-03 | validation loss: 1.8748e-03\n",
      "Epoch: 130700 | training loss: 2.1881e-03 | validation loss: 1.8730e-03\n",
      "Epoch: 130710 | training loss: 2.1880e-03 | validation loss: 1.8739e-03\n",
      "Epoch: 130720 | training loss: 2.1880e-03 | validation loss: 1.8731e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 130730 | training loss: 2.1880e-03 | validation loss: 1.8721e-03\n",
      "Epoch: 130740 | training loss: 2.1934e-03 | validation loss: 1.8659e-03\n",
      "Epoch: 130750 | training loss: 2.6709e-03 | validation loss: 2.0969e-03\n",
      "Epoch: 130760 | training loss: 2.3908e-03 | validation loss: 2.0343e-03\n",
      "Epoch: 130770 | training loss: 2.2674e-03 | validation loss: 1.9711e-03\n",
      "Epoch: 130780 | training loss: 2.1980e-03 | validation loss: 1.8681e-03\n",
      "Epoch: 130790 | training loss: 2.1908e-03 | validation loss: 1.8683e-03\n",
      "Epoch: 130800 | training loss: 2.1936e-03 | validation loss: 1.8776e-03\n",
      "Epoch: 130810 | training loss: 2.1898e-03 | validation loss: 1.8789e-03\n",
      "Epoch: 130820 | training loss: 2.1936e-03 | validation loss: 1.8887e-03\n",
      "Epoch: 130830 | training loss: 2.2612e-03 | validation loss: 1.9494e-03\n",
      "Epoch: 130840 | training loss: 2.8435e-03 | validation loss: 2.2998e-03\n",
      "Epoch: 130850 | training loss: 2.3968e-03 | validation loss: 1.9066e-03\n",
      "Epoch: 130860 | training loss: 2.2681e-03 | validation loss: 1.9491e-03\n",
      "Epoch: 130870 | training loss: 2.2175e-03 | validation loss: 1.8614e-03\n",
      "Epoch: 130880 | training loss: 2.1992e-03 | validation loss: 1.8931e-03\n",
      "Epoch: 130890 | training loss: 2.1911e-03 | validation loss: 1.8659e-03\n",
      "Epoch: 130900 | training loss: 2.1875e-03 | validation loss: 1.8740e-03\n",
      "Epoch: 130910 | training loss: 2.1880e-03 | validation loss: 1.8760e-03\n",
      "Epoch: 130920 | training loss: 2.1874e-03 | validation loss: 1.8713e-03\n",
      "Epoch: 130930 | training loss: 2.1877e-03 | validation loss: 1.8695e-03\n",
      "Epoch: 130940 | training loss: 2.1895e-03 | validation loss: 1.8665e-03\n",
      "Epoch: 130950 | training loss: 2.2295e-03 | validation loss: 1.8633e-03\n",
      "Epoch: 130960 | training loss: 3.0444e-03 | validation loss: 2.1494e-03\n",
      "Epoch: 130970 | training loss: 2.4938e-03 | validation loss: 2.0904e-03\n",
      "Epoch: 130980 | training loss: 2.2935e-03 | validation loss: 1.8858e-03\n",
      "Epoch: 130990 | training loss: 2.1977e-03 | validation loss: 1.8840e-03\n",
      "Epoch: 131000 | training loss: 2.1873e-03 | validation loss: 1.8749e-03\n",
      "Epoch: 131010 | training loss: 2.1879e-03 | validation loss: 1.8688e-03\n",
      "Epoch: 131020 | training loss: 2.1874e-03 | validation loss: 1.8738e-03\n",
      "Epoch: 131030 | training loss: 2.1870e-03 | validation loss: 1.8718e-03\n",
      "Epoch: 131040 | training loss: 2.1870e-03 | validation loss: 1.8702e-03\n",
      "Epoch: 131050 | training loss: 2.1871e-03 | validation loss: 1.8733e-03\n",
      "Epoch: 131060 | training loss: 2.1869e-03 | validation loss: 1.8718e-03\n",
      "Epoch: 131070 | training loss: 2.1869e-03 | validation loss: 1.8707e-03\n",
      "Epoch: 131080 | training loss: 2.1869e-03 | validation loss: 1.8700e-03\n",
      "Epoch: 131090 | training loss: 2.1887e-03 | validation loss: 1.8670e-03\n",
      "Epoch: 131100 | training loss: 2.2556e-03 | validation loss: 1.8726e-03\n",
      "Epoch: 131110 | training loss: 3.3853e-03 | validation loss: 2.3042e-03\n",
      "Epoch: 131120 | training loss: 2.3619e-03 | validation loss: 2.0088e-03\n",
      "Epoch: 131130 | training loss: 2.2287e-03 | validation loss: 1.9068e-03\n",
      "Epoch: 131140 | training loss: 2.2279e-03 | validation loss: 1.8731e-03\n",
      "Epoch: 131150 | training loss: 2.1866e-03 | validation loss: 1.8708e-03\n",
      "Epoch: 131160 | training loss: 2.1913e-03 | validation loss: 1.8800e-03\n",
      "Epoch: 131170 | training loss: 2.1887e-03 | validation loss: 1.8675e-03\n",
      "Epoch: 131180 | training loss: 2.1869e-03 | validation loss: 1.8737e-03\n",
      "Epoch: 131190 | training loss: 2.1865e-03 | validation loss: 1.8701e-03\n",
      "Epoch: 131200 | training loss: 2.1864e-03 | validation loss: 1.8713e-03\n",
      "Epoch: 131210 | training loss: 2.1864e-03 | validation loss: 1.8701e-03\n",
      "Epoch: 131220 | training loss: 2.1864e-03 | validation loss: 1.8707e-03\n",
      "Epoch: 131230 | training loss: 2.1865e-03 | validation loss: 1.8683e-03\n",
      "Epoch: 131240 | training loss: 2.1982e-03 | validation loss: 1.8604e-03\n",
      "Epoch: 131250 | training loss: 2.7537e-03 | validation loss: 2.1170e-03\n",
      "Epoch: 131260 | training loss: 2.2912e-03 | validation loss: 1.9713e-03\n",
      "Epoch: 131270 | training loss: 2.2483e-03 | validation loss: 1.9519e-03\n",
      "Epoch: 131280 | training loss: 2.1974e-03 | validation loss: 1.8588e-03\n",
      "Epoch: 131290 | training loss: 2.1942e-03 | validation loss: 1.8703e-03\n",
      "Epoch: 131300 | training loss: 2.1894e-03 | validation loss: 1.8759e-03\n",
      "Epoch: 131310 | training loss: 2.1871e-03 | validation loss: 1.8765e-03\n",
      "Epoch: 131320 | training loss: 2.1864e-03 | validation loss: 1.8738e-03\n",
      "Epoch: 131330 | training loss: 2.1896e-03 | validation loss: 1.8792e-03\n",
      "Epoch: 131340 | training loss: 2.3062e-03 | validation loss: 1.9782e-03\n",
      "Epoch: 131350 | training loss: 2.8903e-03 | validation loss: 2.3176e-03\n",
      "Epoch: 131360 | training loss: 2.2215e-03 | validation loss: 1.8739e-03\n",
      "Epoch: 131370 | training loss: 2.2318e-03 | validation loss: 1.8645e-03\n",
      "Epoch: 131380 | training loss: 2.2210e-03 | validation loss: 1.9072e-03\n",
      "Epoch: 131390 | training loss: 2.1889e-03 | validation loss: 1.8624e-03\n",
      "Epoch: 131400 | training loss: 2.1859e-03 | validation loss: 1.8693e-03\n",
      "Epoch: 131410 | training loss: 2.1862e-03 | validation loss: 1.8732e-03\n",
      "Epoch: 131420 | training loss: 2.1860e-03 | validation loss: 1.8671e-03\n",
      "Epoch: 131430 | training loss: 2.1857e-03 | validation loss: 1.8705e-03\n",
      "Epoch: 131440 | training loss: 2.1856e-03 | validation loss: 1.8698e-03\n",
      "Epoch: 131450 | training loss: 2.1856e-03 | validation loss: 1.8684e-03\n",
      "Epoch: 131460 | training loss: 2.1856e-03 | validation loss: 1.8696e-03\n",
      "Epoch: 131470 | training loss: 2.1855e-03 | validation loss: 1.8698e-03\n",
      "Epoch: 131480 | training loss: 2.1855e-03 | validation loss: 1.8701e-03\n",
      "Epoch: 131490 | training loss: 2.1859e-03 | validation loss: 1.8721e-03\n",
      "Epoch: 131500 | training loss: 2.1990e-03 | validation loss: 1.8906e-03\n",
      "Epoch: 131510 | training loss: 2.9025e-03 | validation loss: 2.3175e-03\n",
      "Epoch: 131520 | training loss: 2.5374e-03 | validation loss: 1.9564e-03\n",
      "Epoch: 131530 | training loss: 2.3135e-03 | validation loss: 1.9732e-03\n",
      "Epoch: 131540 | training loss: 2.2187e-03 | validation loss: 1.9122e-03\n",
      "Epoch: 131550 | training loss: 2.2031e-03 | validation loss: 1.8586e-03\n",
      "Epoch: 131560 | training loss: 2.1869e-03 | validation loss: 1.8631e-03\n",
      "Epoch: 131570 | training loss: 2.1890e-03 | validation loss: 1.8802e-03\n",
      "Epoch: 131580 | training loss: 2.1858e-03 | validation loss: 1.8652e-03\n",
      "Epoch: 131590 | training loss: 2.1851e-03 | validation loss: 1.8693e-03\n",
      "Epoch: 131600 | training loss: 2.1851e-03 | validation loss: 1.8693e-03\n",
      "Epoch: 131610 | training loss: 2.1851e-03 | validation loss: 1.8681e-03\n",
      "Epoch: 131620 | training loss: 2.1850e-03 | validation loss: 1.8687e-03\n",
      "Epoch: 131630 | training loss: 2.1850e-03 | validation loss: 1.8688e-03\n",
      "Epoch: 131640 | training loss: 2.1850e-03 | validation loss: 1.8683e-03\n",
      "Epoch: 131650 | training loss: 2.1849e-03 | validation loss: 1.8689e-03\n",
      "Epoch: 131660 | training loss: 2.1854e-03 | validation loss: 1.8715e-03\n",
      "Epoch: 131670 | training loss: 2.2231e-03 | validation loss: 1.9195e-03\n",
      "Epoch: 131680 | training loss: 2.4279e-03 | validation loss: 2.1019e-03\n",
      "Epoch: 131690 | training loss: 2.2717e-03 | validation loss: 1.9735e-03\n",
      "Epoch: 131700 | training loss: 2.2061e-03 | validation loss: 1.8898e-03\n",
      "Epoch: 131710 | training loss: 2.2140e-03 | validation loss: 1.8886e-03\n",
      "Epoch: 131720 | training loss: 2.2371e-03 | validation loss: 1.9230e-03\n",
      "Epoch: 131730 | training loss: 2.5145e-03 | validation loss: 2.1140e-03\n",
      "Epoch: 131740 | training loss: 2.1855e-03 | validation loss: 1.8708e-03\n",
      "Epoch: 131750 | training loss: 2.2113e-03 | validation loss: 1.8561e-03\n",
      "Epoch: 131760 | training loss: 2.2110e-03 | validation loss: 1.9023e-03\n",
      "Epoch: 131770 | training loss: 2.1858e-03 | validation loss: 1.8626e-03\n",
      "Epoch: 131780 | training loss: 2.1909e-03 | validation loss: 1.8593e-03\n",
      "Epoch: 131790 | training loss: 2.1869e-03 | validation loss: 1.8614e-03\n",
      "Epoch: 131800 | training loss: 2.1890e-03 | validation loss: 1.8595e-03\n",
      "Epoch: 131810 | training loss: 2.2392e-03 | validation loss: 1.8590e-03\n",
      "Epoch: 131820 | training loss: 2.8773e-03 | validation loss: 2.0743e-03\n",
      "Epoch: 131830 | training loss: 2.3822e-03 | validation loss: 2.0207e-03\n",
      "Epoch: 131840 | training loss: 2.2742e-03 | validation loss: 1.8727e-03\n",
      "Epoch: 131850 | training loss: 2.2198e-03 | validation loss: 1.9073e-03\n",
      "Epoch: 131860 | training loss: 2.1978e-03 | validation loss: 1.8575e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 131870 | training loss: 2.1891e-03 | validation loss: 1.8793e-03\n",
      "Epoch: 131880 | training loss: 2.1849e-03 | validation loss: 1.8635e-03\n",
      "Epoch: 131890 | training loss: 2.1843e-03 | validation loss: 1.8654e-03\n",
      "Epoch: 131900 | training loss: 2.1845e-03 | validation loss: 1.8697e-03\n",
      "Epoch: 131910 | training loss: 2.1844e-03 | validation loss: 1.8695e-03\n",
      "Epoch: 131920 | training loss: 2.1847e-03 | validation loss: 1.8704e-03\n",
      "Epoch: 131930 | training loss: 2.1913e-03 | validation loss: 1.8814e-03\n",
      "Epoch: 131940 | training loss: 2.3991e-03 | validation loss: 2.0290e-03\n",
      "Epoch: 131950 | training loss: 2.4310e-03 | validation loss: 2.0451e-03\n",
      "Epoch: 131960 | training loss: 2.1862e-03 | validation loss: 1.8768e-03\n",
      "Epoch: 131970 | training loss: 2.2405e-03 | validation loss: 1.8590e-03\n",
      "Epoch: 131980 | training loss: 2.2116e-03 | validation loss: 1.9012e-03\n",
      "Epoch: 131990 | training loss: 2.1882e-03 | validation loss: 1.8614e-03\n",
      "Epoch: 132000 | training loss: 2.1842e-03 | validation loss: 1.8677e-03\n",
      "Epoch: 132010 | training loss: 2.1838e-03 | validation loss: 1.8665e-03\n",
      "Epoch: 132020 | training loss: 2.1838e-03 | validation loss: 1.8675e-03\n",
      "Epoch: 132030 | training loss: 2.1839e-03 | validation loss: 1.8644e-03\n",
      "Epoch: 132040 | training loss: 2.1838e-03 | validation loss: 1.8680e-03\n",
      "Epoch: 132050 | training loss: 2.1836e-03 | validation loss: 1.8667e-03\n",
      "Epoch: 132060 | training loss: 2.1836e-03 | validation loss: 1.8659e-03\n",
      "Epoch: 132070 | training loss: 2.1839e-03 | validation loss: 1.8667e-03\n",
      "Epoch: 132080 | training loss: 2.1924e-03 | validation loss: 1.8785e-03\n",
      "Epoch: 132090 | training loss: 2.5467e-03 | validation loss: 2.1626e-03\n",
      "Epoch: 132100 | training loss: 2.3573e-03 | validation loss: 1.9308e-03\n",
      "Epoch: 132110 | training loss: 2.4389e-03 | validation loss: 1.9069e-03\n",
      "Epoch: 132120 | training loss: 2.2779e-03 | validation loss: 1.8996e-03\n",
      "Epoch: 132130 | training loss: 2.2193e-03 | validation loss: 1.9190e-03\n",
      "Epoch: 132140 | training loss: 2.1970e-03 | validation loss: 1.8534e-03\n",
      "Epoch: 132150 | training loss: 2.1948e-03 | validation loss: 1.8812e-03\n",
      "Epoch: 132160 | training loss: 2.1899e-03 | validation loss: 1.8553e-03\n",
      "Epoch: 132170 | training loss: 2.1845e-03 | validation loss: 1.8722e-03\n",
      "Epoch: 132180 | training loss: 2.1840e-03 | validation loss: 1.8703e-03\n",
      "Epoch: 132190 | training loss: 2.1833e-03 | validation loss: 1.8639e-03\n",
      "Epoch: 132200 | training loss: 2.1837e-03 | validation loss: 1.8623e-03\n",
      "Epoch: 132210 | training loss: 2.1883e-03 | validation loss: 1.8570e-03\n",
      "Epoch: 132220 | training loss: 2.3182e-03 | validation loss: 1.8743e-03\n",
      "Epoch: 132230 | training loss: 2.7558e-03 | validation loss: 2.0329e-03\n",
      "Epoch: 132240 | training loss: 2.2595e-03 | validation loss: 1.9282e-03\n",
      "Epoch: 132250 | training loss: 2.1863e-03 | validation loss: 1.8768e-03\n",
      "Epoch: 132260 | training loss: 2.2006e-03 | validation loss: 1.8594e-03\n",
      "Epoch: 132270 | training loss: 2.1941e-03 | validation loss: 1.8812e-03\n",
      "Epoch: 132280 | training loss: 2.1873e-03 | validation loss: 1.8588e-03\n",
      "Epoch: 132290 | training loss: 2.1846e-03 | validation loss: 1.8715e-03\n",
      "Epoch: 132300 | training loss: 2.1834e-03 | validation loss: 1.8617e-03\n",
      "Epoch: 132310 | training loss: 2.1828e-03 | validation loss: 1.8657e-03\n",
      "Epoch: 132320 | training loss: 2.1829e-03 | validation loss: 1.8658e-03\n",
      "Epoch: 132330 | training loss: 2.1827e-03 | validation loss: 1.8645e-03\n",
      "Epoch: 132340 | training loss: 2.1828e-03 | validation loss: 1.8637e-03\n",
      "Epoch: 132350 | training loss: 2.1830e-03 | validation loss: 1.8624e-03\n",
      "Epoch: 132360 | training loss: 2.1887e-03 | validation loss: 1.8574e-03\n",
      "Epoch: 132370 | training loss: 2.4516e-03 | validation loss: 1.9257e-03\n",
      "Epoch: 132380 | training loss: 2.2574e-03 | validation loss: 1.8680e-03\n",
      "Epoch: 132390 | training loss: 2.3147e-03 | validation loss: 1.8779e-03\n",
      "Epoch: 132400 | training loss: 2.2481e-03 | validation loss: 1.9278e-03\n",
      "Epoch: 132410 | training loss: 2.1854e-03 | validation loss: 1.8736e-03\n",
      "Epoch: 132420 | training loss: 2.1946e-03 | validation loss: 1.8548e-03\n",
      "Epoch: 132430 | training loss: 2.1848e-03 | validation loss: 1.8723e-03\n",
      "Epoch: 132440 | training loss: 2.1824e-03 | validation loss: 1.8633e-03\n",
      "Epoch: 132450 | training loss: 2.1824e-03 | validation loss: 1.8634e-03\n",
      "Epoch: 132460 | training loss: 2.1824e-03 | validation loss: 1.8650e-03\n",
      "Epoch: 132470 | training loss: 2.1823e-03 | validation loss: 1.8638e-03\n",
      "Epoch: 132480 | training loss: 2.1823e-03 | validation loss: 1.8636e-03\n",
      "Epoch: 132490 | training loss: 2.1822e-03 | validation loss: 1.8643e-03\n",
      "Epoch: 132500 | training loss: 2.1822e-03 | validation loss: 1.8632e-03\n",
      "Epoch: 132510 | training loss: 2.1832e-03 | validation loss: 1.8600e-03\n",
      "Epoch: 132520 | training loss: 2.2780e-03 | validation loss: 1.8895e-03\n",
      "Epoch: 132530 | training loss: 2.2384e-03 | validation loss: 1.8582e-03\n",
      "Epoch: 132540 | training loss: 2.2915e-03 | validation loss: 1.8937e-03\n",
      "Epoch: 132550 | training loss: 2.2119e-03 | validation loss: 1.8762e-03\n",
      "Epoch: 132560 | training loss: 2.1919e-03 | validation loss: 1.8831e-03\n",
      "Epoch: 132570 | training loss: 2.2144e-03 | validation loss: 1.9118e-03\n",
      "Epoch: 132580 | training loss: 2.3716e-03 | validation loss: 2.0225e-03\n",
      "Epoch: 132590 | training loss: 2.3179e-03 | validation loss: 1.9795e-03\n",
      "Epoch: 132600 | training loss: 2.2664e-03 | validation loss: 1.8582e-03\n",
      "Epoch: 132610 | training loss: 2.1926e-03 | validation loss: 1.8827e-03\n",
      "Epoch: 132620 | training loss: 2.1872e-03 | validation loss: 1.8767e-03\n",
      "Epoch: 132630 | training loss: 2.1860e-03 | validation loss: 1.8557e-03\n",
      "Epoch: 132640 | training loss: 2.1890e-03 | validation loss: 1.8536e-03\n",
      "Epoch: 132650 | training loss: 2.2005e-03 | validation loss: 1.8517e-03\n",
      "Epoch: 132660 | training loss: 2.3379e-03 | validation loss: 1.8790e-03\n",
      "Epoch: 132670 | training loss: 2.4657e-03 | validation loss: 1.9230e-03\n",
      "Epoch: 132680 | training loss: 2.3015e-03 | validation loss: 1.9648e-03\n",
      "Epoch: 132690 | training loss: 2.2131e-03 | validation loss: 1.8538e-03\n",
      "Epoch: 132700 | training loss: 2.1824e-03 | validation loss: 1.8670e-03\n",
      "Epoch: 132710 | training loss: 2.1868e-03 | validation loss: 1.8745e-03\n",
      "Epoch: 132720 | training loss: 2.1828e-03 | validation loss: 1.8584e-03\n",
      "Epoch: 132730 | training loss: 2.1850e-03 | validation loss: 1.8560e-03\n",
      "Epoch: 132740 | training loss: 2.1907e-03 | validation loss: 1.8539e-03\n",
      "Epoch: 132750 | training loss: 2.2705e-03 | validation loss: 1.8643e-03\n",
      "Epoch: 132760 | training loss: 2.7440e-03 | validation loss: 2.0300e-03\n",
      "Epoch: 132770 | training loss: 2.3419e-03 | validation loss: 1.9900e-03\n",
      "Epoch: 132780 | training loss: 2.2354e-03 | validation loss: 1.8583e-03\n",
      "Epoch: 132790 | training loss: 2.1962e-03 | validation loss: 1.8839e-03\n",
      "Epoch: 132800 | training loss: 2.1823e-03 | validation loss: 1.8589e-03\n",
      "Epoch: 132810 | training loss: 2.1824e-03 | validation loss: 1.8582e-03\n",
      "Epoch: 132820 | training loss: 2.1831e-03 | validation loss: 1.8684e-03\n",
      "Epoch: 132830 | training loss: 2.1816e-03 | validation loss: 1.8654e-03\n",
      "Epoch: 132840 | training loss: 2.1811e-03 | validation loss: 1.8627e-03\n",
      "Epoch: 132850 | training loss: 2.1811e-03 | validation loss: 1.8627e-03\n",
      "Epoch: 132860 | training loss: 2.1820e-03 | validation loss: 1.8660e-03\n",
      "Epoch: 132870 | training loss: 2.2337e-03 | validation loss: 1.9150e-03\n",
      "Epoch: 132880 | training loss: 3.7258e-03 | validation loss: 2.7508e-03\n",
      "Epoch: 132890 | training loss: 2.2873e-03 | validation loss: 1.8696e-03\n",
      "Epoch: 132900 | training loss: 2.3355e-03 | validation loss: 1.8893e-03\n",
      "Epoch: 132910 | training loss: 2.1809e-03 | validation loss: 1.8613e-03\n",
      "Epoch: 132920 | training loss: 2.2019e-03 | validation loss: 1.8881e-03\n",
      "Epoch: 132930 | training loss: 2.1810e-03 | validation loss: 1.8636e-03\n",
      "Epoch: 132940 | training loss: 2.1837e-03 | validation loss: 1.8566e-03\n",
      "Epoch: 132950 | training loss: 2.1810e-03 | validation loss: 1.8635e-03\n",
      "Epoch: 132960 | training loss: 2.1808e-03 | validation loss: 1.8625e-03\n",
      "Epoch: 132970 | training loss: 2.1808e-03 | validation loss: 1.8602e-03\n",
      "Epoch: 132980 | training loss: 2.1807e-03 | validation loss: 1.8623e-03\n",
      "Epoch: 132990 | training loss: 2.1806e-03 | validation loss: 1.8608e-03\n",
      "Epoch: 133000 | training loss: 2.1806e-03 | validation loss: 1.8616e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 133010 | training loss: 2.1805e-03 | validation loss: 1.8613e-03\n",
      "Epoch: 133020 | training loss: 2.1806e-03 | validation loss: 1.8630e-03\n",
      "Epoch: 133030 | training loss: 2.1935e-03 | validation loss: 1.8875e-03\n",
      "Epoch: 133040 | training loss: 2.8532e-03 | validation loss: 2.4457e-03\n",
      "Epoch: 133050 | training loss: 2.1821e-03 | validation loss: 1.8565e-03\n",
      "Epoch: 133060 | training loss: 2.2190e-03 | validation loss: 1.8598e-03\n",
      "Epoch: 133070 | training loss: 2.2103e-03 | validation loss: 1.8761e-03\n",
      "Epoch: 133080 | training loss: 2.1859e-03 | validation loss: 1.8531e-03\n",
      "Epoch: 133090 | training loss: 2.1820e-03 | validation loss: 1.8553e-03\n",
      "Epoch: 133100 | training loss: 2.1819e-03 | validation loss: 1.8599e-03\n",
      "Epoch: 133110 | training loss: 2.1827e-03 | validation loss: 1.8554e-03\n",
      "Epoch: 133120 | training loss: 2.2171e-03 | validation loss: 1.8476e-03\n",
      "Epoch: 133130 | training loss: 2.9184e-03 | validation loss: 2.0773e-03\n",
      "Epoch: 133140 | training loss: 2.4168e-03 | validation loss: 2.0406e-03\n",
      "Epoch: 133150 | training loss: 2.2961e-03 | validation loss: 1.8703e-03\n",
      "Epoch: 133160 | training loss: 2.2083e-03 | validation loss: 1.8979e-03\n",
      "Epoch: 133170 | training loss: 2.1843e-03 | validation loss: 1.8526e-03\n",
      "Epoch: 133180 | training loss: 2.1809e-03 | validation loss: 1.8639e-03\n",
      "Epoch: 133190 | training loss: 2.1806e-03 | validation loss: 1.8569e-03\n",
      "Epoch: 133200 | training loss: 2.1807e-03 | validation loss: 1.8642e-03\n",
      "Epoch: 133210 | training loss: 2.1803e-03 | validation loss: 1.8574e-03\n",
      "Epoch: 133220 | training loss: 2.1799e-03 | validation loss: 1.8595e-03\n",
      "Epoch: 133230 | training loss: 2.1799e-03 | validation loss: 1.8611e-03\n",
      "Epoch: 133240 | training loss: 2.1800e-03 | validation loss: 1.8617e-03\n",
      "Epoch: 133250 | training loss: 2.1811e-03 | validation loss: 1.8654e-03\n",
      "Epoch: 133260 | training loss: 2.2110e-03 | validation loss: 1.8987e-03\n",
      "Epoch: 133270 | training loss: 3.0985e-03 | validation loss: 2.4229e-03\n",
      "Epoch: 133280 | training loss: 2.5572e-03 | validation loss: 1.9556e-03\n",
      "Epoch: 133290 | training loss: 2.2709e-03 | validation loss: 1.9347e-03\n",
      "Epoch: 133300 | training loss: 2.1836e-03 | validation loss: 1.8731e-03\n",
      "Epoch: 133310 | training loss: 2.1939e-03 | validation loss: 1.8499e-03\n",
      "Epoch: 133320 | training loss: 2.1862e-03 | validation loss: 1.8705e-03\n",
      "Epoch: 133330 | training loss: 2.1814e-03 | validation loss: 1.8581e-03\n",
      "Epoch: 133340 | training loss: 2.1801e-03 | validation loss: 1.8603e-03\n",
      "Epoch: 133350 | training loss: 2.1798e-03 | validation loss: 1.8590e-03\n",
      "Epoch: 133360 | training loss: 2.1796e-03 | validation loss: 1.8599e-03\n",
      "Epoch: 133370 | training loss: 2.1794e-03 | validation loss: 1.8587e-03\n",
      "Epoch: 133380 | training loss: 2.1794e-03 | validation loss: 1.8591e-03\n",
      "Epoch: 133390 | training loss: 2.1793e-03 | validation loss: 1.8591e-03\n",
      "Epoch: 133400 | training loss: 2.1793e-03 | validation loss: 1.8592e-03\n",
      "Epoch: 133410 | training loss: 2.1794e-03 | validation loss: 1.8597e-03\n",
      "Epoch: 133420 | training loss: 2.1833e-03 | validation loss: 1.8649e-03\n",
      "Epoch: 133430 | training loss: 2.3994e-03 | validation loss: 2.0048e-03\n",
      "Epoch: 133440 | training loss: 2.6686e-03 | validation loss: 2.2608e-03\n",
      "Epoch: 133450 | training loss: 2.2855e-03 | validation loss: 1.8992e-03\n",
      "Epoch: 133460 | training loss: 2.2799e-03 | validation loss: 1.8662e-03\n",
      "Epoch: 133470 | training loss: 2.2003e-03 | validation loss: 1.8755e-03\n",
      "Epoch: 133480 | training loss: 2.1792e-03 | validation loss: 1.8593e-03\n",
      "Epoch: 133490 | training loss: 2.1817e-03 | validation loss: 1.8579e-03\n",
      "Epoch: 133500 | training loss: 2.1808e-03 | validation loss: 1.8670e-03\n",
      "Epoch: 133510 | training loss: 2.1793e-03 | validation loss: 1.8560e-03\n",
      "Epoch: 133520 | training loss: 2.1792e-03 | validation loss: 1.8591e-03\n",
      "Epoch: 133530 | training loss: 2.1789e-03 | validation loss: 1.8572e-03\n",
      "Epoch: 133540 | training loss: 2.1789e-03 | validation loss: 1.8586e-03\n",
      "Epoch: 133550 | training loss: 2.1788e-03 | validation loss: 1.8585e-03\n",
      "Epoch: 133560 | training loss: 2.1788e-03 | validation loss: 1.8581e-03\n",
      "Epoch: 133570 | training loss: 2.1788e-03 | validation loss: 1.8579e-03\n",
      "Epoch: 133580 | training loss: 2.1787e-03 | validation loss: 1.8574e-03\n",
      "Epoch: 133590 | training loss: 2.1789e-03 | validation loss: 1.8557e-03\n",
      "Epoch: 133600 | training loss: 2.1925e-03 | validation loss: 1.8462e-03\n",
      "Epoch: 133610 | training loss: 3.2577e-03 | validation loss: 2.2097e-03\n",
      "Epoch: 133620 | training loss: 2.7834e-03 | validation loss: 2.2603e-03\n",
      "Epoch: 133630 | training loss: 2.2713e-03 | validation loss: 1.9690e-03\n",
      "Epoch: 133640 | training loss: 2.2102e-03 | validation loss: 1.8721e-03\n",
      "Epoch: 133650 | training loss: 2.1865e-03 | validation loss: 1.8476e-03\n",
      "Epoch: 133660 | training loss: 2.1889e-03 | validation loss: 1.8538e-03\n",
      "Epoch: 133670 | training loss: 2.1791e-03 | validation loss: 1.8534e-03\n",
      "Epoch: 133680 | training loss: 2.1791e-03 | validation loss: 1.8613e-03\n",
      "Epoch: 133690 | training loss: 2.1787e-03 | validation loss: 1.8607e-03\n",
      "Epoch: 133700 | training loss: 2.1785e-03 | validation loss: 1.8554e-03\n",
      "Epoch: 133710 | training loss: 2.1783e-03 | validation loss: 1.8572e-03\n",
      "Epoch: 133720 | training loss: 2.1783e-03 | validation loss: 1.8578e-03\n",
      "Epoch: 133730 | training loss: 2.1783e-03 | validation loss: 1.8569e-03\n",
      "Epoch: 133740 | training loss: 2.1782e-03 | validation loss: 1.8572e-03\n",
      "Epoch: 133750 | training loss: 2.1782e-03 | validation loss: 1.8570e-03\n",
      "Epoch: 133760 | training loss: 2.1782e-03 | validation loss: 1.8571e-03\n",
      "Epoch: 133770 | training loss: 2.1781e-03 | validation loss: 1.8570e-03\n",
      "Epoch: 133780 | training loss: 2.1781e-03 | validation loss: 1.8570e-03\n",
      "Epoch: 133790 | training loss: 2.1781e-03 | validation loss: 1.8571e-03\n",
      "Epoch: 133800 | training loss: 2.1785e-03 | validation loss: 1.8586e-03\n",
      "Epoch: 133810 | training loss: 2.2255e-03 | validation loss: 1.8962e-03\n",
      "Epoch: 133820 | training loss: 3.7566e-03 | validation loss: 2.7527e-03\n",
      "Epoch: 133830 | training loss: 2.4690e-03 | validation loss: 1.9493e-03\n",
      "Epoch: 133840 | training loss: 2.3246e-03 | validation loss: 1.8701e-03\n",
      "Epoch: 133850 | training loss: 2.1998e-03 | validation loss: 1.8589e-03\n",
      "Epoch: 133860 | training loss: 2.1969e-03 | validation loss: 1.8747e-03\n",
      "Epoch: 133870 | training loss: 2.1794e-03 | validation loss: 1.8530e-03\n",
      "Epoch: 133880 | training loss: 2.1795e-03 | validation loss: 1.8501e-03\n",
      "Epoch: 133890 | training loss: 2.1781e-03 | validation loss: 1.8595e-03\n",
      "Epoch: 133900 | training loss: 2.1778e-03 | validation loss: 1.8577e-03\n",
      "Epoch: 133910 | training loss: 2.1778e-03 | validation loss: 1.8563e-03\n",
      "Epoch: 133920 | training loss: 2.1777e-03 | validation loss: 1.8571e-03\n",
      "Epoch: 133930 | training loss: 2.1777e-03 | validation loss: 1.8555e-03\n",
      "Epoch: 133940 | training loss: 2.1776e-03 | validation loss: 1.8562e-03\n",
      "Epoch: 133950 | training loss: 2.1776e-03 | validation loss: 1.8561e-03\n",
      "Epoch: 133960 | training loss: 2.1775e-03 | validation loss: 1.8559e-03\n",
      "Epoch: 133970 | training loss: 2.1775e-03 | validation loss: 1.8560e-03\n",
      "Epoch: 133980 | training loss: 2.1775e-03 | validation loss: 1.8560e-03\n",
      "Epoch: 133990 | training loss: 2.1774e-03 | validation loss: 1.8560e-03\n",
      "Epoch: 134000 | training loss: 2.1774e-03 | validation loss: 1.8564e-03\n",
      "Epoch: 134010 | training loss: 2.1779e-03 | validation loss: 1.8594e-03\n",
      "Epoch: 134020 | training loss: 2.2212e-03 | validation loss: 1.9076e-03\n",
      "Epoch: 134030 | training loss: 3.9796e-03 | validation loss: 2.8980e-03\n",
      "Epoch: 134040 | training loss: 2.2480e-03 | validation loss: 1.9208e-03\n",
      "Epoch: 134050 | training loss: 2.2878e-03 | validation loss: 1.8721e-03\n",
      "Epoch: 134060 | training loss: 2.2142e-03 | validation loss: 1.8392e-03\n",
      "Epoch: 134070 | training loss: 2.1973e-03 | validation loss: 1.8502e-03\n",
      "Epoch: 134080 | training loss: 2.1798e-03 | validation loss: 1.8532e-03\n",
      "Epoch: 134090 | training loss: 2.1783e-03 | validation loss: 1.8598e-03\n",
      "Epoch: 134100 | training loss: 2.1780e-03 | validation loss: 1.8604e-03\n",
      "Epoch: 134110 | training loss: 2.1772e-03 | validation loss: 1.8559e-03\n",
      "Epoch: 134120 | training loss: 2.1772e-03 | validation loss: 1.8533e-03\n",
      "Epoch: 134130 | training loss: 2.1770e-03 | validation loss: 1.8555e-03\n",
      "Epoch: 134140 | training loss: 2.1770e-03 | validation loss: 1.8557e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 134150 | training loss: 2.1770e-03 | validation loss: 1.8546e-03\n",
      "Epoch: 134160 | training loss: 2.1769e-03 | validation loss: 1.8552e-03\n",
      "Epoch: 134170 | training loss: 2.1769e-03 | validation loss: 1.8549e-03\n",
      "Epoch: 134180 | training loss: 2.1768e-03 | validation loss: 1.8550e-03\n",
      "Epoch: 134190 | training loss: 2.1768e-03 | validation loss: 1.8548e-03\n",
      "Epoch: 134200 | training loss: 2.1768e-03 | validation loss: 1.8549e-03\n",
      "Epoch: 134210 | training loss: 2.1768e-03 | validation loss: 1.8548e-03\n",
      "Epoch: 134220 | training loss: 2.1767e-03 | validation loss: 1.8548e-03\n",
      "Epoch: 134230 | training loss: 2.1768e-03 | validation loss: 1.8551e-03\n",
      "Epoch: 134240 | training loss: 2.1841e-03 | validation loss: 1.8624e-03\n",
      "Epoch: 134250 | training loss: 3.0199e-03 | validation loss: 2.3536e-03\n",
      "Epoch: 134260 | training loss: 2.4033e-03 | validation loss: 1.9471e-03\n",
      "Epoch: 134270 | training loss: 2.3421e-03 | validation loss: 2.0209e-03\n",
      "Epoch: 134280 | training loss: 2.2006e-03 | validation loss: 1.8965e-03\n",
      "Epoch: 134290 | training loss: 2.1894e-03 | validation loss: 1.8485e-03\n",
      "Epoch: 134300 | training loss: 2.1774e-03 | validation loss: 1.8514e-03\n",
      "Epoch: 134310 | training loss: 2.1786e-03 | validation loss: 1.8622e-03\n",
      "Epoch: 134320 | training loss: 2.1767e-03 | validation loss: 1.8524e-03\n",
      "Epoch: 134330 | training loss: 2.1765e-03 | validation loss: 1.8546e-03\n",
      "Epoch: 134340 | training loss: 2.1765e-03 | validation loss: 1.8558e-03\n",
      "Epoch: 134350 | training loss: 2.1764e-03 | validation loss: 1.8540e-03\n",
      "Epoch: 134360 | training loss: 2.1763e-03 | validation loss: 1.8545e-03\n",
      "Epoch: 134370 | training loss: 2.1763e-03 | validation loss: 1.8538e-03\n",
      "Epoch: 134380 | training loss: 2.1762e-03 | validation loss: 1.8535e-03\n",
      "Epoch: 134390 | training loss: 2.1762e-03 | validation loss: 1.8538e-03\n",
      "Epoch: 134400 | training loss: 2.1762e-03 | validation loss: 1.8539e-03\n",
      "Epoch: 134410 | training loss: 2.1761e-03 | validation loss: 1.8538e-03\n",
      "Epoch: 134420 | training loss: 2.1761e-03 | validation loss: 1.8538e-03\n",
      "Epoch: 134430 | training loss: 2.1761e-03 | validation loss: 1.8548e-03\n",
      "Epoch: 134440 | training loss: 2.1798e-03 | validation loss: 1.8643e-03\n",
      "Epoch: 134450 | training loss: 2.6068e-03 | validation loss: 2.1520e-03\n",
      "Epoch: 134460 | training loss: 2.4035e-03 | validation loss: 1.8849e-03\n",
      "Epoch: 134470 | training loss: 2.3149e-03 | validation loss: 1.9487e-03\n",
      "Epoch: 134480 | training loss: 2.2789e-03 | validation loss: 1.9361e-03\n",
      "Epoch: 134490 | training loss: 2.1926e-03 | validation loss: 1.8803e-03\n",
      "Epoch: 134500 | training loss: 2.1794e-03 | validation loss: 1.8598e-03\n",
      "Epoch: 134510 | training loss: 2.1813e-03 | validation loss: 1.8541e-03\n",
      "Epoch: 134520 | training loss: 2.1760e-03 | validation loss: 1.8528e-03\n",
      "Epoch: 134530 | training loss: 2.1764e-03 | validation loss: 1.8547e-03\n",
      "Epoch: 134540 | training loss: 2.1758e-03 | validation loss: 1.8525e-03\n",
      "Epoch: 134550 | training loss: 2.1758e-03 | validation loss: 1.8525e-03\n",
      "Epoch: 134560 | training loss: 2.1757e-03 | validation loss: 1.8538e-03\n",
      "Epoch: 134570 | training loss: 2.1756e-03 | validation loss: 1.8527e-03\n",
      "Epoch: 134580 | training loss: 2.1756e-03 | validation loss: 1.8529e-03\n",
      "Epoch: 134590 | training loss: 2.1756e-03 | validation loss: 1.8530e-03\n",
      "Epoch: 134600 | training loss: 2.1755e-03 | validation loss: 1.8528e-03\n",
      "Epoch: 134610 | training loss: 2.1755e-03 | validation loss: 1.8528e-03\n",
      "Epoch: 134620 | training loss: 2.1755e-03 | validation loss: 1.8527e-03\n",
      "Epoch: 134630 | training loss: 2.1754e-03 | validation loss: 1.8527e-03\n",
      "Epoch: 134640 | training loss: 2.1754e-03 | validation loss: 1.8526e-03\n",
      "Epoch: 134650 | training loss: 2.1754e-03 | validation loss: 1.8525e-03\n",
      "Epoch: 134660 | training loss: 2.1754e-03 | validation loss: 1.8524e-03\n",
      "Epoch: 134670 | training loss: 2.1753e-03 | validation loss: 1.8519e-03\n",
      "Epoch: 134680 | training loss: 2.1772e-03 | validation loss: 1.8487e-03\n",
      "Epoch: 134690 | training loss: 2.4431e-03 | validation loss: 1.9280e-03\n",
      "Epoch: 134700 | training loss: 2.2559e-03 | validation loss: 1.8767e-03\n",
      "Epoch: 134710 | training loss: 2.3568e-03 | validation loss: 1.8727e-03\n",
      "Epoch: 134720 | training loss: 2.2189e-03 | validation loss: 1.8573e-03\n",
      "Epoch: 134730 | training loss: 2.1859e-03 | validation loss: 1.8702e-03\n",
      "Epoch: 134740 | training loss: 2.1806e-03 | validation loss: 1.8680e-03\n",
      "Epoch: 134750 | training loss: 2.1761e-03 | validation loss: 1.8540e-03\n",
      "Epoch: 134760 | training loss: 2.1761e-03 | validation loss: 1.8499e-03\n",
      "Epoch: 134770 | training loss: 2.1755e-03 | validation loss: 1.8539e-03\n",
      "Epoch: 134780 | training loss: 2.1753e-03 | validation loss: 1.8538e-03\n",
      "Epoch: 134790 | training loss: 2.1750e-03 | validation loss: 1.8515e-03\n",
      "Epoch: 134800 | training loss: 2.1749e-03 | validation loss: 1.8523e-03\n",
      "Epoch: 134810 | training loss: 2.1749e-03 | validation loss: 1.8515e-03\n",
      "Epoch: 134820 | training loss: 2.1749e-03 | validation loss: 1.8518e-03\n",
      "Epoch: 134830 | training loss: 2.1748e-03 | validation loss: 1.8516e-03\n",
      "Epoch: 134840 | training loss: 2.1748e-03 | validation loss: 1.8518e-03\n",
      "Epoch: 134850 | training loss: 2.1748e-03 | validation loss: 1.8516e-03\n",
      "Epoch: 134860 | training loss: 2.1747e-03 | validation loss: 1.8515e-03\n",
      "Epoch: 134870 | training loss: 2.1747e-03 | validation loss: 1.8515e-03\n",
      "Epoch: 134880 | training loss: 2.1747e-03 | validation loss: 1.8514e-03\n",
      "Epoch: 134890 | training loss: 2.1746e-03 | validation loss: 1.8513e-03\n",
      "Epoch: 134900 | training loss: 2.1746e-03 | validation loss: 1.8507e-03\n",
      "Epoch: 134910 | training loss: 2.1760e-03 | validation loss: 1.8452e-03\n",
      "Epoch: 134920 | training loss: 2.4213e-03 | validation loss: 1.8917e-03\n",
      "Epoch: 134930 | training loss: 2.3781e-03 | validation loss: 2.0564e-03\n",
      "Epoch: 134940 | training loss: 2.2720e-03 | validation loss: 1.8945e-03\n",
      "Epoch: 134950 | training loss: 2.1844e-03 | validation loss: 1.8735e-03\n",
      "Epoch: 134960 | training loss: 2.1928e-03 | validation loss: 1.8849e-03\n",
      "Epoch: 134970 | training loss: 2.1811e-03 | validation loss: 1.8665e-03\n",
      "Epoch: 134980 | training loss: 2.1781e-03 | validation loss: 1.8576e-03\n",
      "Epoch: 134990 | training loss: 2.1755e-03 | validation loss: 1.8572e-03\n",
      "Epoch: 135000 | training loss: 2.1746e-03 | validation loss: 1.8531e-03\n",
      "Epoch: 135010 | training loss: 2.1743e-03 | validation loss: 1.8507e-03\n",
      "Epoch: 135020 | training loss: 2.1743e-03 | validation loss: 1.8507e-03\n",
      "Epoch: 135030 | training loss: 2.1742e-03 | validation loss: 1.8496e-03\n",
      "Epoch: 135040 | training loss: 2.1742e-03 | validation loss: 1.8500e-03\n",
      "Epoch: 135050 | training loss: 2.1741e-03 | validation loss: 1.8501e-03\n",
      "Epoch: 135060 | training loss: 2.1741e-03 | validation loss: 1.8503e-03\n",
      "Epoch: 135070 | training loss: 2.1741e-03 | validation loss: 1.8503e-03\n",
      "Epoch: 135080 | training loss: 2.1741e-03 | validation loss: 1.8501e-03\n",
      "Epoch: 135090 | training loss: 2.1740e-03 | validation loss: 1.8501e-03\n",
      "Epoch: 135100 | training loss: 2.1740e-03 | validation loss: 1.8503e-03\n",
      "Epoch: 135110 | training loss: 2.1743e-03 | validation loss: 1.8520e-03\n",
      "Epoch: 135120 | training loss: 2.1987e-03 | validation loss: 1.8791e-03\n",
      "Epoch: 135130 | training loss: 3.9887e-03 | validation loss: 2.8749e-03\n",
      "Epoch: 135140 | training loss: 2.4210e-03 | validation loss: 1.8913e-03\n",
      "Epoch: 135150 | training loss: 2.3975e-03 | validation loss: 1.8857e-03\n",
      "Epoch: 135160 | training loss: 2.2173e-03 | validation loss: 1.8388e-03\n",
      "Epoch: 135170 | training loss: 2.1739e-03 | validation loss: 1.8482e-03\n",
      "Epoch: 135180 | training loss: 2.1807e-03 | validation loss: 1.8637e-03\n",
      "Epoch: 135190 | training loss: 2.1763e-03 | validation loss: 1.8583e-03\n",
      "Epoch: 135200 | training loss: 2.1739e-03 | validation loss: 1.8496e-03\n",
      "Epoch: 135210 | training loss: 2.1741e-03 | validation loss: 1.8479e-03\n",
      "Epoch: 135220 | training loss: 2.1737e-03 | validation loss: 1.8503e-03\n",
      "Epoch: 135230 | training loss: 2.1736e-03 | validation loss: 1.8499e-03\n",
      "Epoch: 135240 | training loss: 2.1736e-03 | validation loss: 1.8490e-03\n",
      "Epoch: 135250 | training loss: 2.1735e-03 | validation loss: 1.8498e-03\n",
      "Epoch: 135260 | training loss: 2.1735e-03 | validation loss: 1.8493e-03\n",
      "Epoch: 135270 | training loss: 2.1735e-03 | validation loss: 1.8494e-03\n",
      "Epoch: 135280 | training loss: 2.1734e-03 | validation loss: 1.8493e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 135290 | training loss: 2.1734e-03 | validation loss: 1.8493e-03\n",
      "Epoch: 135300 | training loss: 2.1734e-03 | validation loss: 1.8492e-03\n",
      "Epoch: 135310 | training loss: 2.1733e-03 | validation loss: 1.8492e-03\n",
      "Epoch: 135320 | training loss: 2.1733e-03 | validation loss: 1.8491e-03\n",
      "Epoch: 135330 | training loss: 2.1733e-03 | validation loss: 1.8491e-03\n",
      "Epoch: 135340 | training loss: 2.1732e-03 | validation loss: 1.8491e-03\n",
      "Epoch: 135350 | training loss: 2.1732e-03 | validation loss: 1.8493e-03\n",
      "Epoch: 135360 | training loss: 2.1739e-03 | validation loss: 1.8523e-03\n",
      "Epoch: 135370 | training loss: 2.2736e-03 | validation loss: 1.9341e-03\n",
      "Epoch: 135380 | training loss: 3.1136e-03 | validation loss: 2.4224e-03\n",
      "Epoch: 135390 | training loss: 2.7296e-03 | validation loss: 2.1953e-03\n",
      "Epoch: 135400 | training loss: 2.3418e-03 | validation loss: 1.9617e-03\n",
      "Epoch: 135410 | training loss: 2.2256e-03 | validation loss: 1.8839e-03\n",
      "Epoch: 135420 | training loss: 2.1862e-03 | validation loss: 1.8596e-03\n",
      "Epoch: 135430 | training loss: 2.1747e-03 | validation loss: 1.8534e-03\n",
      "Epoch: 135440 | training loss: 2.1730e-03 | validation loss: 1.8497e-03\n",
      "Epoch: 135450 | training loss: 2.1734e-03 | validation loss: 1.8471e-03\n",
      "Epoch: 135460 | training loss: 2.1733e-03 | validation loss: 1.8477e-03\n",
      "Epoch: 135470 | training loss: 2.1729e-03 | validation loss: 1.8486e-03\n",
      "Epoch: 135480 | training loss: 2.1729e-03 | validation loss: 1.8486e-03\n",
      "Epoch: 135490 | training loss: 2.1728e-03 | validation loss: 1.8488e-03\n",
      "Epoch: 135500 | training loss: 2.1728e-03 | validation loss: 1.8483e-03\n",
      "Epoch: 135510 | training loss: 2.1727e-03 | validation loss: 1.8483e-03\n",
      "Epoch: 135520 | training loss: 2.1727e-03 | validation loss: 1.8483e-03\n",
      "Epoch: 135530 | training loss: 2.1727e-03 | validation loss: 1.8482e-03\n",
      "Epoch: 135540 | training loss: 2.1726e-03 | validation loss: 1.8482e-03\n",
      "Epoch: 135550 | training loss: 2.1726e-03 | validation loss: 1.8482e-03\n",
      "Epoch: 135560 | training loss: 2.1726e-03 | validation loss: 1.8481e-03\n",
      "Epoch: 135570 | training loss: 2.1725e-03 | validation loss: 1.8481e-03\n",
      "Epoch: 135580 | training loss: 2.1725e-03 | validation loss: 1.8483e-03\n",
      "Epoch: 135590 | training loss: 2.1727e-03 | validation loss: 1.8510e-03\n",
      "Epoch: 135600 | training loss: 2.2180e-03 | validation loss: 1.9160e-03\n",
      "Epoch: 135610 | training loss: 2.8097e-03 | validation loss: 2.3220e-03\n",
      "Epoch: 135620 | training loss: 2.4432e-03 | validation loss: 2.0296e-03\n",
      "Epoch: 135630 | training loss: 2.2437e-03 | validation loss: 1.9331e-03\n",
      "Epoch: 135640 | training loss: 2.1939e-03 | validation loss: 1.8815e-03\n",
      "Epoch: 135650 | training loss: 2.1804e-03 | validation loss: 1.8551e-03\n",
      "Epoch: 135660 | training loss: 2.1754e-03 | validation loss: 1.8584e-03\n",
      "Epoch: 135670 | training loss: 2.1734e-03 | validation loss: 1.8493e-03\n",
      "Epoch: 135680 | training loss: 2.1726e-03 | validation loss: 1.8493e-03\n",
      "Epoch: 135690 | training loss: 2.1724e-03 | validation loss: 1.8498e-03\n",
      "Epoch: 135700 | training loss: 2.1722e-03 | validation loss: 1.8468e-03\n",
      "Epoch: 135710 | training loss: 2.1722e-03 | validation loss: 1.8458e-03\n",
      "Epoch: 135720 | training loss: 2.1722e-03 | validation loss: 1.8453e-03\n",
      "Epoch: 135730 | training loss: 2.1728e-03 | validation loss: 1.8431e-03\n",
      "Epoch: 135740 | training loss: 2.1937e-03 | validation loss: 1.8358e-03\n",
      "Epoch: 135750 | training loss: 3.0665e-03 | validation loss: 2.1313e-03\n",
      "Epoch: 135760 | training loss: 2.6028e-03 | validation loss: 2.1378e-03\n",
      "Epoch: 135770 | training loss: 2.2450e-03 | validation loss: 1.8473e-03\n",
      "Epoch: 135780 | training loss: 2.1949e-03 | validation loss: 1.8390e-03\n",
      "Epoch: 135790 | training loss: 2.1928e-03 | validation loss: 1.8766e-03\n",
      "Epoch: 135800 | training loss: 2.1722e-03 | validation loss: 1.8447e-03\n",
      "Epoch: 135810 | training loss: 2.1728e-03 | validation loss: 1.8427e-03\n",
      "Epoch: 135820 | training loss: 2.1728e-03 | validation loss: 1.8508e-03\n",
      "Epoch: 135830 | training loss: 2.1722e-03 | validation loss: 1.8438e-03\n",
      "Epoch: 135840 | training loss: 2.1719e-03 | validation loss: 1.8483e-03\n",
      "Epoch: 135850 | training loss: 2.1717e-03 | validation loss: 1.8454e-03\n",
      "Epoch: 135860 | training loss: 2.1717e-03 | validation loss: 1.8465e-03\n",
      "Epoch: 135870 | training loss: 2.1716e-03 | validation loss: 1.8466e-03\n",
      "Epoch: 135880 | training loss: 2.1716e-03 | validation loss: 1.8460e-03\n",
      "Epoch: 135890 | training loss: 2.1716e-03 | validation loss: 1.8458e-03\n",
      "Epoch: 135900 | training loss: 2.1715e-03 | validation loss: 1.8454e-03\n",
      "Epoch: 135910 | training loss: 2.1719e-03 | validation loss: 1.8436e-03\n",
      "Epoch: 135920 | training loss: 2.1883e-03 | validation loss: 1.8369e-03\n",
      "Epoch: 135930 | training loss: 3.1893e-03 | validation loss: 2.1954e-03\n",
      "Epoch: 135940 | training loss: 2.7779e-03 | validation loss: 2.2344e-03\n",
      "Epoch: 135950 | training loss: 2.1754e-03 | validation loss: 1.8480e-03\n",
      "Epoch: 135960 | training loss: 2.2459e-03 | validation loss: 1.8457e-03\n",
      "Epoch: 135970 | training loss: 2.1748e-03 | validation loss: 1.8367e-03\n",
      "Epoch: 135980 | training loss: 2.1800e-03 | validation loss: 1.8621e-03\n",
      "Epoch: 135990 | training loss: 2.1716e-03 | validation loss: 1.8492e-03\n",
      "Epoch: 136000 | training loss: 2.1724e-03 | validation loss: 1.8407e-03\n",
      "Epoch: 136010 | training loss: 2.1715e-03 | validation loss: 1.8481e-03\n",
      "Epoch: 136020 | training loss: 2.1712e-03 | validation loss: 1.8457e-03\n",
      "Epoch: 136030 | training loss: 2.1711e-03 | validation loss: 1.8450e-03\n",
      "Epoch: 136040 | training loss: 2.1711e-03 | validation loss: 1.8459e-03\n",
      "Epoch: 136050 | training loss: 2.1711e-03 | validation loss: 1.8453e-03\n",
      "Epoch: 136060 | training loss: 2.1710e-03 | validation loss: 1.8453e-03\n",
      "Epoch: 136070 | training loss: 2.1710e-03 | validation loss: 1.8456e-03\n",
      "Epoch: 136080 | training loss: 2.1710e-03 | validation loss: 1.8453e-03\n",
      "Epoch: 136090 | training loss: 2.1709e-03 | validation loss: 1.8452e-03\n",
      "Epoch: 136100 | training loss: 2.1709e-03 | validation loss: 1.8452e-03\n",
      "Epoch: 136110 | training loss: 2.1710e-03 | validation loss: 1.8457e-03\n",
      "Epoch: 136120 | training loss: 2.1780e-03 | validation loss: 1.8559e-03\n",
      "Epoch: 136130 | training loss: 2.7022e-03 | validation loss: 2.2555e-03\n",
      "Epoch: 136140 | training loss: 2.4475e-03 | validation loss: 1.9209e-03\n",
      "Epoch: 136150 | training loss: 2.5144e-03 | validation loss: 1.9286e-03\n",
      "Epoch: 136160 | training loss: 2.2957e-03 | validation loss: 1.9356e-03\n",
      "Epoch: 136170 | training loss: 2.2155e-03 | validation loss: 1.8302e-03\n",
      "Epoch: 136180 | training loss: 2.1884e-03 | validation loss: 1.8681e-03\n",
      "Epoch: 136190 | training loss: 2.1768e-03 | validation loss: 1.8350e-03\n",
      "Epoch: 136200 | training loss: 2.1711e-03 | validation loss: 1.8488e-03\n",
      "Epoch: 136210 | training loss: 2.1718e-03 | validation loss: 1.8512e-03\n",
      "Epoch: 136220 | training loss: 2.1706e-03 | validation loss: 1.8444e-03\n",
      "Epoch: 136230 | training loss: 2.1709e-03 | validation loss: 1.8412e-03\n",
      "Epoch: 136240 | training loss: 2.1739e-03 | validation loss: 1.8368e-03\n",
      "Epoch: 136250 | training loss: 2.2464e-03 | validation loss: 1.8386e-03\n",
      "Epoch: 136260 | training loss: 3.0174e-03 | validation loss: 2.1094e-03\n",
      "Epoch: 136270 | training loss: 2.4163e-03 | validation loss: 2.0248e-03\n",
      "Epoch: 136280 | training loss: 2.1954e-03 | validation loss: 1.8342e-03\n",
      "Epoch: 136290 | training loss: 2.1710e-03 | validation loss: 1.8423e-03\n",
      "Epoch: 136300 | training loss: 2.1745e-03 | validation loss: 1.8554e-03\n",
      "Epoch: 136310 | training loss: 2.1729e-03 | validation loss: 1.8380e-03\n",
      "Epoch: 136320 | training loss: 2.1711e-03 | validation loss: 1.8480e-03\n",
      "Epoch: 136330 | training loss: 2.1703e-03 | validation loss: 1.8431e-03\n",
      "Epoch: 136340 | training loss: 2.1702e-03 | validation loss: 1.8428e-03\n",
      "Epoch: 136350 | training loss: 2.1703e-03 | validation loss: 1.8455e-03\n",
      "Epoch: 136360 | training loss: 2.1701e-03 | validation loss: 1.8445e-03\n",
      "Epoch: 136370 | training loss: 2.1701e-03 | validation loss: 1.8436e-03\n",
      "Epoch: 136380 | training loss: 2.1700e-03 | validation loss: 1.8432e-03\n",
      "Epoch: 136390 | training loss: 2.1703e-03 | validation loss: 1.8415e-03\n",
      "Epoch: 136400 | training loss: 2.1840e-03 | validation loss: 1.8342e-03\n",
      "Epoch: 136410 | training loss: 3.2098e-03 | validation loss: 2.1980e-03\n",
      "Epoch: 136420 | training loss: 2.8496e-03 | validation loss: 2.2753e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 136430 | training loss: 2.1843e-03 | validation loss: 1.8743e-03\n",
      "Epoch: 136440 | training loss: 2.2226e-03 | validation loss: 1.8407e-03\n",
      "Epoch: 136450 | training loss: 2.1926e-03 | validation loss: 1.8281e-03\n",
      "Epoch: 136460 | training loss: 2.1710e-03 | validation loss: 1.8444e-03\n",
      "Epoch: 136470 | training loss: 2.1736e-03 | validation loss: 1.8568e-03\n",
      "Epoch: 136480 | training loss: 2.1698e-03 | validation loss: 1.8424e-03\n",
      "Epoch: 136490 | training loss: 2.1701e-03 | validation loss: 1.8399e-03\n",
      "Epoch: 136500 | training loss: 2.1698e-03 | validation loss: 1.8457e-03\n",
      "Epoch: 136510 | training loss: 2.1696e-03 | validation loss: 1.8426e-03\n",
      "Epoch: 136520 | training loss: 2.1696e-03 | validation loss: 1.8431e-03\n",
      "Epoch: 136530 | training loss: 2.1696e-03 | validation loss: 1.8433e-03\n",
      "Epoch: 136540 | training loss: 2.1695e-03 | validation loss: 1.8429e-03\n",
      "Epoch: 136550 | training loss: 2.1695e-03 | validation loss: 1.8430e-03\n",
      "Epoch: 136560 | training loss: 2.1695e-03 | validation loss: 1.8430e-03\n",
      "Epoch: 136570 | training loss: 2.1694e-03 | validation loss: 1.8428e-03\n",
      "Epoch: 136580 | training loss: 2.1694e-03 | validation loss: 1.8428e-03\n",
      "Epoch: 136590 | training loss: 2.1694e-03 | validation loss: 1.8430e-03\n",
      "Epoch: 136600 | training loss: 2.1697e-03 | validation loss: 1.8446e-03\n",
      "Epoch: 136610 | training loss: 2.2227e-03 | validation loss: 1.8984e-03\n",
      "Epoch: 136620 | training loss: 2.2627e-03 | validation loss: 1.8876e-03\n",
      "Epoch: 136630 | training loss: 2.5304e-03 | validation loss: 1.9978e-03\n",
      "Epoch: 136640 | training loss: 2.2330e-03 | validation loss: 1.8964e-03\n",
      "Epoch: 136650 | training loss: 2.2251e-03 | validation loss: 1.9162e-03\n",
      "Epoch: 136660 | training loss: 2.1766e-03 | validation loss: 1.8457e-03\n",
      "Epoch: 136670 | training loss: 2.1802e-03 | validation loss: 1.8342e-03\n",
      "Epoch: 136680 | training loss: 2.1769e-03 | validation loss: 1.8321e-03\n",
      "Epoch: 136690 | training loss: 2.1938e-03 | validation loss: 1.8286e-03\n",
      "Epoch: 136700 | training loss: 2.4012e-03 | validation loss: 1.8772e-03\n",
      "Epoch: 136710 | training loss: 2.2645e-03 | validation loss: 1.8415e-03\n",
      "Epoch: 136720 | training loss: 2.2447e-03 | validation loss: 1.9180e-03\n",
      "Epoch: 136730 | training loss: 2.2072e-03 | validation loss: 1.8312e-03\n",
      "Epoch: 136740 | training loss: 2.1778e-03 | validation loss: 1.8597e-03\n",
      "Epoch: 136750 | training loss: 2.1694e-03 | validation loss: 1.8452e-03\n",
      "Epoch: 136760 | training loss: 2.1720e-03 | validation loss: 1.8349e-03\n",
      "Epoch: 136770 | training loss: 2.1711e-03 | validation loss: 1.8358e-03\n",
      "Epoch: 136780 | training loss: 2.1729e-03 | validation loss: 1.8342e-03\n",
      "Epoch: 136790 | training loss: 2.2109e-03 | validation loss: 1.8315e-03\n",
      "Epoch: 136800 | training loss: 2.7755e-03 | validation loss: 2.0156e-03\n",
      "Epoch: 136810 | training loss: 2.2858e-03 | validation loss: 1.9447e-03\n",
      "Epoch: 136820 | training loss: 2.2330e-03 | validation loss: 1.8393e-03\n",
      "Epoch: 136830 | training loss: 2.1987e-03 | validation loss: 1.8780e-03\n",
      "Epoch: 136840 | training loss: 2.1797e-03 | validation loss: 1.8318e-03\n",
      "Epoch: 136850 | training loss: 2.1711e-03 | validation loss: 1.8496e-03\n",
      "Epoch: 136860 | training loss: 2.1686e-03 | validation loss: 1.8412e-03\n",
      "Epoch: 136870 | training loss: 2.1693e-03 | validation loss: 1.8377e-03\n",
      "Epoch: 136880 | training loss: 2.1686e-03 | validation loss: 1.8426e-03\n",
      "Epoch: 136890 | training loss: 2.1689e-03 | validation loss: 1.8441e-03\n",
      "Epoch: 136900 | training loss: 2.1699e-03 | validation loss: 1.8470e-03\n",
      "Epoch: 136910 | training loss: 2.1883e-03 | validation loss: 1.8692e-03\n",
      "Epoch: 136920 | training loss: 2.6729e-03 | validation loss: 2.1736e-03\n",
      "Epoch: 136930 | training loss: 2.2066e-03 | validation loss: 1.8304e-03\n",
      "Epoch: 136940 | training loss: 2.2648e-03 | validation loss: 1.9279e-03\n",
      "Epoch: 136950 | training loss: 2.2242e-03 | validation loss: 1.8382e-03\n",
      "Epoch: 136960 | training loss: 2.1831e-03 | validation loss: 1.8609e-03\n",
      "Epoch: 136970 | training loss: 2.1715e-03 | validation loss: 1.8377e-03\n",
      "Epoch: 136980 | training loss: 2.1694e-03 | validation loss: 1.8440e-03\n",
      "Epoch: 136990 | training loss: 2.1690e-03 | validation loss: 1.8388e-03\n",
      "Epoch: 137000 | training loss: 2.1686e-03 | validation loss: 1.8432e-03\n",
      "Epoch: 137010 | training loss: 2.1681e-03 | validation loss: 1.8398e-03\n",
      "Epoch: 137020 | training loss: 2.1681e-03 | validation loss: 1.8399e-03\n",
      "Epoch: 137030 | training loss: 2.1680e-03 | validation loss: 1.8404e-03\n",
      "Epoch: 137040 | training loss: 2.1680e-03 | validation loss: 1.8403e-03\n",
      "Epoch: 137050 | training loss: 2.1681e-03 | validation loss: 1.8398e-03\n",
      "Epoch: 137060 | training loss: 2.1717e-03 | validation loss: 1.8379e-03\n",
      "Epoch: 137070 | training loss: 2.4496e-03 | validation loss: 1.9408e-03\n",
      "Epoch: 137080 | training loss: 2.4882e-03 | validation loss: 1.9454e-03\n",
      "Epoch: 137090 | training loss: 2.2655e-03 | validation loss: 1.8613e-03\n",
      "Epoch: 137100 | training loss: 2.2409e-03 | validation loss: 1.9339e-03\n",
      "Epoch: 137110 | training loss: 2.1912e-03 | validation loss: 1.8773e-03\n",
      "Epoch: 137120 | training loss: 2.1823e-03 | validation loss: 1.8442e-03\n",
      "Epoch: 137130 | training loss: 2.1711e-03 | validation loss: 1.8496e-03\n",
      "Epoch: 137140 | training loss: 2.1685e-03 | validation loss: 1.8448e-03\n",
      "Epoch: 137150 | training loss: 2.1679e-03 | validation loss: 1.8377e-03\n",
      "Epoch: 137160 | training loss: 2.1678e-03 | validation loss: 1.8409e-03\n",
      "Epoch: 137170 | training loss: 2.1677e-03 | validation loss: 1.8382e-03\n",
      "Epoch: 137180 | training loss: 2.1676e-03 | validation loss: 1.8404e-03\n",
      "Epoch: 137190 | training loss: 2.1675e-03 | validation loss: 1.8398e-03\n",
      "Epoch: 137200 | training loss: 2.1675e-03 | validation loss: 1.8394e-03\n",
      "Epoch: 137210 | training loss: 2.1675e-03 | validation loss: 1.8397e-03\n",
      "Epoch: 137220 | training loss: 2.1674e-03 | validation loss: 1.8398e-03\n",
      "Epoch: 137230 | training loss: 2.1674e-03 | validation loss: 1.8399e-03\n",
      "Epoch: 137240 | training loss: 2.1674e-03 | validation loss: 1.8409e-03\n",
      "Epoch: 137250 | training loss: 2.1704e-03 | validation loss: 1.8498e-03\n",
      "Epoch: 137260 | training loss: 2.4182e-03 | validation loss: 2.0410e-03\n",
      "Epoch: 137270 | training loss: 2.2340e-03 | validation loss: 1.8755e-03\n",
      "Epoch: 137280 | training loss: 2.3832e-03 | validation loss: 2.0188e-03\n",
      "Epoch: 137290 | training loss: 2.2372e-03 | validation loss: 1.9198e-03\n",
      "Epoch: 137300 | training loss: 2.1874e-03 | validation loss: 1.8569e-03\n",
      "Epoch: 137310 | training loss: 2.1697e-03 | validation loss: 1.8410e-03\n",
      "Epoch: 137320 | training loss: 2.1706e-03 | validation loss: 1.8350e-03\n",
      "Epoch: 137330 | training loss: 2.1683e-03 | validation loss: 1.8336e-03\n",
      "Epoch: 137340 | training loss: 2.1674e-03 | validation loss: 1.8422e-03\n",
      "Epoch: 137350 | training loss: 2.1671e-03 | validation loss: 1.8402e-03\n",
      "Epoch: 137360 | training loss: 2.1670e-03 | validation loss: 1.8380e-03\n",
      "Epoch: 137370 | training loss: 2.1670e-03 | validation loss: 1.8385e-03\n",
      "Epoch: 137380 | training loss: 2.1670e-03 | validation loss: 1.8393e-03\n",
      "Epoch: 137390 | training loss: 2.1669e-03 | validation loss: 1.8382e-03\n",
      "Epoch: 137400 | training loss: 2.1669e-03 | validation loss: 1.8388e-03\n",
      "Epoch: 137410 | training loss: 2.1669e-03 | validation loss: 1.8386e-03\n",
      "Epoch: 137420 | training loss: 2.1668e-03 | validation loss: 1.8387e-03\n",
      "Epoch: 137430 | training loss: 2.1668e-03 | validation loss: 1.8386e-03\n",
      "Epoch: 137440 | training loss: 2.1668e-03 | validation loss: 1.8393e-03\n",
      "Epoch: 137450 | training loss: 2.1716e-03 | validation loss: 1.8473e-03\n",
      "Epoch: 137460 | training loss: 2.6541e-03 | validation loss: 2.1456e-03\n",
      "Epoch: 137470 | training loss: 2.4378e-03 | validation loss: 1.9742e-03\n",
      "Epoch: 137480 | training loss: 2.3414e-03 | validation loss: 2.0080e-03\n",
      "Epoch: 137490 | training loss: 2.2047e-03 | validation loss: 1.8824e-03\n",
      "Epoch: 137500 | training loss: 2.1791e-03 | validation loss: 1.8278e-03\n",
      "Epoch: 137510 | training loss: 2.1799e-03 | validation loss: 1.8262e-03\n",
      "Epoch: 137520 | training loss: 2.1686e-03 | validation loss: 1.8386e-03\n",
      "Epoch: 137530 | training loss: 2.1673e-03 | validation loss: 1.8425e-03\n",
      "Epoch: 137540 | training loss: 2.1669e-03 | validation loss: 1.8366e-03\n",
      "Epoch: 137550 | training loss: 2.1665e-03 | validation loss: 1.8390e-03\n",
      "Epoch: 137560 | training loss: 2.1664e-03 | validation loss: 1.8384e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 137570 | training loss: 2.1664e-03 | validation loss: 1.8370e-03\n",
      "Epoch: 137580 | training loss: 2.1663e-03 | validation loss: 1.8383e-03\n",
      "Epoch: 137590 | training loss: 2.1663e-03 | validation loss: 1.8376e-03\n",
      "Epoch: 137600 | training loss: 2.1663e-03 | validation loss: 1.8377e-03\n",
      "Epoch: 137610 | training loss: 2.1662e-03 | validation loss: 1.8378e-03\n",
      "Epoch: 137620 | training loss: 2.1662e-03 | validation loss: 1.8375e-03\n",
      "Epoch: 137630 | training loss: 2.1662e-03 | validation loss: 1.8375e-03\n",
      "Epoch: 137640 | training loss: 2.1662e-03 | validation loss: 1.8374e-03\n",
      "Epoch: 137650 | training loss: 2.1661e-03 | validation loss: 1.8371e-03\n",
      "Epoch: 137660 | training loss: 2.1663e-03 | validation loss: 1.8352e-03\n",
      "Epoch: 137670 | training loss: 2.1813e-03 | validation loss: 1.8245e-03\n",
      "Epoch: 137680 | training loss: 3.5349e-03 | validation loss: 2.3084e-03\n",
      "Epoch: 137690 | training loss: 2.5641e-03 | validation loss: 2.1247e-03\n",
      "Epoch: 137700 | training loss: 2.3650e-03 | validation loss: 2.0427e-03\n",
      "Epoch: 137710 | training loss: 2.2423e-03 | validation loss: 1.9314e-03\n",
      "Epoch: 137720 | training loss: 2.1885e-03 | validation loss: 1.8774e-03\n",
      "Epoch: 137730 | training loss: 2.1703e-03 | validation loss: 1.8516e-03\n",
      "Epoch: 137740 | training loss: 2.1663e-03 | validation loss: 1.8380e-03\n",
      "Epoch: 137750 | training loss: 2.1662e-03 | validation loss: 1.8346e-03\n",
      "Epoch: 137760 | training loss: 2.1662e-03 | validation loss: 1.8334e-03\n",
      "Epoch: 137770 | training loss: 2.1658e-03 | validation loss: 1.8354e-03\n",
      "Epoch: 137780 | training loss: 2.1657e-03 | validation loss: 1.8374e-03\n",
      "Epoch: 137790 | training loss: 2.1657e-03 | validation loss: 1.8373e-03\n",
      "Epoch: 137800 | training loss: 2.1657e-03 | validation loss: 1.8362e-03\n",
      "Epoch: 137810 | training loss: 2.1656e-03 | validation loss: 1.8363e-03\n",
      "Epoch: 137820 | training loss: 2.1656e-03 | validation loss: 1.8366e-03\n",
      "Epoch: 137830 | training loss: 2.1656e-03 | validation loss: 1.8363e-03\n",
      "Epoch: 137840 | training loss: 2.1655e-03 | validation loss: 1.8364e-03\n",
      "Epoch: 137850 | training loss: 2.1655e-03 | validation loss: 1.8363e-03\n",
      "Epoch: 137860 | training loss: 2.1655e-03 | validation loss: 1.8364e-03\n",
      "Epoch: 137870 | training loss: 2.1658e-03 | validation loss: 1.8374e-03\n",
      "Epoch: 137880 | training loss: 2.2259e-03 | validation loss: 1.8833e-03\n",
      "Epoch: 137890 | training loss: 3.5324e-03 | validation loss: 2.6335e-03\n",
      "Epoch: 137900 | training loss: 2.2921e-03 | validation loss: 1.9331e-03\n",
      "Epoch: 137910 | training loss: 2.1729e-03 | validation loss: 1.8319e-03\n",
      "Epoch: 137920 | training loss: 2.2033e-03 | validation loss: 1.8445e-03\n",
      "Epoch: 137930 | training loss: 2.1877e-03 | validation loss: 1.8427e-03\n",
      "Epoch: 137940 | training loss: 2.1695e-03 | validation loss: 1.8417e-03\n",
      "Epoch: 137950 | training loss: 2.1673e-03 | validation loss: 1.8451e-03\n",
      "Epoch: 137960 | training loss: 2.1661e-03 | validation loss: 1.8420e-03\n",
      "Epoch: 137970 | training loss: 2.1654e-03 | validation loss: 1.8364e-03\n",
      "Epoch: 137980 | training loss: 2.1653e-03 | validation loss: 1.8357e-03\n",
      "Epoch: 137990 | training loss: 2.1652e-03 | validation loss: 1.8369e-03\n",
      "Epoch: 138000 | training loss: 2.1651e-03 | validation loss: 1.8358e-03\n",
      "Epoch: 138010 | training loss: 2.1650e-03 | validation loss: 1.8354e-03\n",
      "Epoch: 138020 | training loss: 2.1650e-03 | validation loss: 1.8356e-03\n",
      "Epoch: 138030 | training loss: 2.1650e-03 | validation loss: 1.8354e-03\n",
      "Epoch: 138040 | training loss: 2.1650e-03 | validation loss: 1.8355e-03\n",
      "Epoch: 138050 | training loss: 2.1649e-03 | validation loss: 1.8354e-03\n",
      "Epoch: 138060 | training loss: 2.1649e-03 | validation loss: 1.8354e-03\n",
      "Epoch: 138070 | training loss: 2.1649e-03 | validation loss: 1.8354e-03\n",
      "Epoch: 138080 | training loss: 2.1648e-03 | validation loss: 1.8353e-03\n",
      "Epoch: 138090 | training loss: 2.1648e-03 | validation loss: 1.8353e-03\n",
      "Epoch: 138100 | training loss: 2.1648e-03 | validation loss: 1.8353e-03\n",
      "Epoch: 138110 | training loss: 2.1647e-03 | validation loss: 1.8354e-03\n",
      "Epoch: 138120 | training loss: 2.1650e-03 | validation loss: 1.8376e-03\n",
      "Epoch: 138130 | training loss: 2.2082e-03 | validation loss: 1.8838e-03\n",
      "Epoch: 138140 | training loss: 4.4358e-03 | validation loss: 3.1074e-03\n",
      "Epoch: 138150 | training loss: 2.4044e-03 | validation loss: 2.0276e-03\n",
      "Epoch: 138160 | training loss: 2.2027e-03 | validation loss: 1.8922e-03\n",
      "Epoch: 138170 | training loss: 2.1767e-03 | validation loss: 1.8610e-03\n",
      "Epoch: 138180 | training loss: 2.1681e-03 | validation loss: 1.8437e-03\n",
      "Epoch: 138190 | training loss: 2.1657e-03 | validation loss: 1.8330e-03\n",
      "Epoch: 138200 | training loss: 2.1661e-03 | validation loss: 1.8284e-03\n",
      "Epoch: 138210 | training loss: 2.1658e-03 | validation loss: 1.8288e-03\n",
      "Epoch: 138220 | training loss: 2.1647e-03 | validation loss: 1.8320e-03\n",
      "Epoch: 138230 | training loss: 2.1644e-03 | validation loss: 1.8353e-03\n",
      "Epoch: 138240 | training loss: 2.1644e-03 | validation loss: 1.8360e-03\n",
      "Epoch: 138250 | training loss: 2.1643e-03 | validation loss: 1.8349e-03\n",
      "Epoch: 138260 | training loss: 2.1643e-03 | validation loss: 1.8342e-03\n",
      "Epoch: 138270 | training loss: 2.1643e-03 | validation loss: 1.8345e-03\n",
      "Epoch: 138280 | training loss: 2.1642e-03 | validation loss: 1.8347e-03\n",
      "Epoch: 138290 | training loss: 2.1642e-03 | validation loss: 1.8344e-03\n",
      "Epoch: 138300 | training loss: 2.1642e-03 | validation loss: 1.8344e-03\n",
      "Epoch: 138310 | training loss: 2.1641e-03 | validation loss: 1.8344e-03\n",
      "Epoch: 138320 | training loss: 2.1641e-03 | validation loss: 1.8343e-03\n",
      "Epoch: 138330 | training loss: 2.1641e-03 | validation loss: 1.8343e-03\n",
      "Epoch: 138340 | training loss: 2.1640e-03 | validation loss: 1.8342e-03\n",
      "Epoch: 138350 | training loss: 2.1640e-03 | validation loss: 1.8341e-03\n",
      "Epoch: 138360 | training loss: 2.1640e-03 | validation loss: 1.8341e-03\n",
      "Epoch: 138370 | training loss: 2.1639e-03 | validation loss: 1.8340e-03\n",
      "Epoch: 138380 | training loss: 2.1639e-03 | validation loss: 1.8336e-03\n",
      "Epoch: 138390 | training loss: 2.1697e-03 | validation loss: 1.8320e-03\n",
      "Epoch: 138400 | training loss: 2.9752e-03 | validation loss: 2.3135e-03\n",
      "Epoch: 138410 | training loss: 2.3543e-03 | validation loss: 1.9904e-03\n",
      "Epoch: 138420 | training loss: 2.1653e-03 | validation loss: 1.8371e-03\n",
      "Epoch: 138430 | training loss: 2.1945e-03 | validation loss: 1.8178e-03\n",
      "Epoch: 138440 | training loss: 2.1712e-03 | validation loss: 1.8414e-03\n",
      "Epoch: 138450 | training loss: 2.1734e-03 | validation loss: 1.8478e-03\n",
      "Epoch: 138460 | training loss: 2.1683e-03 | validation loss: 1.8429e-03\n",
      "Epoch: 138470 | training loss: 2.1751e-03 | validation loss: 1.8538e-03\n",
      "Epoch: 138480 | training loss: 2.3047e-03 | validation loss: 1.9566e-03\n",
      "Epoch: 138490 | training loss: 2.5275e-03 | validation loss: 2.0939e-03\n",
      "Epoch: 138500 | training loss: 2.3007e-03 | validation loss: 1.8424e-03\n",
      "Epoch: 138510 | training loss: 2.2150e-03 | validation loss: 1.8906e-03\n",
      "Epoch: 138520 | training loss: 2.1848e-03 | validation loss: 1.8215e-03\n",
      "Epoch: 138530 | training loss: 2.1710e-03 | validation loss: 1.8495e-03\n",
      "Epoch: 138540 | training loss: 2.1639e-03 | validation loss: 1.8302e-03\n",
      "Epoch: 138550 | training loss: 2.1645e-03 | validation loss: 1.8287e-03\n",
      "Epoch: 138560 | training loss: 2.1635e-03 | validation loss: 1.8345e-03\n",
      "Epoch: 138570 | training loss: 2.1641e-03 | validation loss: 1.8373e-03\n",
      "Epoch: 138580 | training loss: 2.1680e-03 | validation loss: 1.8450e-03\n",
      "Epoch: 138590 | training loss: 2.2477e-03 | validation loss: 1.9151e-03\n",
      "Epoch: 138600 | training loss: 2.9167e-03 | validation loss: 2.3114e-03\n",
      "Epoch: 138610 | training loss: 2.3994e-03 | validation loss: 1.8801e-03\n",
      "Epoch: 138620 | training loss: 2.2140e-03 | validation loss: 1.8874e-03\n",
      "Epoch: 138630 | training loss: 2.1688e-03 | validation loss: 1.8241e-03\n",
      "Epoch: 138640 | training loss: 2.1637e-03 | validation loss: 1.8353e-03\n",
      "Epoch: 138650 | training loss: 2.1634e-03 | validation loss: 1.8304e-03\n",
      "Epoch: 138660 | training loss: 2.1638e-03 | validation loss: 1.8366e-03\n",
      "Epoch: 138670 | training loss: 2.1638e-03 | validation loss: 1.8290e-03\n",
      "Epoch: 138680 | training loss: 2.1631e-03 | validation loss: 1.8337e-03\n",
      "Epoch: 138690 | training loss: 2.1632e-03 | validation loss: 1.8343e-03\n",
      "Epoch: 138700 | training loss: 2.1630e-03 | validation loss: 1.8334e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 138710 | training loss: 2.1631e-03 | validation loss: 1.8341e-03\n",
      "Epoch: 138720 | training loss: 2.1656e-03 | validation loss: 1.8404e-03\n",
      "Epoch: 138730 | training loss: 2.2771e-03 | validation loss: 1.9317e-03\n",
      "Epoch: 138740 | training loss: 3.0173e-03 | validation loss: 2.3567e-03\n",
      "Epoch: 138750 | training loss: 2.1686e-03 | validation loss: 1.8379e-03\n",
      "Epoch: 138760 | training loss: 2.2728e-03 | validation loss: 1.8410e-03\n",
      "Epoch: 138770 | training loss: 2.1712e-03 | validation loss: 1.8408e-03\n",
      "Epoch: 138780 | training loss: 2.1710e-03 | validation loss: 1.8489e-03\n",
      "Epoch: 138790 | training loss: 2.1678e-03 | validation loss: 1.8276e-03\n",
      "Epoch: 138800 | training loss: 2.1632e-03 | validation loss: 1.8328e-03\n",
      "Epoch: 138810 | training loss: 2.1627e-03 | validation loss: 1.8333e-03\n",
      "Epoch: 138820 | training loss: 2.1626e-03 | validation loss: 1.8308e-03\n",
      "Epoch: 138830 | training loss: 2.1626e-03 | validation loss: 1.8325e-03\n",
      "Epoch: 138840 | training loss: 2.1625e-03 | validation loss: 1.8315e-03\n",
      "Epoch: 138850 | training loss: 2.1625e-03 | validation loss: 1.8316e-03\n",
      "Epoch: 138860 | training loss: 2.1625e-03 | validation loss: 1.8318e-03\n",
      "Epoch: 138870 | training loss: 2.1624e-03 | validation loss: 1.8316e-03\n",
      "Epoch: 138880 | training loss: 2.1624e-03 | validation loss: 1.8316e-03\n",
      "Epoch: 138890 | training loss: 2.1624e-03 | validation loss: 1.8315e-03\n",
      "Epoch: 138900 | training loss: 2.1623e-03 | validation loss: 1.8315e-03\n",
      "Epoch: 138910 | training loss: 2.1625e-03 | validation loss: 1.8313e-03\n",
      "Epoch: 138920 | training loss: 2.1799e-03 | validation loss: 1.8369e-03\n",
      "Epoch: 138930 | training loss: 3.5595e-03 | validation loss: 2.4463e-03\n",
      "Epoch: 138940 | training loss: 2.7475e-03 | validation loss: 2.2945e-03\n",
      "Epoch: 138950 | training loss: 2.2807e-03 | validation loss: 1.9594e-03\n",
      "Epoch: 138960 | training loss: 2.2410e-03 | validation loss: 1.8856e-03\n",
      "Epoch: 138970 | training loss: 2.1829e-03 | validation loss: 1.8372e-03\n",
      "Epoch: 138980 | training loss: 2.1622e-03 | validation loss: 1.8325e-03\n",
      "Epoch: 138990 | training loss: 2.1646e-03 | validation loss: 1.8354e-03\n",
      "Epoch: 139000 | training loss: 2.1626e-03 | validation loss: 1.8280e-03\n",
      "Epoch: 139010 | training loss: 2.1625e-03 | validation loss: 1.8271e-03\n",
      "Epoch: 139020 | training loss: 2.1622e-03 | validation loss: 1.8308e-03\n",
      "Epoch: 139030 | training loss: 2.1620e-03 | validation loss: 1.8298e-03\n",
      "Epoch: 139040 | training loss: 2.1619e-03 | validation loss: 1.8301e-03\n",
      "Epoch: 139050 | training loss: 2.1619e-03 | validation loss: 1.8305e-03\n",
      "Epoch: 139060 | training loss: 2.1619e-03 | validation loss: 1.8304e-03\n",
      "Epoch: 139070 | training loss: 2.1618e-03 | validation loss: 1.8305e-03\n",
      "Epoch: 139080 | training loss: 2.1618e-03 | validation loss: 1.8305e-03\n",
      "Epoch: 139090 | training loss: 2.1618e-03 | validation loss: 1.8303e-03\n",
      "Epoch: 139100 | training loss: 2.1618e-03 | validation loss: 1.8303e-03\n",
      "Epoch: 139110 | training loss: 2.1617e-03 | validation loss: 1.8303e-03\n",
      "Epoch: 139120 | training loss: 2.1617e-03 | validation loss: 1.8302e-03\n",
      "Epoch: 139130 | training loss: 2.1617e-03 | validation loss: 1.8302e-03\n",
      "Epoch: 139140 | training loss: 2.1616e-03 | validation loss: 1.8301e-03\n",
      "Epoch: 139150 | training loss: 2.1616e-03 | validation loss: 1.8293e-03\n",
      "Epoch: 139160 | training loss: 2.1682e-03 | validation loss: 1.8216e-03\n",
      "Epoch: 139170 | training loss: 4.0015e-03 | validation loss: 2.5050e-03\n",
      "Epoch: 139180 | training loss: 2.5521e-03 | validation loss: 2.0819e-03\n",
      "Epoch: 139190 | training loss: 2.2606e-03 | validation loss: 1.8911e-03\n",
      "Epoch: 139200 | training loss: 2.1953e-03 | validation loss: 1.8274e-03\n",
      "Epoch: 139210 | training loss: 2.1830e-03 | validation loss: 1.8168e-03\n",
      "Epoch: 139220 | training loss: 2.1706e-03 | validation loss: 1.8178e-03\n",
      "Epoch: 139230 | training loss: 2.1642e-03 | validation loss: 1.8217e-03\n",
      "Epoch: 139240 | training loss: 2.1622e-03 | validation loss: 1.8251e-03\n",
      "Epoch: 139250 | training loss: 2.1616e-03 | validation loss: 1.8272e-03\n",
      "Epoch: 139260 | training loss: 2.1615e-03 | validation loss: 1.8282e-03\n",
      "Epoch: 139270 | training loss: 2.1614e-03 | validation loss: 1.8286e-03\n",
      "Epoch: 139280 | training loss: 2.1613e-03 | validation loss: 1.8289e-03\n",
      "Epoch: 139290 | training loss: 2.1612e-03 | validation loss: 1.8292e-03\n",
      "Epoch: 139300 | training loss: 2.1612e-03 | validation loss: 1.8295e-03\n",
      "Epoch: 139310 | training loss: 2.1611e-03 | validation loss: 1.8296e-03\n",
      "Epoch: 139320 | training loss: 2.1611e-03 | validation loss: 1.8296e-03\n",
      "Epoch: 139330 | training loss: 2.1611e-03 | validation loss: 1.8294e-03\n",
      "Epoch: 139340 | training loss: 2.1611e-03 | validation loss: 1.8294e-03\n",
      "Epoch: 139350 | training loss: 2.1610e-03 | validation loss: 1.8294e-03\n",
      "Epoch: 139360 | training loss: 2.1610e-03 | validation loss: 1.8293e-03\n",
      "Epoch: 139370 | training loss: 2.1610e-03 | validation loss: 1.8293e-03\n",
      "Epoch: 139380 | training loss: 2.1609e-03 | validation loss: 1.8292e-03\n",
      "Epoch: 139390 | training loss: 2.1609e-03 | validation loss: 1.8292e-03\n",
      "Epoch: 139400 | training loss: 2.1609e-03 | validation loss: 1.8291e-03\n",
      "Epoch: 139410 | training loss: 2.1608e-03 | validation loss: 1.8291e-03\n",
      "Epoch: 139420 | training loss: 2.1608e-03 | validation loss: 1.8290e-03\n",
      "Epoch: 139430 | training loss: 2.1608e-03 | validation loss: 1.8289e-03\n",
      "Epoch: 139440 | training loss: 2.1607e-03 | validation loss: 1.8289e-03\n",
      "Epoch: 139450 | training loss: 2.1607e-03 | validation loss: 1.8288e-03\n",
      "Epoch: 139460 | training loss: 2.1607e-03 | validation loss: 1.8288e-03\n",
      "Epoch: 139470 | training loss: 2.1606e-03 | validation loss: 1.8287e-03\n",
      "Epoch: 139480 | training loss: 2.1607e-03 | validation loss: 1.8286e-03\n",
      "Epoch: 139490 | training loss: 2.1794e-03 | validation loss: 1.8370e-03\n",
      "Epoch: 139500 | training loss: 4.0377e-03 | validation loss: 2.6531e-03\n",
      "Epoch: 139510 | training loss: 2.7554e-03 | validation loss: 2.3039e-03\n",
      "Epoch: 139520 | training loss: 2.3161e-03 | validation loss: 1.9332e-03\n",
      "Epoch: 139530 | training loss: 2.1995e-03 | validation loss: 1.8444e-03\n",
      "Epoch: 139540 | training loss: 2.1781e-03 | validation loss: 1.8426e-03\n",
      "Epoch: 139550 | training loss: 2.1670e-03 | validation loss: 1.8243e-03\n",
      "Epoch: 139560 | training loss: 2.1633e-03 | validation loss: 1.8219e-03\n",
      "Epoch: 139570 | training loss: 2.1609e-03 | validation loss: 1.8298e-03\n",
      "Epoch: 139580 | training loss: 2.1607e-03 | validation loss: 1.8312e-03\n",
      "Epoch: 139590 | training loss: 2.1605e-03 | validation loss: 1.8277e-03\n",
      "Epoch: 139600 | training loss: 2.1603e-03 | validation loss: 1.8276e-03\n",
      "Epoch: 139610 | training loss: 2.1603e-03 | validation loss: 1.8279e-03\n",
      "Epoch: 139620 | training loss: 2.1602e-03 | validation loss: 1.8272e-03\n",
      "Epoch: 139630 | training loss: 2.1602e-03 | validation loss: 1.8278e-03\n",
      "Epoch: 139640 | training loss: 2.1602e-03 | validation loss: 1.8276e-03\n",
      "Epoch: 139650 | training loss: 2.1601e-03 | validation loss: 1.8277e-03\n",
      "Epoch: 139660 | training loss: 2.1601e-03 | validation loss: 1.8276e-03\n",
      "Epoch: 139670 | training loss: 2.1601e-03 | validation loss: 1.8276e-03\n",
      "Epoch: 139680 | training loss: 2.1601e-03 | validation loss: 1.8276e-03\n",
      "Epoch: 139690 | training loss: 2.1600e-03 | validation loss: 1.8275e-03\n",
      "Epoch: 139700 | training loss: 2.1600e-03 | validation loss: 1.8274e-03\n",
      "Epoch: 139710 | training loss: 2.1600e-03 | validation loss: 1.8274e-03\n",
      "Epoch: 139720 | training loss: 2.1599e-03 | validation loss: 1.8273e-03\n",
      "Epoch: 139730 | training loss: 2.1599e-03 | validation loss: 1.8270e-03\n",
      "Epoch: 139740 | training loss: 2.1608e-03 | validation loss: 1.8232e-03\n",
      "Epoch: 139750 | training loss: 2.4177e-03 | validation loss: 1.8737e-03\n",
      "Epoch: 139760 | training loss: 2.3664e-03 | validation loss: 2.0055e-03\n",
      "Epoch: 139770 | training loss: 2.1799e-03 | validation loss: 1.8651e-03\n",
      "Epoch: 139780 | training loss: 2.1791e-03 | validation loss: 1.8553e-03\n",
      "Epoch: 139790 | training loss: 2.1833e-03 | validation loss: 1.8515e-03\n",
      "Epoch: 139800 | training loss: 2.1731e-03 | validation loss: 1.8436e-03\n",
      "Epoch: 139810 | training loss: 2.1647e-03 | validation loss: 1.8374e-03\n",
      "Epoch: 139820 | training loss: 2.1612e-03 | validation loss: 1.8333e-03\n",
      "Epoch: 139830 | training loss: 2.1600e-03 | validation loss: 1.8303e-03\n",
      "Epoch: 139840 | training loss: 2.1596e-03 | validation loss: 1.8282e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 139850 | training loss: 2.1596e-03 | validation loss: 1.8269e-03\n",
      "Epoch: 139860 | training loss: 2.1596e-03 | validation loss: 1.8264e-03\n",
      "Epoch: 139870 | training loss: 2.1595e-03 | validation loss: 1.8265e-03\n",
      "Epoch: 139880 | training loss: 2.1595e-03 | validation loss: 1.8269e-03\n",
      "Epoch: 139890 | training loss: 2.1595e-03 | validation loss: 1.8270e-03\n",
      "Epoch: 139900 | training loss: 2.1594e-03 | validation loss: 1.8268e-03\n",
      "Epoch: 139910 | training loss: 2.1594e-03 | validation loss: 1.8266e-03\n",
      "Epoch: 139920 | training loss: 2.1594e-03 | validation loss: 1.8267e-03\n",
      "Epoch: 139930 | training loss: 2.1593e-03 | validation loss: 1.8267e-03\n",
      "Epoch: 139940 | training loss: 2.1593e-03 | validation loss: 1.8266e-03\n",
      "Epoch: 139950 | training loss: 2.1593e-03 | validation loss: 1.8266e-03\n",
      "Epoch: 139960 | training loss: 2.1592e-03 | validation loss: 1.8265e-03\n",
      "Epoch: 139970 | training loss: 2.1592e-03 | validation loss: 1.8264e-03\n",
      "Epoch: 139980 | training loss: 2.1592e-03 | validation loss: 1.8264e-03\n",
      "Epoch: 139990 | training loss: 2.1592e-03 | validation loss: 1.8263e-03\n",
      "Epoch: 140000 | training loss: 2.1591e-03 | validation loss: 1.8263e-03\n",
      "Epoch: 140010 | training loss: 2.1591e-03 | validation loss: 1.8262e-03\n",
      "Epoch: 140020 | training loss: 2.1591e-03 | validation loss: 1.8262e-03\n",
      "Epoch: 140030 | training loss: 2.1590e-03 | validation loss: 1.8261e-03\n",
      "Epoch: 140040 | training loss: 2.1590e-03 | validation loss: 1.8261e-03\n",
      "Epoch: 140050 | training loss: 2.1590e-03 | validation loss: 1.8259e-03\n",
      "Epoch: 140060 | training loss: 2.1590e-03 | validation loss: 1.8250e-03\n",
      "Epoch: 140070 | training loss: 2.1730e-03 | validation loss: 1.8168e-03\n",
      "Epoch: 140080 | training loss: 5.0341e-03 | validation loss: 2.9671e-03\n",
      "Epoch: 140090 | training loss: 2.2366e-03 | validation loss: 1.8160e-03\n",
      "Epoch: 140100 | training loss: 2.2737e-03 | validation loss: 1.8221e-03\n",
      "Epoch: 140110 | training loss: 2.2630e-03 | validation loss: 1.8206e-03\n",
      "Epoch: 140120 | training loss: 2.2020e-03 | validation loss: 1.8124e-03\n",
      "Epoch: 140130 | training loss: 2.1698e-03 | validation loss: 1.8153e-03\n",
      "Epoch: 140140 | training loss: 2.1610e-03 | validation loss: 1.8204e-03\n",
      "Epoch: 140150 | training loss: 2.1591e-03 | validation loss: 1.8228e-03\n",
      "Epoch: 140160 | training loss: 2.1588e-03 | validation loss: 1.8237e-03\n",
      "Epoch: 140170 | training loss: 2.1587e-03 | validation loss: 1.8243e-03\n",
      "Epoch: 140180 | training loss: 2.1587e-03 | validation loss: 1.8246e-03\n",
      "Epoch: 140190 | training loss: 2.1586e-03 | validation loss: 1.8246e-03\n",
      "Epoch: 140200 | training loss: 2.1585e-03 | validation loss: 1.8249e-03\n",
      "Epoch: 140210 | training loss: 2.1585e-03 | validation loss: 1.8253e-03\n",
      "Epoch: 140220 | training loss: 2.1585e-03 | validation loss: 1.8255e-03\n",
      "Epoch: 140230 | training loss: 2.1584e-03 | validation loss: 1.8254e-03\n",
      "Epoch: 140240 | training loss: 2.1584e-03 | validation loss: 1.8253e-03\n",
      "Epoch: 140250 | training loss: 2.1584e-03 | validation loss: 1.8252e-03\n",
      "Epoch: 140260 | training loss: 2.1584e-03 | validation loss: 1.8252e-03\n",
      "Epoch: 140270 | training loss: 2.1583e-03 | validation loss: 1.8252e-03\n",
      "Epoch: 140280 | training loss: 2.1583e-03 | validation loss: 1.8251e-03\n",
      "Epoch: 140290 | training loss: 2.1583e-03 | validation loss: 1.8251e-03\n",
      "Epoch: 140300 | training loss: 2.1582e-03 | validation loss: 1.8250e-03\n",
      "Epoch: 140310 | training loss: 2.1582e-03 | validation loss: 1.8252e-03\n",
      "Epoch: 140320 | training loss: 2.1591e-03 | validation loss: 1.8288e-03\n",
      "Epoch: 140330 | training loss: 2.3750e-03 | validation loss: 2.0255e-03\n",
      "Epoch: 140340 | training loss: 2.4891e-03 | validation loss: 1.9970e-03\n",
      "Epoch: 140350 | training loss: 2.2218e-03 | validation loss: 1.8454e-03\n",
      "Epoch: 140360 | training loss: 2.1742e-03 | validation loss: 1.8225e-03\n",
      "Epoch: 140370 | training loss: 2.1616e-03 | validation loss: 1.8212e-03\n",
      "Epoch: 140380 | training loss: 2.1583e-03 | validation loss: 1.8228e-03\n",
      "Epoch: 140390 | training loss: 2.1580e-03 | validation loss: 1.8248e-03\n",
      "Epoch: 140400 | training loss: 2.1582e-03 | validation loss: 1.8260e-03\n",
      "Epoch: 140410 | training loss: 2.1582e-03 | validation loss: 1.8258e-03\n",
      "Epoch: 140420 | training loss: 2.1579e-03 | validation loss: 1.8250e-03\n",
      "Epoch: 140430 | training loss: 2.1579e-03 | validation loss: 1.8252e-03\n",
      "Epoch: 140440 | training loss: 2.1624e-03 | validation loss: 1.8362e-03\n",
      "Epoch: 140450 | training loss: 2.6788e-03 | validation loss: 2.1805e-03\n",
      "Epoch: 140460 | training loss: 2.5523e-03 | validation loss: 1.9134e-03\n",
      "Epoch: 140470 | training loss: 2.2250e-03 | validation loss: 1.8739e-03\n",
      "Epoch: 140480 | training loss: 2.2613e-03 | validation loss: 1.9055e-03\n",
      "Epoch: 140490 | training loss: 2.1828e-03 | validation loss: 1.8500e-03\n",
      "Epoch: 140500 | training loss: 2.1587e-03 | validation loss: 1.8186e-03\n",
      "Epoch: 140510 | training loss: 2.1625e-03 | validation loss: 1.8152e-03\n",
      "Epoch: 140520 | training loss: 2.1578e-03 | validation loss: 1.8212e-03\n",
      "Epoch: 140530 | training loss: 2.1581e-03 | validation loss: 1.8272e-03\n",
      "Epoch: 140540 | training loss: 2.1575e-03 | validation loss: 1.8242e-03\n",
      "Epoch: 140550 | training loss: 2.1576e-03 | validation loss: 1.8228e-03\n",
      "Epoch: 140560 | training loss: 2.1575e-03 | validation loss: 1.8242e-03\n",
      "Epoch: 140570 | training loss: 2.1574e-03 | validation loss: 1.8231e-03\n",
      "Epoch: 140580 | training loss: 2.1574e-03 | validation loss: 1.8235e-03\n",
      "Epoch: 140590 | training loss: 2.1574e-03 | validation loss: 1.8234e-03\n",
      "Epoch: 140600 | training loss: 2.1573e-03 | validation loss: 1.8234e-03\n",
      "Epoch: 140610 | training loss: 2.1573e-03 | validation loss: 1.8232e-03\n",
      "Epoch: 140620 | training loss: 2.1573e-03 | validation loss: 1.8233e-03\n",
      "Epoch: 140630 | training loss: 2.1573e-03 | validation loss: 1.8232e-03\n",
      "Epoch: 140640 | training loss: 2.1572e-03 | validation loss: 1.8232e-03\n",
      "Epoch: 140650 | training loss: 2.1572e-03 | validation loss: 1.8231e-03\n",
      "Epoch: 140660 | training loss: 2.1572e-03 | validation loss: 1.8231e-03\n",
      "Epoch: 140670 | training loss: 2.1571e-03 | validation loss: 1.8230e-03\n",
      "Epoch: 140680 | training loss: 2.1571e-03 | validation loss: 1.8227e-03\n",
      "Epoch: 140690 | training loss: 2.1578e-03 | validation loss: 1.8197e-03\n",
      "Epoch: 140700 | training loss: 2.3892e-03 | validation loss: 1.8690e-03\n",
      "Epoch: 140710 | training loss: 2.3356e-03 | validation loss: 1.9787e-03\n",
      "Epoch: 140720 | training loss: 2.2350e-03 | validation loss: 1.9144e-03\n",
      "Epoch: 140730 | training loss: 2.2956e-03 | validation loss: 1.9541e-03\n",
      "Epoch: 140740 | training loss: 2.2348e-03 | validation loss: 1.9064e-03\n",
      "Epoch: 140750 | training loss: 2.1761e-03 | validation loss: 1.8543e-03\n",
      "Epoch: 140760 | training loss: 2.1592e-03 | validation loss: 1.8309e-03\n",
      "Epoch: 140770 | training loss: 2.1570e-03 | validation loss: 1.8236e-03\n",
      "Epoch: 140780 | training loss: 2.1568e-03 | validation loss: 1.8225e-03\n",
      "Epoch: 140790 | training loss: 2.1568e-03 | validation loss: 1.8227e-03\n",
      "Epoch: 140800 | training loss: 2.1568e-03 | validation loss: 1.8227e-03\n",
      "Epoch: 140810 | training loss: 2.1567e-03 | validation loss: 1.8228e-03\n",
      "Epoch: 140820 | training loss: 2.1567e-03 | validation loss: 1.8230e-03\n",
      "Epoch: 140830 | training loss: 2.1567e-03 | validation loss: 1.8229e-03\n",
      "Epoch: 140840 | training loss: 2.1566e-03 | validation loss: 1.8226e-03\n",
      "Epoch: 140850 | training loss: 2.1566e-03 | validation loss: 1.8224e-03\n",
      "Epoch: 140860 | training loss: 2.1566e-03 | validation loss: 1.8223e-03\n",
      "Epoch: 140870 | training loss: 2.1566e-03 | validation loss: 1.8223e-03\n",
      "Epoch: 140880 | training loss: 2.1565e-03 | validation loss: 1.8223e-03\n",
      "Epoch: 140890 | training loss: 2.1565e-03 | validation loss: 1.8222e-03\n",
      "Epoch: 140900 | training loss: 2.1565e-03 | validation loss: 1.8221e-03\n",
      "Epoch: 140910 | training loss: 2.1564e-03 | validation loss: 1.8221e-03\n",
      "Epoch: 140920 | training loss: 2.1564e-03 | validation loss: 1.8220e-03\n",
      "Epoch: 140930 | training loss: 2.1564e-03 | validation loss: 1.8220e-03\n",
      "Epoch: 140940 | training loss: 2.1563e-03 | validation loss: 1.8216e-03\n",
      "Epoch: 140950 | training loss: 2.1582e-03 | validation loss: 1.8186e-03\n",
      "Epoch: 140960 | training loss: 2.5425e-03 | validation loss: 2.0258e-03\n",
      "Epoch: 140970 | training loss: 2.4463e-03 | validation loss: 2.0736e-03\n",
      "Epoch: 140980 | training loss: 2.2654e-03 | validation loss: 1.9249e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 140990 | training loss: 2.1934e-03 | validation loss: 1.8640e-03\n",
      "Epoch: 141000 | training loss: 2.1682e-03 | validation loss: 1.8385e-03\n",
      "Epoch: 141010 | training loss: 2.1593e-03 | validation loss: 1.8284e-03\n",
      "Epoch: 141020 | training loss: 2.1565e-03 | validation loss: 1.8234e-03\n",
      "Epoch: 141030 | training loss: 2.1561e-03 | validation loss: 1.8210e-03\n",
      "Epoch: 141040 | training loss: 2.1562e-03 | validation loss: 1.8202e-03\n",
      "Epoch: 141050 | training loss: 2.1561e-03 | validation loss: 1.8203e-03\n",
      "Epoch: 141060 | training loss: 2.1560e-03 | validation loss: 1.8204e-03\n",
      "Epoch: 141070 | training loss: 2.1566e-03 | validation loss: 1.8176e-03\n",
      "Epoch: 141080 | training loss: 2.2340e-03 | validation loss: 1.8149e-03\n",
      "Epoch: 141090 | training loss: 3.3514e-03 | validation loss: 2.2295e-03\n",
      "Epoch: 141100 | training loss: 2.4299e-03 | validation loss: 1.8708e-03\n",
      "Epoch: 141110 | training loss: 2.1633e-03 | validation loss: 1.8104e-03\n",
      "Epoch: 141120 | training loss: 2.1760e-03 | validation loss: 1.8421e-03\n",
      "Epoch: 141130 | training loss: 2.1763e-03 | validation loss: 1.8455e-03\n",
      "Epoch: 141140 | training loss: 2.1601e-03 | validation loss: 1.8288e-03\n",
      "Epoch: 141150 | training loss: 2.1562e-03 | validation loss: 1.8172e-03\n",
      "Epoch: 141160 | training loss: 2.1568e-03 | validation loss: 1.8158e-03\n",
      "Epoch: 141170 | training loss: 2.1557e-03 | validation loss: 1.8199e-03\n",
      "Epoch: 141180 | training loss: 2.1558e-03 | validation loss: 1.8221e-03\n",
      "Epoch: 141190 | training loss: 2.1556e-03 | validation loss: 1.8204e-03\n",
      "Epoch: 141200 | training loss: 2.1556e-03 | validation loss: 1.8204e-03\n",
      "Epoch: 141210 | training loss: 2.1556e-03 | validation loss: 1.8208e-03\n",
      "Epoch: 141220 | training loss: 2.1555e-03 | validation loss: 1.8202e-03\n",
      "Epoch: 141230 | training loss: 2.1555e-03 | validation loss: 1.8205e-03\n",
      "Epoch: 141240 | training loss: 2.1555e-03 | validation loss: 1.8203e-03\n",
      "Epoch: 141250 | training loss: 2.1554e-03 | validation loss: 1.8203e-03\n",
      "Epoch: 141260 | training loss: 2.1554e-03 | validation loss: 1.8202e-03\n",
      "Epoch: 141270 | training loss: 2.1554e-03 | validation loss: 1.8202e-03\n",
      "Epoch: 141280 | training loss: 2.1553e-03 | validation loss: 1.8202e-03\n",
      "Epoch: 141290 | training loss: 2.1553e-03 | validation loss: 1.8201e-03\n",
      "Epoch: 141300 | training loss: 2.1553e-03 | validation loss: 1.8201e-03\n",
      "Epoch: 141310 | training loss: 2.1552e-03 | validation loss: 1.8202e-03\n",
      "Epoch: 141320 | training loss: 2.1553e-03 | validation loss: 1.8215e-03\n",
      "Epoch: 141330 | training loss: 2.1710e-03 | validation loss: 1.8438e-03\n",
      "Epoch: 141340 | training loss: 4.2119e-03 | validation loss: 2.9802e-03\n",
      "Epoch: 141350 | training loss: 2.3540e-03 | validation loss: 1.8680e-03\n",
      "Epoch: 141360 | training loss: 2.3176e-03 | validation loss: 1.8432e-03\n",
      "Epoch: 141370 | training loss: 2.2270e-03 | validation loss: 1.8120e-03\n",
      "Epoch: 141380 | training loss: 2.1869e-03 | validation loss: 1.8060e-03\n",
      "Epoch: 141390 | training loss: 2.1681e-03 | validation loss: 1.8087e-03\n",
      "Epoch: 141400 | training loss: 2.1591e-03 | validation loss: 1.8129e-03\n",
      "Epoch: 141410 | training loss: 2.1555e-03 | validation loss: 1.8165e-03\n",
      "Epoch: 141420 | training loss: 2.1550e-03 | validation loss: 1.8203e-03\n",
      "Epoch: 141430 | training loss: 2.1551e-03 | validation loss: 1.8220e-03\n",
      "Epoch: 141440 | training loss: 2.1549e-03 | validation loss: 1.8204e-03\n",
      "Epoch: 141450 | training loss: 2.1549e-03 | validation loss: 1.8191e-03\n",
      "Epoch: 141460 | training loss: 2.1548e-03 | validation loss: 1.8190e-03\n",
      "Epoch: 141470 | training loss: 2.1548e-03 | validation loss: 1.8196e-03\n",
      "Epoch: 141480 | training loss: 2.1548e-03 | validation loss: 1.8194e-03\n",
      "Epoch: 141490 | training loss: 2.1547e-03 | validation loss: 1.8192e-03\n",
      "Epoch: 141500 | training loss: 2.1547e-03 | validation loss: 1.8193e-03\n",
      "Epoch: 141510 | training loss: 2.1547e-03 | validation loss: 1.8192e-03\n",
      "Epoch: 141520 | training loss: 2.1546e-03 | validation loss: 1.8192e-03\n",
      "Epoch: 141530 | training loss: 2.1546e-03 | validation loss: 1.8192e-03\n",
      "Epoch: 141540 | training loss: 2.1546e-03 | validation loss: 1.8199e-03\n",
      "Epoch: 141550 | training loss: 2.1593e-03 | validation loss: 1.8302e-03\n",
      "Epoch: 141560 | training loss: 2.7166e-03 | validation loss: 2.2938e-03\n",
      "Epoch: 141570 | training loss: 2.2635e-03 | validation loss: 1.8651e-03\n",
      "Epoch: 141580 | training loss: 2.2359e-03 | validation loss: 1.8452e-03\n",
      "Epoch: 141590 | training loss: 2.1811e-03 | validation loss: 1.8239e-03\n",
      "Epoch: 141600 | training loss: 2.1585e-03 | validation loss: 1.8164e-03\n",
      "Epoch: 141610 | training loss: 2.1544e-03 | validation loss: 1.8181e-03\n",
      "Epoch: 141620 | training loss: 2.1553e-03 | validation loss: 1.8199e-03\n",
      "Epoch: 141630 | training loss: 2.1559e-03 | validation loss: 1.8156e-03\n",
      "Epoch: 141640 | training loss: 2.1936e-03 | validation loss: 1.8063e-03\n",
      "Epoch: 141650 | training loss: 3.2553e-03 | validation loss: 2.1786e-03\n",
      "Epoch: 141660 | training loss: 2.5246e-03 | validation loss: 2.0731e-03\n",
      "Epoch: 141670 | training loss: 2.1567e-03 | validation loss: 1.8121e-03\n",
      "Epoch: 141680 | training loss: 2.1928e-03 | validation loss: 1.8101e-03\n",
      "Epoch: 141690 | training loss: 2.1668e-03 | validation loss: 1.8410e-03\n",
      "Epoch: 141700 | training loss: 2.1542e-03 | validation loss: 1.8180e-03\n",
      "Epoch: 141710 | training loss: 2.1547e-03 | validation loss: 1.8149e-03\n",
      "Epoch: 141720 | training loss: 2.1546e-03 | validation loss: 1.8213e-03\n",
      "Epoch: 141730 | training loss: 2.1543e-03 | validation loss: 1.8158e-03\n",
      "Epoch: 141740 | training loss: 2.1540e-03 | validation loss: 1.8192e-03\n",
      "Epoch: 141750 | training loss: 2.1539e-03 | validation loss: 1.8178e-03\n",
      "Epoch: 141760 | training loss: 2.1539e-03 | validation loss: 1.8172e-03\n",
      "Epoch: 141770 | training loss: 2.1539e-03 | validation loss: 1.8181e-03\n",
      "Epoch: 141780 | training loss: 2.1539e-03 | validation loss: 1.8182e-03\n",
      "Epoch: 141790 | training loss: 2.1539e-03 | validation loss: 1.8184e-03\n",
      "Epoch: 141800 | training loss: 2.1541e-03 | validation loss: 1.8199e-03\n",
      "Epoch: 141810 | training loss: 2.1622e-03 | validation loss: 1.8341e-03\n",
      "Epoch: 141820 | training loss: 2.6766e-03 | validation loss: 2.1631e-03\n",
      "Epoch: 141830 | training loss: 2.3236e-03 | validation loss: 1.8357e-03\n",
      "Epoch: 141840 | training loss: 2.3590e-03 | validation loss: 1.9686e-03\n",
      "Epoch: 141850 | training loss: 2.1836e-03 | validation loss: 1.8635e-03\n",
      "Epoch: 141860 | training loss: 2.1722e-03 | validation loss: 1.8128e-03\n",
      "Epoch: 141870 | training loss: 2.1591e-03 | validation loss: 1.8070e-03\n",
      "Epoch: 141880 | training loss: 2.1561e-03 | validation loss: 1.8259e-03\n",
      "Epoch: 141890 | training loss: 2.1540e-03 | validation loss: 1.8198e-03\n",
      "Epoch: 141900 | training loss: 2.1540e-03 | validation loss: 1.8138e-03\n",
      "Epoch: 141910 | training loss: 2.1537e-03 | validation loss: 1.8200e-03\n",
      "Epoch: 141920 | training loss: 2.1535e-03 | validation loss: 1.8159e-03\n",
      "Epoch: 141930 | training loss: 2.1534e-03 | validation loss: 1.8180e-03\n",
      "Epoch: 141940 | training loss: 2.1534e-03 | validation loss: 1.8165e-03\n",
      "Epoch: 141950 | training loss: 2.1534e-03 | validation loss: 1.8174e-03\n",
      "Epoch: 141960 | training loss: 2.1533e-03 | validation loss: 1.8170e-03\n",
      "Epoch: 141970 | training loss: 2.1533e-03 | validation loss: 1.8168e-03\n",
      "Epoch: 141980 | training loss: 2.1533e-03 | validation loss: 1.8170e-03\n",
      "Epoch: 141990 | training loss: 2.1533e-03 | validation loss: 1.8172e-03\n",
      "Epoch: 142000 | training loss: 2.1553e-03 | validation loss: 1.8205e-03\n",
      "Epoch: 142010 | training loss: 2.3315e-03 | validation loss: 1.9518e-03\n",
      "Epoch: 142020 | training loss: 2.6476e-03 | validation loss: 1.9681e-03\n",
      "Epoch: 142030 | training loss: 2.1759e-03 | validation loss: 1.8573e-03\n",
      "Epoch: 142040 | training loss: 2.1756e-03 | validation loss: 1.8291e-03\n",
      "Epoch: 142050 | training loss: 2.1661e-03 | validation loss: 1.8419e-03\n",
      "Epoch: 142060 | training loss: 2.1588e-03 | validation loss: 1.8249e-03\n",
      "Epoch: 142070 | training loss: 2.1560e-03 | validation loss: 1.8196e-03\n",
      "Epoch: 142080 | training loss: 2.1552e-03 | validation loss: 1.8259e-03\n",
      "Epoch: 142090 | training loss: 2.1531e-03 | validation loss: 1.8146e-03\n",
      "Epoch: 142100 | training loss: 2.1535e-03 | validation loss: 1.8123e-03\n",
      "Epoch: 142110 | training loss: 2.1535e-03 | validation loss: 1.8123e-03\n",
      "Epoch: 142120 | training loss: 2.1561e-03 | validation loss: 1.8091e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 142130 | training loss: 2.2220e-03 | validation loss: 1.8094e-03\n",
      "Epoch: 142140 | training loss: 3.0216e-03 | validation loss: 2.0899e-03\n",
      "Epoch: 142150 | training loss: 2.4202e-03 | validation loss: 2.0100e-03\n",
      "Epoch: 142160 | training loss: 2.1865e-03 | validation loss: 1.8066e-03\n",
      "Epoch: 142170 | training loss: 2.1529e-03 | validation loss: 1.8157e-03\n",
      "Epoch: 142180 | training loss: 2.1562e-03 | validation loss: 1.8263e-03\n",
      "Epoch: 142190 | training loss: 2.1551e-03 | validation loss: 1.8101e-03\n",
      "Epoch: 142200 | training loss: 2.1535e-03 | validation loss: 1.8198e-03\n",
      "Epoch: 142210 | training loss: 2.1527e-03 | validation loss: 1.8148e-03\n",
      "Epoch: 142220 | training loss: 2.1526e-03 | validation loss: 1.8146e-03\n",
      "Epoch: 142230 | training loss: 2.1527e-03 | validation loss: 1.8173e-03\n",
      "Epoch: 142240 | training loss: 2.1525e-03 | validation loss: 1.8162e-03\n",
      "Epoch: 142250 | training loss: 2.1525e-03 | validation loss: 1.8153e-03\n",
      "Epoch: 142260 | training loss: 2.1525e-03 | validation loss: 1.8147e-03\n",
      "Epoch: 142270 | training loss: 2.1532e-03 | validation loss: 1.8120e-03\n",
      "Epoch: 142280 | training loss: 2.1920e-03 | validation loss: 1.8072e-03\n",
      "Epoch: 142290 | training loss: 3.6256e-03 | validation loss: 2.3521e-03\n",
      "Epoch: 142300 | training loss: 2.4541e-03 | validation loss: 2.0241e-03\n",
      "Epoch: 142310 | training loss: 2.2412e-03 | validation loss: 1.9041e-03\n",
      "Epoch: 142320 | training loss: 2.1727e-03 | validation loss: 1.8148e-03\n",
      "Epoch: 142330 | training loss: 2.1686e-03 | validation loss: 1.8018e-03\n",
      "Epoch: 142340 | training loss: 2.1547e-03 | validation loss: 1.8197e-03\n",
      "Epoch: 142350 | training loss: 2.1540e-03 | validation loss: 1.8242e-03\n",
      "Epoch: 142360 | training loss: 2.1529e-03 | validation loss: 1.8108e-03\n",
      "Epoch: 142370 | training loss: 2.1522e-03 | validation loss: 1.8159e-03\n",
      "Epoch: 142380 | training loss: 2.1521e-03 | validation loss: 1.8159e-03\n",
      "Epoch: 142390 | training loss: 2.1521e-03 | validation loss: 1.8143e-03\n",
      "Epoch: 142400 | training loss: 2.1520e-03 | validation loss: 1.8154e-03\n",
      "Epoch: 142410 | training loss: 2.1520e-03 | validation loss: 1.8149e-03\n",
      "Epoch: 142420 | training loss: 2.1520e-03 | validation loss: 1.8147e-03\n",
      "Epoch: 142430 | training loss: 2.1519e-03 | validation loss: 1.8150e-03\n",
      "Epoch: 142440 | training loss: 2.1519e-03 | validation loss: 1.8148e-03\n",
      "Epoch: 142450 | training loss: 2.1519e-03 | validation loss: 1.8146e-03\n",
      "Epoch: 142460 | training loss: 2.1519e-03 | validation loss: 1.8142e-03\n",
      "Epoch: 142470 | training loss: 2.1534e-03 | validation loss: 1.8123e-03\n",
      "Epoch: 142480 | training loss: 2.3188e-03 | validation loss: 1.8962e-03\n",
      "Epoch: 142490 | training loss: 2.3866e-03 | validation loss: 2.0436e-03\n",
      "Epoch: 142500 | training loss: 2.4459e-03 | validation loss: 2.0225e-03\n",
      "Epoch: 142510 | training loss: 2.2772e-03 | validation loss: 1.8163e-03\n",
      "Epoch: 142520 | training loss: 2.2009e-03 | validation loss: 1.8581e-03\n",
      "Epoch: 142530 | training loss: 2.1657e-03 | validation loss: 1.8012e-03\n",
      "Epoch: 142540 | training loss: 2.1537e-03 | validation loss: 1.8109e-03\n",
      "Epoch: 142550 | training loss: 2.1547e-03 | validation loss: 1.8219e-03\n",
      "Epoch: 142560 | training loss: 2.1516e-03 | validation loss: 1.8143e-03\n",
      "Epoch: 142570 | training loss: 2.1523e-03 | validation loss: 1.8113e-03\n",
      "Epoch: 142580 | training loss: 2.1558e-03 | validation loss: 1.8065e-03\n",
      "Epoch: 142590 | training loss: 2.2260e-03 | validation loss: 1.8079e-03\n",
      "Epoch: 142600 | training loss: 2.9002e-03 | validation loss: 2.0403e-03\n",
      "Epoch: 142610 | training loss: 2.3975e-03 | validation loss: 1.9968e-03\n",
      "Epoch: 142620 | training loss: 2.2193e-03 | validation loss: 1.8095e-03\n",
      "Epoch: 142630 | training loss: 2.1645e-03 | validation loss: 1.8364e-03\n",
      "Epoch: 142640 | training loss: 2.1543e-03 | validation loss: 1.8081e-03\n",
      "Epoch: 142650 | training loss: 2.1529e-03 | validation loss: 1.8198e-03\n",
      "Epoch: 142660 | training loss: 2.1527e-03 | validation loss: 1.8089e-03\n",
      "Epoch: 142670 | training loss: 2.1520e-03 | validation loss: 1.8178e-03\n",
      "Epoch: 142680 | training loss: 2.1512e-03 | validation loss: 1.8137e-03\n",
      "Epoch: 142690 | training loss: 2.1514e-03 | validation loss: 1.8116e-03\n",
      "Epoch: 142700 | training loss: 2.1514e-03 | validation loss: 1.8114e-03\n",
      "Epoch: 142710 | training loss: 2.1525e-03 | validation loss: 1.8089e-03\n",
      "Epoch: 142720 | training loss: 2.1784e-03 | validation loss: 1.8036e-03\n",
      "Epoch: 142730 | training loss: 2.9283e-03 | validation loss: 2.0610e-03\n",
      "Epoch: 142740 | training loss: 2.4289e-03 | validation loss: 2.0172e-03\n",
      "Epoch: 142750 | training loss: 2.2903e-03 | validation loss: 1.8359e-03\n",
      "Epoch: 142760 | training loss: 2.1606e-03 | validation loss: 1.8237e-03\n",
      "Epoch: 142770 | training loss: 2.1537e-03 | validation loss: 1.8223e-03\n",
      "Epoch: 142780 | training loss: 2.1557e-03 | validation loss: 1.8081e-03\n",
      "Epoch: 142790 | training loss: 2.1534e-03 | validation loss: 1.8193e-03\n",
      "Epoch: 142800 | training loss: 2.1518e-03 | validation loss: 1.8107e-03\n",
      "Epoch: 142810 | training loss: 2.1511e-03 | validation loss: 1.8148e-03\n",
      "Epoch: 142820 | training loss: 2.1508e-03 | validation loss: 1.8130e-03\n",
      "Epoch: 142830 | training loss: 2.1508e-03 | validation loss: 1.8120e-03\n",
      "Epoch: 142840 | training loss: 2.1507e-03 | validation loss: 1.8134e-03\n",
      "Epoch: 142850 | training loss: 2.1507e-03 | validation loss: 1.8137e-03\n",
      "Epoch: 142860 | training loss: 2.1508e-03 | validation loss: 1.8142e-03\n",
      "Epoch: 142870 | training loss: 2.1524e-03 | validation loss: 1.8186e-03\n",
      "Epoch: 142880 | training loss: 2.2198e-03 | validation loss: 1.8768e-03\n",
      "Epoch: 142890 | training loss: 3.3865e-03 | validation loss: 2.5383e-03\n",
      "Epoch: 142900 | training loss: 2.2985e-03 | validation loss: 1.8311e-03\n",
      "Epoch: 142910 | training loss: 2.2159e-03 | validation loss: 1.8290e-03\n",
      "Epoch: 142920 | training loss: 2.1818e-03 | validation loss: 1.8421e-03\n",
      "Epoch: 142930 | training loss: 2.1530e-03 | validation loss: 1.8172e-03\n",
      "Epoch: 142940 | training loss: 2.1569e-03 | validation loss: 1.8099e-03\n",
      "Epoch: 142950 | training loss: 2.1515e-03 | validation loss: 1.8156e-03\n",
      "Epoch: 142960 | training loss: 2.1504e-03 | validation loss: 1.8119e-03\n",
      "Epoch: 142970 | training loss: 2.1504e-03 | validation loss: 1.8124e-03\n",
      "Epoch: 142980 | training loss: 2.1503e-03 | validation loss: 1.8129e-03\n",
      "Epoch: 142990 | training loss: 2.1503e-03 | validation loss: 1.8121e-03\n",
      "Epoch: 143000 | training loss: 2.1502e-03 | validation loss: 1.8119e-03\n",
      "Epoch: 143010 | training loss: 2.1502e-03 | validation loss: 1.8116e-03\n",
      "Epoch: 143020 | training loss: 2.1513e-03 | validation loss: 1.8072e-03\n",
      "Epoch: 143030 | training loss: 2.2420e-03 | validation loss: 1.8222e-03\n",
      "Epoch: 143040 | training loss: 2.3852e-03 | validation loss: 1.8473e-03\n",
      "Epoch: 143050 | training loss: 2.3348e-03 | validation loss: 1.9221e-03\n",
      "Epoch: 143060 | training loss: 2.1878e-03 | validation loss: 1.8005e-03\n",
      "Epoch: 143070 | training loss: 2.1520e-03 | validation loss: 1.8193e-03\n",
      "Epoch: 143080 | training loss: 2.1551e-03 | validation loss: 1.8253e-03\n",
      "Epoch: 143090 | training loss: 2.1530e-03 | validation loss: 1.8115e-03\n",
      "Epoch: 143100 | training loss: 2.1504e-03 | validation loss: 1.8080e-03\n",
      "Epoch: 143110 | training loss: 2.1502e-03 | validation loss: 1.8092e-03\n",
      "Epoch: 143120 | training loss: 2.1500e-03 | validation loss: 1.8109e-03\n",
      "Epoch: 143130 | training loss: 2.1509e-03 | validation loss: 1.8077e-03\n",
      "Epoch: 143140 | training loss: 2.2059e-03 | validation loss: 1.8029e-03\n",
      "Epoch: 143150 | training loss: 3.6075e-03 | validation loss: 2.3299e-03\n",
      "Epoch: 143160 | training loss: 2.2305e-03 | validation loss: 1.8740e-03\n",
      "Epoch: 143170 | training loss: 2.2933e-03 | validation loss: 1.9239e-03\n",
      "Epoch: 143180 | training loss: 2.1503e-03 | validation loss: 1.8122e-03\n",
      "Epoch: 143190 | training loss: 2.1701e-03 | validation loss: 1.8072e-03\n",
      "Epoch: 143200 | training loss: 2.1499e-03 | validation loss: 1.8139e-03\n",
      "Epoch: 143210 | training loss: 2.1520e-03 | validation loss: 1.8180e-03\n",
      "Epoch: 143220 | training loss: 2.1502e-03 | validation loss: 1.8075e-03\n",
      "Epoch: 143230 | training loss: 2.1496e-03 | validation loss: 1.8112e-03\n",
      "Epoch: 143240 | training loss: 2.1496e-03 | validation loss: 1.8121e-03\n",
      "Epoch: 143250 | training loss: 2.1495e-03 | validation loss: 1.8102e-03\n",
      "Epoch: 143260 | training loss: 2.1495e-03 | validation loss: 1.8113e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 143270 | training loss: 2.1495e-03 | validation loss: 1.8108e-03\n",
      "Epoch: 143280 | training loss: 2.1494e-03 | validation loss: 1.8107e-03\n",
      "Epoch: 143290 | training loss: 2.1494e-03 | validation loss: 1.8110e-03\n",
      "Epoch: 143300 | training loss: 2.1494e-03 | validation loss: 1.8107e-03\n",
      "Epoch: 143310 | training loss: 2.1493e-03 | validation loss: 1.8106e-03\n",
      "Epoch: 143320 | training loss: 2.1493e-03 | validation loss: 1.8105e-03\n",
      "Epoch: 143330 | training loss: 2.1493e-03 | validation loss: 1.8100e-03\n",
      "Epoch: 143340 | training loss: 2.1502e-03 | validation loss: 1.8071e-03\n",
      "Epoch: 143350 | training loss: 2.2416e-03 | validation loss: 1.8158e-03\n",
      "Epoch: 143360 | training loss: 3.3307e-03 | validation loss: 2.2365e-03\n",
      "Epoch: 143370 | training loss: 2.4137e-03 | validation loss: 1.8675e-03\n",
      "Epoch: 143380 | training loss: 2.1497e-03 | validation loss: 1.8055e-03\n",
      "Epoch: 143390 | training loss: 2.1738e-03 | validation loss: 1.8424e-03\n",
      "Epoch: 143400 | training loss: 2.1715e-03 | validation loss: 1.8415e-03\n",
      "Epoch: 143410 | training loss: 2.1524e-03 | validation loss: 1.8194e-03\n",
      "Epoch: 143420 | training loss: 2.1496e-03 | validation loss: 1.8076e-03\n",
      "Epoch: 143430 | training loss: 2.1500e-03 | validation loss: 1.8065e-03\n",
      "Epoch: 143440 | training loss: 2.1490e-03 | validation loss: 1.8107e-03\n",
      "Epoch: 143450 | training loss: 2.1491e-03 | validation loss: 1.8118e-03\n",
      "Epoch: 143460 | training loss: 2.1489e-03 | validation loss: 1.8095e-03\n",
      "Epoch: 143470 | training loss: 2.1489e-03 | validation loss: 1.8099e-03\n",
      "Epoch: 143480 | training loss: 2.1488e-03 | validation loss: 1.8103e-03\n",
      "Epoch: 143490 | training loss: 2.1488e-03 | validation loss: 1.8098e-03\n",
      "Epoch: 143500 | training loss: 2.1488e-03 | validation loss: 1.8101e-03\n",
      "Epoch: 143510 | training loss: 2.1487e-03 | validation loss: 1.8098e-03\n",
      "Epoch: 143520 | training loss: 2.1487e-03 | validation loss: 1.8100e-03\n",
      "Epoch: 143530 | training loss: 2.1487e-03 | validation loss: 1.8107e-03\n",
      "Epoch: 143540 | training loss: 2.1534e-03 | validation loss: 1.8221e-03\n",
      "Epoch: 143550 | training loss: 2.7484e-03 | validation loss: 2.3307e-03\n",
      "Epoch: 143560 | training loss: 2.2454e-03 | validation loss: 1.8556e-03\n",
      "Epoch: 143570 | training loss: 2.2331e-03 | validation loss: 1.8280e-03\n",
      "Epoch: 143580 | training loss: 2.1817e-03 | validation loss: 1.8222e-03\n",
      "Epoch: 143590 | training loss: 2.1580e-03 | validation loss: 1.8109e-03\n",
      "Epoch: 143600 | training loss: 2.1496e-03 | validation loss: 1.8052e-03\n",
      "Epoch: 143610 | training loss: 2.1496e-03 | validation loss: 1.8051e-03\n",
      "Epoch: 143620 | training loss: 2.1596e-03 | validation loss: 1.8002e-03\n",
      "Epoch: 143630 | training loss: 2.3929e-03 | validation loss: 1.8497e-03\n",
      "Epoch: 143640 | training loss: 2.2579e-03 | validation loss: 1.8155e-03\n",
      "Epoch: 143650 | training loss: 2.1502e-03 | validation loss: 1.8154e-03\n",
      "Epoch: 143660 | training loss: 2.1574e-03 | validation loss: 1.8257e-03\n",
      "Epoch: 143670 | training loss: 2.1591e-03 | validation loss: 1.7982e-03\n",
      "Epoch: 143680 | training loss: 2.1535e-03 | validation loss: 1.8211e-03\n",
      "Epoch: 143690 | training loss: 2.1497e-03 | validation loss: 1.8042e-03\n",
      "Epoch: 143700 | training loss: 2.1483e-03 | validation loss: 1.8101e-03\n",
      "Epoch: 143710 | training loss: 2.1484e-03 | validation loss: 1.8111e-03\n",
      "Epoch: 143720 | training loss: 2.1483e-03 | validation loss: 1.8067e-03\n",
      "Epoch: 143730 | training loss: 2.1482e-03 | validation loss: 1.8071e-03\n",
      "Epoch: 143740 | training loss: 2.1482e-03 | validation loss: 1.8074e-03\n",
      "Epoch: 143750 | training loss: 2.1485e-03 | validation loss: 1.8056e-03\n",
      "Epoch: 143760 | training loss: 2.1600e-03 | validation loss: 1.7987e-03\n",
      "Epoch: 143770 | training loss: 2.7010e-03 | validation loss: 1.9646e-03\n",
      "Epoch: 143780 | training loss: 2.2896e-03 | validation loss: 1.9344e-03\n",
      "Epoch: 143790 | training loss: 2.3497e-03 | validation loss: 1.8510e-03\n",
      "Epoch: 143800 | training loss: 2.1524e-03 | validation loss: 1.8077e-03\n",
      "Epoch: 143810 | training loss: 2.1686e-03 | validation loss: 1.8338e-03\n",
      "Epoch: 143820 | training loss: 2.1550e-03 | validation loss: 1.8051e-03\n",
      "Epoch: 143830 | training loss: 2.1479e-03 | validation loss: 1.8095e-03\n",
      "Epoch: 143840 | training loss: 2.1481e-03 | validation loss: 1.8092e-03\n",
      "Epoch: 143850 | training loss: 2.1480e-03 | validation loss: 1.8072e-03\n",
      "Epoch: 143860 | training loss: 2.1478e-03 | validation loss: 1.8091e-03\n",
      "Epoch: 143870 | training loss: 2.1477e-03 | validation loss: 1.8078e-03\n",
      "Epoch: 143880 | training loss: 2.1477e-03 | validation loss: 1.8078e-03\n",
      "Epoch: 143890 | training loss: 2.1477e-03 | validation loss: 1.8085e-03\n",
      "Epoch: 143900 | training loss: 2.1476e-03 | validation loss: 1.8079e-03\n",
      "Epoch: 143910 | training loss: 2.1476e-03 | validation loss: 1.8077e-03\n",
      "Epoch: 143920 | training loss: 2.1476e-03 | validation loss: 1.8074e-03\n",
      "Epoch: 143930 | training loss: 2.1478e-03 | validation loss: 1.8061e-03\n",
      "Epoch: 143940 | training loss: 2.1575e-03 | validation loss: 1.8006e-03\n",
      "Epoch: 143950 | training loss: 2.8466e-03 | validation loss: 2.0407e-03\n",
      "Epoch: 143960 | training loss: 2.5741e-03 | validation loss: 2.0869e-03\n",
      "Epoch: 143970 | training loss: 2.2260e-03 | validation loss: 1.8076e-03\n",
      "Epoch: 143980 | training loss: 2.2311e-03 | validation loss: 1.8236e-03\n",
      "Epoch: 143990 | training loss: 2.1474e-03 | validation loss: 1.8079e-03\n",
      "Epoch: 144000 | training loss: 2.1588e-03 | validation loss: 1.8222e-03\n",
      "Epoch: 144010 | training loss: 2.1473e-03 | validation loss: 1.8088e-03\n",
      "Epoch: 144020 | training loss: 2.1488e-03 | validation loss: 1.8043e-03\n",
      "Epoch: 144030 | training loss: 2.1475e-03 | validation loss: 1.8093e-03\n",
      "Epoch: 144040 | training loss: 2.1472e-03 | validation loss: 1.8076e-03\n",
      "Epoch: 144050 | training loss: 2.1472e-03 | validation loss: 1.8068e-03\n",
      "Epoch: 144060 | training loss: 2.1472e-03 | validation loss: 1.8079e-03\n",
      "Epoch: 144070 | training loss: 2.1471e-03 | validation loss: 1.8071e-03\n",
      "Epoch: 144080 | training loss: 2.1471e-03 | validation loss: 1.8075e-03\n",
      "Epoch: 144090 | training loss: 2.1471e-03 | validation loss: 1.8077e-03\n",
      "Epoch: 144100 | training loss: 2.1474e-03 | validation loss: 1.8104e-03\n",
      "Epoch: 144110 | training loss: 2.1753e-03 | validation loss: 1.8540e-03\n",
      "Epoch: 144120 | training loss: 2.7456e-03 | validation loss: 2.3142e-03\n",
      "Epoch: 144130 | training loss: 2.2087e-03 | validation loss: 1.8338e-03\n",
      "Epoch: 144140 | training loss: 2.1598e-03 | validation loss: 1.8127e-03\n",
      "Epoch: 144150 | training loss: 2.1657e-03 | validation loss: 1.8084e-03\n",
      "Epoch: 144160 | training loss: 2.1549e-03 | validation loss: 1.7958e-03\n",
      "Epoch: 144170 | training loss: 2.1475e-03 | validation loss: 1.8114e-03\n",
      "Epoch: 144180 | training loss: 2.1488e-03 | validation loss: 1.8157e-03\n",
      "Epoch: 144190 | training loss: 2.1478e-03 | validation loss: 1.8121e-03\n",
      "Epoch: 144200 | training loss: 2.1522e-03 | validation loss: 1.8187e-03\n",
      "Epoch: 144210 | training loss: 2.2519e-03 | validation loss: 1.9042e-03\n",
      "Epoch: 144220 | training loss: 2.8222e-03 | validation loss: 2.2420e-03\n",
      "Epoch: 144230 | training loss: 2.3268e-03 | validation loss: 1.8385e-03\n",
      "Epoch: 144240 | training loss: 2.1679e-03 | validation loss: 1.8349e-03\n",
      "Epoch: 144250 | training loss: 2.1469e-03 | validation loss: 1.8027e-03\n",
      "Epoch: 144260 | training loss: 2.1469e-03 | validation loss: 1.8038e-03\n",
      "Epoch: 144270 | training loss: 2.1466e-03 | validation loss: 1.8081e-03\n",
      "Epoch: 144280 | training loss: 2.1466e-03 | validation loss: 1.8072e-03\n",
      "Epoch: 144290 | training loss: 2.1470e-03 | validation loss: 1.8032e-03\n",
      "Epoch: 144300 | training loss: 2.1467e-03 | validation loss: 1.8084e-03\n",
      "Epoch: 144310 | training loss: 2.1465e-03 | validation loss: 1.8072e-03\n",
      "Epoch: 144320 | training loss: 2.1464e-03 | validation loss: 1.8060e-03\n",
      "Epoch: 144330 | training loss: 2.1464e-03 | validation loss: 1.8053e-03\n",
      "Epoch: 144340 | training loss: 2.1466e-03 | validation loss: 1.8038e-03\n",
      "Epoch: 144350 | training loss: 2.1580e-03 | validation loss: 1.7969e-03\n",
      "Epoch: 144360 | training loss: 2.9630e-03 | validation loss: 2.0724e-03\n",
      "Epoch: 144370 | training loss: 2.6891e-03 | validation loss: 2.1637e-03\n",
      "Epoch: 144380 | training loss: 2.1797e-03 | validation loss: 1.8101e-03\n",
      "Epoch: 144390 | training loss: 2.2284e-03 | validation loss: 1.8021e-03\n",
      "Epoch: 144400 | training loss: 2.1511e-03 | validation loss: 1.7954e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 144410 | training loss: 2.1546e-03 | validation loss: 1.8249e-03\n",
      "Epoch: 144420 | training loss: 2.1469e-03 | validation loss: 1.8112e-03\n",
      "Epoch: 144430 | training loss: 2.1475e-03 | validation loss: 1.7995e-03\n",
      "Epoch: 144440 | training loss: 2.1461e-03 | validation loss: 1.8073e-03\n",
      "Epoch: 144450 | training loss: 2.1461e-03 | validation loss: 1.8064e-03\n",
      "Epoch: 144460 | training loss: 2.1460e-03 | validation loss: 1.8044e-03\n",
      "Epoch: 144470 | training loss: 2.1460e-03 | validation loss: 1.8061e-03\n",
      "Epoch: 144480 | training loss: 2.1459e-03 | validation loss: 1.8048e-03\n",
      "Epoch: 144490 | training loss: 2.1459e-03 | validation loss: 1.8057e-03\n",
      "Epoch: 144500 | training loss: 2.1459e-03 | validation loss: 1.8052e-03\n",
      "Epoch: 144510 | training loss: 2.1458e-03 | validation loss: 1.8051e-03\n",
      "Epoch: 144520 | training loss: 2.1458e-03 | validation loss: 1.8052e-03\n",
      "Epoch: 144530 | training loss: 2.1458e-03 | validation loss: 1.8050e-03\n",
      "Epoch: 144540 | training loss: 2.1462e-03 | validation loss: 1.8035e-03\n",
      "Epoch: 144550 | training loss: 2.2155e-03 | validation loss: 1.8291e-03\n",
      "Epoch: 144560 | training loss: 2.1569e-03 | validation loss: 1.8190e-03\n",
      "Epoch: 144570 | training loss: 2.2996e-03 | validation loss: 1.8988e-03\n",
      "Epoch: 144580 | training loss: 2.3513e-03 | validation loss: 1.9464e-03\n",
      "Epoch: 144590 | training loss: 2.2130e-03 | validation loss: 1.8599e-03\n",
      "Epoch: 144600 | training loss: 2.1731e-03 | validation loss: 1.7896e-03\n",
      "Epoch: 144610 | training loss: 2.1652e-03 | validation loss: 1.7916e-03\n",
      "Epoch: 144620 | training loss: 2.1482e-03 | validation loss: 1.7992e-03\n",
      "Epoch: 144630 | training loss: 2.1471e-03 | validation loss: 1.8013e-03\n",
      "Epoch: 144640 | training loss: 2.1582e-03 | validation loss: 1.7941e-03\n",
      "Epoch: 144650 | training loss: 2.4870e-03 | validation loss: 1.8774e-03\n",
      "Epoch: 144660 | training loss: 2.1519e-03 | validation loss: 1.8004e-03\n",
      "Epoch: 144670 | training loss: 2.1800e-03 | validation loss: 1.7950e-03\n",
      "Epoch: 144680 | training loss: 2.1962e-03 | validation loss: 1.8595e-03\n",
      "Epoch: 144690 | training loss: 2.1674e-03 | validation loss: 1.7932e-03\n",
      "Epoch: 144700 | training loss: 2.1521e-03 | validation loss: 1.8184e-03\n",
      "Epoch: 144710 | training loss: 2.1477e-03 | validation loss: 1.7983e-03\n",
      "Epoch: 144720 | training loss: 2.1464e-03 | validation loss: 1.8096e-03\n",
      "Epoch: 144730 | training loss: 2.1456e-03 | validation loss: 1.8014e-03\n",
      "Epoch: 144740 | training loss: 2.1452e-03 | validation loss: 1.8043e-03\n",
      "Epoch: 144750 | training loss: 2.1452e-03 | validation loss: 1.8055e-03\n",
      "Epoch: 144760 | training loss: 2.1451e-03 | validation loss: 1.8047e-03\n",
      "Epoch: 144770 | training loss: 2.1451e-03 | validation loss: 1.8045e-03\n",
      "Epoch: 144780 | training loss: 2.1452e-03 | validation loss: 1.8059e-03\n",
      "Epoch: 144790 | training loss: 2.1521e-03 | validation loss: 1.8188e-03\n",
      "Epoch: 144800 | training loss: 2.6108e-03 | validation loss: 2.1183e-03\n",
      "Epoch: 144810 | training loss: 2.2587e-03 | validation loss: 1.8054e-03\n",
      "Epoch: 144820 | training loss: 2.3777e-03 | validation loss: 1.9706e-03\n",
      "Epoch: 144830 | training loss: 2.1704e-03 | validation loss: 1.8429e-03\n",
      "Epoch: 144840 | training loss: 2.1671e-03 | validation loss: 1.8036e-03\n",
      "Epoch: 144850 | training loss: 2.1488e-03 | validation loss: 1.7952e-03\n",
      "Epoch: 144860 | training loss: 2.1486e-03 | validation loss: 1.8097e-03\n",
      "Epoch: 144870 | training loss: 2.1451e-03 | validation loss: 1.8055e-03\n",
      "Epoch: 144880 | training loss: 2.1450e-03 | validation loss: 1.8019e-03\n",
      "Epoch: 144890 | training loss: 2.1449e-03 | validation loss: 1.8044e-03\n",
      "Epoch: 144900 | training loss: 2.1448e-03 | validation loss: 1.8032e-03\n",
      "Epoch: 144910 | training loss: 2.1447e-03 | validation loss: 1.8035e-03\n",
      "Epoch: 144920 | training loss: 2.1446e-03 | validation loss: 1.8033e-03\n",
      "Epoch: 144930 | training loss: 2.1446e-03 | validation loss: 1.8033e-03\n",
      "Epoch: 144940 | training loss: 2.1446e-03 | validation loss: 1.8033e-03\n",
      "Epoch: 144950 | training loss: 2.1445e-03 | validation loss: 1.8032e-03\n",
      "Epoch: 144960 | training loss: 2.1445e-03 | validation loss: 1.8031e-03\n",
      "Epoch: 144970 | training loss: 2.1445e-03 | validation loss: 1.8030e-03\n",
      "Epoch: 144980 | training loss: 2.1445e-03 | validation loss: 1.8027e-03\n",
      "Epoch: 144990 | training loss: 2.1451e-03 | validation loss: 1.8011e-03\n",
      "Epoch: 145000 | training loss: 2.1962e-03 | validation loss: 1.8095e-03\n",
      "Epoch: 145010 | training loss: 3.7989e-03 | validation loss: 2.4375e-03\n",
      "Epoch: 145020 | training loss: 2.4015e-03 | validation loss: 2.0601e-03\n",
      "Epoch: 145030 | training loss: 2.2142e-03 | validation loss: 1.8939e-03\n",
      "Epoch: 145040 | training loss: 2.1619e-03 | validation loss: 1.8064e-03\n",
      "Epoch: 145050 | training loss: 2.1545e-03 | validation loss: 1.8009e-03\n",
      "Epoch: 145060 | training loss: 2.1474e-03 | validation loss: 1.8141e-03\n",
      "Epoch: 145070 | training loss: 2.1446e-03 | validation loss: 1.8056e-03\n",
      "Epoch: 145080 | training loss: 2.1445e-03 | validation loss: 1.7998e-03\n",
      "Epoch: 145090 | training loss: 2.1443e-03 | validation loss: 1.8041e-03\n",
      "Epoch: 145100 | training loss: 2.1442e-03 | validation loss: 1.8010e-03\n",
      "Epoch: 145110 | training loss: 2.1441e-03 | validation loss: 1.8026e-03\n",
      "Epoch: 145120 | training loss: 2.1441e-03 | validation loss: 1.8019e-03\n",
      "Epoch: 145130 | training loss: 2.1440e-03 | validation loss: 1.8028e-03\n",
      "Epoch: 145140 | training loss: 2.1440e-03 | validation loss: 1.8023e-03\n",
      "Epoch: 145150 | training loss: 2.1440e-03 | validation loss: 1.8021e-03\n",
      "Epoch: 145160 | training loss: 2.1439e-03 | validation loss: 1.8022e-03\n",
      "Epoch: 145170 | training loss: 2.1439e-03 | validation loss: 1.8022e-03\n",
      "Epoch: 145180 | training loss: 2.1439e-03 | validation loss: 1.8022e-03\n",
      "Epoch: 145190 | training loss: 2.1439e-03 | validation loss: 1.8027e-03\n",
      "Epoch: 145200 | training loss: 2.1447e-03 | validation loss: 1.8072e-03\n",
      "Epoch: 145210 | training loss: 2.2693e-03 | validation loss: 1.9218e-03\n",
      "Epoch: 145220 | training loss: 2.5274e-03 | validation loss: 2.0575e-03\n",
      "Epoch: 145230 | training loss: 2.4284e-03 | validation loss: 2.0298e-03\n",
      "Epoch: 145240 | training loss: 2.2052e-03 | validation loss: 1.8828e-03\n",
      "Epoch: 145250 | training loss: 2.1723e-03 | validation loss: 1.8338e-03\n",
      "Epoch: 145260 | training loss: 2.1534e-03 | validation loss: 1.8139e-03\n",
      "Epoch: 145270 | training loss: 2.1459e-03 | validation loss: 1.8109e-03\n",
      "Epoch: 145280 | training loss: 2.1454e-03 | validation loss: 1.8099e-03\n",
      "Epoch: 145290 | training loss: 2.1442e-03 | validation loss: 1.8055e-03\n",
      "Epoch: 145300 | training loss: 2.1439e-03 | validation loss: 1.8033e-03\n",
      "Epoch: 145310 | training loss: 2.1436e-03 | validation loss: 1.8030e-03\n",
      "Epoch: 145320 | training loss: 2.1435e-03 | validation loss: 1.8013e-03\n",
      "Epoch: 145330 | training loss: 2.1435e-03 | validation loss: 1.8008e-03\n",
      "Epoch: 145340 | training loss: 2.1434e-03 | validation loss: 1.8011e-03\n",
      "Epoch: 145350 | training loss: 2.1434e-03 | validation loss: 1.8013e-03\n",
      "Epoch: 145360 | training loss: 2.1434e-03 | validation loss: 1.8013e-03\n",
      "Epoch: 145370 | training loss: 2.1433e-03 | validation loss: 1.8011e-03\n",
      "Epoch: 145380 | training loss: 2.1433e-03 | validation loss: 1.8011e-03\n",
      "Epoch: 145390 | training loss: 2.1433e-03 | validation loss: 1.8011e-03\n",
      "Epoch: 145400 | training loss: 2.1433e-03 | validation loss: 1.8010e-03\n",
      "Epoch: 145410 | training loss: 2.1432e-03 | validation loss: 1.8010e-03\n",
      "Epoch: 145420 | training loss: 2.1432e-03 | validation loss: 1.8010e-03\n",
      "Epoch: 145430 | training loss: 2.1432e-03 | validation loss: 1.8013e-03\n",
      "Epoch: 145440 | training loss: 2.1448e-03 | validation loss: 1.8057e-03\n",
      "Epoch: 145450 | training loss: 2.4699e-03 | validation loss: 2.0178e-03\n",
      "Epoch: 145460 | training loss: 2.3740e-03 | validation loss: 1.9249e-03\n",
      "Epoch: 145470 | training loss: 2.2889e-03 | validation loss: 1.9607e-03\n",
      "Epoch: 145480 | training loss: 2.2486e-03 | validation loss: 1.9219e-03\n",
      "Epoch: 145490 | training loss: 2.1952e-03 | validation loss: 1.8723e-03\n",
      "Epoch: 145500 | training loss: 2.1624e-03 | validation loss: 1.8376e-03\n",
      "Epoch: 145510 | training loss: 2.1475e-03 | validation loss: 1.8154e-03\n",
      "Epoch: 145520 | training loss: 2.1437e-03 | validation loss: 1.8035e-03\n",
      "Epoch: 145530 | training loss: 2.1436e-03 | validation loss: 1.7991e-03\n",
      "Epoch: 145540 | training loss: 2.1431e-03 | validation loss: 1.7994e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 145550 | training loss: 2.1429e-03 | validation loss: 1.8011e-03\n",
      "Epoch: 145560 | training loss: 2.1429e-03 | validation loss: 1.8010e-03\n",
      "Epoch: 145570 | training loss: 2.1428e-03 | validation loss: 1.7999e-03\n",
      "Epoch: 145580 | training loss: 2.1428e-03 | validation loss: 1.8002e-03\n",
      "Epoch: 145590 | training loss: 2.1427e-03 | validation loss: 1.8006e-03\n",
      "Epoch: 145600 | training loss: 2.1427e-03 | validation loss: 1.8002e-03\n",
      "Epoch: 145610 | training loss: 2.1427e-03 | validation loss: 1.8003e-03\n",
      "Epoch: 145620 | training loss: 2.1426e-03 | validation loss: 1.8002e-03\n",
      "Epoch: 145630 | training loss: 2.1426e-03 | validation loss: 1.8002e-03\n",
      "Epoch: 145640 | training loss: 2.1426e-03 | validation loss: 1.8001e-03\n",
      "Epoch: 145650 | training loss: 2.1426e-03 | validation loss: 1.8001e-03\n",
      "Epoch: 145660 | training loss: 2.1425e-03 | validation loss: 1.8001e-03\n",
      "Epoch: 145670 | training loss: 2.1425e-03 | validation loss: 1.8000e-03\n",
      "Epoch: 145680 | training loss: 2.1425e-03 | validation loss: 1.8000e-03\n",
      "Epoch: 145690 | training loss: 2.1424e-03 | validation loss: 1.7999e-03\n",
      "Epoch: 145700 | training loss: 2.1424e-03 | validation loss: 1.7998e-03\n",
      "Epoch: 145710 | training loss: 2.1424e-03 | validation loss: 1.7988e-03\n",
      "Epoch: 145720 | training loss: 2.1487e-03 | validation loss: 1.7905e-03\n",
      "Epoch: 145730 | training loss: 3.4637e-03 | validation loss: 2.2547e-03\n",
      "Epoch: 145740 | training loss: 2.8307e-03 | validation loss: 2.2506e-03\n",
      "Epoch: 145750 | training loss: 2.3169e-03 | validation loss: 1.9751e-03\n",
      "Epoch: 145760 | training loss: 2.1629e-03 | validation loss: 1.8350e-03\n",
      "Epoch: 145770 | training loss: 2.1446e-03 | validation loss: 1.7951e-03\n",
      "Epoch: 145780 | training loss: 2.1460e-03 | validation loss: 1.7904e-03\n",
      "Epoch: 145790 | training loss: 2.1453e-03 | validation loss: 1.7920e-03\n",
      "Epoch: 145800 | training loss: 2.1435e-03 | validation loss: 1.7936e-03\n",
      "Epoch: 145810 | training loss: 2.1426e-03 | validation loss: 1.7954e-03\n",
      "Epoch: 145820 | training loss: 2.1422e-03 | validation loss: 1.7971e-03\n",
      "Epoch: 145830 | training loss: 2.1421e-03 | validation loss: 1.7983e-03\n",
      "Epoch: 145840 | training loss: 2.1420e-03 | validation loss: 1.7989e-03\n",
      "Epoch: 145850 | training loss: 2.1420e-03 | validation loss: 1.7994e-03\n",
      "Epoch: 145860 | training loss: 2.1420e-03 | validation loss: 1.7993e-03\n",
      "Epoch: 145870 | training loss: 2.1419e-03 | validation loss: 1.7991e-03\n",
      "Epoch: 145880 | training loss: 2.1419e-03 | validation loss: 1.7989e-03\n",
      "Epoch: 145890 | training loss: 2.1419e-03 | validation loss: 1.7988e-03\n",
      "Epoch: 145900 | training loss: 2.1418e-03 | validation loss: 1.7988e-03\n",
      "Epoch: 145910 | training loss: 2.1418e-03 | validation loss: 1.7988e-03\n",
      "Epoch: 145920 | training loss: 2.1418e-03 | validation loss: 1.7987e-03\n",
      "Epoch: 145930 | training loss: 2.1418e-03 | validation loss: 1.7987e-03\n",
      "Epoch: 145940 | training loss: 2.1417e-03 | validation loss: 1.7987e-03\n",
      "Epoch: 145950 | training loss: 2.1417e-03 | validation loss: 1.7985e-03\n",
      "Epoch: 145960 | training loss: 2.1452e-03 | validation loss: 1.7993e-03\n",
      "Epoch: 145970 | training loss: 2.7448e-03 | validation loss: 2.1187e-03\n",
      "Epoch: 145980 | training loss: 2.3658e-03 | validation loss: 1.9446e-03\n",
      "Epoch: 145990 | training loss: 2.3176e-03 | validation loss: 1.8142e-03\n",
      "Epoch: 146000 | training loss: 2.1569e-03 | validation loss: 1.7893e-03\n",
      "Epoch: 146010 | training loss: 2.1635e-03 | validation loss: 1.8192e-03\n",
      "Epoch: 146020 | training loss: 2.1454e-03 | validation loss: 1.7895e-03\n",
      "Epoch: 146030 | training loss: 2.1427e-03 | validation loss: 1.7933e-03\n",
      "Epoch: 146040 | training loss: 2.1426e-03 | validation loss: 1.7997e-03\n",
      "Epoch: 146050 | training loss: 2.1419e-03 | validation loss: 1.7945e-03\n",
      "Epoch: 146060 | training loss: 2.1416e-03 | validation loss: 1.7992e-03\n",
      "Epoch: 146070 | training loss: 2.1414e-03 | validation loss: 1.7973e-03\n",
      "Epoch: 146080 | training loss: 2.1414e-03 | validation loss: 1.7987e-03\n",
      "Epoch: 146090 | training loss: 2.1413e-03 | validation loss: 1.7983e-03\n",
      "Epoch: 146100 | training loss: 2.1413e-03 | validation loss: 1.7976e-03\n",
      "Epoch: 146110 | training loss: 2.1413e-03 | validation loss: 1.7977e-03\n",
      "Epoch: 146120 | training loss: 2.1412e-03 | validation loss: 1.7977e-03\n",
      "Epoch: 146130 | training loss: 2.1412e-03 | validation loss: 1.7972e-03\n",
      "Epoch: 146140 | training loss: 2.1418e-03 | validation loss: 1.7947e-03\n",
      "Epoch: 146150 | training loss: 2.1853e-03 | validation loss: 1.7901e-03\n",
      "Epoch: 146160 | training loss: 3.9164e-03 | validation loss: 2.4595e-03\n",
      "Epoch: 146170 | training loss: 2.1887e-03 | validation loss: 1.8384e-03\n",
      "Epoch: 146180 | training loss: 2.3189e-03 | validation loss: 1.9304e-03\n",
      "Epoch: 146190 | training loss: 2.1772e-03 | validation loss: 1.8388e-03\n",
      "Epoch: 146200 | training loss: 2.1429e-03 | validation loss: 1.7975e-03\n",
      "Epoch: 146210 | training loss: 2.1497e-03 | validation loss: 1.7929e-03\n",
      "Epoch: 146220 | training loss: 2.1412e-03 | validation loss: 1.7959e-03\n",
      "Epoch: 146230 | training loss: 2.1420e-03 | validation loss: 1.8013e-03\n",
      "Epoch: 146240 | training loss: 2.1409e-03 | validation loss: 1.7974e-03\n",
      "Epoch: 146250 | training loss: 2.1410e-03 | validation loss: 1.7966e-03\n",
      "Epoch: 146260 | training loss: 2.1409e-03 | validation loss: 1.7983e-03\n",
      "Epoch: 146270 | training loss: 2.1408e-03 | validation loss: 1.7970e-03\n",
      "Epoch: 146280 | training loss: 2.1408e-03 | validation loss: 1.7975e-03\n",
      "Epoch: 146290 | training loss: 2.1407e-03 | validation loss: 1.7972e-03\n",
      "Epoch: 146300 | training loss: 2.1407e-03 | validation loss: 1.7974e-03\n",
      "Epoch: 146310 | training loss: 2.1407e-03 | validation loss: 1.7972e-03\n",
      "Epoch: 146320 | training loss: 2.1407e-03 | validation loss: 1.7973e-03\n",
      "Epoch: 146330 | training loss: 2.1406e-03 | validation loss: 1.7972e-03\n",
      "Epoch: 146340 | training loss: 2.1406e-03 | validation loss: 1.7971e-03\n",
      "Epoch: 146350 | training loss: 2.1406e-03 | validation loss: 1.7971e-03\n",
      "Epoch: 146360 | training loss: 2.1405e-03 | validation loss: 1.7970e-03\n",
      "Epoch: 146370 | training loss: 2.1405e-03 | validation loss: 1.7968e-03\n",
      "Epoch: 146380 | training loss: 2.1407e-03 | validation loss: 1.7952e-03\n",
      "Epoch: 146390 | training loss: 2.1841e-03 | validation loss: 1.7934e-03\n",
      "Epoch: 146400 | training loss: 4.5477e-03 | validation loss: 2.7489e-03\n",
      "Epoch: 146410 | training loss: 2.6203e-03 | validation loss: 1.9753e-03\n",
      "Epoch: 146420 | training loss: 2.3072e-03 | validation loss: 1.8606e-03\n",
      "Epoch: 146430 | training loss: 2.2123e-03 | validation loss: 1.8220e-03\n",
      "Epoch: 146440 | training loss: 2.1687e-03 | validation loss: 1.8020e-03\n",
      "Epoch: 146450 | training loss: 2.1506e-03 | validation loss: 1.7948e-03\n",
      "Epoch: 146460 | training loss: 2.1436e-03 | validation loss: 1.7944e-03\n",
      "Epoch: 146470 | training loss: 2.1411e-03 | validation loss: 1.7959e-03\n",
      "Epoch: 146480 | training loss: 2.1403e-03 | validation loss: 1.7963e-03\n",
      "Epoch: 146490 | training loss: 2.1402e-03 | validation loss: 1.7966e-03\n",
      "Epoch: 146500 | training loss: 2.1402e-03 | validation loss: 1.7971e-03\n",
      "Epoch: 146510 | training loss: 2.1401e-03 | validation loss: 1.7967e-03\n",
      "Epoch: 146520 | training loss: 2.1401e-03 | validation loss: 1.7965e-03\n",
      "Epoch: 146530 | training loss: 2.1401e-03 | validation loss: 1.7963e-03\n",
      "Epoch: 146540 | training loss: 2.1400e-03 | validation loss: 1.7964e-03\n",
      "Epoch: 146550 | training loss: 2.1400e-03 | validation loss: 1.7964e-03\n",
      "Epoch: 146560 | training loss: 2.1400e-03 | validation loss: 1.7963e-03\n",
      "Epoch: 146570 | training loss: 2.1399e-03 | validation loss: 1.7962e-03\n",
      "Epoch: 146580 | training loss: 2.1399e-03 | validation loss: 1.7962e-03\n",
      "Epoch: 146590 | training loss: 2.1399e-03 | validation loss: 1.7962e-03\n",
      "Epoch: 146600 | training loss: 2.1399e-03 | validation loss: 1.7962e-03\n",
      "Epoch: 146610 | training loss: 2.1398e-03 | validation loss: 1.7969e-03\n",
      "Epoch: 146620 | training loss: 2.1423e-03 | validation loss: 1.8066e-03\n",
      "Epoch: 146630 | training loss: 2.6240e-03 | validation loss: 2.2345e-03\n",
      "Epoch: 146640 | training loss: 2.4486e-03 | validation loss: 1.9172e-03\n",
      "Epoch: 146650 | training loss: 2.3191e-03 | validation loss: 1.9050e-03\n",
      "Epoch: 146660 | training loss: 2.1976e-03 | validation loss: 1.7929e-03\n",
      "Epoch: 146670 | training loss: 2.1547e-03 | validation loss: 1.7967e-03\n",
      "Epoch: 146680 | training loss: 2.1450e-03 | validation loss: 1.7935e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 146690 | training loss: 2.1417e-03 | validation loss: 1.7902e-03\n",
      "Epoch: 146700 | training loss: 2.1401e-03 | validation loss: 1.7944e-03\n",
      "Epoch: 146710 | training loss: 2.1396e-03 | validation loss: 1.7947e-03\n",
      "Epoch: 146720 | training loss: 2.1396e-03 | validation loss: 1.7944e-03\n",
      "Epoch: 146730 | training loss: 2.1395e-03 | validation loss: 1.7964e-03\n",
      "Epoch: 146740 | training loss: 2.1395e-03 | validation loss: 1.7959e-03\n",
      "Epoch: 146750 | training loss: 2.1394e-03 | validation loss: 1.7953e-03\n",
      "Epoch: 146760 | training loss: 2.1394e-03 | validation loss: 1.7953e-03\n",
      "Epoch: 146770 | training loss: 2.1396e-03 | validation loss: 1.7971e-03\n",
      "Epoch: 146780 | training loss: 2.1491e-03 | validation loss: 1.8140e-03\n",
      "Epoch: 146790 | training loss: 2.9505e-03 | validation loss: 2.3088e-03\n",
      "Epoch: 146800 | training loss: 2.7283e-03 | validation loss: 1.9645e-03\n",
      "Epoch: 146810 | training loss: 2.1605e-03 | validation loss: 1.8070e-03\n",
      "Epoch: 146820 | training loss: 2.2252e-03 | validation loss: 1.8644e-03\n",
      "Epoch: 146830 | training loss: 2.1497e-03 | validation loss: 1.8086e-03\n",
      "Epoch: 146840 | training loss: 2.1443e-03 | validation loss: 1.7877e-03\n",
      "Epoch: 146850 | training loss: 2.1416e-03 | validation loss: 1.7910e-03\n",
      "Epoch: 146860 | training loss: 2.1401e-03 | validation loss: 1.8001e-03\n",
      "Epoch: 146870 | training loss: 2.1392e-03 | validation loss: 1.7958e-03\n",
      "Epoch: 146880 | training loss: 2.1393e-03 | validation loss: 1.7926e-03\n",
      "Epoch: 146890 | training loss: 2.1391e-03 | validation loss: 1.7955e-03\n",
      "Epoch: 146900 | training loss: 2.1390e-03 | validation loss: 1.7942e-03\n",
      "Epoch: 146910 | training loss: 2.1390e-03 | validation loss: 1.7947e-03\n",
      "Epoch: 146920 | training loss: 2.1390e-03 | validation loss: 1.7942e-03\n",
      "Epoch: 146930 | training loss: 2.1389e-03 | validation loss: 1.7946e-03\n",
      "Epoch: 146940 | training loss: 2.1389e-03 | validation loss: 1.7942e-03\n",
      "Epoch: 146950 | training loss: 2.1389e-03 | validation loss: 1.7943e-03\n",
      "Epoch: 146960 | training loss: 2.1388e-03 | validation loss: 1.7943e-03\n",
      "Epoch: 146970 | training loss: 2.1388e-03 | validation loss: 1.7943e-03\n",
      "Epoch: 146980 | training loss: 2.1388e-03 | validation loss: 1.7944e-03\n",
      "Epoch: 146990 | training loss: 2.1388e-03 | validation loss: 1.7952e-03\n",
      "Epoch: 147000 | training loss: 2.1430e-03 | validation loss: 1.8045e-03\n",
      "Epoch: 147010 | training loss: 2.6484e-03 | validation loss: 2.1273e-03\n",
      "Epoch: 147020 | training loss: 2.5285e-03 | validation loss: 1.8953e-03\n",
      "Epoch: 147030 | training loss: 2.1733e-03 | validation loss: 1.8294e-03\n",
      "Epoch: 147040 | training loss: 2.2232e-03 | validation loss: 1.8728e-03\n",
      "Epoch: 147050 | training loss: 2.1829e-03 | validation loss: 1.8455e-03\n",
      "Epoch: 147060 | training loss: 2.1453e-03 | validation loss: 1.8085e-03\n",
      "Epoch: 147070 | training loss: 2.1389e-03 | validation loss: 1.7913e-03\n",
      "Epoch: 147080 | training loss: 2.1405e-03 | validation loss: 1.7889e-03\n",
      "Epoch: 147090 | training loss: 2.1386e-03 | validation loss: 1.7921e-03\n",
      "Epoch: 147100 | training loss: 2.1387e-03 | validation loss: 1.7961e-03\n",
      "Epoch: 147110 | training loss: 2.1384e-03 | validation loss: 1.7941e-03\n",
      "Epoch: 147120 | training loss: 2.1384e-03 | validation loss: 1.7929e-03\n",
      "Epoch: 147130 | training loss: 2.1384e-03 | validation loss: 1.7939e-03\n",
      "Epoch: 147140 | training loss: 2.1383e-03 | validation loss: 1.7936e-03\n",
      "Epoch: 147150 | training loss: 2.1383e-03 | validation loss: 1.7935e-03\n",
      "Epoch: 147160 | training loss: 2.1383e-03 | validation loss: 1.7936e-03\n",
      "Epoch: 147170 | training loss: 2.1382e-03 | validation loss: 1.7935e-03\n",
      "Epoch: 147180 | training loss: 2.1382e-03 | validation loss: 1.7936e-03\n",
      "Epoch: 147190 | training loss: 2.1383e-03 | validation loss: 1.7951e-03\n",
      "Epoch: 147200 | training loss: 2.1585e-03 | validation loss: 1.8253e-03\n",
      "Epoch: 147210 | training loss: 2.6473e-03 | validation loss: 2.2333e-03\n",
      "Epoch: 147220 | training loss: 2.2050e-03 | validation loss: 1.8579e-03\n",
      "Epoch: 147230 | training loss: 2.1400e-03 | validation loss: 1.7866e-03\n",
      "Epoch: 147240 | training loss: 2.1471e-03 | validation loss: 1.7954e-03\n",
      "Epoch: 147250 | training loss: 2.1461e-03 | validation loss: 1.7942e-03\n",
      "Epoch: 147260 | training loss: 2.1399e-03 | validation loss: 1.7925e-03\n",
      "Epoch: 147270 | training loss: 2.1385e-03 | validation loss: 1.7969e-03\n",
      "Epoch: 147280 | training loss: 2.1487e-03 | validation loss: 1.8152e-03\n",
      "Epoch: 147290 | training loss: 2.5544e-03 | validation loss: 2.0890e-03\n",
      "Epoch: 147300 | training loss: 2.1631e-03 | validation loss: 1.7777e-03\n",
      "Epoch: 147310 | training loss: 2.2990e-03 | validation loss: 1.9203e-03\n",
      "Epoch: 147320 | training loss: 2.1794e-03 | validation loss: 1.7828e-03\n",
      "Epoch: 147330 | training loss: 2.1381e-03 | validation loss: 1.7909e-03\n",
      "Epoch: 147340 | training loss: 2.1437e-03 | validation loss: 1.8069e-03\n",
      "Epoch: 147350 | training loss: 2.1412e-03 | validation loss: 1.7864e-03\n",
      "Epoch: 147360 | training loss: 2.1390e-03 | validation loss: 1.7977e-03\n",
      "Epoch: 147370 | training loss: 2.1382e-03 | validation loss: 1.7894e-03\n",
      "Epoch: 147380 | training loss: 2.1378e-03 | validation loss: 1.7945e-03\n",
      "Epoch: 147390 | training loss: 2.1376e-03 | validation loss: 1.7915e-03\n",
      "Epoch: 147400 | training loss: 2.1376e-03 | validation loss: 1.7918e-03\n",
      "Epoch: 147410 | training loss: 2.1376e-03 | validation loss: 1.7927e-03\n",
      "Epoch: 147420 | training loss: 2.1375e-03 | validation loss: 1.7928e-03\n",
      "Epoch: 147430 | training loss: 2.1376e-03 | validation loss: 1.7933e-03\n",
      "Epoch: 147440 | training loss: 2.1385e-03 | validation loss: 1.7970e-03\n",
      "Epoch: 147450 | training loss: 2.1803e-03 | validation loss: 1.8406e-03\n",
      "Epoch: 147460 | training loss: 3.5029e-03 | validation loss: 2.5953e-03\n",
      "Epoch: 147470 | training loss: 2.4649e-03 | validation loss: 1.8819e-03\n",
      "Epoch: 147480 | training loss: 2.1806e-03 | validation loss: 1.7770e-03\n",
      "Epoch: 147490 | training loss: 2.1778e-03 | validation loss: 1.8324e-03\n",
      "Epoch: 147500 | training loss: 2.1436e-03 | validation loss: 1.8101e-03\n",
      "Epoch: 147510 | training loss: 2.1428e-03 | validation loss: 1.7839e-03\n",
      "Epoch: 147520 | training loss: 2.1381e-03 | validation loss: 1.7915e-03\n",
      "Epoch: 147530 | training loss: 2.1379e-03 | validation loss: 1.7964e-03\n",
      "Epoch: 147540 | training loss: 2.1375e-03 | validation loss: 1.7888e-03\n",
      "Epoch: 147550 | training loss: 2.1373e-03 | validation loss: 1.7937e-03\n",
      "Epoch: 147560 | training loss: 2.1372e-03 | validation loss: 1.7905e-03\n",
      "Epoch: 147570 | training loss: 2.1371e-03 | validation loss: 1.7924e-03\n",
      "Epoch: 147580 | training loss: 2.1371e-03 | validation loss: 1.7912e-03\n",
      "Epoch: 147590 | training loss: 2.1370e-03 | validation loss: 1.7914e-03\n",
      "Epoch: 147600 | training loss: 2.1370e-03 | validation loss: 1.7917e-03\n",
      "Epoch: 147610 | training loss: 2.1370e-03 | validation loss: 1.7915e-03\n",
      "Epoch: 147620 | training loss: 2.1370e-03 | validation loss: 1.7914e-03\n",
      "Epoch: 147630 | training loss: 2.1373e-03 | validation loss: 1.7914e-03\n",
      "Epoch: 147640 | training loss: 2.1674e-03 | validation loss: 1.8067e-03\n",
      "Epoch: 147650 | training loss: 2.8290e-03 | validation loss: 2.2043e-03\n",
      "Epoch: 147660 | training loss: 2.3444e-03 | validation loss: 1.8966e-03\n",
      "Epoch: 147670 | training loss: 2.2351e-03 | validation loss: 1.8465e-03\n",
      "Epoch: 147680 | training loss: 2.2002e-03 | validation loss: 1.7848e-03\n",
      "Epoch: 147690 | training loss: 2.1627e-03 | validation loss: 1.8106e-03\n",
      "Epoch: 147700 | training loss: 2.1456e-03 | validation loss: 1.7799e-03\n",
      "Epoch: 147710 | training loss: 2.1400e-03 | validation loss: 1.7949e-03\n",
      "Epoch: 147720 | training loss: 2.1380e-03 | validation loss: 1.7852e-03\n",
      "Epoch: 147730 | training loss: 2.1370e-03 | validation loss: 1.7916e-03\n",
      "Epoch: 147740 | training loss: 2.1367e-03 | validation loss: 1.7905e-03\n",
      "Epoch: 147750 | training loss: 2.1367e-03 | validation loss: 1.7894e-03\n",
      "Epoch: 147760 | training loss: 2.1366e-03 | validation loss: 1.7896e-03\n",
      "Epoch: 147770 | training loss: 2.1367e-03 | validation loss: 1.7888e-03\n",
      "Epoch: 147780 | training loss: 2.1390e-03 | validation loss: 1.7843e-03\n",
      "Epoch: 147790 | training loss: 2.2339e-03 | validation loss: 1.7902e-03\n",
      "Epoch: 147800 | training loss: 3.0280e-03 | validation loss: 2.0784e-03\n",
      "Epoch: 147810 | training loss: 2.1791e-03 | validation loss: 1.8329e-03\n",
      "Epoch: 147820 | training loss: 2.2043e-03 | validation loss: 1.8543e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 147830 | training loss: 2.1664e-03 | validation loss: 1.7794e-03\n",
      "Epoch: 147840 | training loss: 2.1364e-03 | validation loss: 1.7895e-03\n",
      "Epoch: 147850 | training loss: 2.1397e-03 | validation loss: 1.8003e-03\n",
      "Epoch: 147860 | training loss: 2.1384e-03 | validation loss: 1.7856e-03\n",
      "Epoch: 147870 | training loss: 2.1370e-03 | validation loss: 1.7942e-03\n",
      "Epoch: 147880 | training loss: 2.1365e-03 | validation loss: 1.7880e-03\n",
      "Epoch: 147890 | training loss: 2.1363e-03 | validation loss: 1.7917e-03\n",
      "Epoch: 147900 | training loss: 2.1362e-03 | validation loss: 1.7894e-03\n",
      "Epoch: 147910 | training loss: 2.1361e-03 | validation loss: 1.7900e-03\n",
      "Epoch: 147920 | training loss: 2.1361e-03 | validation loss: 1.7905e-03\n",
      "Epoch: 147930 | training loss: 2.1361e-03 | validation loss: 1.7903e-03\n",
      "Epoch: 147940 | training loss: 2.1361e-03 | validation loss: 1.7903e-03\n",
      "Epoch: 147950 | training loss: 2.1361e-03 | validation loss: 1.7913e-03\n",
      "Epoch: 147960 | training loss: 2.1394e-03 | validation loss: 1.7992e-03\n",
      "Epoch: 147970 | training loss: 2.3903e-03 | validation loss: 1.9775e-03\n",
      "Epoch: 147980 | training loss: 2.1817e-03 | validation loss: 1.8319e-03\n",
      "Epoch: 147990 | training loss: 2.4654e-03 | validation loss: 2.0218e-03\n",
      "Epoch: 148000 | training loss: 2.1488e-03 | validation loss: 1.8174e-03\n",
      "Epoch: 148010 | training loss: 2.1617e-03 | validation loss: 1.7832e-03\n",
      "Epoch: 148020 | training loss: 2.1460e-03 | validation loss: 1.7773e-03\n",
      "Epoch: 148030 | training loss: 2.1373e-03 | validation loss: 1.7941e-03\n",
      "Epoch: 148040 | training loss: 2.1373e-03 | validation loss: 1.7975e-03\n",
      "Epoch: 148050 | training loss: 2.1362e-03 | validation loss: 1.7863e-03\n",
      "Epoch: 148060 | training loss: 2.1357e-03 | validation loss: 1.7892e-03\n",
      "Epoch: 148070 | training loss: 2.1358e-03 | validation loss: 1.7910e-03\n",
      "Epoch: 148080 | training loss: 2.1357e-03 | validation loss: 1.7884e-03\n",
      "Epoch: 148090 | training loss: 2.1356e-03 | validation loss: 1.7900e-03\n",
      "Epoch: 148100 | training loss: 2.1356e-03 | validation loss: 1.7890e-03\n",
      "Epoch: 148110 | training loss: 2.1356e-03 | validation loss: 1.7895e-03\n",
      "Epoch: 148120 | training loss: 2.1355e-03 | validation loss: 1.7892e-03\n",
      "Epoch: 148130 | training loss: 2.1355e-03 | validation loss: 1.7892e-03\n",
      "Epoch: 148140 | training loss: 2.1355e-03 | validation loss: 1.7893e-03\n",
      "Epoch: 148150 | training loss: 2.1355e-03 | validation loss: 1.7894e-03\n",
      "Epoch: 148160 | training loss: 2.1356e-03 | validation loss: 1.7904e-03\n",
      "Epoch: 148170 | training loss: 2.1483e-03 | validation loss: 1.8083e-03\n",
      "Epoch: 148180 | training loss: 2.7589e-03 | validation loss: 2.2695e-03\n",
      "Epoch: 148190 | training loss: 2.2289e-03 | validation loss: 1.7868e-03\n",
      "Epoch: 148200 | training loss: 2.4623e-03 | validation loss: 1.8568e-03\n",
      "Epoch: 148210 | training loss: 2.2228e-03 | validation loss: 1.8553e-03\n",
      "Epoch: 148220 | training loss: 2.1476e-03 | validation loss: 1.7809e-03\n",
      "Epoch: 148230 | training loss: 2.1471e-03 | validation loss: 1.7758e-03\n",
      "Epoch: 148240 | training loss: 2.1413e-03 | validation loss: 1.7999e-03\n",
      "Epoch: 148250 | training loss: 2.1384e-03 | validation loss: 1.7984e-03\n",
      "Epoch: 148260 | training loss: 2.1363e-03 | validation loss: 1.7949e-03\n",
      "Epoch: 148270 | training loss: 2.1389e-03 | validation loss: 1.8001e-03\n",
      "Epoch: 148280 | training loss: 2.2037e-03 | validation loss: 1.8598e-03\n",
      "Epoch: 148290 | training loss: 2.9510e-03 | validation loss: 2.3052e-03\n",
      "Epoch: 148300 | training loss: 2.4012e-03 | validation loss: 1.8443e-03\n",
      "Epoch: 148310 | training loss: 2.1929e-03 | validation loss: 1.8489e-03\n",
      "Epoch: 148320 | training loss: 2.1393e-03 | validation loss: 1.7804e-03\n",
      "Epoch: 148330 | training loss: 2.1350e-03 | validation loss: 1.7879e-03\n",
      "Epoch: 148340 | training loss: 2.1349e-03 | validation loss: 1.7887e-03\n",
      "Epoch: 148350 | training loss: 2.1350e-03 | validation loss: 1.7895e-03\n",
      "Epoch: 148360 | training loss: 2.1353e-03 | validation loss: 1.7856e-03\n",
      "Epoch: 148370 | training loss: 2.1352e-03 | validation loss: 1.7907e-03\n",
      "Epoch: 148380 | training loss: 2.1348e-03 | validation loss: 1.7884e-03\n",
      "Epoch: 148390 | training loss: 2.1348e-03 | validation loss: 1.7869e-03\n",
      "Epoch: 148400 | training loss: 2.1350e-03 | validation loss: 1.7859e-03\n",
      "Epoch: 148410 | training loss: 2.1372e-03 | validation loss: 1.7823e-03\n",
      "Epoch: 148420 | training loss: 2.2086e-03 | validation loss: 1.7857e-03\n",
      "Epoch: 148430 | training loss: 3.1967e-03 | validation loss: 2.1554e-03\n",
      "Epoch: 148440 | training loss: 2.3480e-03 | validation loss: 1.9435e-03\n",
      "Epoch: 148450 | training loss: 2.1401e-03 | validation loss: 1.8038e-03\n",
      "Epoch: 148460 | training loss: 2.1723e-03 | validation loss: 1.7822e-03\n",
      "Epoch: 148470 | training loss: 2.1459e-03 | validation loss: 1.8029e-03\n",
      "Epoch: 148480 | training loss: 2.1351e-03 | validation loss: 1.7868e-03\n",
      "Epoch: 148490 | training loss: 2.1345e-03 | validation loss: 1.7869e-03\n",
      "Epoch: 148500 | training loss: 2.1346e-03 | validation loss: 1.7883e-03\n",
      "Epoch: 148510 | training loss: 2.1345e-03 | validation loss: 1.7877e-03\n",
      "Epoch: 148520 | training loss: 2.1344e-03 | validation loss: 1.7869e-03\n",
      "Epoch: 148530 | training loss: 2.1344e-03 | validation loss: 1.7884e-03\n",
      "Epoch: 148540 | training loss: 2.1344e-03 | validation loss: 1.7872e-03\n",
      "Epoch: 148550 | training loss: 2.1343e-03 | validation loss: 1.7868e-03\n",
      "Epoch: 148560 | training loss: 2.1343e-03 | validation loss: 1.7868e-03\n",
      "Epoch: 148570 | training loss: 2.1345e-03 | validation loss: 1.7859e-03\n",
      "Epoch: 148580 | training loss: 2.1389e-03 | validation loss: 1.7816e-03\n",
      "Epoch: 148590 | training loss: 2.3951e-03 | validation loss: 1.8529e-03\n",
      "Epoch: 148600 | training loss: 2.2105e-03 | validation loss: 1.7805e-03\n",
      "Epoch: 148610 | training loss: 2.3633e-03 | validation loss: 1.8589e-03\n",
      "Epoch: 148620 | training loss: 2.1493e-03 | validation loss: 1.8104e-03\n",
      "Epoch: 148630 | training loss: 2.1692e-03 | validation loss: 1.8142e-03\n",
      "Epoch: 148640 | training loss: 2.1355e-03 | validation loss: 1.7873e-03\n",
      "Epoch: 148650 | training loss: 2.1377e-03 | validation loss: 1.7841e-03\n",
      "Epoch: 148660 | training loss: 2.1353e-03 | validation loss: 1.7907e-03\n",
      "Epoch: 148670 | training loss: 2.1340e-03 | validation loss: 1.7863e-03\n",
      "Epoch: 148680 | training loss: 2.1340e-03 | validation loss: 1.7868e-03\n",
      "Epoch: 148690 | training loss: 2.1340e-03 | validation loss: 1.7875e-03\n",
      "Epoch: 148700 | training loss: 2.1339e-03 | validation loss: 1.7864e-03\n",
      "Epoch: 148710 | training loss: 2.1339e-03 | validation loss: 1.7867e-03\n",
      "Epoch: 148720 | training loss: 2.1339e-03 | validation loss: 1.7862e-03\n",
      "Epoch: 148730 | training loss: 2.1341e-03 | validation loss: 1.7839e-03\n",
      "Epoch: 148740 | training loss: 2.1494e-03 | validation loss: 1.7755e-03\n",
      "Epoch: 148750 | training loss: 2.8290e-03 | validation loss: 2.0773e-03\n",
      "Epoch: 148760 | training loss: 2.2074e-03 | validation loss: 1.8784e-03\n",
      "Epoch: 148770 | training loss: 2.2021e-03 | validation loss: 1.8513e-03\n",
      "Epoch: 148780 | training loss: 2.1391e-03 | validation loss: 1.7984e-03\n",
      "Epoch: 148790 | training loss: 2.1371e-03 | validation loss: 1.7878e-03\n",
      "Epoch: 148800 | training loss: 2.1388e-03 | validation loss: 1.7769e-03\n",
      "Epoch: 148810 | training loss: 2.1337e-03 | validation loss: 1.7875e-03\n",
      "Epoch: 148820 | training loss: 2.1346e-03 | validation loss: 1.7925e-03\n",
      "Epoch: 148830 | training loss: 2.1345e-03 | validation loss: 1.7907e-03\n",
      "Epoch: 148840 | training loss: 2.1398e-03 | validation loss: 1.7997e-03\n",
      "Epoch: 148850 | training loss: 2.2663e-03 | validation loss: 1.9012e-03\n",
      "Epoch: 148860 | training loss: 2.7008e-03 | validation loss: 2.1559e-03\n",
      "Epoch: 148870 | training loss: 2.2516e-03 | validation loss: 1.8016e-03\n",
      "Epoch: 148880 | training loss: 2.1377e-03 | validation loss: 1.7954e-03\n",
      "Epoch: 148890 | training loss: 2.1353e-03 | validation loss: 1.7904e-03\n",
      "Epoch: 148900 | training loss: 2.1361e-03 | validation loss: 1.7807e-03\n",
      "Epoch: 148910 | training loss: 2.1343e-03 | validation loss: 1.7909e-03\n",
      "Epoch: 148920 | training loss: 2.1333e-03 | validation loss: 1.7844e-03\n",
      "Epoch: 148930 | training loss: 2.1334e-03 | validation loss: 1.7842e-03\n",
      "Epoch: 148940 | training loss: 2.1335e-03 | validation loss: 1.7881e-03\n",
      "Epoch: 148950 | training loss: 2.1332e-03 | validation loss: 1.7860e-03\n",
      "Epoch: 148960 | training loss: 2.1332e-03 | validation loss: 1.7847e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 148970 | training loss: 2.1334e-03 | validation loss: 1.7836e-03\n",
      "Epoch: 148980 | training loss: 2.1367e-03 | validation loss: 1.7794e-03\n",
      "Epoch: 148990 | training loss: 2.2670e-03 | validation loss: 1.8016e-03\n",
      "Epoch: 149000 | training loss: 2.8208e-03 | validation loss: 2.0094e-03\n",
      "Epoch: 149010 | training loss: 2.1375e-03 | validation loss: 1.7895e-03\n",
      "Epoch: 149020 | training loss: 2.2231e-03 | validation loss: 1.8690e-03\n",
      "Epoch: 149030 | training loss: 2.1514e-03 | validation loss: 1.7775e-03\n",
      "Epoch: 149040 | training loss: 2.1347e-03 | validation loss: 1.7787e-03\n",
      "Epoch: 149050 | training loss: 2.1375e-03 | validation loss: 1.7983e-03\n",
      "Epoch: 149060 | training loss: 2.1348e-03 | validation loss: 1.7792e-03\n",
      "Epoch: 149070 | training loss: 2.1334e-03 | validation loss: 1.7893e-03\n",
      "Epoch: 149080 | training loss: 2.1330e-03 | validation loss: 1.7830e-03\n",
      "Epoch: 149090 | training loss: 2.1329e-03 | validation loss: 1.7867e-03\n",
      "Epoch: 149100 | training loss: 2.1328e-03 | validation loss: 1.7842e-03\n",
      "Epoch: 149110 | training loss: 2.1327e-03 | validation loss: 1.7852e-03\n",
      "Epoch: 149120 | training loss: 2.1327e-03 | validation loss: 1.7855e-03\n",
      "Epoch: 149130 | training loss: 2.1327e-03 | validation loss: 1.7854e-03\n",
      "Epoch: 149140 | training loss: 2.1328e-03 | validation loss: 1.7867e-03\n",
      "Epoch: 149150 | training loss: 2.1466e-03 | validation loss: 1.8087e-03\n",
      "Epoch: 149160 | training loss: 2.7159e-03 | validation loss: 2.2747e-03\n",
      "Epoch: 149170 | training loss: 2.2091e-03 | validation loss: 1.8576e-03\n",
      "Epoch: 149180 | training loss: 2.3582e-03 | validation loss: 1.9376e-03\n",
      "Epoch: 149190 | training loss: 2.1578e-03 | validation loss: 1.7859e-03\n",
      "Epoch: 149200 | training loss: 2.1687e-03 | validation loss: 1.7696e-03\n",
      "Epoch: 149210 | training loss: 2.1362e-03 | validation loss: 1.7768e-03\n",
      "Epoch: 149220 | training loss: 2.1334e-03 | validation loss: 1.7894e-03\n",
      "Epoch: 149230 | training loss: 2.1366e-03 | validation loss: 1.7973e-03\n",
      "Epoch: 149240 | training loss: 2.2065e-03 | validation loss: 1.8592e-03\n",
      "Epoch: 149250 | training loss: 2.9202e-03 | validation loss: 2.2835e-03\n",
      "Epoch: 149260 | training loss: 2.3863e-03 | validation loss: 1.8359e-03\n",
      "Epoch: 149270 | training loss: 2.1895e-03 | validation loss: 1.8441e-03\n",
      "Epoch: 149280 | training loss: 2.1381e-03 | validation loss: 1.7757e-03\n",
      "Epoch: 149290 | training loss: 2.1325e-03 | validation loss: 1.7860e-03\n",
      "Epoch: 149300 | training loss: 2.1323e-03 | validation loss: 1.7827e-03\n",
      "Epoch: 149310 | training loss: 2.1326e-03 | validation loss: 1.7869e-03\n",
      "Epoch: 149320 | training loss: 2.1328e-03 | validation loss: 1.7808e-03\n",
      "Epoch: 149330 | training loss: 2.1323e-03 | validation loss: 1.7860e-03\n",
      "Epoch: 149340 | training loss: 2.1322e-03 | validation loss: 1.7852e-03\n",
      "Epoch: 149350 | training loss: 2.1321e-03 | validation loss: 1.7838e-03\n",
      "Epoch: 149360 | training loss: 2.1321e-03 | validation loss: 1.7830e-03\n",
      "Epoch: 149370 | training loss: 2.1325e-03 | validation loss: 1.7809e-03\n",
      "Epoch: 149380 | training loss: 2.1508e-03 | validation loss: 1.7739e-03\n",
      "Epoch: 149390 | training loss: 3.1340e-03 | validation loss: 2.1228e-03\n",
      "Epoch: 149400 | training loss: 2.6843e-03 | validation loss: 2.1442e-03\n",
      "Epoch: 149410 | training loss: 2.1524e-03 | validation loss: 1.7907e-03\n",
      "Epoch: 149420 | training loss: 2.1978e-03 | validation loss: 1.7814e-03\n",
      "Epoch: 149430 | training loss: 2.1353e-03 | validation loss: 1.7824e-03\n",
      "Epoch: 149440 | training loss: 2.1391e-03 | validation loss: 1.7984e-03\n",
      "Epoch: 149450 | training loss: 2.1337e-03 | validation loss: 1.7830e-03\n",
      "Epoch: 149460 | training loss: 2.1320e-03 | validation loss: 1.7809e-03\n",
      "Epoch: 149470 | training loss: 2.1320e-03 | validation loss: 1.7858e-03\n",
      "Epoch: 149480 | training loss: 2.1318e-03 | validation loss: 1.7825e-03\n",
      "Epoch: 149490 | training loss: 2.1317e-03 | validation loss: 1.7838e-03\n",
      "Epoch: 149500 | training loss: 2.1317e-03 | validation loss: 1.7831e-03\n",
      "Epoch: 149510 | training loss: 2.1316e-03 | validation loss: 1.7834e-03\n",
      "Epoch: 149520 | training loss: 2.1316e-03 | validation loss: 1.7831e-03\n",
      "Epoch: 149530 | training loss: 2.1316e-03 | validation loss: 1.7832e-03\n",
      "Epoch: 149540 | training loss: 2.1315e-03 | validation loss: 1.7832e-03\n",
      "Epoch: 149550 | training loss: 2.1315e-03 | validation loss: 1.7831e-03\n",
      "Epoch: 149560 | training loss: 2.1315e-03 | validation loss: 1.7829e-03\n",
      "Epoch: 149570 | training loss: 2.1318e-03 | validation loss: 1.7822e-03\n",
      "Epoch: 149580 | training loss: 2.1575e-03 | validation loss: 1.7886e-03\n",
      "Epoch: 149590 | training loss: 3.4683e-03 | validation loss: 2.3465e-03\n",
      "Epoch: 149600 | training loss: 2.5054e-03 | validation loss: 2.0957e-03\n",
      "Epoch: 149610 | training loss: 2.1538e-03 | validation loss: 1.7902e-03\n",
      "Epoch: 149620 | training loss: 2.1704e-03 | validation loss: 1.7867e-03\n",
      "Epoch: 149630 | training loss: 2.1395e-03 | validation loss: 1.8028e-03\n",
      "Epoch: 149640 | training loss: 2.1364e-03 | validation loss: 1.7979e-03\n",
      "Epoch: 149650 | training loss: 2.1335e-03 | validation loss: 1.7809e-03\n",
      "Epoch: 149660 | training loss: 2.1315e-03 | validation loss: 1.7854e-03\n",
      "Epoch: 149670 | training loss: 2.1312e-03 | validation loss: 1.7831e-03\n",
      "Epoch: 149680 | training loss: 2.1312e-03 | validation loss: 1.7816e-03\n",
      "Epoch: 149690 | training loss: 2.1312e-03 | validation loss: 1.7823e-03\n",
      "Epoch: 149700 | training loss: 2.1311e-03 | validation loss: 1.7820e-03\n",
      "Epoch: 149710 | training loss: 2.1311e-03 | validation loss: 1.7820e-03\n",
      "Epoch: 149720 | training loss: 2.1310e-03 | validation loss: 1.7826e-03\n",
      "Epoch: 149730 | training loss: 2.1310e-03 | validation loss: 1.7823e-03\n",
      "Epoch: 149740 | training loss: 2.1310e-03 | validation loss: 1.7821e-03\n",
      "Epoch: 149750 | training loss: 2.1310e-03 | validation loss: 1.7820e-03\n",
      "Epoch: 149760 | training loss: 2.1309e-03 | validation loss: 1.7816e-03\n",
      "Epoch: 149770 | training loss: 2.1315e-03 | validation loss: 1.7787e-03\n",
      "Epoch: 149780 | training loss: 2.1873e-03 | validation loss: 1.7735e-03\n",
      "Epoch: 149790 | training loss: 3.8183e-03 | validation loss: 2.3997e-03\n",
      "Epoch: 149800 | training loss: 2.2021e-03 | validation loss: 1.7750e-03\n",
      "Epoch: 149810 | training loss: 2.2184e-03 | validation loss: 1.8396e-03\n",
      "Epoch: 149820 | training loss: 2.1826e-03 | validation loss: 1.8262e-03\n",
      "Epoch: 149830 | training loss: 2.1359e-03 | validation loss: 1.7948e-03\n",
      "Epoch: 149840 | training loss: 2.1342e-03 | validation loss: 1.7853e-03\n",
      "Epoch: 149850 | training loss: 2.1335e-03 | validation loss: 1.7829e-03\n",
      "Epoch: 149860 | training loss: 2.1307e-03 | validation loss: 1.7835e-03\n",
      "Epoch: 149870 | training loss: 2.1310e-03 | validation loss: 1.7826e-03\n",
      "Epoch: 149880 | training loss: 2.1307e-03 | validation loss: 1.7803e-03\n",
      "Epoch: 149890 | training loss: 2.1306e-03 | validation loss: 1.7818e-03\n",
      "Epoch: 149900 | training loss: 2.1306e-03 | validation loss: 1.7822e-03\n",
      "Epoch: 149910 | training loss: 2.1305e-03 | validation loss: 1.7811e-03\n",
      "Epoch: 149920 | training loss: 2.1305e-03 | validation loss: 1.7817e-03\n",
      "Epoch: 149930 | training loss: 2.1305e-03 | validation loss: 1.7814e-03\n",
      "Epoch: 149940 | training loss: 2.1304e-03 | validation loss: 1.7815e-03\n",
      "Epoch: 149950 | training loss: 2.1304e-03 | validation loss: 1.7814e-03\n",
      "Epoch: 149960 | training loss: 2.1304e-03 | validation loss: 1.7813e-03\n",
      "Epoch: 149970 | training loss: 2.1303e-03 | validation loss: 1.7813e-03\n",
      "Epoch: 149980 | training loss: 2.1303e-03 | validation loss: 1.7813e-03\n",
      "Epoch: 149990 | training loss: 2.1303e-03 | validation loss: 1.7813e-03\n",
      "Epoch: 150000 | training loss: 2.1303e-03 | validation loss: 1.7813e-03\n",
      "Epoch: 150010 | training loss: 2.1304e-03 | validation loss: 1.7824e-03\n",
      "Epoch: 150020 | training loss: 2.1467e-03 | validation loss: 1.8025e-03\n",
      "Epoch: 150030 | training loss: 4.1795e-03 | validation loss: 2.9264e-03\n",
      "Epoch: 150040 | training loss: 2.3563e-03 | validation loss: 1.8141e-03\n",
      "Epoch: 150050 | training loss: 2.2260e-03 | validation loss: 1.7781e-03\n",
      "Epoch: 150060 | training loss: 2.1370e-03 | validation loss: 1.7690e-03\n",
      "Epoch: 150070 | training loss: 2.1328e-03 | validation loss: 1.7853e-03\n",
      "Epoch: 150080 | training loss: 2.1352e-03 | validation loss: 1.7900e-03\n",
      "Epoch: 150090 | training loss: 2.1318e-03 | validation loss: 1.7826e-03\n",
      "Epoch: 150100 | training loss: 2.1308e-03 | validation loss: 1.7779e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150110 | training loss: 2.1302e-03 | validation loss: 1.7795e-03\n",
      "Epoch: 150120 | training loss: 2.1300e-03 | validation loss: 1.7817e-03\n",
      "Epoch: 150130 | training loss: 2.1299e-03 | validation loss: 1.7806e-03\n",
      "Epoch: 150140 | training loss: 2.1299e-03 | validation loss: 1.7806e-03\n",
      "Epoch: 150150 | training loss: 2.1299e-03 | validation loss: 1.7810e-03\n",
      "Epoch: 150160 | training loss: 2.1298e-03 | validation loss: 1.7805e-03\n",
      "Epoch: 150170 | training loss: 2.1298e-03 | validation loss: 1.7806e-03\n",
      "Epoch: 150180 | training loss: 2.1298e-03 | validation loss: 1.7804e-03\n",
      "Epoch: 150190 | training loss: 2.1297e-03 | validation loss: 1.7805e-03\n",
      "Epoch: 150200 | training loss: 2.1297e-03 | validation loss: 1.7805e-03\n",
      "Epoch: 150210 | training loss: 2.1297e-03 | validation loss: 1.7804e-03\n",
      "Epoch: 150220 | training loss: 2.1297e-03 | validation loss: 1.7803e-03\n",
      "Epoch: 150230 | training loss: 2.1296e-03 | validation loss: 1.7803e-03\n",
      "Epoch: 150240 | training loss: 2.1296e-03 | validation loss: 1.7801e-03\n",
      "Epoch: 150250 | training loss: 2.1296e-03 | validation loss: 1.7792e-03\n",
      "Epoch: 150260 | training loss: 2.1333e-03 | validation loss: 1.7713e-03\n",
      "Epoch: 150270 | training loss: 2.7460e-03 | validation loss: 1.9653e-03\n",
      "Epoch: 150280 | training loss: 2.7614e-03 | validation loss: 2.2760e-03\n",
      "Epoch: 150290 | training loss: 2.3581e-03 | validation loss: 1.9853e-03\n",
      "Epoch: 150300 | training loss: 2.2158e-03 | validation loss: 1.8676e-03\n",
      "Epoch: 150310 | training loss: 2.1592e-03 | validation loss: 1.8268e-03\n",
      "Epoch: 150320 | training loss: 2.1407e-03 | validation loss: 1.8045e-03\n",
      "Epoch: 150330 | training loss: 2.1334e-03 | validation loss: 1.7882e-03\n",
      "Epoch: 150340 | training loss: 2.1306e-03 | validation loss: 1.7850e-03\n",
      "Epoch: 150350 | training loss: 2.1298e-03 | validation loss: 1.7837e-03\n",
      "Epoch: 150360 | training loss: 2.1294e-03 | validation loss: 1.7801e-03\n",
      "Epoch: 150370 | training loss: 2.1293e-03 | validation loss: 1.7800e-03\n",
      "Epoch: 150380 | training loss: 2.1292e-03 | validation loss: 1.7787e-03\n",
      "Epoch: 150390 | training loss: 2.1292e-03 | validation loss: 1.7791e-03\n",
      "Epoch: 150400 | training loss: 2.1292e-03 | validation loss: 1.7791e-03\n",
      "Epoch: 150410 | training loss: 2.1291e-03 | validation loss: 1.7794e-03\n",
      "Epoch: 150420 | training loss: 2.1291e-03 | validation loss: 1.7793e-03\n",
      "Epoch: 150430 | training loss: 2.1291e-03 | validation loss: 1.7792e-03\n",
      "Epoch: 150440 | training loss: 2.1290e-03 | validation loss: 1.7792e-03\n",
      "Epoch: 150450 | training loss: 2.1290e-03 | validation loss: 1.7793e-03\n",
      "Epoch: 150460 | training loss: 2.1291e-03 | validation loss: 1.7804e-03\n",
      "Epoch: 150470 | training loss: 2.1407e-03 | validation loss: 1.7964e-03\n",
      "Epoch: 150480 | training loss: 3.4817e-03 | validation loss: 2.5618e-03\n",
      "Epoch: 150490 | training loss: 2.8641e-03 | validation loss: 2.0125e-03\n",
      "Epoch: 150500 | training loss: 2.3310e-03 | validation loss: 1.8166e-03\n",
      "Epoch: 150510 | training loss: 2.1460e-03 | validation loss: 1.7723e-03\n",
      "Epoch: 150520 | training loss: 2.1312e-03 | validation loss: 1.7883e-03\n",
      "Epoch: 150530 | training loss: 2.1381e-03 | validation loss: 1.7983e-03\n",
      "Epoch: 150540 | training loss: 2.1327e-03 | validation loss: 1.7906e-03\n",
      "Epoch: 150550 | training loss: 2.1288e-03 | validation loss: 1.7802e-03\n",
      "Epoch: 150560 | training loss: 2.1293e-03 | validation loss: 1.7764e-03\n",
      "Epoch: 150570 | training loss: 2.1287e-03 | validation loss: 1.7779e-03\n",
      "Epoch: 150580 | training loss: 2.1288e-03 | validation loss: 1.7796e-03\n",
      "Epoch: 150590 | training loss: 2.1287e-03 | validation loss: 1.7786e-03\n",
      "Epoch: 150600 | training loss: 2.1286e-03 | validation loss: 1.7785e-03\n",
      "Epoch: 150610 | training loss: 2.1286e-03 | validation loss: 1.7788e-03\n",
      "Epoch: 150620 | training loss: 2.1286e-03 | validation loss: 1.7785e-03\n",
      "Epoch: 150630 | training loss: 2.1285e-03 | validation loss: 1.7787e-03\n",
      "Epoch: 150640 | training loss: 2.1285e-03 | validation loss: 1.7785e-03\n",
      "Epoch: 150650 | training loss: 2.1285e-03 | validation loss: 1.7785e-03\n",
      "Epoch: 150660 | training loss: 2.1285e-03 | validation loss: 1.7784e-03\n",
      "Epoch: 150670 | training loss: 2.1284e-03 | validation loss: 1.7784e-03\n",
      "Epoch: 150680 | training loss: 2.1284e-03 | validation loss: 1.7784e-03\n",
      "Epoch: 150690 | training loss: 2.1284e-03 | validation loss: 1.7783e-03\n",
      "Epoch: 150700 | training loss: 2.1283e-03 | validation loss: 1.7783e-03\n",
      "Epoch: 150710 | training loss: 2.1283e-03 | validation loss: 1.7782e-03\n",
      "Epoch: 150720 | training loss: 2.1283e-03 | validation loss: 1.7781e-03\n",
      "Epoch: 150730 | training loss: 2.1284e-03 | validation loss: 1.7767e-03\n",
      "Epoch: 150740 | training loss: 2.1718e-03 | validation loss: 1.7739e-03\n",
      "Epoch: 150750 | training loss: 4.6540e-03 | validation loss: 2.7817e-03\n",
      "Epoch: 150760 | training loss: 2.9838e-03 | validation loss: 2.0868e-03\n",
      "Epoch: 150770 | training loss: 2.3710e-03 | validation loss: 1.8450e-03\n",
      "Epoch: 150780 | training loss: 2.1549e-03 | validation loss: 1.7757e-03\n",
      "Epoch: 150790 | training loss: 2.1282e-03 | validation loss: 1.7801e-03\n",
      "Epoch: 150800 | training loss: 2.1313e-03 | validation loss: 1.7853e-03\n",
      "Epoch: 150810 | training loss: 2.1307e-03 | validation loss: 1.7837e-03\n",
      "Epoch: 150820 | training loss: 2.1293e-03 | validation loss: 1.7815e-03\n",
      "Epoch: 150830 | training loss: 2.1285e-03 | validation loss: 1.7802e-03\n",
      "Epoch: 150840 | training loss: 2.1281e-03 | validation loss: 1.7792e-03\n",
      "Epoch: 150850 | training loss: 2.1280e-03 | validation loss: 1.7783e-03\n",
      "Epoch: 150860 | training loss: 2.1279e-03 | validation loss: 1.7779e-03\n",
      "Epoch: 150870 | training loss: 2.1279e-03 | validation loss: 1.7777e-03\n",
      "Epoch: 150880 | training loss: 2.1279e-03 | validation loss: 1.7775e-03\n",
      "Epoch: 150890 | training loss: 2.1278e-03 | validation loss: 1.7776e-03\n",
      "Epoch: 150900 | training loss: 2.1278e-03 | validation loss: 1.7776e-03\n",
      "Epoch: 150910 | training loss: 2.1278e-03 | validation loss: 1.7776e-03\n",
      "Epoch: 150920 | training loss: 2.1277e-03 | validation loss: 1.7776e-03\n",
      "Epoch: 150930 | training loss: 2.1277e-03 | validation loss: 1.7775e-03\n",
      "Epoch: 150940 | training loss: 2.1277e-03 | validation loss: 1.7775e-03\n",
      "Epoch: 150950 | training loss: 2.1277e-03 | validation loss: 1.7774e-03\n",
      "Epoch: 150960 | training loss: 2.1276e-03 | validation loss: 1.7774e-03\n",
      "Epoch: 150970 | training loss: 2.1276e-03 | validation loss: 1.7773e-03\n",
      "Epoch: 150980 | training loss: 2.1276e-03 | validation loss: 1.7772e-03\n",
      "Epoch: 150990 | training loss: 2.1276e-03 | validation loss: 1.7764e-03\n",
      "Epoch: 151000 | training loss: 2.1363e-03 | validation loss: 1.7680e-03\n",
      "Epoch: 151010 | training loss: 3.2959e-03 | validation loss: 2.3576e-03\n",
      "Epoch: 151020 | training loss: 2.4384e-03 | validation loss: 1.9662e-03\n",
      "Epoch: 151030 | training loss: 2.2287e-03 | validation loss: 1.8006e-03\n",
      "Epoch: 151040 | training loss: 2.1600e-03 | validation loss: 1.7793e-03\n",
      "Epoch: 151050 | training loss: 2.1378e-03 | validation loss: 1.7821e-03\n",
      "Epoch: 151060 | training loss: 2.1305e-03 | validation loss: 1.7689e-03\n",
      "Epoch: 151070 | training loss: 2.1283e-03 | validation loss: 1.7793e-03\n",
      "Epoch: 151080 | training loss: 2.1276e-03 | validation loss: 1.7737e-03\n",
      "Epoch: 151090 | training loss: 2.1273e-03 | validation loss: 1.7763e-03\n",
      "Epoch: 151100 | training loss: 2.1273e-03 | validation loss: 1.7768e-03\n",
      "Epoch: 151110 | training loss: 2.1273e-03 | validation loss: 1.7754e-03\n",
      "Epoch: 151120 | training loss: 2.1272e-03 | validation loss: 1.7753e-03\n",
      "Epoch: 151130 | training loss: 2.1272e-03 | validation loss: 1.7751e-03\n",
      "Epoch: 151140 | training loss: 2.1278e-03 | validation loss: 1.7728e-03\n",
      "Epoch: 151150 | training loss: 2.1485e-03 | validation loss: 1.7643e-03\n",
      "Epoch: 151160 | training loss: 3.0812e-03 | validation loss: 2.0814e-03\n",
      "Epoch: 151170 | training loss: 2.5975e-03 | validation loss: 2.0928e-03\n",
      "Epoch: 151180 | training loss: 2.1624e-03 | validation loss: 1.7695e-03\n",
      "Epoch: 151190 | training loss: 2.1689e-03 | validation loss: 1.7699e-03\n",
      "Epoch: 151200 | training loss: 2.1404e-03 | validation loss: 1.7994e-03\n",
      "Epoch: 151210 | training loss: 2.1276e-03 | validation loss: 1.7804e-03\n",
      "Epoch: 151220 | training loss: 2.1295e-03 | validation loss: 1.7705e-03\n",
      "Epoch: 151230 | training loss: 2.1280e-03 | validation loss: 1.7806e-03\n",
      "Epoch: 151240 | training loss: 2.1271e-03 | validation loss: 1.7736e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 151250 | training loss: 2.1269e-03 | validation loss: 1.7772e-03\n",
      "Epoch: 151260 | training loss: 2.1269e-03 | validation loss: 1.7750e-03\n",
      "Epoch: 151270 | training loss: 2.1268e-03 | validation loss: 1.7764e-03\n",
      "Epoch: 151280 | training loss: 2.1268e-03 | validation loss: 1.7755e-03\n",
      "Epoch: 151290 | training loss: 2.1267e-03 | validation loss: 1.7754e-03\n",
      "Epoch: 151300 | training loss: 2.1267e-03 | validation loss: 1.7756e-03\n",
      "Epoch: 151310 | training loss: 2.1267e-03 | validation loss: 1.7757e-03\n",
      "Epoch: 151320 | training loss: 2.1266e-03 | validation loss: 1.7760e-03\n",
      "Epoch: 151330 | training loss: 2.1269e-03 | validation loss: 1.7780e-03\n",
      "Epoch: 151340 | training loss: 2.1480e-03 | validation loss: 1.8053e-03\n",
      "Epoch: 151350 | training loss: 3.7564e-03 | validation loss: 2.7185e-03\n",
      "Epoch: 151360 | training loss: 2.5659e-03 | validation loss: 1.9082e-03\n",
      "Epoch: 151370 | training loss: 2.3290e-03 | validation loss: 1.8048e-03\n",
      "Epoch: 151380 | training loss: 2.1491e-03 | validation loss: 1.7593e-03\n",
      "Epoch: 151390 | training loss: 2.1319e-03 | validation loss: 1.7825e-03\n",
      "Epoch: 151400 | training loss: 2.1356e-03 | validation loss: 1.7969e-03\n",
      "Epoch: 151410 | training loss: 2.1280e-03 | validation loss: 1.7836e-03\n",
      "Epoch: 151420 | training loss: 2.1270e-03 | validation loss: 1.7711e-03\n",
      "Epoch: 151430 | training loss: 2.1266e-03 | validation loss: 1.7726e-03\n",
      "Epoch: 151440 | training loss: 2.1264e-03 | validation loss: 1.7772e-03\n",
      "Epoch: 151450 | training loss: 2.1263e-03 | validation loss: 1.7753e-03\n",
      "Epoch: 151460 | training loss: 2.1263e-03 | validation loss: 1.7745e-03\n",
      "Epoch: 151470 | training loss: 2.1262e-03 | validation loss: 1.7755e-03\n",
      "Epoch: 151480 | training loss: 2.1262e-03 | validation loss: 1.7748e-03\n",
      "Epoch: 151490 | training loss: 2.1262e-03 | validation loss: 1.7751e-03\n",
      "Epoch: 151500 | training loss: 2.1261e-03 | validation loss: 1.7748e-03\n",
      "Epoch: 151510 | training loss: 2.1261e-03 | validation loss: 1.7750e-03\n",
      "Epoch: 151520 | training loss: 2.1261e-03 | validation loss: 1.7748e-03\n",
      "Epoch: 151530 | training loss: 2.1261e-03 | validation loss: 1.7748e-03\n",
      "Epoch: 151540 | training loss: 2.1260e-03 | validation loss: 1.7749e-03\n",
      "Epoch: 151550 | training loss: 2.1261e-03 | validation loss: 1.7754e-03\n",
      "Epoch: 151560 | training loss: 2.1343e-03 | validation loss: 1.7874e-03\n",
      "Epoch: 151570 | training loss: 2.8637e-03 | validation loss: 2.3211e-03\n",
      "Epoch: 151580 | training loss: 2.2817e-03 | validation loss: 1.7889e-03\n",
      "Epoch: 151590 | training loss: 2.1632e-03 | validation loss: 1.7614e-03\n",
      "Epoch: 151600 | training loss: 2.1642e-03 | validation loss: 1.8120e-03\n",
      "Epoch: 151610 | training loss: 2.1311e-03 | validation loss: 1.7781e-03\n",
      "Epoch: 151620 | training loss: 2.1327e-03 | validation loss: 1.7636e-03\n",
      "Epoch: 151630 | training loss: 2.1429e-03 | validation loss: 1.7606e-03\n",
      "Epoch: 151640 | training loss: 2.2255e-03 | validation loss: 1.7723e-03\n",
      "Epoch: 151650 | training loss: 2.4620e-03 | validation loss: 1.8460e-03\n",
      "Epoch: 151660 | training loss: 2.1875e-03 | validation loss: 1.8390e-03\n",
      "Epoch: 151670 | training loss: 2.1259e-03 | validation loss: 1.7716e-03\n",
      "Epoch: 151680 | training loss: 2.1380e-03 | validation loss: 1.7635e-03\n",
      "Epoch: 151690 | training loss: 2.1308e-03 | validation loss: 1.7871e-03\n",
      "Epoch: 151700 | training loss: 2.1302e-03 | validation loss: 1.7861e-03\n",
      "Epoch: 151710 | training loss: 2.1278e-03 | validation loss: 1.7818e-03\n",
      "Epoch: 151720 | training loss: 2.1342e-03 | validation loss: 1.7915e-03\n",
      "Epoch: 151730 | training loss: 2.2686e-03 | validation loss: 1.8967e-03\n",
      "Epoch: 151740 | training loss: 2.5799e-03 | validation loss: 2.0824e-03\n",
      "Epoch: 151750 | training loss: 2.2580e-03 | validation loss: 1.7889e-03\n",
      "Epoch: 151760 | training loss: 2.1540e-03 | validation loss: 1.8107e-03\n",
      "Epoch: 151770 | training loss: 2.1321e-03 | validation loss: 1.7651e-03\n",
      "Epoch: 151780 | training loss: 2.1287e-03 | validation loss: 1.7825e-03\n",
      "Epoch: 151790 | training loss: 2.1282e-03 | validation loss: 1.7679e-03\n",
      "Epoch: 151800 | training loss: 2.1269e-03 | validation loss: 1.7799e-03\n",
      "Epoch: 151810 | training loss: 2.1253e-03 | validation loss: 1.7730e-03\n",
      "Epoch: 151820 | training loss: 2.1257e-03 | validation loss: 1.7710e-03\n",
      "Epoch: 151830 | training loss: 2.1255e-03 | validation loss: 1.7713e-03\n",
      "Epoch: 151840 | training loss: 2.1262e-03 | validation loss: 1.7696e-03\n",
      "Epoch: 151850 | training loss: 2.1420e-03 | validation loss: 1.7639e-03\n",
      "Epoch: 151860 | training loss: 2.6550e-03 | validation loss: 1.9270e-03\n",
      "Epoch: 151870 | training loss: 2.1951e-03 | validation loss: 1.8463e-03\n",
      "Epoch: 151880 | training loss: 2.2755e-03 | validation loss: 1.7945e-03\n",
      "Epoch: 151890 | training loss: 2.1700e-03 | validation loss: 1.8168e-03\n",
      "Epoch: 151900 | training loss: 2.1264e-03 | validation loss: 1.7733e-03\n",
      "Epoch: 151910 | training loss: 2.1263e-03 | validation loss: 1.7689e-03\n",
      "Epoch: 151920 | training loss: 2.1265e-03 | validation loss: 1.7782e-03\n",
      "Epoch: 151930 | training loss: 2.1256e-03 | validation loss: 1.7710e-03\n",
      "Epoch: 151940 | training loss: 2.1251e-03 | validation loss: 1.7742e-03\n",
      "Epoch: 151950 | training loss: 2.1249e-03 | validation loss: 1.7735e-03\n",
      "Epoch: 151960 | training loss: 2.1249e-03 | validation loss: 1.7721e-03\n",
      "Epoch: 151970 | training loss: 2.1249e-03 | validation loss: 1.7732e-03\n",
      "Epoch: 151980 | training loss: 2.1248e-03 | validation loss: 1.7737e-03\n",
      "Epoch: 151990 | training loss: 2.1249e-03 | validation loss: 1.7742e-03\n",
      "Epoch: 152000 | training loss: 2.1260e-03 | validation loss: 1.7776e-03\n",
      "Epoch: 152010 | training loss: 2.1663e-03 | validation loss: 1.8166e-03\n",
      "Epoch: 152020 | training loss: 3.3762e-03 | validation loss: 2.5051e-03\n",
      "Epoch: 152030 | training loss: 2.5176e-03 | validation loss: 1.8871e-03\n",
      "Epoch: 152040 | training loss: 2.1332e-03 | validation loss: 1.7821e-03\n",
      "Epoch: 152050 | training loss: 2.1796e-03 | validation loss: 1.8154e-03\n",
      "Epoch: 152060 | training loss: 2.1251e-03 | validation loss: 1.7720e-03\n",
      "Epoch: 152070 | training loss: 2.1299e-03 | validation loss: 1.7699e-03\n",
      "Epoch: 152080 | training loss: 2.1269e-03 | validation loss: 1.7777e-03\n",
      "Epoch: 152090 | training loss: 2.1247e-03 | validation loss: 1.7712e-03\n",
      "Epoch: 152100 | training loss: 2.1245e-03 | validation loss: 1.7729e-03\n",
      "Epoch: 152110 | training loss: 2.1245e-03 | validation loss: 1.7729e-03\n",
      "Epoch: 152120 | training loss: 2.1244e-03 | validation loss: 1.7725e-03\n",
      "Epoch: 152130 | training loss: 2.1244e-03 | validation loss: 1.7722e-03\n",
      "Epoch: 152140 | training loss: 2.1244e-03 | validation loss: 1.7722e-03\n",
      "Epoch: 152150 | training loss: 2.1246e-03 | validation loss: 1.7700e-03\n",
      "Epoch: 152160 | training loss: 2.1422e-03 | validation loss: 1.7613e-03\n",
      "Epoch: 152170 | training loss: 2.9660e-03 | validation loss: 2.1358e-03\n",
      "Epoch: 152180 | training loss: 2.1624e-03 | validation loss: 1.8140e-03\n",
      "Epoch: 152190 | training loss: 2.1599e-03 | validation loss: 1.7958e-03\n",
      "Epoch: 152200 | training loss: 2.1485e-03 | validation loss: 1.8104e-03\n",
      "Epoch: 152210 | training loss: 2.1337e-03 | validation loss: 1.7875e-03\n",
      "Epoch: 152220 | training loss: 2.1266e-03 | validation loss: 1.7728e-03\n",
      "Epoch: 152230 | training loss: 2.1250e-03 | validation loss: 1.7755e-03\n",
      "Epoch: 152240 | training loss: 2.1247e-03 | validation loss: 1.7717e-03\n",
      "Epoch: 152250 | training loss: 2.1241e-03 | validation loss: 1.7705e-03\n",
      "Epoch: 152260 | training loss: 2.1242e-03 | validation loss: 1.7706e-03\n",
      "Epoch: 152270 | training loss: 2.1265e-03 | validation loss: 1.7660e-03\n",
      "Epoch: 152280 | training loss: 2.2282e-03 | validation loss: 1.7743e-03\n",
      "Epoch: 152290 | training loss: 3.0197e-03 | validation loss: 2.0673e-03\n",
      "Epoch: 152300 | training loss: 2.1386e-03 | validation loss: 1.7864e-03\n",
      "Epoch: 152310 | training loss: 2.2255e-03 | validation loss: 1.8573e-03\n",
      "Epoch: 152320 | training loss: 2.1375e-03 | validation loss: 1.7617e-03\n",
      "Epoch: 152330 | training loss: 2.1290e-03 | validation loss: 1.7657e-03\n",
      "Epoch: 152340 | training loss: 2.1296e-03 | validation loss: 1.7849e-03\n",
      "Epoch: 152350 | training loss: 2.1248e-03 | validation loss: 1.7678e-03\n",
      "Epoch: 152360 | training loss: 2.1238e-03 | validation loss: 1.7719e-03\n",
      "Epoch: 152370 | training loss: 2.1237e-03 | validation loss: 1.7713e-03\n",
      "Epoch: 152380 | training loss: 2.1237e-03 | validation loss: 1.7715e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 152390 | training loss: 2.1237e-03 | validation loss: 1.7707e-03\n",
      "Epoch: 152400 | training loss: 2.1237e-03 | validation loss: 1.7717e-03\n",
      "Epoch: 152410 | training loss: 2.1236e-03 | validation loss: 1.7709e-03\n",
      "Epoch: 152420 | training loss: 2.1236e-03 | validation loss: 1.7708e-03\n",
      "Epoch: 152430 | training loss: 2.1236e-03 | validation loss: 1.7708e-03\n",
      "Epoch: 152440 | training loss: 2.1236e-03 | validation loss: 1.7706e-03\n",
      "Epoch: 152450 | training loss: 2.1237e-03 | validation loss: 1.7691e-03\n",
      "Epoch: 152460 | training loss: 2.1353e-03 | validation loss: 1.7626e-03\n",
      "Epoch: 152470 | training loss: 3.0549e-03 | validation loss: 2.0895e-03\n",
      "Epoch: 152480 | training loss: 2.8005e-03 | validation loss: 2.2000e-03\n",
      "Epoch: 152490 | training loss: 2.1256e-03 | validation loss: 1.7789e-03\n",
      "Epoch: 152500 | training loss: 2.1901e-03 | validation loss: 1.7677e-03\n",
      "Epoch: 152510 | training loss: 2.1460e-03 | validation loss: 1.7580e-03\n",
      "Epoch: 152520 | training loss: 2.1242e-03 | validation loss: 1.7754e-03\n",
      "Epoch: 152530 | training loss: 2.1275e-03 | validation loss: 1.7827e-03\n",
      "Epoch: 152540 | training loss: 2.1234e-03 | validation loss: 1.7690e-03\n",
      "Epoch: 152550 | training loss: 2.1237e-03 | validation loss: 1.7683e-03\n",
      "Epoch: 152560 | training loss: 2.1234e-03 | validation loss: 1.7724e-03\n",
      "Epoch: 152570 | training loss: 2.1232e-03 | validation loss: 1.7704e-03\n",
      "Epoch: 152580 | training loss: 2.1232e-03 | validation loss: 1.7702e-03\n",
      "Epoch: 152590 | training loss: 2.1231e-03 | validation loss: 1.7708e-03\n",
      "Epoch: 152600 | training loss: 2.1231e-03 | validation loss: 1.7704e-03\n",
      "Epoch: 152610 | training loss: 2.1231e-03 | validation loss: 1.7705e-03\n",
      "Epoch: 152620 | training loss: 2.1231e-03 | validation loss: 1.7705e-03\n",
      "Epoch: 152630 | training loss: 2.1230e-03 | validation loss: 1.7703e-03\n",
      "Epoch: 152640 | training loss: 2.1230e-03 | validation loss: 1.7702e-03\n",
      "Epoch: 152650 | training loss: 2.1230e-03 | validation loss: 1.7697e-03\n",
      "Epoch: 152660 | training loss: 2.1269e-03 | validation loss: 1.7659e-03\n",
      "Epoch: 152670 | training loss: 2.7730e-03 | validation loss: 2.1298e-03\n",
      "Epoch: 152680 | training loss: 2.2289e-03 | validation loss: 1.8259e-03\n",
      "Epoch: 152690 | training loss: 2.1886e-03 | validation loss: 1.8113e-03\n",
      "Epoch: 152700 | training loss: 2.1446e-03 | validation loss: 1.7930e-03\n",
      "Epoch: 152710 | training loss: 2.1324e-03 | validation loss: 1.7869e-03\n",
      "Epoch: 152720 | training loss: 2.1332e-03 | validation loss: 1.7932e-03\n",
      "Epoch: 152730 | training loss: 2.2805e-03 | validation loss: 1.9085e-03\n",
      "Epoch: 152740 | training loss: 2.5183e-03 | validation loss: 2.0503e-03\n",
      "Epoch: 152750 | training loss: 2.1632e-03 | validation loss: 1.7610e-03\n",
      "Epoch: 152760 | training loss: 2.1259e-03 | validation loss: 1.7635e-03\n",
      "Epoch: 152770 | training loss: 2.1362e-03 | validation loss: 1.7928e-03\n",
      "Epoch: 152780 | training loss: 2.1311e-03 | validation loss: 1.7606e-03\n",
      "Epoch: 152790 | training loss: 2.1261e-03 | validation loss: 1.7798e-03\n",
      "Epoch: 152800 | training loss: 2.1237e-03 | validation loss: 1.7653e-03\n",
      "Epoch: 152810 | training loss: 2.1226e-03 | validation loss: 1.7709e-03\n",
      "Epoch: 152820 | training loss: 2.1226e-03 | validation loss: 1.7707e-03\n",
      "Epoch: 152830 | training loss: 2.1226e-03 | validation loss: 1.7679e-03\n",
      "Epoch: 152840 | training loss: 2.1225e-03 | validation loss: 1.7680e-03\n",
      "Epoch: 152850 | training loss: 2.1225e-03 | validation loss: 1.7678e-03\n",
      "Epoch: 152860 | training loss: 2.1234e-03 | validation loss: 1.7652e-03\n",
      "Epoch: 152870 | training loss: 2.1518e-03 | validation loss: 1.7586e-03\n",
      "Epoch: 152880 | training loss: 3.1515e-03 | validation loss: 2.1133e-03\n",
      "Epoch: 152890 | training loss: 2.5708e-03 | validation loss: 2.0692e-03\n",
      "Epoch: 152900 | training loss: 2.1572e-03 | validation loss: 1.7666e-03\n",
      "Epoch: 152910 | training loss: 2.1522e-03 | validation loss: 1.7639e-03\n",
      "Epoch: 152920 | training loss: 2.1402e-03 | validation loss: 1.7951e-03\n",
      "Epoch: 152930 | training loss: 2.1227e-03 | validation loss: 1.7657e-03\n",
      "Epoch: 152940 | training loss: 2.1229e-03 | validation loss: 1.7655e-03\n",
      "Epoch: 152950 | training loss: 2.1229e-03 | validation loss: 1.7731e-03\n",
      "Epoch: 152960 | training loss: 2.1225e-03 | validation loss: 1.7669e-03\n",
      "Epoch: 152970 | training loss: 2.1222e-03 | validation loss: 1.7702e-03\n",
      "Epoch: 152980 | training loss: 2.1221e-03 | validation loss: 1.7684e-03\n",
      "Epoch: 152990 | training loss: 2.1221e-03 | validation loss: 1.7686e-03\n",
      "Epoch: 153000 | training loss: 2.1220e-03 | validation loss: 1.7692e-03\n",
      "Epoch: 153010 | training loss: 2.1220e-03 | validation loss: 1.7689e-03\n",
      "Epoch: 153020 | training loss: 2.1220e-03 | validation loss: 1.7687e-03\n",
      "Epoch: 153030 | training loss: 2.1219e-03 | validation loss: 1.7687e-03\n",
      "Epoch: 153040 | training loss: 2.1219e-03 | validation loss: 1.7687e-03\n",
      "Epoch: 153050 | training loss: 2.1219e-03 | validation loss: 1.7695e-03\n",
      "Epoch: 153060 | training loss: 2.1283e-03 | validation loss: 1.7819e-03\n",
      "Epoch: 153070 | training loss: 3.3180e-03 | validation loss: 2.4776e-03\n",
      "Epoch: 153080 | training loss: 3.1499e-03 | validation loss: 2.1293e-03\n",
      "Epoch: 153090 | training loss: 2.4833e-03 | validation loss: 1.8557e-03\n",
      "Epoch: 153100 | training loss: 2.2480e-03 | validation loss: 1.7746e-03\n",
      "Epoch: 153110 | training loss: 2.1659e-03 | validation loss: 1.7574e-03\n",
      "Epoch: 153120 | training loss: 2.1360e-03 | validation loss: 1.7584e-03\n",
      "Epoch: 153130 | training loss: 2.1253e-03 | validation loss: 1.7622e-03\n",
      "Epoch: 153140 | training loss: 2.1220e-03 | validation loss: 1.7656e-03\n",
      "Epoch: 153150 | training loss: 2.1217e-03 | validation loss: 1.7690e-03\n",
      "Epoch: 153160 | training loss: 2.1218e-03 | validation loss: 1.7705e-03\n",
      "Epoch: 153170 | training loss: 2.1217e-03 | validation loss: 1.7695e-03\n",
      "Epoch: 153180 | training loss: 2.1215e-03 | validation loss: 1.7682e-03\n",
      "Epoch: 153190 | training loss: 2.1215e-03 | validation loss: 1.7676e-03\n",
      "Epoch: 153200 | training loss: 2.1215e-03 | validation loss: 1.7681e-03\n",
      "Epoch: 153210 | training loss: 2.1215e-03 | validation loss: 1.7682e-03\n",
      "Epoch: 153220 | training loss: 2.1214e-03 | validation loss: 1.7680e-03\n",
      "Epoch: 153230 | training loss: 2.1214e-03 | validation loss: 1.7680e-03\n",
      "Epoch: 153240 | training loss: 2.1214e-03 | validation loss: 1.7680e-03\n",
      "Epoch: 153250 | training loss: 2.1213e-03 | validation loss: 1.7679e-03\n",
      "Epoch: 153260 | training loss: 2.1213e-03 | validation loss: 1.7679e-03\n",
      "Epoch: 153270 | training loss: 2.1213e-03 | validation loss: 1.7677e-03\n",
      "Epoch: 153280 | training loss: 2.1213e-03 | validation loss: 1.7669e-03\n",
      "Epoch: 153290 | training loss: 2.1305e-03 | validation loss: 1.7630e-03\n",
      "Epoch: 153300 | training loss: 2.8656e-03 | validation loss: 2.1826e-03\n",
      "Epoch: 153310 | training loss: 2.1352e-03 | validation loss: 1.7717e-03\n",
      "Epoch: 153320 | training loss: 2.1237e-03 | validation loss: 1.7763e-03\n",
      "Epoch: 153330 | training loss: 2.1277e-03 | validation loss: 1.7729e-03\n",
      "Epoch: 153340 | training loss: 2.1266e-03 | validation loss: 1.7784e-03\n",
      "Epoch: 153350 | training loss: 2.1247e-03 | validation loss: 1.7771e-03\n",
      "Epoch: 153360 | training loss: 2.1225e-03 | validation loss: 1.7736e-03\n",
      "Epoch: 153370 | training loss: 2.1231e-03 | validation loss: 1.7756e-03\n",
      "Epoch: 153380 | training loss: 2.1664e-03 | validation loss: 1.8202e-03\n",
      "Epoch: 153390 | training loss: 3.0146e-03 | validation loss: 2.3316e-03\n",
      "Epoch: 153400 | training loss: 2.4430e-03 | validation loss: 1.8394e-03\n",
      "Epoch: 153410 | training loss: 2.1839e-03 | validation loss: 1.8318e-03\n",
      "Epoch: 153420 | training loss: 2.1210e-03 | validation loss: 1.7650e-03\n",
      "Epoch: 153430 | training loss: 2.1257e-03 | validation loss: 1.7588e-03\n",
      "Epoch: 153440 | training loss: 2.1249e-03 | validation loss: 1.7774e-03\n",
      "Epoch: 153450 | training loss: 2.1226e-03 | validation loss: 1.7619e-03\n",
      "Epoch: 153460 | training loss: 2.1213e-03 | validation loss: 1.7704e-03\n",
      "Epoch: 153470 | training loss: 2.1208e-03 | validation loss: 1.7660e-03\n",
      "Epoch: 153480 | training loss: 2.1208e-03 | validation loss: 1.7657e-03\n",
      "Epoch: 153490 | training loss: 2.1207e-03 | validation loss: 1.7678e-03\n",
      "Epoch: 153500 | training loss: 2.1207e-03 | validation loss: 1.7676e-03\n",
      "Epoch: 153510 | training loss: 2.1207e-03 | validation loss: 1.7677e-03\n",
      "Epoch: 153520 | training loss: 2.1211e-03 | validation loss: 1.7697e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 153530 | training loss: 2.1333e-03 | validation loss: 1.7882e-03\n",
      "Epoch: 153540 | training loss: 2.7614e-03 | validation loss: 2.1808e-03\n",
      "Epoch: 153550 | training loss: 2.3743e-03 | validation loss: 1.8136e-03\n",
      "Epoch: 153560 | training loss: 2.2934e-03 | validation loss: 1.8949e-03\n",
      "Epoch: 153570 | training loss: 2.1312e-03 | validation loss: 1.7904e-03\n",
      "Epoch: 153580 | training loss: 2.1459e-03 | validation loss: 1.7631e-03\n",
      "Epoch: 153590 | training loss: 2.1224e-03 | validation loss: 1.7663e-03\n",
      "Epoch: 153600 | training loss: 2.1216e-03 | validation loss: 1.7714e-03\n",
      "Epoch: 153610 | training loss: 2.1216e-03 | validation loss: 1.7646e-03\n",
      "Epoch: 153620 | training loss: 2.1208e-03 | validation loss: 1.7677e-03\n",
      "Epoch: 153630 | training loss: 2.1205e-03 | validation loss: 1.7657e-03\n",
      "Epoch: 153640 | training loss: 2.1204e-03 | validation loss: 1.7668e-03\n",
      "Epoch: 153650 | training loss: 2.1203e-03 | validation loss: 1.7659e-03\n",
      "Epoch: 153660 | training loss: 2.1202e-03 | validation loss: 1.7661e-03\n",
      "Epoch: 153670 | training loss: 2.1202e-03 | validation loss: 1.7664e-03\n",
      "Epoch: 153680 | training loss: 2.1202e-03 | validation loss: 1.7660e-03\n",
      "Epoch: 153690 | training loss: 2.1201e-03 | validation loss: 1.7658e-03\n",
      "Epoch: 153700 | training loss: 2.1201e-03 | validation loss: 1.7655e-03\n",
      "Epoch: 153710 | training loss: 2.1206e-03 | validation loss: 1.7639e-03\n",
      "Epoch: 153720 | training loss: 2.1487e-03 | validation loss: 1.7626e-03\n",
      "Epoch: 153730 | training loss: 3.6672e-03 | validation loss: 2.3742e-03\n",
      "Epoch: 153740 | training loss: 2.4604e-03 | validation loss: 2.0072e-03\n",
      "Epoch: 153750 | training loss: 2.2751e-03 | validation loss: 1.8522e-03\n",
      "Epoch: 153760 | training loss: 2.1722e-03 | validation loss: 1.7746e-03\n",
      "Epoch: 153770 | training loss: 2.1243e-03 | validation loss: 1.7717e-03\n",
      "Epoch: 153780 | training loss: 2.1207e-03 | validation loss: 1.7705e-03\n",
      "Epoch: 153790 | training loss: 2.1224e-03 | validation loss: 1.7645e-03\n",
      "Epoch: 153800 | training loss: 2.1208e-03 | validation loss: 1.7698e-03\n",
      "Epoch: 153810 | training loss: 2.1199e-03 | validation loss: 1.7643e-03\n",
      "Epoch: 153820 | training loss: 2.1199e-03 | validation loss: 1.7660e-03\n",
      "Epoch: 153830 | training loss: 2.1198e-03 | validation loss: 1.7648e-03\n",
      "Epoch: 153840 | training loss: 2.1198e-03 | validation loss: 1.7660e-03\n",
      "Epoch: 153850 | training loss: 2.1197e-03 | validation loss: 1.7654e-03\n",
      "Epoch: 153860 | training loss: 2.1197e-03 | validation loss: 1.7651e-03\n",
      "Epoch: 153870 | training loss: 2.1197e-03 | validation loss: 1.7652e-03\n",
      "Epoch: 153880 | training loss: 2.1196e-03 | validation loss: 1.7650e-03\n",
      "Epoch: 153890 | training loss: 2.1197e-03 | validation loss: 1.7640e-03\n",
      "Epoch: 153900 | training loss: 2.1217e-03 | validation loss: 1.7582e-03\n",
      "Epoch: 153910 | training loss: 2.3110e-03 | validation loss: 1.7895e-03\n",
      "Epoch: 153920 | training loss: 2.2314e-03 | validation loss: 1.8051e-03\n",
      "Epoch: 153930 | training loss: 2.2782e-03 | validation loss: 1.7886e-03\n",
      "Epoch: 153940 | training loss: 2.2147e-03 | validation loss: 1.8062e-03\n",
      "Epoch: 153950 | training loss: 2.1346e-03 | validation loss: 1.7534e-03\n",
      "Epoch: 153960 | training loss: 2.1203e-03 | validation loss: 1.7602e-03\n",
      "Epoch: 153970 | training loss: 2.1216e-03 | validation loss: 1.7741e-03\n",
      "Epoch: 153980 | training loss: 2.1210e-03 | validation loss: 1.7686e-03\n",
      "Epoch: 153990 | training loss: 2.1195e-03 | validation loss: 1.7671e-03\n",
      "Epoch: 154000 | training loss: 2.1195e-03 | validation loss: 1.7624e-03\n",
      "Epoch: 154010 | training loss: 2.1193e-03 | validation loss: 1.7644e-03\n",
      "Epoch: 154020 | training loss: 2.1193e-03 | validation loss: 1.7651e-03\n",
      "Epoch: 154030 | training loss: 2.1192e-03 | validation loss: 1.7642e-03\n",
      "Epoch: 154040 | training loss: 2.1192e-03 | validation loss: 1.7642e-03\n",
      "Epoch: 154050 | training loss: 2.1192e-03 | validation loss: 1.7646e-03\n",
      "Epoch: 154060 | training loss: 2.1191e-03 | validation loss: 1.7644e-03\n",
      "Epoch: 154070 | training loss: 2.1191e-03 | validation loss: 1.7647e-03\n",
      "Epoch: 154080 | training loss: 2.1199e-03 | validation loss: 1.7676e-03\n",
      "Epoch: 154090 | training loss: 2.2256e-03 | validation loss: 1.8509e-03\n",
      "Epoch: 154100 | training loss: 2.9601e-03 | validation loss: 2.2869e-03\n",
      "Epoch: 154110 | training loss: 2.5843e-03 | validation loss: 2.0665e-03\n",
      "Epoch: 154120 | training loss: 2.2066e-03 | validation loss: 1.8305e-03\n",
      "Epoch: 154130 | training loss: 2.1346e-03 | validation loss: 1.7673e-03\n",
      "Epoch: 154140 | training loss: 2.1288e-03 | validation loss: 1.7539e-03\n",
      "Epoch: 154150 | training loss: 2.1261e-03 | validation loss: 1.7530e-03\n",
      "Epoch: 154160 | training loss: 2.1204e-03 | validation loss: 1.7581e-03\n",
      "Epoch: 154170 | training loss: 2.1190e-03 | validation loss: 1.7657e-03\n",
      "Epoch: 154180 | training loss: 2.1192e-03 | validation loss: 1.7680e-03\n",
      "Epoch: 154190 | training loss: 2.1188e-03 | validation loss: 1.7646e-03\n",
      "Epoch: 154200 | training loss: 2.1188e-03 | validation loss: 1.7628e-03\n",
      "Epoch: 154210 | training loss: 2.1188e-03 | validation loss: 1.7640e-03\n",
      "Epoch: 154220 | training loss: 2.1187e-03 | validation loss: 1.7642e-03\n",
      "Epoch: 154230 | training loss: 2.1187e-03 | validation loss: 1.7637e-03\n",
      "Epoch: 154240 | training loss: 2.1187e-03 | validation loss: 1.7639e-03\n",
      "Epoch: 154250 | training loss: 2.1186e-03 | validation loss: 1.7637e-03\n",
      "Epoch: 154260 | training loss: 2.1186e-03 | validation loss: 1.7638e-03\n",
      "Epoch: 154270 | training loss: 2.1186e-03 | validation loss: 1.7637e-03\n",
      "Epoch: 154280 | training loss: 2.1186e-03 | validation loss: 1.7637e-03\n",
      "Epoch: 154290 | training loss: 2.1185e-03 | validation loss: 1.7637e-03\n",
      "Epoch: 154300 | training loss: 2.1185e-03 | validation loss: 1.7636e-03\n",
      "Epoch: 154310 | training loss: 2.1185e-03 | validation loss: 1.7636e-03\n",
      "Epoch: 154320 | training loss: 2.1184e-03 | validation loss: 1.7635e-03\n",
      "Epoch: 154330 | training loss: 2.1184e-03 | validation loss: 1.7634e-03\n",
      "Epoch: 154340 | training loss: 2.1184e-03 | validation loss: 1.7624e-03\n",
      "Epoch: 154350 | training loss: 2.1234e-03 | validation loss: 1.7542e-03\n",
      "Epoch: 154360 | training loss: 3.1000e-03 | validation loss: 2.0838e-03\n",
      "Epoch: 154370 | training loss: 2.9171e-03 | validation loss: 2.3565e-03\n",
      "Epoch: 154380 | training loss: 2.3809e-03 | validation loss: 2.0169e-03\n",
      "Epoch: 154390 | training loss: 2.1453e-03 | validation loss: 1.8048e-03\n",
      "Epoch: 154400 | training loss: 2.1197e-03 | validation loss: 1.7578e-03\n",
      "Epoch: 154410 | training loss: 2.1260e-03 | validation loss: 1.7550e-03\n",
      "Epoch: 154420 | training loss: 2.1230e-03 | validation loss: 1.7546e-03\n",
      "Epoch: 154430 | training loss: 2.1201e-03 | validation loss: 1.7564e-03\n",
      "Epoch: 154440 | training loss: 2.1188e-03 | validation loss: 1.7592e-03\n",
      "Epoch: 154450 | training loss: 2.1183e-03 | validation loss: 1.7605e-03\n",
      "Epoch: 154460 | training loss: 2.1182e-03 | validation loss: 1.7611e-03\n",
      "Epoch: 154470 | training loss: 2.1181e-03 | validation loss: 1.7618e-03\n",
      "Epoch: 154480 | training loss: 2.1180e-03 | validation loss: 1.7621e-03\n",
      "Epoch: 154490 | training loss: 2.1180e-03 | validation loss: 1.7624e-03\n",
      "Epoch: 154500 | training loss: 2.1180e-03 | validation loss: 1.7626e-03\n",
      "Epoch: 154510 | training loss: 2.1179e-03 | validation loss: 1.7626e-03\n",
      "Epoch: 154520 | training loss: 2.1179e-03 | validation loss: 1.7625e-03\n",
      "Epoch: 154530 | training loss: 2.1179e-03 | validation loss: 1.7624e-03\n",
      "Epoch: 154540 | training loss: 2.1179e-03 | validation loss: 1.7624e-03\n",
      "Epoch: 154550 | training loss: 2.1178e-03 | validation loss: 1.7624e-03\n",
      "Epoch: 154560 | training loss: 2.1178e-03 | validation loss: 1.7623e-03\n",
      "Epoch: 154570 | training loss: 2.1178e-03 | validation loss: 1.7622e-03\n",
      "Epoch: 154580 | training loss: 2.1181e-03 | validation loss: 1.7613e-03\n",
      "Epoch: 154590 | training loss: 2.1967e-03 | validation loss: 1.7856e-03\n",
      "Epoch: 154600 | training loss: 2.9659e-03 | validation loss: 2.0243e-03\n",
      "Epoch: 154610 | training loss: 2.5964e-03 | validation loss: 1.8797e-03\n",
      "Epoch: 154620 | training loss: 2.2560e-03 | validation loss: 1.7664e-03\n",
      "Epoch: 154630 | training loss: 2.1415e-03 | validation loss: 1.7466e-03\n",
      "Epoch: 154640 | training loss: 2.1179e-03 | validation loss: 1.7600e-03\n",
      "Epoch: 154650 | training loss: 2.1200e-03 | validation loss: 1.7714e-03\n",
      "Epoch: 154660 | training loss: 2.1203e-03 | validation loss: 1.7720e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 154670 | training loss: 2.1182e-03 | validation loss: 1.7670e-03\n",
      "Epoch: 154680 | training loss: 2.1177e-03 | validation loss: 1.7627e-03\n",
      "Epoch: 154690 | training loss: 2.1176e-03 | validation loss: 1.7616e-03\n",
      "Epoch: 154700 | training loss: 2.1175e-03 | validation loss: 1.7625e-03\n",
      "Epoch: 154710 | training loss: 2.1174e-03 | validation loss: 1.7626e-03\n",
      "Epoch: 154720 | training loss: 2.1174e-03 | validation loss: 1.7617e-03\n",
      "Epoch: 154730 | training loss: 2.1174e-03 | validation loss: 1.7618e-03\n",
      "Epoch: 154740 | training loss: 2.1174e-03 | validation loss: 1.7618e-03\n",
      "Epoch: 154750 | training loss: 2.1173e-03 | validation loss: 1.7617e-03\n",
      "Epoch: 154760 | training loss: 2.1173e-03 | validation loss: 1.7618e-03\n",
      "Epoch: 154770 | training loss: 2.1173e-03 | validation loss: 1.7617e-03\n",
      "Epoch: 154780 | training loss: 2.1172e-03 | validation loss: 1.7617e-03\n",
      "Epoch: 154790 | training loss: 2.1172e-03 | validation loss: 1.7616e-03\n",
      "Epoch: 154800 | training loss: 2.1172e-03 | validation loss: 1.7616e-03\n",
      "Epoch: 154810 | training loss: 2.1172e-03 | validation loss: 1.7616e-03\n",
      "Epoch: 154820 | training loss: 2.1171e-03 | validation loss: 1.7615e-03\n",
      "Epoch: 154830 | training loss: 2.1171e-03 | validation loss: 1.7615e-03\n",
      "Epoch: 154840 | training loss: 2.1171e-03 | validation loss: 1.7613e-03\n",
      "Epoch: 154850 | training loss: 2.1171e-03 | validation loss: 1.7603e-03\n",
      "Epoch: 154860 | training loss: 2.1297e-03 | validation loss: 1.7537e-03\n",
      "Epoch: 154870 | training loss: 4.2338e-03 | validation loss: 2.5900e-03\n",
      "Epoch: 154880 | training loss: 2.2907e-03 | validation loss: 1.9004e-03\n",
      "Epoch: 154890 | training loss: 2.2163e-03 | validation loss: 1.8498e-03\n",
      "Epoch: 154900 | training loss: 2.1453e-03 | validation loss: 1.7969e-03\n",
      "Epoch: 154910 | training loss: 2.1257e-03 | validation loss: 1.7781e-03\n",
      "Epoch: 154920 | training loss: 2.1208e-03 | validation loss: 1.7713e-03\n",
      "Epoch: 154930 | training loss: 2.1193e-03 | validation loss: 1.7678e-03\n",
      "Epoch: 154940 | training loss: 2.1184e-03 | validation loss: 1.7656e-03\n",
      "Epoch: 154950 | training loss: 2.1175e-03 | validation loss: 1.7639e-03\n",
      "Epoch: 154960 | training loss: 2.1169e-03 | validation loss: 1.7624e-03\n",
      "Epoch: 154970 | training loss: 2.1167e-03 | validation loss: 1.7612e-03\n",
      "Epoch: 154980 | training loss: 2.1167e-03 | validation loss: 1.7606e-03\n",
      "Epoch: 154990 | training loss: 2.1167e-03 | validation loss: 1.7607e-03\n",
      "Epoch: 155000 | training loss: 2.1167e-03 | validation loss: 1.7612e-03\n",
      "Epoch: 155010 | training loss: 2.1166e-03 | validation loss: 1.7611e-03\n",
      "Epoch: 155020 | training loss: 2.1166e-03 | validation loss: 1.7609e-03\n",
      "Epoch: 155030 | training loss: 2.1166e-03 | validation loss: 1.7609e-03\n",
      "Epoch: 155040 | training loss: 2.1166e-03 | validation loss: 1.7609e-03\n",
      "Epoch: 155050 | training loss: 2.1165e-03 | validation loss: 1.7608e-03\n",
      "Epoch: 155060 | training loss: 2.1165e-03 | validation loss: 1.7608e-03\n",
      "Epoch: 155070 | training loss: 2.1165e-03 | validation loss: 1.7608e-03\n",
      "Epoch: 155080 | training loss: 2.1164e-03 | validation loss: 1.7607e-03\n",
      "Epoch: 155090 | training loss: 2.1164e-03 | validation loss: 1.7607e-03\n",
      "Epoch: 155100 | training loss: 2.1164e-03 | validation loss: 1.7606e-03\n",
      "Epoch: 155110 | training loss: 2.1164e-03 | validation loss: 1.7606e-03\n",
      "Epoch: 155120 | training loss: 2.1163e-03 | validation loss: 1.7606e-03\n",
      "Epoch: 155130 | training loss: 2.1163e-03 | validation loss: 1.7610e-03\n",
      "Epoch: 155140 | training loss: 2.1183e-03 | validation loss: 1.7686e-03\n",
      "Epoch: 155150 | training loss: 2.6787e-03 | validation loss: 2.2577e-03\n",
      "Epoch: 155160 | training loss: 2.5344e-03 | validation loss: 2.0122e-03\n",
      "Epoch: 155170 | training loss: 2.2145e-03 | validation loss: 1.7609e-03\n",
      "Epoch: 155180 | training loss: 2.1428e-03 | validation loss: 1.8008e-03\n",
      "Epoch: 155190 | training loss: 2.1356e-03 | validation loss: 1.7669e-03\n",
      "Epoch: 155200 | training loss: 2.1237e-03 | validation loss: 1.7762e-03\n",
      "Epoch: 155210 | training loss: 2.1189e-03 | validation loss: 1.7691e-03\n",
      "Epoch: 155220 | training loss: 2.1171e-03 | validation loss: 1.7590e-03\n",
      "Epoch: 155230 | training loss: 2.1170e-03 | validation loss: 1.7570e-03\n",
      "Epoch: 155240 | training loss: 2.1183e-03 | validation loss: 1.7544e-03\n",
      "Epoch: 155250 | training loss: 2.1444e-03 | validation loss: 1.7488e-03\n",
      "Epoch: 155260 | training loss: 2.6931e-03 | validation loss: 1.9215e-03\n",
      "Epoch: 155270 | training loss: 2.2170e-03 | validation loss: 1.8519e-03\n",
      "Epoch: 155280 | training loss: 2.2062e-03 | validation loss: 1.7594e-03\n",
      "Epoch: 155290 | training loss: 2.1608e-03 | validation loss: 1.8098e-03\n",
      "Epoch: 155300 | training loss: 2.1319e-03 | validation loss: 1.7492e-03\n",
      "Epoch: 155310 | training loss: 2.1219e-03 | validation loss: 1.7730e-03\n",
      "Epoch: 155320 | training loss: 2.1184e-03 | validation loss: 1.7536e-03\n",
      "Epoch: 155330 | training loss: 2.1165e-03 | validation loss: 1.7634e-03\n",
      "Epoch: 155340 | training loss: 2.1158e-03 | validation loss: 1.7597e-03\n",
      "Epoch: 155350 | training loss: 2.1160e-03 | validation loss: 1.7574e-03\n",
      "Epoch: 155360 | training loss: 2.1158e-03 | validation loss: 1.7581e-03\n",
      "Epoch: 155370 | training loss: 2.1158e-03 | validation loss: 1.7579e-03\n",
      "Epoch: 155380 | training loss: 2.1167e-03 | validation loss: 1.7552e-03\n",
      "Epoch: 155390 | training loss: 2.1503e-03 | validation loss: 1.7497e-03\n",
      "Epoch: 155400 | training loss: 3.2703e-03 | validation loss: 2.1580e-03\n",
      "Epoch: 155410 | training loss: 2.5603e-03 | validation loss: 2.0551e-03\n",
      "Epoch: 155420 | training loss: 2.1181e-03 | validation loss: 1.7535e-03\n",
      "Epoch: 155430 | training loss: 2.1661e-03 | validation loss: 1.7544e-03\n",
      "Epoch: 155440 | training loss: 2.1234e-03 | validation loss: 1.7751e-03\n",
      "Epoch: 155450 | training loss: 2.1166e-03 | validation loss: 1.7645e-03\n",
      "Epoch: 155460 | training loss: 2.1179e-03 | validation loss: 1.7539e-03\n",
      "Epoch: 155470 | training loss: 2.1164e-03 | validation loss: 1.7634e-03\n",
      "Epoch: 155480 | training loss: 2.1157e-03 | validation loss: 1.7568e-03\n",
      "Epoch: 155490 | training loss: 2.1155e-03 | validation loss: 1.7605e-03\n",
      "Epoch: 155500 | training loss: 2.1154e-03 | validation loss: 1.7580e-03\n",
      "Epoch: 155510 | training loss: 2.1153e-03 | validation loss: 1.7594e-03\n",
      "Epoch: 155520 | training loss: 2.1153e-03 | validation loss: 1.7589e-03\n",
      "Epoch: 155530 | training loss: 2.1153e-03 | validation loss: 1.7585e-03\n",
      "Epoch: 155540 | training loss: 2.1152e-03 | validation loss: 1.7585e-03\n",
      "Epoch: 155550 | training loss: 2.1152e-03 | validation loss: 1.7582e-03\n",
      "Epoch: 155560 | training loss: 2.1154e-03 | validation loss: 1.7568e-03\n",
      "Epoch: 155570 | training loss: 2.1261e-03 | validation loss: 1.7505e-03\n",
      "Epoch: 155580 | training loss: 2.8912e-03 | validation loss: 2.0150e-03\n",
      "Epoch: 155590 | training loss: 2.6295e-03 | validation loss: 2.0971e-03\n",
      "Epoch: 155600 | training loss: 2.1574e-03 | validation loss: 1.7573e-03\n",
      "Epoch: 155610 | training loss: 2.2021e-03 | validation loss: 1.7606e-03\n",
      "Epoch: 155620 | training loss: 2.1169e-03 | validation loss: 1.7514e-03\n",
      "Epoch: 155630 | training loss: 2.1251e-03 | validation loss: 1.7768e-03\n",
      "Epoch: 155640 | training loss: 2.1154e-03 | validation loss: 1.7626e-03\n",
      "Epoch: 155650 | training loss: 2.1165e-03 | validation loss: 1.7533e-03\n",
      "Epoch: 155660 | training loss: 2.1150e-03 | validation loss: 1.7600e-03\n",
      "Epoch: 155670 | training loss: 2.1149e-03 | validation loss: 1.7593e-03\n",
      "Epoch: 155680 | training loss: 2.1149e-03 | validation loss: 1.7573e-03\n",
      "Epoch: 155690 | training loss: 2.1148e-03 | validation loss: 1.7591e-03\n",
      "Epoch: 155700 | training loss: 2.1148e-03 | validation loss: 1.7578e-03\n",
      "Epoch: 155710 | training loss: 2.1148e-03 | validation loss: 1.7584e-03\n",
      "Epoch: 155720 | training loss: 2.1147e-03 | validation loss: 1.7582e-03\n",
      "Epoch: 155730 | training loss: 2.1147e-03 | validation loss: 1.7581e-03\n",
      "Epoch: 155740 | training loss: 2.1147e-03 | validation loss: 1.7582e-03\n",
      "Epoch: 155750 | training loss: 2.1147e-03 | validation loss: 1.7583e-03\n",
      "Epoch: 155760 | training loss: 2.1147e-03 | validation loss: 1.7590e-03\n",
      "Epoch: 155770 | training loss: 2.1180e-03 | validation loss: 1.7674e-03\n",
      "Epoch: 155780 | training loss: 2.4763e-03 | validation loss: 2.0792e-03\n",
      "Epoch: 155790 | training loss: 2.4371e-03 | validation loss: 1.9488e-03\n",
      "Epoch: 155800 | training loss: 2.4026e-03 | validation loss: 1.9534e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 155810 | training loss: 2.1194e-03 | validation loss: 1.7480e-03\n",
      "Epoch: 155820 | training loss: 2.1395e-03 | validation loss: 1.7482e-03\n",
      "Epoch: 155830 | training loss: 2.1255e-03 | validation loss: 1.7815e-03\n",
      "Epoch: 155840 | training loss: 2.1247e-03 | validation loss: 1.7803e-03\n",
      "Epoch: 155850 | training loss: 2.1181e-03 | validation loss: 1.7695e-03\n",
      "Epoch: 155860 | training loss: 2.1236e-03 | validation loss: 1.7756e-03\n",
      "Epoch: 155870 | training loss: 2.2431e-03 | validation loss: 1.8698e-03\n",
      "Epoch: 155880 | training loss: 2.5901e-03 | validation loss: 2.0792e-03\n",
      "Epoch: 155890 | training loss: 2.2761e-03 | validation loss: 1.7790e-03\n",
      "Epoch: 155900 | training loss: 2.1670e-03 | validation loss: 1.8147e-03\n",
      "Epoch: 155910 | training loss: 2.1340e-03 | validation loss: 1.7473e-03\n",
      "Epoch: 155920 | training loss: 2.1235e-03 | validation loss: 1.7746e-03\n",
      "Epoch: 155930 | training loss: 2.1176e-03 | validation loss: 1.7506e-03\n",
      "Epoch: 155940 | training loss: 2.1142e-03 | validation loss: 1.7584e-03\n",
      "Epoch: 155950 | training loss: 2.1149e-03 | validation loss: 1.7614e-03\n",
      "Epoch: 155960 | training loss: 2.1141e-03 | validation loss: 1.7580e-03\n",
      "Epoch: 155970 | training loss: 2.1141e-03 | validation loss: 1.7567e-03\n",
      "Epoch: 155980 | training loss: 2.1142e-03 | validation loss: 1.7555e-03\n",
      "Epoch: 155990 | training loss: 2.1181e-03 | validation loss: 1.7501e-03\n",
      "Epoch: 156000 | training loss: 2.3738e-03 | validation loss: 1.8102e-03\n",
      "Epoch: 156010 | training loss: 2.1705e-03 | validation loss: 1.7647e-03\n",
      "Epoch: 156020 | training loss: 2.3745e-03 | validation loss: 1.8131e-03\n",
      "Epoch: 156030 | training loss: 2.1241e-03 | validation loss: 1.7620e-03\n",
      "Epoch: 156040 | training loss: 2.1508e-03 | validation loss: 1.7953e-03\n",
      "Epoch: 156050 | training loss: 2.1149e-03 | validation loss: 1.7593e-03\n",
      "Epoch: 156060 | training loss: 2.1182e-03 | validation loss: 1.7526e-03\n",
      "Epoch: 156070 | training loss: 2.1149e-03 | validation loss: 1.7594e-03\n",
      "Epoch: 156080 | training loss: 2.1138e-03 | validation loss: 1.7571e-03\n",
      "Epoch: 156090 | training loss: 2.1139e-03 | validation loss: 1.7561e-03\n",
      "Epoch: 156100 | training loss: 2.1138e-03 | validation loss: 1.7574e-03\n",
      "Epoch: 156110 | training loss: 2.1137e-03 | validation loss: 1.7562e-03\n",
      "Epoch: 156120 | training loss: 2.1137e-03 | validation loss: 1.7569e-03\n",
      "Epoch: 156130 | training loss: 2.1136e-03 | validation loss: 1.7565e-03\n",
      "Epoch: 156140 | training loss: 2.1136e-03 | validation loss: 1.7564e-03\n",
      "Epoch: 156150 | training loss: 2.1136e-03 | validation loss: 1.7566e-03\n",
      "Epoch: 156160 | training loss: 2.1136e-03 | validation loss: 1.7566e-03\n",
      "Epoch: 156170 | training loss: 2.1135e-03 | validation loss: 1.7566e-03\n",
      "Epoch: 156180 | training loss: 2.1135e-03 | validation loss: 1.7568e-03\n",
      "Epoch: 156190 | training loss: 2.1140e-03 | validation loss: 1.7591e-03\n",
      "Epoch: 156200 | training loss: 2.1555e-03 | validation loss: 1.7987e-03\n",
      "Epoch: 156210 | training loss: 3.9457e-03 | validation loss: 2.7920e-03\n",
      "Epoch: 156220 | training loss: 2.1857e-03 | validation loss: 1.7470e-03\n",
      "Epoch: 156230 | training loss: 2.2395e-03 | validation loss: 1.7934e-03\n",
      "Epoch: 156240 | training loss: 2.1798e-03 | validation loss: 1.8066e-03\n",
      "Epoch: 156250 | training loss: 2.1297e-03 | validation loss: 1.7705e-03\n",
      "Epoch: 156260 | training loss: 2.1145e-03 | validation loss: 1.7525e-03\n",
      "Epoch: 156270 | training loss: 2.1142e-03 | validation loss: 1.7579e-03\n",
      "Epoch: 156280 | training loss: 2.1143e-03 | validation loss: 1.7568e-03\n",
      "Epoch: 156290 | training loss: 2.1133e-03 | validation loss: 1.7556e-03\n",
      "Epoch: 156300 | training loss: 2.1133e-03 | validation loss: 1.7565e-03\n",
      "Epoch: 156310 | training loss: 2.1132e-03 | validation loss: 1.7558e-03\n",
      "Epoch: 156320 | training loss: 2.1132e-03 | validation loss: 1.7559e-03\n",
      "Epoch: 156330 | training loss: 2.1131e-03 | validation loss: 1.7560e-03\n",
      "Epoch: 156340 | training loss: 2.1131e-03 | validation loss: 1.7558e-03\n",
      "Epoch: 156350 | training loss: 2.1131e-03 | validation loss: 1.7557e-03\n",
      "Epoch: 156360 | training loss: 2.1130e-03 | validation loss: 1.7556e-03\n",
      "Epoch: 156370 | training loss: 2.1130e-03 | validation loss: 1.7553e-03\n",
      "Epoch: 156380 | training loss: 2.1131e-03 | validation loss: 1.7538e-03\n",
      "Epoch: 156390 | training loss: 2.1210e-03 | validation loss: 1.7440e-03\n",
      "Epoch: 156400 | training loss: 2.8731e-03 | validation loss: 2.0279e-03\n",
      "Epoch: 156410 | training loss: 2.4985e-03 | validation loss: 2.0880e-03\n",
      "Epoch: 156420 | training loss: 2.2762e-03 | validation loss: 1.8792e-03\n",
      "Epoch: 156430 | training loss: 2.1380e-03 | validation loss: 1.7968e-03\n",
      "Epoch: 156440 | training loss: 2.1166e-03 | validation loss: 1.7679e-03\n",
      "Epoch: 156450 | training loss: 2.1148e-03 | validation loss: 1.7494e-03\n",
      "Epoch: 156460 | training loss: 2.1147e-03 | validation loss: 1.7558e-03\n",
      "Epoch: 156470 | training loss: 2.1138e-03 | validation loss: 1.7499e-03\n",
      "Epoch: 156480 | training loss: 2.1129e-03 | validation loss: 1.7554e-03\n",
      "Epoch: 156490 | training loss: 2.1127e-03 | validation loss: 1.7551e-03\n",
      "Epoch: 156500 | training loss: 2.1127e-03 | validation loss: 1.7555e-03\n",
      "Epoch: 156510 | training loss: 2.1126e-03 | validation loss: 1.7551e-03\n",
      "Epoch: 156520 | training loss: 2.1126e-03 | validation loss: 1.7549e-03\n",
      "Epoch: 156530 | training loss: 2.1126e-03 | validation loss: 1.7552e-03\n",
      "Epoch: 156540 | training loss: 2.1126e-03 | validation loss: 1.7554e-03\n",
      "Epoch: 156550 | training loss: 2.1133e-03 | validation loss: 1.7586e-03\n",
      "Epoch: 156560 | training loss: 2.1712e-03 | validation loss: 1.8125e-03\n",
      "Epoch: 156570 | training loss: 3.7640e-03 | validation loss: 2.7009e-03\n",
      "Epoch: 156580 | training loss: 2.1130e-03 | validation loss: 1.7509e-03\n",
      "Epoch: 156590 | training loss: 2.2524e-03 | validation loss: 1.7717e-03\n",
      "Epoch: 156600 | training loss: 2.1599e-03 | validation loss: 1.7489e-03\n",
      "Epoch: 156610 | training loss: 2.1125e-03 | validation loss: 1.7562e-03\n",
      "Epoch: 156620 | training loss: 2.1201e-03 | validation loss: 1.7698e-03\n",
      "Epoch: 156630 | training loss: 2.1127e-03 | validation loss: 1.7574e-03\n",
      "Epoch: 156640 | training loss: 2.1133e-03 | validation loss: 1.7510e-03\n",
      "Epoch: 156650 | training loss: 2.1123e-03 | validation loss: 1.7545e-03\n",
      "Epoch: 156660 | training loss: 2.1123e-03 | validation loss: 1.7559e-03\n",
      "Epoch: 156670 | training loss: 2.1123e-03 | validation loss: 1.7536e-03\n",
      "Epoch: 156680 | training loss: 2.1122e-03 | validation loss: 1.7548e-03\n",
      "Epoch: 156690 | training loss: 2.1122e-03 | validation loss: 1.7542e-03\n",
      "Epoch: 156700 | training loss: 2.1121e-03 | validation loss: 1.7544e-03\n",
      "Epoch: 156710 | training loss: 2.1121e-03 | validation loss: 1.7542e-03\n",
      "Epoch: 156720 | training loss: 2.1121e-03 | validation loss: 1.7543e-03\n",
      "Epoch: 156730 | training loss: 2.1120e-03 | validation loss: 1.7542e-03\n",
      "Epoch: 156740 | training loss: 2.1120e-03 | validation loss: 1.7542e-03\n",
      "Epoch: 156750 | training loss: 2.1120e-03 | validation loss: 1.7541e-03\n",
      "Epoch: 156760 | training loss: 2.1120e-03 | validation loss: 1.7541e-03\n",
      "Epoch: 156770 | training loss: 2.1119e-03 | validation loss: 1.7538e-03\n",
      "Epoch: 156780 | training loss: 2.1122e-03 | validation loss: 1.7521e-03\n",
      "Epoch: 156790 | training loss: 2.1423e-03 | validation loss: 1.7473e-03\n",
      "Epoch: 156800 | training loss: 4.2811e-03 | validation loss: 2.6085e-03\n",
      "Epoch: 156810 | training loss: 2.1290e-03 | validation loss: 1.7821e-03\n",
      "Epoch: 156820 | training loss: 2.2264e-03 | validation loss: 1.8475e-03\n",
      "Epoch: 156830 | training loss: 2.1878e-03 | validation loss: 1.8192e-03\n",
      "Epoch: 156840 | training loss: 2.1409e-03 | validation loss: 1.7868e-03\n",
      "Epoch: 156850 | training loss: 2.1161e-03 | validation loss: 1.7636e-03\n",
      "Epoch: 156860 | training loss: 2.1118e-03 | validation loss: 1.7526e-03\n",
      "Epoch: 156870 | training loss: 2.1129e-03 | validation loss: 1.7507e-03\n",
      "Epoch: 156880 | training loss: 2.1119e-03 | validation loss: 1.7522e-03\n",
      "Epoch: 156890 | training loss: 2.1117e-03 | validation loss: 1.7547e-03\n",
      "Epoch: 156900 | training loss: 2.1116e-03 | validation loss: 1.7544e-03\n",
      "Epoch: 156910 | training loss: 2.1116e-03 | validation loss: 1.7532e-03\n",
      "Epoch: 156920 | training loss: 2.1115e-03 | validation loss: 1.7535e-03\n",
      "Epoch: 156930 | training loss: 2.1115e-03 | validation loss: 1.7537e-03\n",
      "Epoch: 156940 | training loss: 2.1115e-03 | validation loss: 1.7534e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 156950 | training loss: 2.1115e-03 | validation loss: 1.7535e-03\n",
      "Epoch: 156960 | training loss: 2.1114e-03 | validation loss: 1.7533e-03\n",
      "Epoch: 156970 | training loss: 2.1114e-03 | validation loss: 1.7525e-03\n",
      "Epoch: 156980 | training loss: 2.1176e-03 | validation loss: 1.7461e-03\n",
      "Epoch: 156990 | training loss: 2.8307e-03 | validation loss: 2.1198e-03\n",
      "Epoch: 157000 | training loss: 2.1554e-03 | validation loss: 1.8166e-03\n",
      "Epoch: 157010 | training loss: 2.1616e-03 | validation loss: 1.7863e-03\n",
      "Epoch: 157020 | training loss: 2.1375e-03 | validation loss: 1.7918e-03\n",
      "Epoch: 157030 | training loss: 2.1216e-03 | validation loss: 1.7659e-03\n",
      "Epoch: 157040 | training loss: 2.1144e-03 | validation loss: 1.7569e-03\n",
      "Epoch: 157050 | training loss: 2.1120e-03 | validation loss: 1.7584e-03\n",
      "Epoch: 157060 | training loss: 2.1113e-03 | validation loss: 1.7525e-03\n",
      "Epoch: 157070 | training loss: 2.1114e-03 | validation loss: 1.7502e-03\n",
      "Epoch: 157080 | training loss: 2.1117e-03 | validation loss: 1.7491e-03\n",
      "Epoch: 157090 | training loss: 2.1190e-03 | validation loss: 1.7433e-03\n",
      "Epoch: 157100 | training loss: 2.3744e-03 | validation loss: 1.8003e-03\n",
      "Epoch: 157110 | training loss: 2.1832e-03 | validation loss: 1.7553e-03\n",
      "Epoch: 157120 | training loss: 2.1410e-03 | validation loss: 1.7411e-03\n",
      "Epoch: 157130 | training loss: 2.1792e-03 | validation loss: 1.8172e-03\n",
      "Epoch: 157140 | training loss: 2.1308e-03 | validation loss: 1.7412e-03\n",
      "Epoch: 157150 | training loss: 2.1126e-03 | validation loss: 1.7593e-03\n",
      "Epoch: 157160 | training loss: 2.1110e-03 | validation loss: 1.7526e-03\n",
      "Epoch: 157170 | training loss: 2.1109e-03 | validation loss: 1.7523e-03\n",
      "Epoch: 157180 | training loss: 2.1109e-03 | validation loss: 1.7513e-03\n",
      "Epoch: 157190 | training loss: 2.1110e-03 | validation loss: 1.7541e-03\n",
      "Epoch: 157200 | training loss: 2.1109e-03 | validation loss: 1.7511e-03\n",
      "Epoch: 157210 | training loss: 2.1108e-03 | validation loss: 1.7519e-03\n",
      "Epoch: 157220 | training loss: 2.1108e-03 | validation loss: 1.7527e-03\n",
      "Epoch: 157230 | training loss: 2.1108e-03 | validation loss: 1.7532e-03\n",
      "Epoch: 157240 | training loss: 2.1113e-03 | validation loss: 1.7559e-03\n",
      "Epoch: 157250 | training loss: 2.1324e-03 | validation loss: 1.7823e-03\n",
      "Epoch: 157260 | training loss: 3.0964e-03 | validation loss: 2.3524e-03\n",
      "Epoch: 157270 | training loss: 2.6098e-03 | validation loss: 1.8948e-03\n",
      "Epoch: 157280 | training loss: 2.1502e-03 | validation loss: 1.7848e-03\n",
      "Epoch: 157290 | training loss: 2.1605e-03 | validation loss: 1.8111e-03\n",
      "Epoch: 157300 | training loss: 2.1191e-03 | validation loss: 1.7490e-03\n",
      "Epoch: 157310 | training loss: 2.1151e-03 | validation loss: 1.7425e-03\n",
      "Epoch: 157320 | training loss: 2.1131e-03 | validation loss: 1.7622e-03\n",
      "Epoch: 157330 | training loss: 2.1107e-03 | validation loss: 1.7498e-03\n",
      "Epoch: 157340 | training loss: 2.1104e-03 | validation loss: 1.7514e-03\n",
      "Epoch: 157350 | training loss: 2.1104e-03 | validation loss: 1.7527e-03\n",
      "Epoch: 157360 | training loss: 2.1104e-03 | validation loss: 1.7514e-03\n",
      "Epoch: 157370 | training loss: 2.1103e-03 | validation loss: 1.7516e-03\n",
      "Epoch: 157380 | training loss: 2.1103e-03 | validation loss: 1.7521e-03\n",
      "Epoch: 157390 | training loss: 2.1103e-03 | validation loss: 1.7514e-03\n",
      "Epoch: 157400 | training loss: 2.1103e-03 | validation loss: 1.7517e-03\n",
      "Epoch: 157410 | training loss: 2.1103e-03 | validation loss: 1.7521e-03\n",
      "Epoch: 157420 | training loss: 2.1116e-03 | validation loss: 1.7550e-03\n",
      "Epoch: 157430 | training loss: 2.2010e-03 | validation loss: 1.8316e-03\n",
      "Epoch: 157440 | training loss: 2.2096e-03 | validation loss: 1.7574e-03\n",
      "Epoch: 157450 | training loss: 2.4974e-03 | validation loss: 1.9054e-03\n",
      "Epoch: 157460 | training loss: 2.1906e-03 | validation loss: 1.8433e-03\n",
      "Epoch: 157470 | training loss: 2.1142e-03 | validation loss: 1.7418e-03\n",
      "Epoch: 157480 | training loss: 2.1211e-03 | validation loss: 1.7395e-03\n",
      "Epoch: 157490 | training loss: 2.1167e-03 | validation loss: 1.7618e-03\n",
      "Epoch: 157500 | training loss: 2.1128e-03 | validation loss: 1.7610e-03\n",
      "Epoch: 157510 | training loss: 2.1112e-03 | validation loss: 1.7578e-03\n",
      "Epoch: 157520 | training loss: 2.1135e-03 | validation loss: 1.7613e-03\n",
      "Epoch: 157530 | training loss: 2.1782e-03 | validation loss: 1.8217e-03\n",
      "Epoch: 157540 | training loss: 2.9470e-03 | validation loss: 2.2810e-03\n",
      "Epoch: 157550 | training loss: 2.3790e-03 | validation loss: 1.8114e-03\n",
      "Epoch: 157560 | training loss: 2.1629e-03 | validation loss: 1.8042e-03\n",
      "Epoch: 157570 | training loss: 2.1124e-03 | validation loss: 1.7435e-03\n",
      "Epoch: 157580 | training loss: 2.1100e-03 | validation loss: 1.7496e-03\n",
      "Epoch: 157590 | training loss: 2.1101e-03 | validation loss: 1.7538e-03\n",
      "Epoch: 157600 | training loss: 2.1097e-03 | validation loss: 1.7502e-03\n",
      "Epoch: 157610 | training loss: 2.1099e-03 | validation loss: 1.7489e-03\n",
      "Epoch: 157620 | training loss: 2.1100e-03 | validation loss: 1.7535e-03\n",
      "Epoch: 157630 | training loss: 2.1097e-03 | validation loss: 1.7502e-03\n",
      "Epoch: 157640 | training loss: 2.1097e-03 | validation loss: 1.7494e-03\n",
      "Epoch: 157650 | training loss: 2.1098e-03 | validation loss: 1.7487e-03\n",
      "Epoch: 157660 | training loss: 2.1115e-03 | validation loss: 1.7457e-03\n",
      "Epoch: 157670 | training loss: 2.1608e-03 | validation loss: 1.7448e-03\n",
      "Epoch: 157680 | training loss: 3.2057e-03 | validation loss: 2.1356e-03\n",
      "Epoch: 157690 | training loss: 2.4654e-03 | validation loss: 1.9947e-03\n",
      "Epoch: 157700 | training loss: 2.1235e-03 | validation loss: 1.7529e-03\n",
      "Epoch: 157710 | training loss: 2.1343e-03 | validation loss: 1.7366e-03\n",
      "Epoch: 157720 | training loss: 2.1248e-03 | validation loss: 1.7745e-03\n",
      "Epoch: 157730 | training loss: 2.1118e-03 | validation loss: 1.7474e-03\n",
      "Epoch: 157740 | training loss: 2.1097e-03 | validation loss: 1.7496e-03\n",
      "Epoch: 157750 | training loss: 2.1095e-03 | validation loss: 1.7520e-03\n",
      "Epoch: 157760 | training loss: 2.1093e-03 | validation loss: 1.7498e-03\n",
      "Epoch: 157770 | training loss: 2.1093e-03 | validation loss: 1.7496e-03\n",
      "Epoch: 157780 | training loss: 2.1093e-03 | validation loss: 1.7516e-03\n",
      "Epoch: 157790 | training loss: 2.1092e-03 | validation loss: 1.7498e-03\n",
      "Epoch: 157800 | training loss: 2.1092e-03 | validation loss: 1.7495e-03\n",
      "Epoch: 157810 | training loss: 2.1092e-03 | validation loss: 1.7495e-03\n",
      "Epoch: 157820 | training loss: 2.1092e-03 | validation loss: 1.7486e-03\n",
      "Epoch: 157830 | training loss: 2.1136e-03 | validation loss: 1.7444e-03\n",
      "Epoch: 157840 | training loss: 2.4554e-03 | validation loss: 1.9238e-03\n",
      "Epoch: 157850 | training loss: 2.3460e-03 | validation loss: 1.9363e-03\n",
      "Epoch: 157860 | training loss: 2.2925e-03 | validation loss: 1.7801e-03\n",
      "Epoch: 157870 | training loss: 2.3504e-03 | validation loss: 1.7878e-03\n",
      "Epoch: 157880 | training loss: 2.2113e-03 | validation loss: 1.8304e-03\n",
      "Epoch: 157890 | training loss: 2.1505e-03 | validation loss: 1.7357e-03\n",
      "Epoch: 157900 | training loss: 2.1274e-03 | validation loss: 1.7726e-03\n",
      "Epoch: 157910 | training loss: 2.1160e-03 | validation loss: 1.7404e-03\n",
      "Epoch: 157920 | training loss: 2.1096e-03 | validation loss: 1.7544e-03\n",
      "Epoch: 157930 | training loss: 2.1099e-03 | validation loss: 1.7555e-03\n",
      "Epoch: 157940 | training loss: 2.1089e-03 | validation loss: 1.7485e-03\n",
      "Epoch: 157950 | training loss: 2.1094e-03 | validation loss: 1.7459e-03\n",
      "Epoch: 157960 | training loss: 2.1133e-03 | validation loss: 1.7421e-03\n",
      "Epoch: 157970 | training loss: 2.2071e-03 | validation loss: 1.7519e-03\n",
      "Epoch: 157980 | training loss: 2.8815e-03 | validation loss: 1.9964e-03\n",
      "Epoch: 157990 | training loss: 2.3002e-03 | validation loss: 1.8939e-03\n",
      "Epoch: 158000 | training loss: 2.1194e-03 | validation loss: 1.7407e-03\n",
      "Epoch: 158010 | training loss: 2.1112e-03 | validation loss: 1.7451e-03\n",
      "Epoch: 158020 | training loss: 2.1138e-03 | validation loss: 1.7612e-03\n",
      "Epoch: 158030 | training loss: 2.1114e-03 | validation loss: 1.7432e-03\n",
      "Epoch: 158040 | training loss: 2.1094e-03 | validation loss: 1.7532e-03\n",
      "Epoch: 158050 | training loss: 2.1086e-03 | validation loss: 1.7486e-03\n",
      "Epoch: 158060 | training loss: 2.1086e-03 | validation loss: 1.7476e-03\n",
      "Epoch: 158070 | training loss: 2.1086e-03 | validation loss: 1.7506e-03\n",
      "Epoch: 158080 | training loss: 2.1085e-03 | validation loss: 1.7500e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 158090 | training loss: 2.1085e-03 | validation loss: 1.7496e-03\n",
      "Epoch: 158100 | training loss: 2.1085e-03 | validation loss: 1.7504e-03\n",
      "Epoch: 158110 | training loss: 2.1112e-03 | validation loss: 1.7571e-03\n",
      "Epoch: 158120 | training loss: 2.2639e-03 | validation loss: 1.8736e-03\n",
      "Epoch: 158130 | training loss: 2.6288e-03 | validation loss: 2.0862e-03\n",
      "Epoch: 158140 | training loss: 2.1998e-03 | validation loss: 1.8359e-03\n",
      "Epoch: 158150 | training loss: 2.1865e-03 | validation loss: 1.7521e-03\n",
      "Epoch: 158160 | training loss: 2.1283e-03 | validation loss: 1.7357e-03\n",
      "Epoch: 158170 | training loss: 2.1174e-03 | validation loss: 1.7655e-03\n",
      "Epoch: 158180 | training loss: 2.1096e-03 | validation loss: 1.7560e-03\n",
      "Epoch: 158190 | training loss: 2.1103e-03 | validation loss: 1.7424e-03\n",
      "Epoch: 158200 | training loss: 2.1084e-03 | validation loss: 1.7516e-03\n",
      "Epoch: 158210 | training loss: 2.1081e-03 | validation loss: 1.7485e-03\n",
      "Epoch: 158220 | training loss: 2.1081e-03 | validation loss: 1.7482e-03\n",
      "Epoch: 158230 | training loss: 2.1081e-03 | validation loss: 1.7488e-03\n",
      "Epoch: 158240 | training loss: 2.1080e-03 | validation loss: 1.7484e-03\n",
      "Epoch: 158250 | training loss: 2.1080e-03 | validation loss: 1.7484e-03\n",
      "Epoch: 158260 | training loss: 2.1080e-03 | validation loss: 1.7487e-03\n",
      "Epoch: 158270 | training loss: 2.1079e-03 | validation loss: 1.7484e-03\n",
      "Epoch: 158280 | training loss: 2.1079e-03 | validation loss: 1.7483e-03\n",
      "Epoch: 158290 | training loss: 2.1079e-03 | validation loss: 1.7485e-03\n",
      "Epoch: 158300 | training loss: 2.1093e-03 | validation loss: 1.7525e-03\n",
      "Epoch: 158310 | training loss: 2.3207e-03 | validation loss: 1.9394e-03\n",
      "Epoch: 158320 | training loss: 2.3544e-03 | validation loss: 1.8835e-03\n",
      "Epoch: 158330 | training loss: 2.1732e-03 | validation loss: 1.8108e-03\n",
      "Epoch: 158340 | training loss: 2.4528e-03 | validation loss: 2.0048e-03\n",
      "Epoch: 158350 | training loss: 2.1291e-03 | validation loss: 1.7429e-03\n",
      "Epoch: 158360 | training loss: 2.1161e-03 | validation loss: 1.7454e-03\n",
      "Epoch: 158370 | training loss: 2.1289e-03 | validation loss: 1.7829e-03\n",
      "Epoch: 158380 | training loss: 2.1114e-03 | validation loss: 1.7442e-03\n",
      "Epoch: 158390 | training loss: 2.1130e-03 | validation loss: 1.7410e-03\n",
      "Epoch: 158400 | training loss: 2.1096e-03 | validation loss: 1.7422e-03\n",
      "Epoch: 158410 | training loss: 2.1128e-03 | validation loss: 1.7391e-03\n",
      "Epoch: 158420 | training loss: 2.1830e-03 | validation loss: 1.7426e-03\n",
      "Epoch: 158430 | training loss: 2.8050e-03 | validation loss: 1.9575e-03\n",
      "Epoch: 158440 | training loss: 2.3378e-03 | validation loss: 1.9218e-03\n",
      "Epoch: 158450 | training loss: 2.1867e-03 | validation loss: 1.7456e-03\n",
      "Epoch: 158460 | training loss: 2.1321e-03 | validation loss: 1.7810e-03\n",
      "Epoch: 158470 | training loss: 2.1167e-03 | validation loss: 1.7390e-03\n",
      "Epoch: 158480 | training loss: 2.1119e-03 | validation loss: 1.7589e-03\n",
      "Epoch: 158490 | training loss: 2.1090e-03 | validation loss: 1.7425e-03\n",
      "Epoch: 158500 | training loss: 2.1074e-03 | validation loss: 1.7478e-03\n",
      "Epoch: 158510 | training loss: 2.1078e-03 | validation loss: 1.7505e-03\n",
      "Epoch: 158520 | training loss: 2.1074e-03 | validation loss: 1.7488e-03\n",
      "Epoch: 158530 | training loss: 2.1073e-03 | validation loss: 1.7486e-03\n",
      "Epoch: 158540 | training loss: 2.1080e-03 | validation loss: 1.7515e-03\n",
      "Epoch: 158550 | training loss: 2.1331e-03 | validation loss: 1.7816e-03\n",
      "Epoch: 158560 | training loss: 3.1743e-03 | validation loss: 2.3939e-03\n",
      "Epoch: 158570 | training loss: 2.6033e-03 | validation loss: 1.8922e-03\n",
      "Epoch: 158580 | training loss: 2.1301e-03 | validation loss: 1.7659e-03\n",
      "Epoch: 158590 | training loss: 2.1561e-03 | validation loss: 1.7971e-03\n",
      "Epoch: 158600 | training loss: 2.1174e-03 | validation loss: 1.7454e-03\n",
      "Epoch: 158610 | training loss: 2.1086e-03 | validation loss: 1.7428e-03\n",
      "Epoch: 158620 | training loss: 2.1100e-03 | validation loss: 1.7534e-03\n",
      "Epoch: 158630 | training loss: 2.1079e-03 | validation loss: 1.7449e-03\n",
      "Epoch: 158640 | training loss: 2.1072e-03 | validation loss: 1.7485e-03\n",
      "Epoch: 158650 | training loss: 2.1070e-03 | validation loss: 1.7461e-03\n",
      "Epoch: 158660 | training loss: 2.1070e-03 | validation loss: 1.7477e-03\n",
      "Epoch: 158670 | training loss: 2.1069e-03 | validation loss: 1.7463e-03\n",
      "Epoch: 158680 | training loss: 2.1069e-03 | validation loss: 1.7472e-03\n",
      "Epoch: 158690 | training loss: 2.1068e-03 | validation loss: 1.7470e-03\n",
      "Epoch: 158700 | training loss: 2.1068e-03 | validation loss: 1.7467e-03\n",
      "Epoch: 158710 | training loss: 2.1068e-03 | validation loss: 1.7465e-03\n",
      "Epoch: 158720 | training loss: 2.1068e-03 | validation loss: 1.7461e-03\n",
      "Epoch: 158730 | training loss: 2.1078e-03 | validation loss: 1.7435e-03\n",
      "Epoch: 158740 | training loss: 2.1770e-03 | validation loss: 1.7505e-03\n",
      "Epoch: 158750 | training loss: 3.5829e-03 | validation loss: 2.3013e-03\n",
      "Epoch: 158760 | training loss: 2.1253e-03 | validation loss: 1.7748e-03\n",
      "Epoch: 158770 | training loss: 2.2205e-03 | validation loss: 1.8328e-03\n",
      "Epoch: 158780 | training loss: 2.1560e-03 | validation loss: 1.7781e-03\n",
      "Epoch: 158790 | training loss: 2.1068e-03 | validation loss: 1.7476e-03\n",
      "Epoch: 158800 | training loss: 2.1127e-03 | validation loss: 1.7477e-03\n",
      "Epoch: 158810 | training loss: 2.1075e-03 | validation loss: 1.7443e-03\n",
      "Epoch: 158820 | training loss: 2.1072e-03 | validation loss: 1.7488e-03\n",
      "Epoch: 158830 | training loss: 2.1065e-03 | validation loss: 1.7463e-03\n",
      "Epoch: 158840 | training loss: 2.1066e-03 | validation loss: 1.7462e-03\n",
      "Epoch: 158850 | training loss: 2.1065e-03 | validation loss: 1.7465e-03\n",
      "Epoch: 158860 | training loss: 2.1064e-03 | validation loss: 1.7461e-03\n",
      "Epoch: 158870 | training loss: 2.1064e-03 | validation loss: 1.7462e-03\n",
      "Epoch: 158880 | training loss: 2.1063e-03 | validation loss: 1.7463e-03\n",
      "Epoch: 158890 | training loss: 2.1063e-03 | validation loss: 1.7464e-03\n",
      "Epoch: 158900 | training loss: 2.1063e-03 | validation loss: 1.7470e-03\n",
      "Epoch: 158910 | training loss: 2.1075e-03 | validation loss: 1.7531e-03\n",
      "Epoch: 158920 | training loss: 2.2300e-03 | validation loss: 1.8870e-03\n",
      "Epoch: 158930 | training loss: 2.3266e-03 | validation loss: 1.9088e-03\n",
      "Epoch: 158940 | training loss: 2.2389e-03 | validation loss: 1.8467e-03\n",
      "Epoch: 158950 | training loss: 2.1596e-03 | validation loss: 1.8115e-03\n",
      "Epoch: 158960 | training loss: 2.1192e-03 | validation loss: 1.7665e-03\n",
      "Epoch: 158970 | training loss: 2.1069e-03 | validation loss: 1.7486e-03\n",
      "Epoch: 158980 | training loss: 2.1066e-03 | validation loss: 1.7422e-03\n",
      "Epoch: 158990 | training loss: 2.1073e-03 | validation loss: 1.7459e-03\n",
      "Epoch: 159000 | training loss: 2.1063e-03 | validation loss: 1.7425e-03\n",
      "Epoch: 159010 | training loss: 2.1061e-03 | validation loss: 1.7453e-03\n",
      "Epoch: 159020 | training loss: 2.1060e-03 | validation loss: 1.7463e-03\n",
      "Epoch: 159030 | training loss: 2.1060e-03 | validation loss: 1.7462e-03\n",
      "Epoch: 159040 | training loss: 2.1068e-03 | validation loss: 1.7498e-03\n",
      "Epoch: 159050 | training loss: 2.1399e-03 | validation loss: 1.7869e-03\n",
      "Epoch: 159060 | training loss: 3.3552e-03 | validation loss: 2.4886e-03\n",
      "Epoch: 159070 | training loss: 2.5517e-03 | validation loss: 1.8754e-03\n",
      "Epoch: 159080 | training loss: 2.1094e-03 | validation loss: 1.7361e-03\n",
      "Epoch: 159090 | training loss: 2.1654e-03 | validation loss: 1.8002e-03\n",
      "Epoch: 159100 | training loss: 2.1061e-03 | validation loss: 1.7420e-03\n",
      "Epoch: 159110 | training loss: 2.1119e-03 | validation loss: 1.7392e-03\n",
      "Epoch: 159120 | training loss: 2.1080e-03 | validation loss: 1.7529e-03\n",
      "Epoch: 159130 | training loss: 2.1058e-03 | validation loss: 1.7436e-03\n",
      "Epoch: 159140 | training loss: 2.1057e-03 | validation loss: 1.7445e-03\n",
      "Epoch: 159150 | training loss: 2.1057e-03 | validation loss: 1.7458e-03\n",
      "Epoch: 159160 | training loss: 2.1056e-03 | validation loss: 1.7447e-03\n",
      "Epoch: 159170 | training loss: 2.1056e-03 | validation loss: 1.7448e-03\n",
      "Epoch: 159180 | training loss: 2.1056e-03 | validation loss: 1.7453e-03\n",
      "Epoch: 159190 | training loss: 2.1055e-03 | validation loss: 1.7447e-03\n",
      "Epoch: 159200 | training loss: 2.1055e-03 | validation loss: 1.7447e-03\n",
      "Epoch: 159210 | training loss: 2.1055e-03 | validation loss: 1.7447e-03\n",
      "Epoch: 159220 | training loss: 2.1055e-03 | validation loss: 1.7444e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 159230 | training loss: 2.1056e-03 | validation loss: 1.7430e-03\n",
      "Epoch: 159240 | training loss: 2.1175e-03 | validation loss: 1.7367e-03\n",
      "Epoch: 159250 | training loss: 3.1244e-03 | validation loss: 2.1036e-03\n",
      "Epoch: 159260 | training loss: 2.8467e-03 | validation loss: 2.2075e-03\n",
      "Epoch: 159270 | training loss: 2.1215e-03 | validation loss: 1.7708e-03\n",
      "Epoch: 159280 | training loss: 2.1453e-03 | validation loss: 1.7371e-03\n",
      "Epoch: 159290 | training loss: 2.1374e-03 | validation loss: 1.7349e-03\n",
      "Epoch: 159300 | training loss: 2.1059e-03 | validation loss: 1.7419e-03\n",
      "Epoch: 159310 | training loss: 2.1087e-03 | validation loss: 1.7541e-03\n",
      "Epoch: 159320 | training loss: 2.1056e-03 | validation loss: 1.7471e-03\n",
      "Epoch: 159330 | training loss: 2.1057e-03 | validation loss: 1.7418e-03\n",
      "Epoch: 159340 | training loss: 2.1051e-03 | validation loss: 1.7446e-03\n",
      "Epoch: 159350 | training loss: 2.1052e-03 | validation loss: 1.7453e-03\n",
      "Epoch: 159360 | training loss: 2.1051e-03 | validation loss: 1.7437e-03\n",
      "Epoch: 159370 | training loss: 2.1051e-03 | validation loss: 1.7447e-03\n",
      "Epoch: 159380 | training loss: 2.1050e-03 | validation loss: 1.7440e-03\n",
      "Epoch: 159390 | training loss: 2.1050e-03 | validation loss: 1.7444e-03\n",
      "Epoch: 159400 | training loss: 2.1050e-03 | validation loss: 1.7440e-03\n",
      "Epoch: 159410 | training loss: 2.1050e-03 | validation loss: 1.7438e-03\n",
      "Epoch: 159420 | training loss: 2.1057e-03 | validation loss: 1.7412e-03\n",
      "Epoch: 159430 | training loss: 2.2115e-03 | validation loss: 1.7781e-03\n",
      "Epoch: 159440 | training loss: 2.1781e-03 | validation loss: 1.7665e-03\n",
      "Epoch: 159450 | training loss: 2.1616e-03 | validation loss: 1.7702e-03\n",
      "Epoch: 159460 | training loss: 2.1494e-03 | validation loss: 1.7627e-03\n",
      "Epoch: 159470 | training loss: 2.1185e-03 | validation loss: 1.7379e-03\n",
      "Epoch: 159480 | training loss: 2.1082e-03 | validation loss: 1.7353e-03\n",
      "Epoch: 159490 | training loss: 2.1144e-03 | validation loss: 1.7359e-03\n",
      "Epoch: 159500 | training loss: 2.2070e-03 | validation loss: 1.7463e-03\n",
      "Epoch: 159510 | training loss: 2.6003e-03 | validation loss: 1.8760e-03\n",
      "Epoch: 159520 | training loss: 2.2719e-03 | validation loss: 1.8793e-03\n",
      "Epoch: 159530 | training loss: 2.1653e-03 | validation loss: 1.7358e-03\n",
      "Epoch: 159540 | training loss: 2.1249e-03 | validation loss: 1.7729e-03\n",
      "Epoch: 159550 | training loss: 2.1082e-03 | validation loss: 1.7369e-03\n",
      "Epoch: 159560 | training loss: 2.1048e-03 | validation loss: 1.7418e-03\n",
      "Epoch: 159570 | training loss: 2.1065e-03 | validation loss: 1.7504e-03\n",
      "Epoch: 159580 | training loss: 2.1046e-03 | validation loss: 1.7449e-03\n",
      "Epoch: 159590 | training loss: 2.1046e-03 | validation loss: 1.7422e-03\n",
      "Epoch: 159600 | training loss: 2.1052e-03 | validation loss: 1.7397e-03\n",
      "Epoch: 159610 | training loss: 2.1212e-03 | validation loss: 1.7329e-03\n",
      "Epoch: 159620 | training loss: 2.7207e-03 | validation loss: 1.9250e-03\n",
      "Epoch: 159630 | training loss: 2.2727e-03 | validation loss: 1.8843e-03\n",
      "Epoch: 159640 | training loss: 2.2795e-03 | validation loss: 1.7774e-03\n",
      "Epoch: 159650 | training loss: 2.1190e-03 | validation loss: 1.7589e-03\n",
      "Epoch: 159660 | training loss: 2.1097e-03 | validation loss: 1.7531e-03\n",
      "Epoch: 159670 | training loss: 2.1123e-03 | validation loss: 1.7382e-03\n",
      "Epoch: 159680 | training loss: 2.1073e-03 | validation loss: 1.7512e-03\n",
      "Epoch: 159690 | training loss: 2.1052e-03 | validation loss: 1.7390e-03\n",
      "Epoch: 159700 | training loss: 2.1046e-03 | validation loss: 1.7462e-03\n",
      "Epoch: 159710 | training loss: 2.1044e-03 | validation loss: 1.7411e-03\n",
      "Epoch: 159720 | training loss: 2.1042e-03 | validation loss: 1.7439e-03\n",
      "Epoch: 159730 | training loss: 2.1041e-03 | validation loss: 1.7431e-03\n",
      "Epoch: 159740 | training loss: 2.1041e-03 | validation loss: 1.7425e-03\n",
      "Epoch: 159750 | training loss: 2.1041e-03 | validation loss: 1.7422e-03\n",
      "Epoch: 159760 | training loss: 2.1041e-03 | validation loss: 1.7416e-03\n",
      "Epoch: 159770 | training loss: 2.1059e-03 | validation loss: 1.7383e-03\n",
      "Epoch: 159780 | training loss: 2.1885e-03 | validation loss: 1.7471e-03\n",
      "Epoch: 159790 | training loss: 3.2851e-03 | validation loss: 2.1717e-03\n",
      "Epoch: 159800 | training loss: 2.1317e-03 | validation loss: 1.7762e-03\n",
      "Epoch: 159810 | training loss: 2.2295e-03 | validation loss: 1.8464e-03\n",
      "Epoch: 159820 | training loss: 2.1083e-03 | validation loss: 1.7358e-03\n",
      "Epoch: 159830 | training loss: 2.1185e-03 | validation loss: 1.7345e-03\n",
      "Epoch: 159840 | training loss: 2.1071e-03 | validation loss: 1.7513e-03\n",
      "Epoch: 159850 | training loss: 2.1039e-03 | validation loss: 1.7440e-03\n",
      "Epoch: 159860 | training loss: 2.1044e-03 | validation loss: 1.7397e-03\n",
      "Epoch: 159870 | training loss: 2.1041e-03 | validation loss: 1.7450e-03\n",
      "Epoch: 159880 | training loss: 2.1039e-03 | validation loss: 1.7411e-03\n",
      "Epoch: 159890 | training loss: 2.1038e-03 | validation loss: 1.7433e-03\n",
      "Epoch: 159900 | training loss: 2.1037e-03 | validation loss: 1.7418e-03\n",
      "Epoch: 159910 | training loss: 2.1036e-03 | validation loss: 1.7425e-03\n",
      "Epoch: 159920 | training loss: 2.1036e-03 | validation loss: 1.7424e-03\n",
      "Epoch: 159930 | training loss: 2.1036e-03 | validation loss: 1.7420e-03\n",
      "Epoch: 159940 | training loss: 2.1037e-03 | validation loss: 1.7404e-03\n",
      "Epoch: 159950 | training loss: 2.1478e-03 | validation loss: 1.7449e-03\n",
      "Epoch: 159960 | training loss: 2.3017e-03 | validation loss: 1.7758e-03\n",
      "Epoch: 159970 | training loss: 2.2042e-03 | validation loss: 1.7845e-03\n",
      "Epoch: 159980 | training loss: 2.1397e-03 | validation loss: 1.7637e-03\n",
      "Epoch: 159990 | training loss: 2.1134e-03 | validation loss: 1.7297e-03\n",
      "Epoch: 160000 | training loss: 2.1053e-03 | validation loss: 1.7351e-03\n",
      "Epoch: 160010 | training loss: 2.1038e-03 | validation loss: 1.7429e-03\n",
      "Epoch: 160020 | training loss: 2.1055e-03 | validation loss: 1.7482e-03\n",
      "Epoch: 160030 | training loss: 2.1280e-03 | validation loss: 1.7751e-03\n",
      "Epoch: 160040 | training loss: 2.6062e-03 | validation loss: 2.0823e-03\n",
      "Epoch: 160050 | training loss: 2.1517e-03 | validation loss: 1.7316e-03\n",
      "Epoch: 160060 | training loss: 2.1664e-03 | validation loss: 1.8060e-03\n",
      "Epoch: 160070 | training loss: 2.1439e-03 | validation loss: 1.7315e-03\n",
      "Epoch: 160080 | training loss: 2.1204e-03 | validation loss: 1.7676e-03\n",
      "Epoch: 160090 | training loss: 2.1098e-03 | validation loss: 1.7328e-03\n",
      "Epoch: 160100 | training loss: 2.1054e-03 | validation loss: 1.7489e-03\n",
      "Epoch: 160110 | training loss: 2.1034e-03 | validation loss: 1.7392e-03\n",
      "Epoch: 160120 | training loss: 2.1033e-03 | validation loss: 1.7396e-03\n",
      "Epoch: 160130 | training loss: 2.1032e-03 | validation loss: 1.7431e-03\n",
      "Epoch: 160140 | training loss: 2.1033e-03 | validation loss: 1.7435e-03\n",
      "Epoch: 160150 | training loss: 2.1036e-03 | validation loss: 1.7450e-03\n",
      "Epoch: 160160 | training loss: 2.1109e-03 | validation loss: 1.7575e-03\n",
      "Epoch: 160170 | training loss: 2.3556e-03 | validation loss: 1.9312e-03\n",
      "Epoch: 160180 | training loss: 2.2239e-03 | validation loss: 1.8410e-03\n",
      "Epoch: 160190 | training loss: 2.1220e-03 | validation loss: 1.7690e-03\n",
      "Epoch: 160200 | training loss: 2.1723e-03 | validation loss: 1.7415e-03\n",
      "Epoch: 160210 | training loss: 2.1249e-03 | validation loss: 1.7715e-03\n",
      "Epoch: 160220 | training loss: 2.1046e-03 | validation loss: 1.7363e-03\n",
      "Epoch: 160230 | training loss: 2.1028e-03 | validation loss: 1.7408e-03\n",
      "Epoch: 160240 | training loss: 2.1028e-03 | validation loss: 1.7415e-03\n",
      "Epoch: 160250 | training loss: 2.1028e-03 | validation loss: 1.7415e-03\n",
      "Epoch: 160260 | training loss: 2.1028e-03 | validation loss: 1.7397e-03\n",
      "Epoch: 160270 | training loss: 2.1028e-03 | validation loss: 1.7423e-03\n",
      "Epoch: 160280 | training loss: 2.1027e-03 | validation loss: 1.7409e-03\n",
      "Epoch: 160290 | training loss: 2.1027e-03 | validation loss: 1.7402e-03\n",
      "Epoch: 160300 | training loss: 2.1027e-03 | validation loss: 1.7398e-03\n",
      "Epoch: 160310 | training loss: 2.1033e-03 | validation loss: 1.7378e-03\n",
      "Epoch: 160320 | training loss: 2.1214e-03 | validation loss: 1.7320e-03\n",
      "Epoch: 160330 | training loss: 2.9328e-03 | validation loss: 2.0192e-03\n",
      "Epoch: 160340 | training loss: 2.5072e-03 | validation loss: 2.0163e-03\n",
      "Epoch: 160350 | training loss: 2.2045e-03 | validation loss: 1.7525e-03\n",
      "Epoch: 160360 | training loss: 2.1249e-03 | validation loss: 1.7283e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 160370 | training loss: 2.1241e-03 | validation loss: 1.7689e-03\n",
      "Epoch: 160380 | training loss: 2.1029e-03 | validation loss: 1.7423e-03\n",
      "Epoch: 160390 | training loss: 2.1043e-03 | validation loss: 1.7344e-03\n",
      "Epoch: 160400 | training loss: 2.1036e-03 | validation loss: 1.7464e-03\n",
      "Epoch: 160410 | training loss: 2.1027e-03 | validation loss: 1.7378e-03\n",
      "Epoch: 160420 | training loss: 2.1025e-03 | validation loss: 1.7423e-03\n",
      "Epoch: 160430 | training loss: 2.1024e-03 | validation loss: 1.7391e-03\n",
      "Epoch: 160440 | training loss: 2.1023e-03 | validation loss: 1.7413e-03\n",
      "Epoch: 160450 | training loss: 2.1022e-03 | validation loss: 1.7402e-03\n",
      "Epoch: 160460 | training loss: 2.1022e-03 | validation loss: 1.7399e-03\n",
      "Epoch: 160470 | training loss: 2.1022e-03 | validation loss: 1.7401e-03\n",
      "Epoch: 160480 | training loss: 2.1022e-03 | validation loss: 1.7400e-03\n",
      "Epoch: 160490 | training loss: 2.1026e-03 | validation loss: 1.7388e-03\n",
      "Epoch: 160500 | training loss: 2.1344e-03 | validation loss: 1.7470e-03\n",
      "Epoch: 160510 | training loss: 2.4149e-03 | validation loss: 1.9100e-03\n",
      "Epoch: 160520 | training loss: 2.1647e-03 | validation loss: 1.7926e-03\n",
      "Epoch: 160530 | training loss: 2.7634e-03 | validation loss: 2.2011e-03\n",
      "Epoch: 160540 | training loss: 2.2997e-03 | validation loss: 1.7909e-03\n",
      "Epoch: 160550 | training loss: 2.1897e-03 | validation loss: 1.8348e-03\n",
      "Epoch: 160560 | training loss: 2.1353e-03 | validation loss: 1.7332e-03\n",
      "Epoch: 160570 | training loss: 2.1141e-03 | validation loss: 1.7612e-03\n",
      "Epoch: 160580 | training loss: 2.1063e-03 | validation loss: 1.7310e-03\n",
      "Epoch: 160590 | training loss: 2.1025e-03 | validation loss: 1.7418e-03\n",
      "Epoch: 160600 | training loss: 2.1021e-03 | validation loss: 1.7422e-03\n",
      "Epoch: 160610 | training loss: 2.1021e-03 | validation loss: 1.7378e-03\n",
      "Epoch: 160620 | training loss: 2.1022e-03 | validation loss: 1.7370e-03\n",
      "Epoch: 160630 | training loss: 2.1026e-03 | validation loss: 1.7356e-03\n",
      "Epoch: 160640 | training loss: 2.1115e-03 | validation loss: 1.7300e-03\n",
      "Epoch: 160650 | training loss: 2.3827e-03 | validation loss: 1.7967e-03\n",
      "Epoch: 160660 | training loss: 2.1723e-03 | validation loss: 1.7446e-03\n",
      "Epoch: 160670 | training loss: 2.1167e-03 | validation loss: 1.7270e-03\n",
      "Epoch: 160680 | training loss: 2.1542e-03 | validation loss: 1.7907e-03\n",
      "Epoch: 160690 | training loss: 2.1269e-03 | validation loss: 1.7320e-03\n",
      "Epoch: 160700 | training loss: 2.1084e-03 | validation loss: 1.7542e-03\n",
      "Epoch: 160710 | training loss: 2.1035e-03 | validation loss: 1.7340e-03\n",
      "Epoch: 160720 | training loss: 2.1025e-03 | validation loss: 1.7436e-03\n",
      "Epoch: 160730 | training loss: 2.1021e-03 | validation loss: 1.7368e-03\n",
      "Epoch: 160740 | training loss: 2.1016e-03 | validation loss: 1.7407e-03\n",
      "Epoch: 160750 | training loss: 2.1015e-03 | validation loss: 1.7399e-03\n",
      "Epoch: 160760 | training loss: 2.1015e-03 | validation loss: 1.7384e-03\n",
      "Epoch: 160770 | training loss: 2.1015e-03 | validation loss: 1.7380e-03\n",
      "Epoch: 160780 | training loss: 2.1020e-03 | validation loss: 1.7363e-03\n",
      "Epoch: 160790 | training loss: 2.1142e-03 | validation loss: 1.7307e-03\n",
      "Epoch: 160800 | training loss: 2.6401e-03 | validation loss: 1.9020e-03\n",
      "Epoch: 160810 | training loss: 2.2079e-03 | validation loss: 1.8356e-03\n",
      "Epoch: 160820 | training loss: 2.3061e-03 | validation loss: 1.7774e-03\n",
      "Epoch: 160830 | training loss: 2.1090e-03 | validation loss: 1.7494e-03\n",
      "Epoch: 160840 | training loss: 2.1173e-03 | validation loss: 1.7663e-03\n",
      "Epoch: 160850 | training loss: 2.1101e-03 | validation loss: 1.7292e-03\n",
      "Epoch: 160860 | training loss: 2.1020e-03 | validation loss: 1.7436e-03\n",
      "Epoch: 160870 | training loss: 2.1012e-03 | validation loss: 1.7388e-03\n",
      "Epoch: 160880 | training loss: 2.1012e-03 | validation loss: 1.7383e-03\n",
      "Epoch: 160890 | training loss: 2.1011e-03 | validation loss: 1.7391e-03\n",
      "Epoch: 160900 | training loss: 2.1011e-03 | validation loss: 1.7389e-03\n",
      "Epoch: 160910 | training loss: 2.1011e-03 | validation loss: 1.7381e-03\n",
      "Epoch: 160920 | training loss: 2.1010e-03 | validation loss: 1.7393e-03\n",
      "Epoch: 160930 | training loss: 2.1010e-03 | validation loss: 1.7395e-03\n",
      "Epoch: 160940 | training loss: 2.1015e-03 | validation loss: 1.7417e-03\n",
      "Epoch: 160950 | training loss: 2.1398e-03 | validation loss: 1.7878e-03\n",
      "Epoch: 160960 | training loss: 2.3415e-03 | validation loss: 1.9625e-03\n",
      "Epoch: 160970 | training loss: 2.1851e-03 | validation loss: 1.8390e-03\n",
      "Epoch: 160980 | training loss: 2.2106e-03 | validation loss: 1.8260e-03\n",
      "Epoch: 160990 | training loss: 2.1750e-03 | validation loss: 1.7977e-03\n",
      "Epoch: 161000 | training loss: 2.1028e-03 | validation loss: 1.7415e-03\n",
      "Epoch: 161010 | training loss: 2.1067e-03 | validation loss: 1.7347e-03\n",
      "Epoch: 161020 | training loss: 2.1405e-03 | validation loss: 1.7295e-03\n",
      "Epoch: 161030 | training loss: 2.5016e-03 | validation loss: 1.8350e-03\n",
      "Epoch: 161040 | training loss: 2.1042e-03 | validation loss: 1.7480e-03\n",
      "Epoch: 161050 | training loss: 2.1029e-03 | validation loss: 1.7462e-03\n",
      "Epoch: 161060 | training loss: 2.1093e-03 | validation loss: 1.7293e-03\n",
      "Epoch: 161070 | training loss: 2.1121e-03 | validation loss: 1.7579e-03\n",
      "Epoch: 161080 | training loss: 2.1043e-03 | validation loss: 1.7312e-03\n",
      "Epoch: 161090 | training loss: 2.1015e-03 | validation loss: 1.7343e-03\n",
      "Epoch: 161100 | training loss: 2.1009e-03 | validation loss: 1.7404e-03\n",
      "Epoch: 161110 | training loss: 2.1025e-03 | validation loss: 1.7449e-03\n",
      "Epoch: 161120 | training loss: 2.1198e-03 | validation loss: 1.7661e-03\n",
      "Epoch: 161130 | training loss: 2.4899e-03 | validation loss: 2.0081e-03\n",
      "Epoch: 161140 | training loss: 2.1036e-03 | validation loss: 1.7385e-03\n",
      "Epoch: 161150 | training loss: 2.1121e-03 | validation loss: 1.7599e-03\n",
      "Epoch: 161160 | training loss: 2.1202e-03 | validation loss: 1.7296e-03\n",
      "Epoch: 161170 | training loss: 2.1124e-03 | validation loss: 1.7551e-03\n",
      "Epoch: 161180 | training loss: 2.1047e-03 | validation loss: 1.7329e-03\n",
      "Epoch: 161190 | training loss: 2.1010e-03 | validation loss: 1.7406e-03\n",
      "Epoch: 161200 | training loss: 2.1004e-03 | validation loss: 1.7390e-03\n",
      "Epoch: 161210 | training loss: 2.1008e-03 | validation loss: 1.7347e-03\n",
      "Epoch: 161220 | training loss: 2.1002e-03 | validation loss: 1.7378e-03\n",
      "Epoch: 161230 | training loss: 2.1004e-03 | validation loss: 1.7389e-03\n",
      "Epoch: 161240 | training loss: 2.1009e-03 | validation loss: 1.7411e-03\n",
      "Epoch: 161250 | training loss: 2.1108e-03 | validation loss: 1.7554e-03\n",
      "Epoch: 161260 | training loss: 2.4579e-03 | validation loss: 1.9824e-03\n",
      "Epoch: 161270 | training loss: 2.1050e-03 | validation loss: 1.7475e-03\n",
      "Epoch: 161280 | training loss: 2.1979e-03 | validation loss: 1.8234e-03\n",
      "Epoch: 161290 | training loss: 2.1717e-03 | validation loss: 1.7371e-03\n",
      "Epoch: 161300 | training loss: 2.1055e-03 | validation loss: 1.7489e-03\n",
      "Epoch: 161310 | training loss: 2.1006e-03 | validation loss: 1.7409e-03\n",
      "Epoch: 161320 | training loss: 2.1016e-03 | validation loss: 1.7329e-03\n",
      "Epoch: 161330 | training loss: 2.1009e-03 | validation loss: 1.7414e-03\n",
      "Epoch: 161340 | training loss: 2.1002e-03 | validation loss: 1.7352e-03\n",
      "Epoch: 161350 | training loss: 2.0999e-03 | validation loss: 1.7379e-03\n",
      "Epoch: 161360 | training loss: 2.0999e-03 | validation loss: 1.7378e-03\n",
      "Epoch: 161370 | training loss: 2.0999e-03 | validation loss: 1.7364e-03\n",
      "Epoch: 161380 | training loss: 2.0999e-03 | validation loss: 1.7374e-03\n",
      "Epoch: 161390 | training loss: 2.1019e-03 | validation loss: 1.7443e-03\n",
      "Epoch: 161400 | training loss: 2.3264e-03 | validation loss: 1.9570e-03\n",
      "Epoch: 161410 | training loss: 2.3888e-03 | validation loss: 1.9137e-03\n",
      "Epoch: 161420 | training loss: 2.1049e-03 | validation loss: 1.7516e-03\n",
      "Epoch: 161430 | training loss: 2.1323e-03 | validation loss: 1.7506e-03\n",
      "Epoch: 161440 | training loss: 2.1216e-03 | validation loss: 1.7446e-03\n",
      "Epoch: 161450 | training loss: 2.1090e-03 | validation loss: 1.7315e-03\n",
      "Epoch: 161460 | training loss: 2.1402e-03 | validation loss: 1.7248e-03\n",
      "Epoch: 161470 | training loss: 2.4929e-03 | validation loss: 1.8296e-03\n",
      "Epoch: 161480 | training loss: 2.1023e-03 | validation loss: 1.7446e-03\n",
      "Epoch: 161490 | training loss: 2.1029e-03 | validation loss: 1.7469e-03\n",
      "Epoch: 161500 | training loss: 2.1101e-03 | validation loss: 1.7277e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 161510 | training loss: 2.1116e-03 | validation loss: 1.7576e-03\n",
      "Epoch: 161520 | training loss: 2.1023e-03 | validation loss: 1.7299e-03\n",
      "Epoch: 161530 | training loss: 2.1009e-03 | validation loss: 1.7316e-03\n",
      "Epoch: 161540 | training loss: 2.0995e-03 | validation loss: 1.7374e-03\n",
      "Epoch: 161550 | training loss: 2.1003e-03 | validation loss: 1.7410e-03\n",
      "Epoch: 161560 | training loss: 2.1104e-03 | validation loss: 1.7561e-03\n",
      "Epoch: 161570 | training loss: 2.3846e-03 | validation loss: 1.9458e-03\n",
      "Epoch: 161580 | training loss: 2.1728e-03 | validation loss: 1.8016e-03\n",
      "Epoch: 161590 | training loss: 2.1052e-03 | validation loss: 1.7516e-03\n",
      "Epoch: 161600 | training loss: 2.1362e-03 | validation loss: 1.7310e-03\n",
      "Epoch: 161610 | training loss: 2.1234e-03 | validation loss: 1.7655e-03\n",
      "Epoch: 161620 | training loss: 2.1084e-03 | validation loss: 1.7279e-03\n",
      "Epoch: 161630 | training loss: 2.1027e-03 | validation loss: 1.7459e-03\n",
      "Epoch: 161640 | training loss: 2.1006e-03 | validation loss: 1.7316e-03\n",
      "Epoch: 161650 | training loss: 2.0995e-03 | validation loss: 1.7386e-03\n",
      "Epoch: 161660 | training loss: 2.0991e-03 | validation loss: 1.7363e-03\n",
      "Epoch: 161670 | training loss: 2.0992e-03 | validation loss: 1.7346e-03\n",
      "Epoch: 161680 | training loss: 2.0991e-03 | validation loss: 1.7349e-03\n",
      "Epoch: 161690 | training loss: 2.0991e-03 | validation loss: 1.7348e-03\n",
      "Epoch: 161700 | training loss: 2.0999e-03 | validation loss: 1.7324e-03\n",
      "Epoch: 161710 | training loss: 2.1289e-03 | validation loss: 1.7282e-03\n",
      "Epoch: 161720 | training loss: 3.2438e-03 | validation loss: 2.1472e-03\n",
      "Epoch: 161730 | training loss: 2.5952e-03 | validation loss: 2.0613e-03\n",
      "Epoch: 161740 | training loss: 2.1039e-03 | validation loss: 1.7328e-03\n",
      "Epoch: 161750 | training loss: 2.1563e-03 | validation loss: 1.7293e-03\n",
      "Epoch: 161760 | training loss: 2.1037e-03 | validation loss: 1.7466e-03\n",
      "Epoch: 161770 | training loss: 2.1022e-03 | validation loss: 1.7458e-03\n",
      "Epoch: 161780 | training loss: 2.1018e-03 | validation loss: 1.7292e-03\n",
      "Epoch: 161790 | training loss: 2.0994e-03 | validation loss: 1.7394e-03\n",
      "Epoch: 161800 | training loss: 2.0988e-03 | validation loss: 1.7346e-03\n",
      "Epoch: 161810 | training loss: 2.0987e-03 | validation loss: 1.7358e-03\n",
      "Epoch: 161820 | training loss: 2.0987e-03 | validation loss: 1.7351e-03\n",
      "Epoch: 161830 | training loss: 2.0987e-03 | validation loss: 1.7361e-03\n",
      "Epoch: 161840 | training loss: 2.0986e-03 | validation loss: 1.7351e-03\n",
      "Epoch: 161850 | training loss: 2.0986e-03 | validation loss: 1.7357e-03\n",
      "Epoch: 161860 | training loss: 2.0988e-03 | validation loss: 1.7377e-03\n",
      "Epoch: 161870 | training loss: 2.1177e-03 | validation loss: 1.7656e-03\n",
      "Epoch: 161880 | training loss: 2.6045e-03 | validation loss: 2.1706e-03\n",
      "Epoch: 161890 | training loss: 2.1336e-03 | validation loss: 1.7805e-03\n",
      "Epoch: 161900 | training loss: 2.1506e-03 | validation loss: 1.7604e-03\n",
      "Epoch: 161910 | training loss: 2.1119e-03 | validation loss: 1.7341e-03\n",
      "Epoch: 161920 | training loss: 2.0991e-03 | validation loss: 1.7333e-03\n",
      "Epoch: 161930 | training loss: 2.1088e-03 | validation loss: 1.7313e-03\n",
      "Epoch: 161940 | training loss: 2.3728e-03 | validation loss: 1.7877e-03\n",
      "Epoch: 161950 | training loss: 2.1532e-03 | validation loss: 1.7316e-03\n",
      "Epoch: 161960 | training loss: 2.1480e-03 | validation loss: 1.7258e-03\n",
      "Epoch: 161970 | training loss: 2.1731e-03 | validation loss: 1.8040e-03\n",
      "Epoch: 161980 | training loss: 2.1115e-03 | validation loss: 1.7240e-03\n",
      "Epoch: 161990 | training loss: 2.0983e-03 | validation loss: 1.7357e-03\n",
      "Epoch: 162000 | training loss: 2.0988e-03 | validation loss: 1.7386e-03\n",
      "Epoch: 162010 | training loss: 2.0986e-03 | validation loss: 1.7323e-03\n",
      "Epoch: 162020 | training loss: 2.0983e-03 | validation loss: 1.7356e-03\n",
      "Epoch: 162030 | training loss: 2.0981e-03 | validation loss: 1.7348e-03\n",
      "Epoch: 162040 | training loss: 2.0982e-03 | validation loss: 1.7335e-03\n",
      "Epoch: 162050 | training loss: 2.0981e-03 | validation loss: 1.7351e-03\n",
      "Epoch: 162060 | training loss: 2.0981e-03 | validation loss: 1.7352e-03\n",
      "Epoch: 162070 | training loss: 2.0981e-03 | validation loss: 1.7351e-03\n",
      "Epoch: 162080 | training loss: 2.0982e-03 | validation loss: 1.7361e-03\n",
      "Epoch: 162090 | training loss: 2.1017e-03 | validation loss: 1.7443e-03\n",
      "Epoch: 162100 | training loss: 2.3036e-03 | validation loss: 1.8922e-03\n",
      "Epoch: 162110 | training loss: 2.3312e-03 | validation loss: 1.9015e-03\n",
      "Epoch: 162120 | training loss: 2.2588e-03 | validation loss: 1.8697e-03\n",
      "Epoch: 162130 | training loss: 2.1471e-03 | validation loss: 1.7372e-03\n",
      "Epoch: 162140 | training loss: 2.1218e-03 | validation loss: 1.7202e-03\n",
      "Epoch: 162150 | training loss: 2.1046e-03 | validation loss: 1.7454e-03\n",
      "Epoch: 162160 | training loss: 2.1000e-03 | validation loss: 1.7439e-03\n",
      "Epoch: 162170 | training loss: 2.0994e-03 | validation loss: 1.7282e-03\n",
      "Epoch: 162180 | training loss: 2.0980e-03 | validation loss: 1.7363e-03\n",
      "Epoch: 162190 | training loss: 2.0978e-03 | validation loss: 1.7344e-03\n",
      "Epoch: 162200 | training loss: 2.0977e-03 | validation loss: 1.7335e-03\n",
      "Epoch: 162210 | training loss: 2.0977e-03 | validation loss: 1.7343e-03\n",
      "Epoch: 162220 | training loss: 2.0977e-03 | validation loss: 1.7341e-03\n",
      "Epoch: 162230 | training loss: 2.0976e-03 | validation loss: 1.7337e-03\n",
      "Epoch: 162240 | training loss: 2.0976e-03 | validation loss: 1.7341e-03\n",
      "Epoch: 162250 | training loss: 2.0976e-03 | validation loss: 1.7340e-03\n",
      "Epoch: 162260 | training loss: 2.0975e-03 | validation loss: 1.7337e-03\n",
      "Epoch: 162270 | training loss: 2.0976e-03 | validation loss: 1.7333e-03\n",
      "Epoch: 162280 | training loss: 2.0999e-03 | validation loss: 1.7317e-03\n",
      "Epoch: 162290 | training loss: 2.3299e-03 | validation loss: 1.8566e-03\n",
      "Epoch: 162300 | training loss: 2.4558e-03 | validation loss: 2.0579e-03\n",
      "Epoch: 162310 | training loss: 2.2860e-03 | validation loss: 1.8897e-03\n",
      "Epoch: 162320 | training loss: 2.1902e-03 | validation loss: 1.7294e-03\n",
      "Epoch: 162330 | training loss: 2.1408e-03 | validation loss: 1.7760e-03\n",
      "Epoch: 162340 | training loss: 2.1142e-03 | validation loss: 1.7200e-03\n",
      "Epoch: 162350 | training loss: 2.1000e-03 | validation loss: 1.7345e-03\n",
      "Epoch: 162360 | training loss: 2.0995e-03 | validation loss: 1.7391e-03\n",
      "Epoch: 162370 | training loss: 2.0975e-03 | validation loss: 1.7316e-03\n",
      "Epoch: 162380 | training loss: 2.0986e-03 | validation loss: 1.7295e-03\n",
      "Epoch: 162390 | training loss: 2.1042e-03 | validation loss: 1.7246e-03\n",
      "Epoch: 162400 | training loss: 2.2097e-03 | validation loss: 1.7371e-03\n",
      "Epoch: 162410 | training loss: 2.6642e-03 | validation loss: 1.8952e-03\n",
      "Epoch: 162420 | training loss: 2.2783e-03 | validation loss: 1.8752e-03\n",
      "Epoch: 162430 | training loss: 2.1464e-03 | validation loss: 1.7255e-03\n",
      "Epoch: 162440 | training loss: 2.1111e-03 | validation loss: 1.7562e-03\n",
      "Epoch: 162450 | training loss: 2.1032e-03 | validation loss: 1.7257e-03\n",
      "Epoch: 162460 | training loss: 2.1008e-03 | validation loss: 1.7430e-03\n",
      "Epoch: 162470 | training loss: 2.0985e-03 | validation loss: 1.7284e-03\n",
      "Epoch: 162480 | training loss: 2.0970e-03 | validation loss: 1.7329e-03\n",
      "Epoch: 162490 | training loss: 2.0974e-03 | validation loss: 1.7359e-03\n",
      "Epoch: 162500 | training loss: 2.0973e-03 | validation loss: 1.7357e-03\n",
      "Epoch: 162510 | training loss: 2.0981e-03 | validation loss: 1.7382e-03\n",
      "Epoch: 162520 | training loss: 2.1156e-03 | validation loss: 1.7603e-03\n",
      "Epoch: 162530 | training loss: 2.6443e-03 | validation loss: 2.0926e-03\n",
      "Epoch: 162540 | training loss: 2.1749e-03 | validation loss: 1.7287e-03\n",
      "Epoch: 162550 | training loss: 2.2344e-03 | validation loss: 1.8454e-03\n",
      "Epoch: 162560 | training loss: 2.1455e-03 | validation loss: 1.7336e-03\n",
      "Epoch: 162570 | training loss: 2.1010e-03 | validation loss: 1.7387e-03\n",
      "Epoch: 162580 | training loss: 2.0968e-03 | validation loss: 1.7345e-03\n",
      "Epoch: 162590 | training loss: 2.0970e-03 | validation loss: 1.7311e-03\n",
      "Epoch: 162600 | training loss: 2.0968e-03 | validation loss: 1.7332e-03\n",
      "Epoch: 162610 | training loss: 2.0967e-03 | validation loss: 1.7334e-03\n",
      "Epoch: 162620 | training loss: 2.0968e-03 | validation loss: 1.7312e-03\n",
      "Epoch: 162630 | training loss: 2.0967e-03 | validation loss: 1.7335e-03\n",
      "Epoch: 162640 | training loss: 2.0966e-03 | validation loss: 1.7334e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 162650 | training loss: 2.0966e-03 | validation loss: 1.7328e-03\n",
      "Epoch: 162660 | training loss: 2.0965e-03 | validation loss: 1.7329e-03\n",
      "Epoch: 162670 | training loss: 2.0968e-03 | validation loss: 1.7346e-03\n",
      "Epoch: 162680 | training loss: 2.1131e-03 | validation loss: 1.7550e-03\n",
      "Epoch: 162690 | training loss: 3.2283e-03 | validation loss: 2.3996e-03\n",
      "Epoch: 162700 | training loss: 2.7606e-03 | validation loss: 1.9613e-03\n",
      "Epoch: 162710 | training loss: 2.1184e-03 | validation loss: 1.7458e-03\n",
      "Epoch: 162720 | training loss: 2.1395e-03 | validation loss: 1.7699e-03\n",
      "Epoch: 162730 | training loss: 2.1214e-03 | validation loss: 1.7533e-03\n",
      "Epoch: 162740 | training loss: 2.0966e-03 | validation loss: 1.7334e-03\n",
      "Epoch: 162750 | training loss: 2.1002e-03 | validation loss: 1.7292e-03\n",
      "Epoch: 162760 | training loss: 2.0963e-03 | validation loss: 1.7329e-03\n",
      "Epoch: 162770 | training loss: 2.0967e-03 | validation loss: 1.7335e-03\n",
      "Epoch: 162780 | training loss: 2.0964e-03 | validation loss: 1.7317e-03\n",
      "Epoch: 162790 | training loss: 2.0962e-03 | validation loss: 1.7321e-03\n",
      "Epoch: 162800 | training loss: 2.0962e-03 | validation loss: 1.7321e-03\n",
      "Epoch: 162810 | training loss: 2.0961e-03 | validation loss: 1.7320e-03\n",
      "Epoch: 162820 | training loss: 2.0961e-03 | validation loss: 1.7321e-03\n",
      "Epoch: 162830 | training loss: 2.0961e-03 | validation loss: 1.7322e-03\n",
      "Epoch: 162840 | training loss: 2.0961e-03 | validation loss: 1.7334e-03\n",
      "Epoch: 162850 | training loss: 2.1008e-03 | validation loss: 1.7467e-03\n",
      "Epoch: 162860 | training loss: 2.5348e-03 | validation loss: 2.1305e-03\n",
      "Epoch: 162870 | training loss: 2.3492e-03 | validation loss: 1.8550e-03\n",
      "Epoch: 162880 | training loss: 2.1299e-03 | validation loss: 1.7377e-03\n",
      "Epoch: 162890 | training loss: 2.0961e-03 | validation loss: 1.7310e-03\n",
      "Epoch: 162900 | training loss: 2.1015e-03 | validation loss: 1.7411e-03\n",
      "Epoch: 162910 | training loss: 2.1010e-03 | validation loss: 1.7454e-03\n",
      "Epoch: 162920 | training loss: 2.0974e-03 | validation loss: 1.7294e-03\n",
      "Epoch: 162930 | training loss: 2.0961e-03 | validation loss: 1.7310e-03\n",
      "Epoch: 162940 | training loss: 2.0963e-03 | validation loss: 1.7329e-03\n",
      "Epoch: 162950 | training loss: 2.0962e-03 | validation loss: 1.7348e-03\n",
      "Epoch: 162960 | training loss: 2.0983e-03 | validation loss: 1.7399e-03\n",
      "Epoch: 162970 | training loss: 2.1490e-03 | validation loss: 1.7890e-03\n",
      "Epoch: 162980 | training loss: 3.0364e-03 | validation loss: 2.3121e-03\n",
      "Epoch: 162990 | training loss: 2.4212e-03 | validation loss: 1.8133e-03\n",
      "Epoch: 163000 | training loss: 2.1521e-03 | validation loss: 1.7853e-03\n",
      "Epoch: 163010 | training loss: 2.0958e-03 | validation loss: 1.7293e-03\n",
      "Epoch: 163020 | training loss: 2.1008e-03 | validation loss: 1.7250e-03\n",
      "Epoch: 163030 | training loss: 2.0997e-03 | validation loss: 1.7420e-03\n",
      "Epoch: 163040 | training loss: 2.0973e-03 | validation loss: 1.7261e-03\n",
      "Epoch: 163050 | training loss: 2.0960e-03 | validation loss: 1.7340e-03\n",
      "Epoch: 163060 | training loss: 2.0955e-03 | validation loss: 1.7306e-03\n",
      "Epoch: 163070 | training loss: 2.0956e-03 | validation loss: 1.7297e-03\n",
      "Epoch: 163080 | training loss: 2.0955e-03 | validation loss: 1.7318e-03\n",
      "Epoch: 163090 | training loss: 2.0955e-03 | validation loss: 1.7318e-03\n",
      "Epoch: 163100 | training loss: 2.0955e-03 | validation loss: 1.7322e-03\n",
      "Epoch: 163110 | training loss: 2.0964e-03 | validation loss: 1.7353e-03\n",
      "Epoch: 163120 | training loss: 2.1263e-03 | validation loss: 1.7680e-03\n",
      "Epoch: 163130 | training loss: 3.1947e-03 | validation loss: 2.3877e-03\n",
      "Epoch: 163140 | training loss: 2.5706e-03 | validation loss: 1.8701e-03\n",
      "Epoch: 163150 | training loss: 2.1125e-03 | validation loss: 1.7494e-03\n",
      "Epoch: 163160 | training loss: 2.1405e-03 | validation loss: 1.7826e-03\n",
      "Epoch: 163170 | training loss: 2.1065e-03 | validation loss: 1.7226e-03\n",
      "Epoch: 163180 | training loss: 2.0960e-03 | validation loss: 1.7262e-03\n",
      "Epoch: 163190 | training loss: 2.0976e-03 | validation loss: 1.7398e-03\n",
      "Epoch: 163200 | training loss: 2.0962e-03 | validation loss: 1.7258e-03\n",
      "Epoch: 163210 | training loss: 2.0955e-03 | validation loss: 1.7338e-03\n",
      "Epoch: 163220 | training loss: 2.0952e-03 | validation loss: 1.7288e-03\n",
      "Epoch: 163230 | training loss: 2.0951e-03 | validation loss: 1.7316e-03\n",
      "Epoch: 163240 | training loss: 2.0951e-03 | validation loss: 1.7299e-03\n",
      "Epoch: 163250 | training loss: 2.0950e-03 | validation loss: 1.7305e-03\n",
      "Epoch: 163260 | training loss: 2.0950e-03 | validation loss: 1.7310e-03\n",
      "Epoch: 163270 | training loss: 2.0950e-03 | validation loss: 1.7317e-03\n",
      "Epoch: 163280 | training loss: 2.0990e-03 | validation loss: 1.7403e-03\n",
      "Epoch: 163290 | training loss: 2.3749e-03 | validation loss: 1.9812e-03\n",
      "Epoch: 163300 | training loss: 2.2907e-03 | validation loss: 1.8389e-03\n",
      "Epoch: 163310 | training loss: 2.1848e-03 | validation loss: 1.8258e-03\n",
      "Epoch: 163320 | training loss: 2.3309e-03 | validation loss: 1.9348e-03\n",
      "Epoch: 163330 | training loss: 2.1001e-03 | validation loss: 1.7341e-03\n",
      "Epoch: 163340 | training loss: 2.1303e-03 | validation loss: 1.7171e-03\n",
      "Epoch: 163350 | training loss: 2.0964e-03 | validation loss: 1.7268e-03\n",
      "Epoch: 163360 | training loss: 2.1026e-03 | validation loss: 1.7465e-03\n",
      "Epoch: 163370 | training loss: 2.1314e-03 | validation loss: 1.7765e-03\n",
      "Epoch: 163380 | training loss: 2.3594e-03 | validation loss: 1.9291e-03\n",
      "Epoch: 163390 | training loss: 2.1363e-03 | validation loss: 1.7757e-03\n",
      "Epoch: 163400 | training loss: 2.1609e-03 | validation loss: 1.7257e-03\n",
      "Epoch: 163410 | training loss: 2.1211e-03 | validation loss: 1.7651e-03\n",
      "Epoch: 163420 | training loss: 2.0946e-03 | validation loss: 1.7296e-03\n",
      "Epoch: 163430 | training loss: 2.1018e-03 | validation loss: 1.7214e-03\n",
      "Epoch: 163440 | training loss: 2.0976e-03 | validation loss: 1.7236e-03\n",
      "Epoch: 163450 | training loss: 2.0986e-03 | validation loss: 1.7227e-03\n",
      "Epoch: 163460 | training loss: 2.1333e-03 | validation loss: 1.7212e-03\n",
      "Epoch: 163470 | training loss: 2.6822e-03 | validation loss: 1.9053e-03\n",
      "Epoch: 163480 | training loss: 2.1853e-03 | validation loss: 1.8149e-03\n",
      "Epoch: 163490 | training loss: 2.1500e-03 | validation loss: 1.7253e-03\n",
      "Epoch: 163500 | training loss: 2.1228e-03 | validation loss: 1.7621e-03\n",
      "Epoch: 163510 | training loss: 2.1051e-03 | validation loss: 1.7232e-03\n",
      "Epoch: 163520 | training loss: 2.0968e-03 | validation loss: 1.7358e-03\n",
      "Epoch: 163530 | training loss: 2.0943e-03 | validation loss: 1.7298e-03\n",
      "Epoch: 163540 | training loss: 2.0951e-03 | validation loss: 1.7259e-03\n",
      "Epoch: 163550 | training loss: 2.0944e-03 | validation loss: 1.7310e-03\n",
      "Epoch: 163560 | training loss: 2.0947e-03 | validation loss: 1.7320e-03\n",
      "Epoch: 163570 | training loss: 2.0955e-03 | validation loss: 1.7343e-03\n",
      "Epoch: 163580 | training loss: 2.1093e-03 | validation loss: 1.7518e-03\n",
      "Epoch: 163590 | training loss: 2.4885e-03 | validation loss: 1.9951e-03\n",
      "Epoch: 163600 | training loss: 2.0945e-03 | validation loss: 1.7306e-03\n",
      "Epoch: 163610 | training loss: 2.1461e-03 | validation loss: 1.7827e-03\n",
      "Epoch: 163620 | training loss: 2.1521e-03 | validation loss: 1.7258e-03\n",
      "Epoch: 163630 | training loss: 2.1154e-03 | validation loss: 1.7576e-03\n",
      "Epoch: 163640 | training loss: 2.0995e-03 | validation loss: 1.7223e-03\n",
      "Epoch: 163650 | training loss: 2.0958e-03 | validation loss: 1.7355e-03\n",
      "Epoch: 163660 | training loss: 2.0950e-03 | validation loss: 1.7255e-03\n",
      "Epoch: 163670 | training loss: 2.0945e-03 | validation loss: 1.7321e-03\n",
      "Epoch: 163680 | training loss: 2.0939e-03 | validation loss: 1.7281e-03\n",
      "Epoch: 163690 | training loss: 2.0940e-03 | validation loss: 1.7276e-03\n",
      "Epoch: 163700 | training loss: 2.0939e-03 | validation loss: 1.7280e-03\n",
      "Epoch: 163710 | training loss: 2.0961e-03 | validation loss: 1.7247e-03\n",
      "Epoch: 163720 | training loss: 2.3080e-03 | validation loss: 1.8198e-03\n",
      "Epoch: 163730 | training loss: 2.3670e-03 | validation loss: 1.8759e-03\n",
      "Epoch: 163740 | training loss: 2.1261e-03 | validation loss: 1.7155e-03\n",
      "Epoch: 163750 | training loss: 2.1340e-03 | validation loss: 1.7540e-03\n",
      "Epoch: 163760 | training loss: 2.1190e-03 | validation loss: 1.7492e-03\n",
      "Epoch: 163770 | training loss: 2.1013e-03 | validation loss: 1.7427e-03\n",
      "Epoch: 163780 | training loss: 2.1159e-03 | validation loss: 1.7634e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 163790 | training loss: 2.3138e-03 | validation loss: 1.9039e-03\n",
      "Epoch: 163800 | training loss: 2.2318e-03 | validation loss: 1.8448e-03\n",
      "Epoch: 163810 | training loss: 2.1798e-03 | validation loss: 1.7269e-03\n",
      "Epoch: 163820 | training loss: 2.1354e-03 | validation loss: 1.7763e-03\n",
      "Epoch: 163830 | training loss: 2.1073e-03 | validation loss: 1.7186e-03\n",
      "Epoch: 163840 | training loss: 2.0937e-03 | validation loss: 1.7303e-03\n",
      "Epoch: 163850 | training loss: 2.0966e-03 | validation loss: 1.7374e-03\n",
      "Epoch: 163860 | training loss: 2.0935e-03 | validation loss: 1.7291e-03\n",
      "Epoch: 163870 | training loss: 2.0937e-03 | validation loss: 1.7259e-03\n",
      "Epoch: 163880 | training loss: 2.0966e-03 | validation loss: 1.7217e-03\n",
      "Epoch: 163890 | training loss: 2.1698e-03 | validation loss: 1.7260e-03\n",
      "Epoch: 163900 | training loss: 3.0190e-03 | validation loss: 2.0387e-03\n",
      "Epoch: 163910 | training loss: 2.3477e-03 | validation loss: 1.9099e-03\n",
      "Epoch: 163920 | training loss: 2.1084e-03 | validation loss: 1.7248e-03\n",
      "Epoch: 163930 | training loss: 2.0990e-03 | validation loss: 1.7218e-03\n",
      "Epoch: 163940 | training loss: 2.1034e-03 | validation loss: 1.7434e-03\n",
      "Epoch: 163950 | training loss: 2.0983e-03 | validation loss: 1.7218e-03\n",
      "Epoch: 163960 | training loss: 2.0952e-03 | validation loss: 1.7349e-03\n",
      "Epoch: 163970 | training loss: 2.0938e-03 | validation loss: 1.7245e-03\n",
      "Epoch: 163980 | training loss: 2.0932e-03 | validation loss: 1.7293e-03\n",
      "Epoch: 163990 | training loss: 2.0932e-03 | validation loss: 1.7285e-03\n",
      "Epoch: 164000 | training loss: 2.0931e-03 | validation loss: 1.7270e-03\n",
      "Epoch: 164010 | training loss: 2.0931e-03 | validation loss: 1.7268e-03\n",
      "Epoch: 164020 | training loss: 2.0931e-03 | validation loss: 1.7265e-03\n",
      "Epoch: 164030 | training loss: 2.0941e-03 | validation loss: 1.7241e-03\n",
      "Epoch: 164040 | training loss: 2.1285e-03 | validation loss: 1.7211e-03\n",
      "Epoch: 164050 | training loss: 3.2875e-03 | validation loss: 2.1620e-03\n",
      "Epoch: 164060 | training loss: 2.5502e-03 | validation loss: 2.0302e-03\n",
      "Epoch: 164070 | training loss: 2.0938e-03 | validation loss: 1.7263e-03\n",
      "Epoch: 164080 | training loss: 2.1485e-03 | validation loss: 1.7221e-03\n",
      "Epoch: 164090 | training loss: 2.0978e-03 | validation loss: 1.7389e-03\n",
      "Epoch: 164100 | training loss: 2.0953e-03 | validation loss: 1.7354e-03\n",
      "Epoch: 164110 | training loss: 2.0957e-03 | validation loss: 1.7218e-03\n",
      "Epoch: 164120 | training loss: 2.0936e-03 | validation loss: 1.7315e-03\n",
      "Epoch: 164130 | training loss: 2.0929e-03 | validation loss: 1.7260e-03\n",
      "Epoch: 164140 | training loss: 2.0928e-03 | validation loss: 1.7284e-03\n",
      "Epoch: 164150 | training loss: 2.0927e-03 | validation loss: 1.7265e-03\n",
      "Epoch: 164160 | training loss: 2.0927e-03 | validation loss: 1.7279e-03\n",
      "Epoch: 164170 | training loss: 2.0927e-03 | validation loss: 1.7269e-03\n",
      "Epoch: 164180 | training loss: 2.0927e-03 | validation loss: 1.7263e-03\n",
      "Epoch: 164190 | training loss: 2.0946e-03 | validation loss: 1.7230e-03\n",
      "Epoch: 164200 | training loss: 2.3003e-03 | validation loss: 1.8157e-03\n",
      "Epoch: 164210 | training loss: 2.3163e-03 | validation loss: 1.8644e-03\n",
      "Epoch: 164220 | training loss: 2.1008e-03 | validation loss: 1.7195e-03\n",
      "Epoch: 164230 | training loss: 2.1284e-03 | validation loss: 1.7468e-03\n",
      "Epoch: 164240 | training loss: 2.1125e-03 | validation loss: 1.7438e-03\n",
      "Epoch: 164250 | training loss: 2.1195e-03 | validation loss: 1.7635e-03\n",
      "Epoch: 164260 | training loss: 2.2816e-03 | validation loss: 1.8849e-03\n",
      "Epoch: 164270 | training loss: 2.2401e-03 | validation loss: 1.8538e-03\n",
      "Epoch: 164280 | training loss: 2.1807e-03 | validation loss: 1.7259e-03\n",
      "Epoch: 164290 | training loss: 2.1067e-03 | validation loss: 1.7496e-03\n",
      "Epoch: 164300 | training loss: 2.0954e-03 | validation loss: 1.7356e-03\n",
      "Epoch: 164310 | training loss: 2.0984e-03 | validation loss: 1.7186e-03\n",
      "Epoch: 164320 | training loss: 2.0975e-03 | validation loss: 1.7189e-03\n",
      "Epoch: 164330 | training loss: 2.1003e-03 | validation loss: 1.7175e-03\n",
      "Epoch: 164340 | training loss: 2.1570e-03 | validation loss: 1.7213e-03\n",
      "Epoch: 164350 | training loss: 2.6339e-03 | validation loss: 1.8814e-03\n",
      "Epoch: 164360 | training loss: 2.1935e-03 | validation loss: 1.8173e-03\n",
      "Epoch: 164370 | training loss: 2.1185e-03 | validation loss: 1.7185e-03\n",
      "Epoch: 164380 | training loss: 2.0957e-03 | validation loss: 1.7350e-03\n",
      "Epoch: 164390 | training loss: 2.0925e-03 | validation loss: 1.7292e-03\n",
      "Epoch: 164400 | training loss: 2.0961e-03 | validation loss: 1.7203e-03\n",
      "Epoch: 164410 | training loss: 2.0929e-03 | validation loss: 1.7303e-03\n",
      "Epoch: 164420 | training loss: 2.0935e-03 | validation loss: 1.7321e-03\n",
      "Epoch: 164430 | training loss: 2.0933e-03 | validation loss: 1.7316e-03\n",
      "Epoch: 164440 | training loss: 2.0997e-03 | validation loss: 1.7414e-03\n",
      "Epoch: 164450 | training loss: 2.2453e-03 | validation loss: 1.8500e-03\n",
      "Epoch: 164460 | training loss: 2.5838e-03 | validation loss: 2.0482e-03\n",
      "Epoch: 164470 | training loss: 2.1911e-03 | validation loss: 1.7385e-03\n",
      "Epoch: 164480 | training loss: 2.0969e-03 | validation loss: 1.7319e-03\n",
      "Epoch: 164490 | training loss: 2.0934e-03 | validation loss: 1.7336e-03\n",
      "Epoch: 164500 | training loss: 2.0935e-03 | validation loss: 1.7202e-03\n",
      "Epoch: 164510 | training loss: 2.0923e-03 | validation loss: 1.7301e-03\n",
      "Epoch: 164520 | training loss: 2.0919e-03 | validation loss: 1.7255e-03\n",
      "Epoch: 164530 | training loss: 2.0922e-03 | validation loss: 1.7246e-03\n",
      "Epoch: 164540 | training loss: 2.0919e-03 | validation loss: 1.7278e-03\n",
      "Epoch: 164550 | training loss: 2.0918e-03 | validation loss: 1.7266e-03\n",
      "Epoch: 164560 | training loss: 2.0917e-03 | validation loss: 1.7259e-03\n",
      "Epoch: 164570 | training loss: 2.0919e-03 | validation loss: 1.7261e-03\n",
      "Epoch: 164580 | training loss: 2.0976e-03 | validation loss: 1.7306e-03\n",
      "Epoch: 164590 | training loss: 2.3302e-03 | validation loss: 1.8751e-03\n",
      "Epoch: 164600 | training loss: 2.8711e-03 | validation loss: 2.3217e-03\n",
      "Epoch: 164610 | training loss: 2.1869e-03 | validation loss: 1.7241e-03\n",
      "Epoch: 164620 | training loss: 2.1509e-03 | validation loss: 1.7301e-03\n",
      "Epoch: 164630 | training loss: 2.1214e-03 | validation loss: 1.7436e-03\n",
      "Epoch: 164640 | training loss: 2.0979e-03 | validation loss: 1.7157e-03\n",
      "Epoch: 164650 | training loss: 2.0943e-03 | validation loss: 1.7363e-03\n",
      "Epoch: 164660 | training loss: 2.0929e-03 | validation loss: 1.7267e-03\n",
      "Epoch: 164670 | training loss: 2.0917e-03 | validation loss: 1.7278e-03\n",
      "Epoch: 164680 | training loss: 2.0916e-03 | validation loss: 1.7227e-03\n",
      "Epoch: 164690 | training loss: 2.0913e-03 | validation loss: 1.7259e-03\n",
      "Epoch: 164700 | training loss: 2.0913e-03 | validation loss: 1.7264e-03\n",
      "Epoch: 164710 | training loss: 2.0913e-03 | validation loss: 1.7250e-03\n",
      "Epoch: 164720 | training loss: 2.0913e-03 | validation loss: 1.7251e-03\n",
      "Epoch: 164730 | training loss: 2.0913e-03 | validation loss: 1.7244e-03\n",
      "Epoch: 164740 | training loss: 2.0925e-03 | validation loss: 1.7210e-03\n",
      "Epoch: 164750 | training loss: 2.1793e-03 | validation loss: 1.7275e-03\n",
      "Epoch: 164760 | training loss: 3.3089e-03 | validation loss: 2.1657e-03\n",
      "Epoch: 164770 | training loss: 2.1319e-03 | validation loss: 1.7114e-03\n",
      "Epoch: 164780 | training loss: 2.1859e-03 | validation loss: 1.8022e-03\n",
      "Epoch: 164790 | training loss: 2.1324e-03 | validation loss: 1.7734e-03\n",
      "Epoch: 164800 | training loss: 2.0941e-03 | validation loss: 1.7307e-03\n",
      "Epoch: 164810 | training loss: 2.0970e-03 | validation loss: 1.7178e-03\n",
      "Epoch: 164820 | training loss: 2.0917e-03 | validation loss: 1.7226e-03\n",
      "Epoch: 164830 | training loss: 2.0916e-03 | validation loss: 1.7293e-03\n",
      "Epoch: 164840 | training loss: 2.0911e-03 | validation loss: 1.7246e-03\n",
      "Epoch: 164850 | training loss: 2.0910e-03 | validation loss: 1.7239e-03\n",
      "Epoch: 164860 | training loss: 2.0909e-03 | validation loss: 1.7259e-03\n",
      "Epoch: 164870 | training loss: 2.0909e-03 | validation loss: 1.7242e-03\n",
      "Epoch: 164880 | training loss: 2.0909e-03 | validation loss: 1.7252e-03\n",
      "Epoch: 164890 | training loss: 2.0908e-03 | validation loss: 1.7245e-03\n",
      "Epoch: 164900 | training loss: 2.0908e-03 | validation loss: 1.7249e-03\n",
      "Epoch: 164910 | training loss: 2.0908e-03 | validation loss: 1.7247e-03\n",
      "Epoch: 164920 | training loss: 2.0908e-03 | validation loss: 1.7246e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 164930 | training loss: 2.0907e-03 | validation loss: 1.7246e-03\n",
      "Epoch: 164940 | training loss: 2.0907e-03 | validation loss: 1.7246e-03\n",
      "Epoch: 164950 | training loss: 2.0907e-03 | validation loss: 1.7244e-03\n",
      "Epoch: 164960 | training loss: 2.0946e-03 | validation loss: 1.7253e-03\n",
      "Epoch: 164970 | training loss: 2.4800e-03 | validation loss: 1.9544e-03\n",
      "Epoch: 164980 | training loss: 3.2298e-03 | validation loss: 2.5747e-03\n",
      "Epoch: 164990 | training loss: 2.3962e-03 | validation loss: 1.8882e-03\n",
      "Epoch: 165000 | training loss: 2.1497e-03 | validation loss: 1.7900e-03\n",
      "Epoch: 165010 | training loss: 2.1151e-03 | validation loss: 1.7645e-03\n",
      "Epoch: 165020 | training loss: 2.1008e-03 | validation loss: 1.7202e-03\n",
      "Epoch: 165030 | training loss: 2.0935e-03 | validation loss: 1.7347e-03\n",
      "Epoch: 165040 | training loss: 2.0914e-03 | validation loss: 1.7207e-03\n",
      "Epoch: 165050 | training loss: 2.0909e-03 | validation loss: 1.7276e-03\n",
      "Epoch: 165060 | training loss: 2.0906e-03 | validation loss: 1.7226e-03\n",
      "Epoch: 165070 | training loss: 2.0904e-03 | validation loss: 1.7251e-03\n",
      "Epoch: 165080 | training loss: 2.0904e-03 | validation loss: 1.7251e-03\n",
      "Epoch: 165090 | training loss: 2.0903e-03 | validation loss: 1.7240e-03\n",
      "Epoch: 165100 | training loss: 2.0903e-03 | validation loss: 1.7234e-03\n",
      "Epoch: 165110 | training loss: 2.0904e-03 | validation loss: 1.7223e-03\n",
      "Epoch: 165120 | training loss: 2.0934e-03 | validation loss: 1.7172e-03\n",
      "Epoch: 165130 | training loss: 2.2700e-03 | validation loss: 1.7482e-03\n",
      "Epoch: 165140 | training loss: 2.3966e-03 | validation loss: 1.7998e-03\n",
      "Epoch: 165150 | training loss: 2.2137e-03 | validation loss: 1.7353e-03\n",
      "Epoch: 165160 | training loss: 2.1541e-03 | validation loss: 1.7849e-03\n",
      "Epoch: 165170 | training loss: 2.1073e-03 | validation loss: 1.7465e-03\n",
      "Epoch: 165180 | training loss: 2.1006e-03 | validation loss: 1.7139e-03\n",
      "Epoch: 165190 | training loss: 2.0904e-03 | validation loss: 1.7211e-03\n",
      "Epoch: 165200 | training loss: 2.0918e-03 | validation loss: 1.7299e-03\n",
      "Epoch: 165210 | training loss: 2.0907e-03 | validation loss: 1.7210e-03\n",
      "Epoch: 165220 | training loss: 2.0901e-03 | validation loss: 1.7252e-03\n",
      "Epoch: 165230 | training loss: 2.0900e-03 | validation loss: 1.7230e-03\n",
      "Epoch: 165240 | training loss: 2.0900e-03 | validation loss: 1.7239e-03\n",
      "Epoch: 165250 | training loss: 2.0899e-03 | validation loss: 1.7232e-03\n",
      "Epoch: 165260 | training loss: 2.0899e-03 | validation loss: 1.7239e-03\n",
      "Epoch: 165270 | training loss: 2.0899e-03 | validation loss: 1.7235e-03\n",
      "Epoch: 165280 | training loss: 2.0899e-03 | validation loss: 1.7233e-03\n",
      "Epoch: 165290 | training loss: 2.0898e-03 | validation loss: 1.7232e-03\n",
      "Epoch: 165300 | training loss: 2.0898e-03 | validation loss: 1.7230e-03\n",
      "Epoch: 165310 | training loss: 2.0900e-03 | validation loss: 1.7215e-03\n",
      "Epoch: 165320 | training loss: 2.1023e-03 | validation loss: 1.7148e-03\n",
      "Epoch: 165330 | training loss: 3.0869e-03 | validation loss: 2.0699e-03\n",
      "Epoch: 165340 | training loss: 2.7928e-03 | validation loss: 2.1670e-03\n",
      "Epoch: 165350 | training loss: 2.0988e-03 | validation loss: 1.7454e-03\n",
      "Epoch: 165360 | training loss: 2.1435e-03 | validation loss: 1.7204e-03\n",
      "Epoch: 165370 | training loss: 2.1160e-03 | validation loss: 1.7100e-03\n",
      "Epoch: 165380 | training loss: 2.0900e-03 | validation loss: 1.7227e-03\n",
      "Epoch: 165390 | training loss: 2.0938e-03 | validation loss: 1.7360e-03\n",
      "Epoch: 165400 | training loss: 2.0896e-03 | validation loss: 1.7233e-03\n",
      "Epoch: 165410 | training loss: 2.0901e-03 | validation loss: 1.7197e-03\n",
      "Epoch: 165420 | training loss: 2.0896e-03 | validation loss: 1.7248e-03\n",
      "Epoch: 165430 | training loss: 2.0895e-03 | validation loss: 1.7231e-03\n",
      "Epoch: 165440 | training loss: 2.0895e-03 | validation loss: 1.7226e-03\n",
      "Epoch: 165450 | training loss: 2.0894e-03 | validation loss: 1.7234e-03\n",
      "Epoch: 165460 | training loss: 2.0894e-03 | validation loss: 1.7226e-03\n",
      "Epoch: 165470 | training loss: 2.0894e-03 | validation loss: 1.7231e-03\n",
      "Epoch: 165480 | training loss: 2.0894e-03 | validation loss: 1.7229e-03\n",
      "Epoch: 165490 | training loss: 2.0893e-03 | validation loss: 1.7228e-03\n",
      "Epoch: 165500 | training loss: 2.0893e-03 | validation loss: 1.7230e-03\n",
      "Epoch: 165510 | training loss: 2.0893e-03 | validation loss: 1.7235e-03\n",
      "Epoch: 165520 | training loss: 2.0923e-03 | validation loss: 1.7303e-03\n",
      "Epoch: 165530 | training loss: 2.4211e-03 | validation loss: 2.0074e-03\n",
      "Epoch: 165540 | training loss: 2.3477e-03 | validation loss: 1.8578e-03\n",
      "Epoch: 165550 | training loss: 2.1094e-03 | validation loss: 1.7304e-03\n",
      "Epoch: 165560 | training loss: 2.1096e-03 | validation loss: 1.7576e-03\n",
      "Epoch: 165570 | training loss: 2.4004e-03 | validation loss: 1.9659e-03\n",
      "Epoch: 165580 | training loss: 2.1230e-03 | validation loss: 1.7700e-03\n",
      "Epoch: 165590 | training loss: 2.0933e-03 | validation loss: 1.7190e-03\n",
      "Epoch: 165600 | training loss: 2.0891e-03 | validation loss: 1.7223e-03\n",
      "Epoch: 165610 | training loss: 2.0891e-03 | validation loss: 1.7216e-03\n",
      "Epoch: 165620 | training loss: 2.0896e-03 | validation loss: 1.7251e-03\n",
      "Epoch: 165630 | training loss: 2.0908e-03 | validation loss: 1.7166e-03\n",
      "Epoch: 165640 | training loss: 2.0900e-03 | validation loss: 1.7274e-03\n",
      "Epoch: 165650 | training loss: 2.0890e-03 | validation loss: 1.7238e-03\n",
      "Epoch: 165660 | training loss: 2.0890e-03 | validation loss: 1.7206e-03\n",
      "Epoch: 165670 | training loss: 2.0895e-03 | validation loss: 1.7187e-03\n",
      "Epoch: 165680 | training loss: 2.0961e-03 | validation loss: 1.7134e-03\n",
      "Epoch: 165690 | training loss: 2.2980e-03 | validation loss: 1.7566e-03\n",
      "Epoch: 165700 | training loss: 2.3338e-03 | validation loss: 1.7771e-03\n",
      "Epoch: 165710 | training loss: 2.0909e-03 | validation loss: 1.7199e-03\n",
      "Epoch: 165720 | training loss: 2.1276e-03 | validation loss: 1.7633e-03\n",
      "Epoch: 165730 | training loss: 2.1174e-03 | validation loss: 1.7155e-03\n",
      "Epoch: 165740 | training loss: 2.0974e-03 | validation loss: 1.7388e-03\n",
      "Epoch: 165750 | training loss: 2.0910e-03 | validation loss: 1.7161e-03\n",
      "Epoch: 165760 | training loss: 2.0896e-03 | validation loss: 1.7262e-03\n",
      "Epoch: 165770 | training loss: 2.0892e-03 | validation loss: 1.7193e-03\n",
      "Epoch: 165780 | training loss: 2.0888e-03 | validation loss: 1.7236e-03\n",
      "Epoch: 165790 | training loss: 2.0886e-03 | validation loss: 1.7220e-03\n",
      "Epoch: 165800 | training loss: 2.0886e-03 | validation loss: 1.7207e-03\n",
      "Epoch: 165810 | training loss: 2.0886e-03 | validation loss: 1.7208e-03\n",
      "Epoch: 165820 | training loss: 2.0887e-03 | validation loss: 1.7200e-03\n",
      "Epoch: 165830 | training loss: 2.0915e-03 | validation loss: 1.7161e-03\n",
      "Epoch: 165840 | training loss: 2.2167e-03 | validation loss: 1.7374e-03\n",
      "Epoch: 165850 | training loss: 2.8366e-03 | validation loss: 1.9729e-03\n",
      "Epoch: 165860 | training loss: 2.0900e-03 | validation loss: 1.7181e-03\n",
      "Epoch: 165870 | training loss: 2.1985e-03 | validation loss: 1.8183e-03\n",
      "Epoch: 165880 | training loss: 2.0949e-03 | validation loss: 1.7158e-03\n",
      "Epoch: 165890 | training loss: 2.0966e-03 | validation loss: 1.7114e-03\n",
      "Epoch: 165900 | training loss: 2.0936e-03 | validation loss: 1.7350e-03\n",
      "Epoch: 165910 | training loss: 2.0888e-03 | validation loss: 1.7180e-03\n",
      "Epoch: 165920 | training loss: 2.0883e-03 | validation loss: 1.7218e-03\n",
      "Epoch: 165930 | training loss: 2.0882e-03 | validation loss: 1.7218e-03\n",
      "Epoch: 165940 | training loss: 2.0882e-03 | validation loss: 1.7211e-03\n",
      "Epoch: 165950 | training loss: 2.0882e-03 | validation loss: 1.7212e-03\n",
      "Epoch: 165960 | training loss: 2.0882e-03 | validation loss: 1.7219e-03\n",
      "Epoch: 165970 | training loss: 2.0881e-03 | validation loss: 1.7211e-03\n",
      "Epoch: 165980 | training loss: 2.0881e-03 | validation loss: 1.7215e-03\n",
      "Epoch: 165990 | training loss: 2.0889e-03 | validation loss: 1.7248e-03\n",
      "Epoch: 166000 | training loss: 2.1577e-03 | validation loss: 1.7990e-03\n",
      "Epoch: 166010 | training loss: 2.1372e-03 | validation loss: 1.7884e-03\n",
      "Epoch: 166020 | training loss: 2.2471e-03 | validation loss: 1.8854e-03\n",
      "Epoch: 166030 | training loss: 2.1463e-03 | validation loss: 1.7904e-03\n",
      "Epoch: 166040 | training loss: 2.1302e-03 | validation loss: 1.7616e-03\n",
      "Epoch: 166050 | training loss: 2.1195e-03 | validation loss: 1.7535e-03\n",
      "Epoch: 166060 | training loss: 2.1469e-03 | validation loss: 1.7839e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 166070 | training loss: 2.2979e-03 | validation loss: 1.8904e-03\n",
      "Epoch: 166080 | training loss: 2.1133e-03 | validation loss: 1.7565e-03\n",
      "Epoch: 166090 | training loss: 2.1339e-03 | validation loss: 1.7110e-03\n",
      "Epoch: 166100 | training loss: 2.0916e-03 | validation loss: 1.7131e-03\n",
      "Epoch: 166110 | training loss: 2.0925e-03 | validation loss: 1.7329e-03\n",
      "Epoch: 166120 | training loss: 2.1098e-03 | validation loss: 1.7523e-03\n",
      "Epoch: 166130 | training loss: 2.2426e-03 | validation loss: 1.8496e-03\n",
      "Epoch: 166140 | training loss: 2.3336e-03 | validation loss: 1.9044e-03\n",
      "Epoch: 166150 | training loss: 2.1818e-03 | validation loss: 1.7248e-03\n",
      "Epoch: 166160 | training loss: 2.0963e-03 | validation loss: 1.7365e-03\n",
      "Epoch: 166170 | training loss: 2.0932e-03 | validation loss: 1.7329e-03\n",
      "Epoch: 166180 | training loss: 2.0941e-03 | validation loss: 1.7130e-03\n",
      "Epoch: 166190 | training loss: 2.0921e-03 | validation loss: 1.7135e-03\n",
      "Epoch: 166200 | training loss: 2.0914e-03 | validation loss: 1.7140e-03\n",
      "Epoch: 166210 | training loss: 2.1117e-03 | validation loss: 1.7111e-03\n",
      "Epoch: 166220 | training loss: 2.4628e-03 | validation loss: 1.8181e-03\n",
      "Epoch: 166230 | training loss: 2.0933e-03 | validation loss: 1.7186e-03\n",
      "Epoch: 166240 | training loss: 2.0884e-03 | validation loss: 1.7178e-03\n",
      "Epoch: 166250 | training loss: 2.0880e-03 | validation loss: 1.7244e-03\n",
      "Epoch: 166260 | training loss: 2.0874e-03 | validation loss: 1.7194e-03\n",
      "Epoch: 166270 | training loss: 2.0884e-03 | validation loss: 1.7172e-03\n",
      "Epoch: 166280 | training loss: 2.0897e-03 | validation loss: 1.7270e-03\n",
      "Epoch: 166290 | training loss: 2.0880e-03 | validation loss: 1.7175e-03\n",
      "Epoch: 166300 | training loss: 2.0877e-03 | validation loss: 1.7175e-03\n",
      "Epoch: 166310 | training loss: 2.0873e-03 | validation loss: 1.7196e-03\n",
      "Epoch: 166320 | training loss: 2.0872e-03 | validation loss: 1.7202e-03\n",
      "Epoch: 166330 | training loss: 2.0872e-03 | validation loss: 1.7205e-03\n",
      "Epoch: 166340 | training loss: 2.0876e-03 | validation loss: 1.7228e-03\n",
      "Epoch: 166350 | training loss: 2.1222e-03 | validation loss: 1.7607e-03\n",
      "Epoch: 166360 | training loss: 4.1762e-03 | validation loss: 2.8963e-03\n",
      "Epoch: 166370 | training loss: 2.1030e-03 | validation loss: 1.7183e-03\n",
      "Epoch: 166380 | training loss: 2.2181e-03 | validation loss: 1.7332e-03\n",
      "Epoch: 166390 | training loss: 2.1663e-03 | validation loss: 1.7188e-03\n",
      "Epoch: 166400 | training loss: 2.1076e-03 | validation loss: 1.7111e-03\n",
      "Epoch: 166410 | training loss: 2.0874e-03 | validation loss: 1.7172e-03\n",
      "Epoch: 166420 | training loss: 2.0888e-03 | validation loss: 1.7264e-03\n",
      "Epoch: 166430 | training loss: 2.0882e-03 | validation loss: 1.7248e-03\n",
      "Epoch: 166440 | training loss: 2.0870e-03 | validation loss: 1.7194e-03\n",
      "Epoch: 166450 | training loss: 2.0871e-03 | validation loss: 1.7180e-03\n",
      "Epoch: 166460 | training loss: 2.0869e-03 | validation loss: 1.7200e-03\n",
      "Epoch: 166470 | training loss: 2.0869e-03 | validation loss: 1.7201e-03\n",
      "Epoch: 166480 | training loss: 2.0868e-03 | validation loss: 1.7192e-03\n",
      "Epoch: 166490 | training loss: 2.0868e-03 | validation loss: 1.7197e-03\n",
      "Epoch: 166500 | training loss: 2.0868e-03 | validation loss: 1.7194e-03\n",
      "Epoch: 166510 | training loss: 2.0868e-03 | validation loss: 1.7192e-03\n",
      "Epoch: 166520 | training loss: 2.0872e-03 | validation loss: 1.7171e-03\n",
      "Epoch: 166530 | training loss: 2.1455e-03 | validation loss: 1.7306e-03\n",
      "Epoch: 166540 | training loss: 2.1519e-03 | validation loss: 1.7168e-03\n",
      "Epoch: 166550 | training loss: 2.2255e-03 | validation loss: 1.7959e-03\n",
      "Epoch: 166560 | training loss: 2.1221e-03 | validation loss: 1.7176e-03\n",
      "Epoch: 166570 | training loss: 2.0886e-03 | validation loss: 1.7224e-03\n",
      "Epoch: 166580 | training loss: 2.0887e-03 | validation loss: 1.7242e-03\n",
      "Epoch: 166590 | training loss: 2.0890e-03 | validation loss: 1.7214e-03\n",
      "Epoch: 166600 | training loss: 2.0866e-03 | validation loss: 1.7187e-03\n",
      "Epoch: 166610 | training loss: 2.0867e-03 | validation loss: 1.7177e-03\n",
      "Epoch: 166620 | training loss: 2.0865e-03 | validation loss: 1.7186e-03\n",
      "Epoch: 166630 | training loss: 2.0865e-03 | validation loss: 1.7194e-03\n",
      "Epoch: 166640 | training loss: 2.0864e-03 | validation loss: 1.7188e-03\n",
      "Epoch: 166650 | training loss: 2.0864e-03 | validation loss: 1.7192e-03\n",
      "Epoch: 166660 | training loss: 2.0884e-03 | validation loss: 1.7267e-03\n",
      "Epoch: 166670 | training loss: 2.9414e-03 | validation loss: 2.2677e-03\n",
      "Epoch: 166680 | training loss: 3.3964e-03 | validation loss: 2.1794e-03\n",
      "Epoch: 166690 | training loss: 2.3318e-03 | validation loss: 1.7806e-03\n",
      "Epoch: 166700 | training loss: 2.2009e-03 | validation loss: 1.8132e-03\n",
      "Epoch: 166710 | training loss: 2.1348e-03 | validation loss: 1.7700e-03\n",
      "Epoch: 166720 | training loss: 2.0892e-03 | validation loss: 1.7144e-03\n",
      "Epoch: 166730 | training loss: 2.0915e-03 | validation loss: 1.7090e-03\n",
      "Epoch: 166740 | training loss: 2.0881e-03 | validation loss: 1.7120e-03\n",
      "Epoch: 166750 | training loss: 2.0864e-03 | validation loss: 1.7163e-03\n",
      "Epoch: 166760 | training loss: 2.0863e-03 | validation loss: 1.7186e-03\n",
      "Epoch: 166770 | training loss: 2.0862e-03 | validation loss: 1.7191e-03\n",
      "Epoch: 166780 | training loss: 2.0862e-03 | validation loss: 1.7190e-03\n",
      "Epoch: 166790 | training loss: 2.0861e-03 | validation loss: 1.7188e-03\n",
      "Epoch: 166800 | training loss: 2.0861e-03 | validation loss: 1.7187e-03\n",
      "Epoch: 166810 | training loss: 2.0860e-03 | validation loss: 1.7184e-03\n",
      "Epoch: 166820 | training loss: 2.0860e-03 | validation loss: 1.7183e-03\n",
      "Epoch: 166830 | training loss: 2.0860e-03 | validation loss: 1.7182e-03\n",
      "Epoch: 166840 | training loss: 2.0860e-03 | validation loss: 1.7182e-03\n",
      "Epoch: 166850 | training loss: 2.0859e-03 | validation loss: 1.7182e-03\n",
      "Epoch: 166860 | training loss: 2.0859e-03 | validation loss: 1.7181e-03\n",
      "Epoch: 166870 | training loss: 2.0859e-03 | validation loss: 1.7181e-03\n",
      "Epoch: 166880 | training loss: 2.0859e-03 | validation loss: 1.7181e-03\n",
      "Epoch: 166890 | training loss: 2.0858e-03 | validation loss: 1.7181e-03\n",
      "Epoch: 166900 | training loss: 2.0858e-03 | validation loss: 1.7180e-03\n",
      "Epoch: 166910 | training loss: 2.0858e-03 | validation loss: 1.7180e-03\n",
      "Epoch: 166920 | training loss: 2.0858e-03 | validation loss: 1.7180e-03\n",
      "Epoch: 166930 | training loss: 2.0857e-03 | validation loss: 1.7180e-03\n",
      "Epoch: 166940 | training loss: 2.0857e-03 | validation loss: 1.7179e-03\n",
      "Epoch: 166950 | training loss: 2.0857e-03 | validation loss: 1.7179e-03\n",
      "Epoch: 166960 | training loss: 2.0857e-03 | validation loss: 1.7179e-03\n",
      "Epoch: 166970 | training loss: 2.0856e-03 | validation loss: 1.7178e-03\n",
      "Epoch: 166980 | training loss: 2.0856e-03 | validation loss: 1.7176e-03\n",
      "Epoch: 166990 | training loss: 2.0862e-03 | validation loss: 1.7157e-03\n",
      "Epoch: 167000 | training loss: 2.3314e-03 | validation loss: 1.7935e-03\n",
      "Epoch: 167010 | training loss: 2.4530e-03 | validation loss: 1.9401e-03\n",
      "Epoch: 167020 | training loss: 2.3008e-03 | validation loss: 1.8249e-03\n",
      "Epoch: 167030 | training loss: 2.1213e-03 | validation loss: 1.7390e-03\n",
      "Epoch: 167040 | training loss: 2.1107e-03 | validation loss: 1.7588e-03\n",
      "Epoch: 167050 | training loss: 2.1044e-03 | validation loss: 1.7519e-03\n",
      "Epoch: 167060 | training loss: 2.0888e-03 | validation loss: 1.7289e-03\n",
      "Epoch: 167070 | training loss: 2.0875e-03 | validation loss: 1.7211e-03\n",
      "Epoch: 167080 | training loss: 2.0862e-03 | validation loss: 1.7185e-03\n",
      "Epoch: 167090 | training loss: 2.0855e-03 | validation loss: 1.7177e-03\n",
      "Epoch: 167100 | training loss: 2.0854e-03 | validation loss: 1.7172e-03\n",
      "Epoch: 167110 | training loss: 2.0854e-03 | validation loss: 1.7169e-03\n",
      "Epoch: 167120 | training loss: 2.0853e-03 | validation loss: 1.7170e-03\n",
      "Epoch: 167130 | training loss: 2.0853e-03 | validation loss: 1.7173e-03\n",
      "Epoch: 167140 | training loss: 2.0853e-03 | validation loss: 1.7175e-03\n",
      "Epoch: 167150 | training loss: 2.0852e-03 | validation loss: 1.7174e-03\n",
      "Epoch: 167160 | training loss: 2.0852e-03 | validation loss: 1.7172e-03\n",
      "Epoch: 167170 | training loss: 2.0852e-03 | validation loss: 1.7172e-03\n",
      "Epoch: 167180 | training loss: 2.0852e-03 | validation loss: 1.7172e-03\n",
      "Epoch: 167190 | training loss: 2.0851e-03 | validation loss: 1.7172e-03\n",
      "Epoch: 167200 | training loss: 2.0851e-03 | validation loss: 1.7172e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 167210 | training loss: 2.0851e-03 | validation loss: 1.7172e-03\n",
      "Epoch: 167220 | training loss: 2.0851e-03 | validation loss: 1.7171e-03\n",
      "Epoch: 167230 | training loss: 2.0850e-03 | validation loss: 1.7171e-03\n",
      "Epoch: 167240 | training loss: 2.0850e-03 | validation loss: 1.7171e-03\n",
      "Epoch: 167250 | training loss: 2.0850e-03 | validation loss: 1.7171e-03\n",
      "Epoch: 167260 | training loss: 2.0850e-03 | validation loss: 1.7170e-03\n",
      "Epoch: 167270 | training loss: 2.0849e-03 | validation loss: 1.7170e-03\n",
      "Epoch: 167280 | training loss: 2.0849e-03 | validation loss: 1.7170e-03\n",
      "Epoch: 167290 | training loss: 2.0849e-03 | validation loss: 1.7169e-03\n",
      "Epoch: 167300 | training loss: 2.0848e-03 | validation loss: 1.7169e-03\n",
      "Epoch: 167310 | training loss: 2.0848e-03 | validation loss: 1.7169e-03\n",
      "Epoch: 167320 | training loss: 2.0848e-03 | validation loss: 1.7166e-03\n",
      "Epoch: 167330 | training loss: 2.0857e-03 | validation loss: 1.7125e-03\n",
      "Epoch: 167340 | training loss: 2.6595e-03 | validation loss: 1.8783e-03\n",
      "Epoch: 167350 | training loss: 3.6950e-03 | validation loss: 2.7318e-03\n",
      "Epoch: 167360 | training loss: 2.2079e-03 | validation loss: 1.8351e-03\n",
      "Epoch: 167370 | training loss: 2.2536e-03 | validation loss: 1.7627e-03\n",
      "Epoch: 167380 | training loss: 2.0920e-03 | validation loss: 1.7121e-03\n",
      "Epoch: 167390 | training loss: 2.1098e-03 | validation loss: 1.7579e-03\n",
      "Epoch: 167400 | training loss: 2.0851e-03 | validation loss: 1.7184e-03\n",
      "Epoch: 167410 | training loss: 2.0878e-03 | validation loss: 1.7097e-03\n",
      "Epoch: 167420 | training loss: 2.0847e-03 | validation loss: 1.7151e-03\n",
      "Epoch: 167430 | training loss: 2.0848e-03 | validation loss: 1.7192e-03\n",
      "Epoch: 167440 | training loss: 2.0846e-03 | validation loss: 1.7177e-03\n",
      "Epoch: 167450 | training loss: 2.0845e-03 | validation loss: 1.7160e-03\n",
      "Epoch: 167460 | training loss: 2.0845e-03 | validation loss: 1.7155e-03\n",
      "Epoch: 167470 | training loss: 2.0845e-03 | validation loss: 1.7156e-03\n",
      "Epoch: 167480 | training loss: 2.0844e-03 | validation loss: 1.7159e-03\n",
      "Epoch: 167490 | training loss: 2.0844e-03 | validation loss: 1.7159e-03\n",
      "Epoch: 167500 | training loss: 2.0844e-03 | validation loss: 1.7160e-03\n",
      "Epoch: 167510 | training loss: 2.0844e-03 | validation loss: 1.7160e-03\n",
      "Epoch: 167520 | training loss: 2.0843e-03 | validation loss: 1.7159e-03\n",
      "Epoch: 167530 | training loss: 2.0843e-03 | validation loss: 1.7159e-03\n",
      "Epoch: 167540 | training loss: 2.0843e-03 | validation loss: 1.7159e-03\n",
      "Epoch: 167550 | training loss: 2.0843e-03 | validation loss: 1.7159e-03\n",
      "Epoch: 167560 | training loss: 2.0842e-03 | validation loss: 1.7158e-03\n",
      "Epoch: 167570 | training loss: 2.0842e-03 | validation loss: 1.7158e-03\n",
      "Epoch: 167580 | training loss: 2.0842e-03 | validation loss: 1.7158e-03\n",
      "Epoch: 167590 | training loss: 2.0842e-03 | validation loss: 1.7158e-03\n",
      "Epoch: 167600 | training loss: 2.0841e-03 | validation loss: 1.7158e-03\n",
      "Epoch: 167610 | training loss: 2.0841e-03 | validation loss: 1.7157e-03\n",
      "Epoch: 167620 | training loss: 2.0841e-03 | validation loss: 1.7157e-03\n",
      "Epoch: 167630 | training loss: 2.0841e-03 | validation loss: 1.7157e-03\n",
      "Epoch: 167640 | training loss: 2.0840e-03 | validation loss: 1.7157e-03\n",
      "Epoch: 167650 | training loss: 2.0840e-03 | validation loss: 1.7159e-03\n",
      "Epoch: 167660 | training loss: 2.0897e-03 | validation loss: 1.7228e-03\n",
      "Epoch: 167670 | training loss: 3.8936e-03 | validation loss: 2.7541e-03\n",
      "Epoch: 167680 | training loss: 2.7170e-03 | validation loss: 1.9197e-03\n",
      "Epoch: 167690 | training loss: 2.3607e-03 | validation loss: 1.7653e-03\n",
      "Epoch: 167700 | training loss: 2.1611e-03 | validation loss: 1.7209e-03\n",
      "Epoch: 167710 | training loss: 2.1218e-03 | validation loss: 1.7301e-03\n",
      "Epoch: 167720 | training loss: 2.0917e-03 | validation loss: 1.7157e-03\n",
      "Epoch: 167730 | training loss: 2.0843e-03 | validation loss: 1.7118e-03\n",
      "Epoch: 167740 | training loss: 2.0845e-03 | validation loss: 1.7148e-03\n",
      "Epoch: 167750 | training loss: 2.0844e-03 | validation loss: 1.7166e-03\n",
      "Epoch: 167760 | training loss: 2.0839e-03 | validation loss: 1.7157e-03\n",
      "Epoch: 167770 | training loss: 2.0838e-03 | validation loss: 1.7145e-03\n",
      "Epoch: 167780 | training loss: 2.0837e-03 | validation loss: 1.7147e-03\n",
      "Epoch: 167790 | training loss: 2.0837e-03 | validation loss: 1.7154e-03\n",
      "Epoch: 167800 | training loss: 2.0837e-03 | validation loss: 1.7153e-03\n",
      "Epoch: 167810 | training loss: 2.0837e-03 | validation loss: 1.7150e-03\n",
      "Epoch: 167820 | training loss: 2.0836e-03 | validation loss: 1.7152e-03\n",
      "Epoch: 167830 | training loss: 2.0836e-03 | validation loss: 1.7151e-03\n",
      "Epoch: 167840 | training loss: 2.0836e-03 | validation loss: 1.7151e-03\n",
      "Epoch: 167850 | training loss: 2.0836e-03 | validation loss: 1.7151e-03\n",
      "Epoch: 167860 | training loss: 2.0835e-03 | validation loss: 1.7151e-03\n",
      "Epoch: 167870 | training loss: 2.0835e-03 | validation loss: 1.7150e-03\n",
      "Epoch: 167880 | training loss: 2.0835e-03 | validation loss: 1.7150e-03\n",
      "Epoch: 167890 | training loss: 2.0835e-03 | validation loss: 1.7150e-03\n",
      "Epoch: 167900 | training loss: 2.0834e-03 | validation loss: 1.7150e-03\n",
      "Epoch: 167910 | training loss: 2.0834e-03 | validation loss: 1.7149e-03\n",
      "Epoch: 167920 | training loss: 2.0834e-03 | validation loss: 1.7150e-03\n",
      "Epoch: 167930 | training loss: 2.0834e-03 | validation loss: 1.7153e-03\n",
      "Epoch: 167940 | training loss: 2.0847e-03 | validation loss: 1.7202e-03\n",
      "Epoch: 167950 | training loss: 2.3985e-03 | validation loss: 1.9354e-03\n",
      "Epoch: 167960 | training loss: 2.3437e-03 | validation loss: 1.7707e-03\n",
      "Epoch: 167970 | training loss: 2.0873e-03 | validation loss: 1.7051e-03\n",
      "Epoch: 167980 | training loss: 2.0887e-03 | validation loss: 1.7035e-03\n",
      "Epoch: 167990 | training loss: 2.0888e-03 | validation loss: 1.7040e-03\n",
      "Epoch: 168000 | training loss: 2.0859e-03 | validation loss: 1.7072e-03\n",
      "Epoch: 168010 | training loss: 2.0839e-03 | validation loss: 1.7105e-03\n",
      "Epoch: 168020 | training loss: 2.0833e-03 | validation loss: 1.7136e-03\n",
      "Epoch: 168030 | training loss: 2.0833e-03 | validation loss: 1.7159e-03\n",
      "Epoch: 168040 | training loss: 2.0833e-03 | validation loss: 1.7170e-03\n",
      "Epoch: 168050 | training loss: 2.0832e-03 | validation loss: 1.7166e-03\n",
      "Epoch: 168060 | training loss: 2.0831e-03 | validation loss: 1.7152e-03\n",
      "Epoch: 168070 | training loss: 2.0830e-03 | validation loss: 1.7142e-03\n",
      "Epoch: 168080 | training loss: 2.0830e-03 | validation loss: 1.7141e-03\n",
      "Epoch: 168090 | training loss: 2.0830e-03 | validation loss: 1.7146e-03\n",
      "Epoch: 168100 | training loss: 2.0830e-03 | validation loss: 1.7146e-03\n",
      "Epoch: 168110 | training loss: 2.0829e-03 | validation loss: 1.7144e-03\n",
      "Epoch: 168120 | training loss: 2.0829e-03 | validation loss: 1.7144e-03\n",
      "Epoch: 168130 | training loss: 2.0829e-03 | validation loss: 1.7144e-03\n",
      "Epoch: 168140 | training loss: 2.0829e-03 | validation loss: 1.7144e-03\n",
      "Epoch: 168150 | training loss: 2.0828e-03 | validation loss: 1.7144e-03\n",
      "Epoch: 168160 | training loss: 2.0828e-03 | validation loss: 1.7143e-03\n",
      "Epoch: 168170 | training loss: 2.0828e-03 | validation loss: 1.7143e-03\n",
      "Epoch: 168180 | training loss: 2.0827e-03 | validation loss: 1.7143e-03\n",
      "Epoch: 168190 | training loss: 2.0827e-03 | validation loss: 1.7142e-03\n",
      "Epoch: 168200 | training loss: 2.0827e-03 | validation loss: 1.7142e-03\n",
      "Epoch: 168210 | training loss: 2.0827e-03 | validation loss: 1.7142e-03\n",
      "Epoch: 168220 | training loss: 2.0826e-03 | validation loss: 1.7141e-03\n",
      "Epoch: 168230 | training loss: 2.0826e-03 | validation loss: 1.7138e-03\n",
      "Epoch: 168240 | training loss: 2.0848e-03 | validation loss: 1.7109e-03\n",
      "Epoch: 168250 | training loss: 2.6562e-03 | validation loss: 2.0339e-03\n",
      "Epoch: 168260 | training loss: 2.2877e-03 | validation loss: 1.8339e-03\n",
      "Epoch: 168270 | training loss: 2.5088e-03 | validation loss: 1.8450e-03\n",
      "Epoch: 168280 | training loss: 2.1233e-03 | validation loss: 1.7626e-03\n",
      "Epoch: 168290 | training loss: 2.0880e-03 | validation loss: 1.7069e-03\n",
      "Epoch: 168300 | training loss: 2.0928e-03 | validation loss: 1.7030e-03\n",
      "Epoch: 168310 | training loss: 2.0934e-03 | validation loss: 1.7285e-03\n",
      "Epoch: 168320 | training loss: 2.0834e-03 | validation loss: 1.7099e-03\n",
      "Epoch: 168330 | training loss: 2.0852e-03 | validation loss: 1.7062e-03\n",
      "Epoch: 168340 | training loss: 2.0866e-03 | validation loss: 1.7053e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 168350 | training loss: 2.1091e-03 | validation loss: 1.7019e-03\n",
      "Epoch: 168360 | training loss: 2.4458e-03 | validation loss: 1.7980e-03\n",
      "Epoch: 168370 | training loss: 2.0835e-03 | validation loss: 1.7091e-03\n",
      "Epoch: 168380 | training loss: 2.0835e-03 | validation loss: 1.7186e-03\n",
      "Epoch: 168390 | training loss: 2.0834e-03 | validation loss: 1.7089e-03\n",
      "Epoch: 168400 | training loss: 2.0853e-03 | validation loss: 1.7224e-03\n",
      "Epoch: 168410 | training loss: 2.0870e-03 | validation loss: 1.7059e-03\n",
      "Epoch: 168420 | training loss: 2.0843e-03 | validation loss: 1.7208e-03\n",
      "Epoch: 168430 | training loss: 2.0824e-03 | validation loss: 1.7155e-03\n",
      "Epoch: 168440 | training loss: 2.0824e-03 | validation loss: 1.7109e-03\n",
      "Epoch: 168450 | training loss: 2.0834e-03 | validation loss: 1.7088e-03\n",
      "Epoch: 168460 | training loss: 2.0943e-03 | validation loss: 1.7035e-03\n",
      "Epoch: 168470 | training loss: 2.3599e-03 | validation loss: 1.7714e-03\n",
      "Epoch: 168480 | training loss: 2.1617e-03 | validation loss: 1.7160e-03\n",
      "Epoch: 168490 | training loss: 2.0820e-03 | validation loss: 1.7124e-03\n",
      "Epoch: 168500 | training loss: 2.0987e-03 | validation loss: 1.7373e-03\n",
      "Epoch: 168510 | training loss: 2.0972e-03 | validation loss: 1.7033e-03\n",
      "Epoch: 168520 | training loss: 2.0890e-03 | validation loss: 1.7273e-03\n",
      "Epoch: 168530 | training loss: 2.0843e-03 | validation loss: 1.7080e-03\n",
      "Epoch: 168540 | training loss: 2.0822e-03 | validation loss: 1.7156e-03\n",
      "Epoch: 168550 | training loss: 2.0819e-03 | validation loss: 1.7141e-03\n",
      "Epoch: 168560 | training loss: 2.0821e-03 | validation loss: 1.7109e-03\n",
      "Epoch: 168570 | training loss: 2.0818e-03 | validation loss: 1.7122e-03\n",
      "Epoch: 168580 | training loss: 2.0818e-03 | validation loss: 1.7129e-03\n",
      "Epoch: 168590 | training loss: 2.0818e-03 | validation loss: 1.7134e-03\n",
      "Epoch: 168600 | training loss: 2.0820e-03 | validation loss: 1.7154e-03\n",
      "Epoch: 168610 | training loss: 2.0999e-03 | validation loss: 1.7391e-03\n",
      "Epoch: 168620 | training loss: 3.3669e-03 | validation loss: 2.4725e-03\n",
      "Epoch: 168630 | training loss: 2.7276e-03 | validation loss: 1.9220e-03\n",
      "Epoch: 168640 | training loss: 2.1441e-03 | validation loss: 1.7041e-03\n",
      "Epoch: 168650 | training loss: 2.1044e-03 | validation loss: 1.7347e-03\n",
      "Epoch: 168660 | training loss: 2.1087e-03 | validation loss: 1.7469e-03\n",
      "Epoch: 168670 | training loss: 2.0830e-03 | validation loss: 1.7201e-03\n",
      "Epoch: 168680 | training loss: 2.0846e-03 | validation loss: 1.7075e-03\n",
      "Epoch: 168690 | training loss: 2.0818e-03 | validation loss: 1.7100e-03\n",
      "Epoch: 168700 | training loss: 2.0819e-03 | validation loss: 1.7160e-03\n",
      "Epoch: 168710 | training loss: 2.0815e-03 | validation loss: 1.7122e-03\n",
      "Epoch: 168720 | training loss: 2.0815e-03 | validation loss: 1.7119e-03\n",
      "Epoch: 168730 | training loss: 2.0814e-03 | validation loss: 1.7133e-03\n",
      "Epoch: 168740 | training loss: 2.0814e-03 | validation loss: 1.7120e-03\n",
      "Epoch: 168750 | training loss: 2.0814e-03 | validation loss: 1.7128e-03\n",
      "Epoch: 168760 | training loss: 2.0813e-03 | validation loss: 1.7123e-03\n",
      "Epoch: 168770 | training loss: 2.0813e-03 | validation loss: 1.7124e-03\n",
      "Epoch: 168780 | training loss: 2.0813e-03 | validation loss: 1.7125e-03\n",
      "Epoch: 168790 | training loss: 2.0813e-03 | validation loss: 1.7124e-03\n",
      "Epoch: 168800 | training loss: 2.0812e-03 | validation loss: 1.7123e-03\n",
      "Epoch: 168810 | training loss: 2.0812e-03 | validation loss: 1.7123e-03\n",
      "Epoch: 168820 | training loss: 2.0812e-03 | validation loss: 1.7122e-03\n",
      "Epoch: 168830 | training loss: 2.0855e-03 | validation loss: 1.7150e-03\n",
      "Epoch: 168840 | training loss: 2.6078e-03 | validation loss: 2.0320e-03\n",
      "Epoch: 168850 | training loss: 2.9322e-03 | validation loss: 2.0536e-03\n",
      "Epoch: 168860 | training loss: 2.1933e-03 | validation loss: 1.7705e-03\n",
      "Epoch: 168870 | training loss: 2.1109e-03 | validation loss: 1.7467e-03\n",
      "Epoch: 168880 | training loss: 2.1074e-03 | validation loss: 1.7127e-03\n",
      "Epoch: 168890 | training loss: 2.0902e-03 | validation loss: 1.7309e-03\n",
      "Epoch: 168900 | training loss: 2.0838e-03 | validation loss: 1.7195e-03\n",
      "Epoch: 168910 | training loss: 2.0818e-03 | validation loss: 1.7122e-03\n",
      "Epoch: 168920 | training loss: 2.0812e-03 | validation loss: 1.7143e-03\n",
      "Epoch: 168930 | training loss: 2.0810e-03 | validation loss: 1.7115e-03\n",
      "Epoch: 168940 | training loss: 2.0809e-03 | validation loss: 1.7115e-03\n",
      "Epoch: 168950 | training loss: 2.0809e-03 | validation loss: 1.7126e-03\n",
      "Epoch: 168960 | training loss: 2.0809e-03 | validation loss: 1.7117e-03\n",
      "Epoch: 168970 | training loss: 2.0808e-03 | validation loss: 1.7113e-03\n",
      "Epoch: 168980 | training loss: 2.0808e-03 | validation loss: 1.7109e-03\n",
      "Epoch: 168990 | training loss: 2.0811e-03 | validation loss: 1.7091e-03\n",
      "Epoch: 169000 | training loss: 2.0919e-03 | validation loss: 1.7015e-03\n",
      "Epoch: 169010 | training loss: 2.7337e-03 | validation loss: 1.9054e-03\n",
      "Epoch: 169020 | training loss: 2.3973e-03 | validation loss: 1.9418e-03\n",
      "Epoch: 169030 | training loss: 2.2206e-03 | validation loss: 1.7323e-03\n",
      "Epoch: 169040 | training loss: 2.1118e-03 | validation loss: 1.7052e-03\n",
      "Epoch: 169050 | training loss: 2.0995e-03 | validation loss: 1.7397e-03\n",
      "Epoch: 169060 | training loss: 2.0823e-03 | validation loss: 1.7183e-03\n",
      "Epoch: 169070 | training loss: 2.0844e-03 | validation loss: 1.7055e-03\n",
      "Epoch: 169080 | training loss: 2.0811e-03 | validation loss: 1.7147e-03\n",
      "Epoch: 169090 | training loss: 2.0805e-03 | validation loss: 1.7111e-03\n",
      "Epoch: 169100 | training loss: 2.0805e-03 | validation loss: 1.7106e-03\n",
      "Epoch: 169110 | training loss: 2.0805e-03 | validation loss: 1.7118e-03\n",
      "Epoch: 169120 | training loss: 2.0805e-03 | validation loss: 1.7111e-03\n",
      "Epoch: 169130 | training loss: 2.0804e-03 | validation loss: 1.7111e-03\n",
      "Epoch: 169140 | training loss: 2.0804e-03 | validation loss: 1.7115e-03\n",
      "Epoch: 169150 | training loss: 2.0804e-03 | validation loss: 1.7111e-03\n",
      "Epoch: 169160 | training loss: 2.0804e-03 | validation loss: 1.7110e-03\n",
      "Epoch: 169170 | training loss: 2.0803e-03 | validation loss: 1.7109e-03\n",
      "Epoch: 169180 | training loss: 2.0804e-03 | validation loss: 1.7103e-03\n",
      "Epoch: 169190 | training loss: 2.0816e-03 | validation loss: 1.7070e-03\n",
      "Epoch: 169200 | training loss: 2.1724e-03 | validation loss: 1.7153e-03\n",
      "Epoch: 169210 | training loss: 3.2550e-03 | validation loss: 2.1374e-03\n",
      "Epoch: 169220 | training loss: 2.1401e-03 | validation loss: 1.7013e-03\n",
      "Epoch: 169230 | training loss: 2.1585e-03 | validation loss: 1.7773e-03\n",
      "Epoch: 169240 | training loss: 2.1277e-03 | validation loss: 1.7646e-03\n",
      "Epoch: 169250 | training loss: 2.0824e-03 | validation loss: 1.7187e-03\n",
      "Epoch: 169260 | training loss: 2.0860e-03 | validation loss: 1.7028e-03\n",
      "Epoch: 169270 | training loss: 2.0807e-03 | validation loss: 1.7071e-03\n",
      "Epoch: 169280 | training loss: 2.0809e-03 | validation loss: 1.7162e-03\n",
      "Epoch: 169290 | training loss: 2.0801e-03 | validation loss: 1.7105e-03\n",
      "Epoch: 169300 | training loss: 2.0801e-03 | validation loss: 1.7095e-03\n",
      "Epoch: 169310 | training loss: 2.0800e-03 | validation loss: 1.7120e-03\n",
      "Epoch: 169320 | training loss: 2.0800e-03 | validation loss: 1.7100e-03\n",
      "Epoch: 169330 | training loss: 2.0800e-03 | validation loss: 1.7111e-03\n",
      "Epoch: 169340 | training loss: 2.0799e-03 | validation loss: 1.7104e-03\n",
      "Epoch: 169350 | training loss: 2.0799e-03 | validation loss: 1.7108e-03\n",
      "Epoch: 169360 | training loss: 2.0799e-03 | validation loss: 1.7106e-03\n",
      "Epoch: 169370 | training loss: 2.0798e-03 | validation loss: 1.7105e-03\n",
      "Epoch: 169380 | training loss: 2.0798e-03 | validation loss: 1.7106e-03\n",
      "Epoch: 169390 | training loss: 2.0798e-03 | validation loss: 1.7107e-03\n",
      "Epoch: 169400 | training loss: 2.0801e-03 | validation loss: 1.7121e-03\n",
      "Epoch: 169410 | training loss: 2.1150e-03 | validation loss: 1.7480e-03\n",
      "Epoch: 169420 | training loss: 2.3779e-03 | validation loss: 1.9107e-03\n",
      "Epoch: 169430 | training loss: 2.5814e-03 | validation loss: 1.8987e-03\n",
      "Epoch: 169440 | training loss: 2.1169e-03 | validation loss: 1.7648e-03\n",
      "Epoch: 169450 | training loss: 2.0830e-03 | validation loss: 1.7184e-03\n",
      "Epoch: 169460 | training loss: 2.0854e-03 | validation loss: 1.7023e-03\n",
      "Epoch: 169470 | training loss: 2.0887e-03 | validation loss: 1.7265e-03\n",
      "Epoch: 169480 | training loss: 2.0818e-03 | validation loss: 1.7032e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 169490 | training loss: 2.0817e-03 | validation loss: 1.7033e-03\n",
      "Epoch: 169500 | training loss: 2.0798e-03 | validation loss: 1.7074e-03\n",
      "Epoch: 169510 | training loss: 2.0796e-03 | validation loss: 1.7087e-03\n",
      "Epoch: 169520 | training loss: 2.0809e-03 | validation loss: 1.7055e-03\n",
      "Epoch: 169530 | training loss: 2.1349e-03 | validation loss: 1.7019e-03\n",
      "Epoch: 169540 | training loss: 3.3028e-03 | validation loss: 2.1347e-03\n",
      "Epoch: 169550 | training loss: 2.2972e-03 | validation loss: 1.8687e-03\n",
      "Epoch: 169560 | training loss: 2.1116e-03 | validation loss: 1.7467e-03\n",
      "Epoch: 169570 | training loss: 2.1238e-03 | validation loss: 1.7035e-03\n",
      "Epoch: 169580 | training loss: 2.0795e-03 | validation loss: 1.7120e-03\n",
      "Epoch: 169590 | training loss: 2.0839e-03 | validation loss: 1.7219e-03\n",
      "Epoch: 169600 | training loss: 2.0816e-03 | validation loss: 1.7047e-03\n",
      "Epoch: 169610 | training loss: 2.0798e-03 | validation loss: 1.7125e-03\n",
      "Epoch: 169620 | training loss: 2.0793e-03 | validation loss: 1.7084e-03\n",
      "Epoch: 169630 | training loss: 2.0793e-03 | validation loss: 1.7107e-03\n",
      "Epoch: 169640 | training loss: 2.0792e-03 | validation loss: 1.7089e-03\n",
      "Epoch: 169650 | training loss: 2.0792e-03 | validation loss: 1.7102e-03\n",
      "Epoch: 169660 | training loss: 2.0791e-03 | validation loss: 1.7096e-03\n",
      "Epoch: 169670 | training loss: 2.0791e-03 | validation loss: 1.7092e-03\n",
      "Epoch: 169680 | training loss: 2.0791e-03 | validation loss: 1.7093e-03\n",
      "Epoch: 169690 | training loss: 2.0791e-03 | validation loss: 1.7090e-03\n",
      "Epoch: 169700 | training loss: 2.0793e-03 | validation loss: 1.7075e-03\n",
      "Epoch: 169710 | training loss: 2.0916e-03 | validation loss: 1.7009e-03\n",
      "Epoch: 169720 | training loss: 2.9869e-03 | validation loss: 2.0192e-03\n",
      "Epoch: 169730 | training loss: 2.7021e-03 | validation loss: 2.1108e-03\n",
      "Epoch: 169740 | training loss: 2.0899e-03 | validation loss: 1.7173e-03\n",
      "Epoch: 169750 | training loss: 2.1548e-03 | validation loss: 1.7076e-03\n",
      "Epoch: 169760 | training loss: 2.0918e-03 | validation loss: 1.6960e-03\n",
      "Epoch: 169770 | training loss: 2.0832e-03 | validation loss: 1.7212e-03\n",
      "Epoch: 169780 | training loss: 2.0815e-03 | validation loss: 1.7196e-03\n",
      "Epoch: 169790 | training loss: 2.0797e-03 | validation loss: 1.7046e-03\n",
      "Epoch: 169800 | training loss: 2.0789e-03 | validation loss: 1.7086e-03\n",
      "Epoch: 169810 | training loss: 2.0790e-03 | validation loss: 1.7112e-03\n",
      "Epoch: 169820 | training loss: 2.0788e-03 | validation loss: 1.7081e-03\n",
      "Epoch: 169830 | training loss: 2.0787e-03 | validation loss: 1.7096e-03\n",
      "Epoch: 169840 | training loss: 2.0787e-03 | validation loss: 1.7089e-03\n",
      "Epoch: 169850 | training loss: 2.0787e-03 | validation loss: 1.7094e-03\n",
      "Epoch: 169860 | training loss: 2.0787e-03 | validation loss: 1.7088e-03\n",
      "Epoch: 169870 | training loss: 2.0786e-03 | validation loss: 1.7091e-03\n",
      "Epoch: 169880 | training loss: 2.0786e-03 | validation loss: 1.7089e-03\n",
      "Epoch: 169890 | training loss: 2.0786e-03 | validation loss: 1.7083e-03\n",
      "Epoch: 169900 | training loss: 2.0823e-03 | validation loss: 1.7061e-03\n",
      "Epoch: 169910 | training loss: 2.4068e-03 | validation loss: 1.8844e-03\n",
      "Epoch: 169920 | training loss: 2.3142e-03 | validation loss: 1.9243e-03\n",
      "Epoch: 169930 | training loss: 2.1115e-03 | validation loss: 1.7571e-03\n",
      "Epoch: 169940 | training loss: 2.2285e-03 | validation loss: 1.8244e-03\n",
      "Epoch: 169950 | training loss: 2.2334e-03 | validation loss: 1.8278e-03\n",
      "Epoch: 169960 | training loss: 2.1083e-03 | validation loss: 1.6951e-03\n",
      "Epoch: 169970 | training loss: 2.0932e-03 | validation loss: 1.6992e-03\n",
      "Epoch: 169980 | training loss: 2.0826e-03 | validation loss: 1.7215e-03\n",
      "Epoch: 169990 | training loss: 2.0939e-03 | validation loss: 1.7345e-03\n",
      "Epoch: 170000 | training loss: 2.1431e-03 | validation loss: 1.7754e-03\n",
      "Epoch: 170010 | training loss: 2.3754e-03 | validation loss: 1.9276e-03\n",
      "Epoch: 170020 | training loss: 2.0783e-03 | validation loss: 1.7082e-03\n",
      "Epoch: 170030 | training loss: 2.1162e-03 | validation loss: 1.6989e-03\n",
      "Epoch: 170040 | training loss: 2.0931e-03 | validation loss: 1.7321e-03\n",
      "Epoch: 170050 | training loss: 2.0842e-03 | validation loss: 1.7220e-03\n",
      "Epoch: 170060 | training loss: 2.0782e-03 | validation loss: 1.7072e-03\n",
      "Epoch: 170070 | training loss: 2.0810e-03 | validation loss: 1.7022e-03\n",
      "Epoch: 170080 | training loss: 2.1141e-03 | validation loss: 1.6992e-03\n",
      "Epoch: 170090 | training loss: 2.6885e-03 | validation loss: 1.8920e-03\n",
      "Epoch: 170100 | training loss: 2.1878e-03 | validation loss: 1.8068e-03\n",
      "Epoch: 170110 | training loss: 2.1524e-03 | validation loss: 1.7095e-03\n",
      "Epoch: 170120 | training loss: 2.1163e-03 | validation loss: 1.7481e-03\n",
      "Epoch: 170130 | training loss: 2.0934e-03 | validation loss: 1.7013e-03\n",
      "Epoch: 170140 | training loss: 2.0834e-03 | validation loss: 1.7197e-03\n",
      "Epoch: 170150 | training loss: 2.0790e-03 | validation loss: 1.7044e-03\n",
      "Epoch: 170160 | training loss: 2.0780e-03 | validation loss: 1.7075e-03\n",
      "Epoch: 170170 | training loss: 2.0784e-03 | validation loss: 1.7110e-03\n",
      "Epoch: 170180 | training loss: 2.0779e-03 | validation loss: 1.7090e-03\n",
      "Epoch: 170190 | training loss: 2.0778e-03 | validation loss: 1.7080e-03\n",
      "Epoch: 170200 | training loss: 2.0778e-03 | validation loss: 1.7072e-03\n",
      "Epoch: 170210 | training loss: 2.0786e-03 | validation loss: 1.7048e-03\n",
      "Epoch: 170220 | training loss: 2.1244e-03 | validation loss: 1.7033e-03\n",
      "Epoch: 170230 | training loss: 3.7242e-03 | validation loss: 2.3368e-03\n",
      "Epoch: 170240 | training loss: 2.1935e-03 | validation loss: 1.8030e-03\n",
      "Epoch: 170250 | training loss: 2.2520e-03 | validation loss: 1.8454e-03\n",
      "Epoch: 170260 | training loss: 2.0811e-03 | validation loss: 1.7175e-03\n",
      "Epoch: 170270 | training loss: 2.0950e-03 | validation loss: 1.6979e-03\n",
      "Epoch: 170280 | training loss: 2.0808e-03 | validation loss: 1.7023e-03\n",
      "Epoch: 170290 | training loss: 2.0797e-03 | validation loss: 1.7147e-03\n",
      "Epoch: 170300 | training loss: 2.0777e-03 | validation loss: 1.7092e-03\n",
      "Epoch: 170310 | training loss: 2.0780e-03 | validation loss: 1.7052e-03\n",
      "Epoch: 170320 | training loss: 2.0776e-03 | validation loss: 1.7091e-03\n",
      "Epoch: 170330 | training loss: 2.0775e-03 | validation loss: 1.7072e-03\n",
      "Epoch: 170340 | training loss: 2.0775e-03 | validation loss: 1.7076e-03\n",
      "Epoch: 170350 | training loss: 2.0774e-03 | validation loss: 1.7075e-03\n",
      "Epoch: 170360 | training loss: 2.0774e-03 | validation loss: 1.7076e-03\n",
      "Epoch: 170370 | training loss: 2.0774e-03 | validation loss: 1.7073e-03\n",
      "Epoch: 170380 | training loss: 2.0774e-03 | validation loss: 1.7073e-03\n",
      "Epoch: 170390 | training loss: 2.0778e-03 | validation loss: 1.7052e-03\n",
      "Epoch: 170400 | training loss: 2.1588e-03 | validation loss: 1.7301e-03\n",
      "Epoch: 170410 | training loss: 2.1365e-03 | validation loss: 1.7093e-03\n",
      "Epoch: 170420 | training loss: 2.1320e-03 | validation loss: 1.7297e-03\n",
      "Epoch: 170430 | training loss: 2.1188e-03 | validation loss: 1.7275e-03\n",
      "Epoch: 170440 | training loss: 2.0950e-03 | validation loss: 1.7045e-03\n",
      "Epoch: 170450 | training loss: 2.0862e-03 | validation loss: 1.6987e-03\n",
      "Epoch: 170460 | training loss: 2.0874e-03 | validation loss: 1.6955e-03\n",
      "Epoch: 170470 | training loss: 2.1740e-03 | validation loss: 1.7068e-03\n",
      "Epoch: 170480 | training loss: 2.5811e-03 | validation loss: 1.8443e-03\n",
      "Epoch: 170490 | training loss: 2.2412e-03 | validation loss: 1.8425e-03\n",
      "Epoch: 170500 | training loss: 2.1356e-03 | validation loss: 1.7004e-03\n",
      "Epoch: 170510 | training loss: 2.0957e-03 | validation loss: 1.7349e-03\n",
      "Epoch: 170520 | training loss: 2.0797e-03 | validation loss: 1.7008e-03\n",
      "Epoch: 170530 | training loss: 2.0774e-03 | validation loss: 1.7040e-03\n",
      "Epoch: 170540 | training loss: 2.0789e-03 | validation loss: 1.7138e-03\n",
      "Epoch: 170550 | training loss: 2.0772e-03 | validation loss: 1.7091e-03\n",
      "Epoch: 170560 | training loss: 2.0769e-03 | validation loss: 1.7065e-03\n",
      "Epoch: 170570 | training loss: 2.0771e-03 | validation loss: 1.7050e-03\n",
      "Epoch: 170580 | training loss: 2.0804e-03 | validation loss: 1.7000e-03\n",
      "Epoch: 170590 | training loss: 2.2573e-03 | validation loss: 1.7327e-03\n",
      "Epoch: 170600 | training loss: 2.4129e-03 | validation loss: 1.7969e-03\n",
      "Epoch: 170610 | training loss: 2.1567e-03 | validation loss: 1.7029e-03\n",
      "Epoch: 170620 | training loss: 2.1659e-03 | validation loss: 1.7797e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 170630 | training loss: 2.0806e-03 | validation loss: 1.7143e-03\n",
      "Epoch: 170640 | training loss: 2.0908e-03 | validation loss: 1.7006e-03\n",
      "Epoch: 170650 | training loss: 2.0778e-03 | validation loss: 1.7122e-03\n",
      "Epoch: 170660 | training loss: 2.0770e-03 | validation loss: 1.7084e-03\n",
      "Epoch: 170670 | training loss: 2.0772e-03 | validation loss: 1.7034e-03\n",
      "Epoch: 170680 | training loss: 2.0769e-03 | validation loss: 1.7089e-03\n",
      "Epoch: 170690 | training loss: 2.0767e-03 | validation loss: 1.7052e-03\n",
      "Epoch: 170700 | training loss: 2.0766e-03 | validation loss: 1.7070e-03\n",
      "Epoch: 170710 | training loss: 2.0766e-03 | validation loss: 1.7062e-03\n",
      "Epoch: 170720 | training loss: 2.0766e-03 | validation loss: 1.7061e-03\n",
      "Epoch: 170730 | training loss: 2.0765e-03 | validation loss: 1.7065e-03\n",
      "Epoch: 170740 | training loss: 2.0765e-03 | validation loss: 1.7065e-03\n",
      "Epoch: 170750 | training loss: 2.0765e-03 | validation loss: 1.7066e-03\n",
      "Epoch: 170760 | training loss: 2.0766e-03 | validation loss: 1.7076e-03\n",
      "Epoch: 170770 | training loss: 2.0806e-03 | validation loss: 1.7163e-03\n",
      "Epoch: 170780 | training loss: 2.3963e-03 | validation loss: 1.9293e-03\n",
      "Epoch: 170790 | training loss: 2.0775e-03 | validation loss: 1.7014e-03\n",
      "Epoch: 170800 | training loss: 2.3983e-03 | validation loss: 1.9311e-03\n",
      "Epoch: 170810 | training loss: 2.1057e-03 | validation loss: 1.7442e-03\n",
      "Epoch: 170820 | training loss: 2.0920e-03 | validation loss: 1.6964e-03\n",
      "Epoch: 170830 | training loss: 2.0888e-03 | validation loss: 1.6968e-03\n",
      "Epoch: 170840 | training loss: 2.0769e-03 | validation loss: 1.7101e-03\n",
      "Epoch: 170850 | training loss: 2.0778e-03 | validation loss: 1.7118e-03\n",
      "Epoch: 170860 | training loss: 2.0766e-03 | validation loss: 1.7037e-03\n",
      "Epoch: 170870 | training loss: 2.0762e-03 | validation loss: 1.7056e-03\n",
      "Epoch: 170880 | training loss: 2.0762e-03 | validation loss: 1.7070e-03\n",
      "Epoch: 170890 | training loss: 2.0762e-03 | validation loss: 1.7051e-03\n",
      "Epoch: 170900 | training loss: 2.0761e-03 | validation loss: 1.7064e-03\n",
      "Epoch: 170910 | training loss: 2.0761e-03 | validation loss: 1.7055e-03\n",
      "Epoch: 170920 | training loss: 2.0761e-03 | validation loss: 1.7059e-03\n",
      "Epoch: 170930 | training loss: 2.0760e-03 | validation loss: 1.7055e-03\n",
      "Epoch: 170940 | training loss: 2.0763e-03 | validation loss: 1.7040e-03\n",
      "Epoch: 170950 | training loss: 2.1008e-03 | validation loss: 1.7042e-03\n",
      "Epoch: 170960 | training loss: 2.5219e-03 | validation loss: 1.9236e-03\n",
      "Epoch: 170970 | training loss: 2.1313e-03 | validation loss: 1.7195e-03\n",
      "Epoch: 170980 | training loss: 2.0857e-03 | validation loss: 1.7279e-03\n",
      "Epoch: 170990 | training loss: 2.0910e-03 | validation loss: 1.7285e-03\n",
      "Epoch: 171000 | training loss: 2.0826e-03 | validation loss: 1.7130e-03\n",
      "Epoch: 171010 | training loss: 2.0791e-03 | validation loss: 1.6991e-03\n",
      "Epoch: 171020 | training loss: 2.1243e-03 | validation loss: 1.6943e-03\n",
      "Epoch: 171030 | training loss: 2.7896e-03 | validation loss: 1.9211e-03\n",
      "Epoch: 171040 | training loss: 2.2960e-03 | validation loss: 1.8743e-03\n",
      "Epoch: 171050 | training loss: 2.1749e-03 | validation loss: 1.7104e-03\n",
      "Epoch: 171060 | training loss: 2.1092e-03 | validation loss: 1.7475e-03\n",
      "Epoch: 171070 | training loss: 2.0867e-03 | validation loss: 1.6962e-03\n",
      "Epoch: 171080 | training loss: 2.0804e-03 | validation loss: 1.7163e-03\n",
      "Epoch: 171090 | training loss: 2.0778e-03 | validation loss: 1.6996e-03\n",
      "Epoch: 171100 | training loss: 2.0760e-03 | validation loss: 1.7079e-03\n",
      "Epoch: 171110 | training loss: 2.0758e-03 | validation loss: 1.7068e-03\n",
      "Epoch: 171120 | training loss: 2.0757e-03 | validation loss: 1.7037e-03\n",
      "Epoch: 171130 | training loss: 2.0758e-03 | validation loss: 1.7029e-03\n",
      "Epoch: 171140 | training loss: 2.0771e-03 | validation loss: 1.7002e-03\n",
      "Epoch: 171150 | training loss: 2.1075e-03 | validation loss: 1.6957e-03\n",
      "Epoch: 171160 | training loss: 2.9034e-03 | validation loss: 1.9763e-03\n",
      "Epoch: 171170 | training loss: 2.3794e-03 | validation loss: 1.9232e-03\n",
      "Epoch: 171180 | training loss: 2.2003e-03 | validation loss: 1.7262e-03\n",
      "Epoch: 171190 | training loss: 2.0842e-03 | validation loss: 1.7137e-03\n",
      "Epoch: 171200 | training loss: 2.0773e-03 | validation loss: 1.7119e-03\n",
      "Epoch: 171210 | training loss: 2.0791e-03 | validation loss: 1.7008e-03\n",
      "Epoch: 171220 | training loss: 2.0774e-03 | validation loss: 1.7098e-03\n",
      "Epoch: 171230 | training loss: 2.0760e-03 | validation loss: 1.7028e-03\n",
      "Epoch: 171240 | training loss: 2.0754e-03 | validation loss: 1.7056e-03\n",
      "Epoch: 171250 | training loss: 2.0753e-03 | validation loss: 1.7055e-03\n",
      "Epoch: 171260 | training loss: 2.0753e-03 | validation loss: 1.7035e-03\n",
      "Epoch: 171270 | training loss: 2.0752e-03 | validation loss: 1.7045e-03\n",
      "Epoch: 171280 | training loss: 2.0752e-03 | validation loss: 1.7049e-03\n",
      "Epoch: 171290 | training loss: 2.0752e-03 | validation loss: 1.7054e-03\n",
      "Epoch: 171300 | training loss: 2.0761e-03 | validation loss: 1.7085e-03\n",
      "Epoch: 171310 | training loss: 2.1215e-03 | validation loss: 1.7518e-03\n",
      "Epoch: 171320 | training loss: 3.6113e-03 | validation loss: 2.5885e-03\n",
      "Epoch: 171330 | training loss: 2.2759e-03 | validation loss: 1.7432e-03\n",
      "Epoch: 171340 | training loss: 2.2044e-03 | validation loss: 1.7355e-03\n",
      "Epoch: 171350 | training loss: 2.0793e-03 | validation loss: 1.7134e-03\n",
      "Epoch: 171360 | training loss: 2.0968e-03 | validation loss: 1.7271e-03\n",
      "Epoch: 171370 | training loss: 2.0752e-03 | validation loss: 1.7044e-03\n",
      "Epoch: 171380 | training loss: 2.0775e-03 | validation loss: 1.7005e-03\n",
      "Epoch: 171390 | training loss: 2.0755e-03 | validation loss: 1.7072e-03\n",
      "Epoch: 171400 | training loss: 2.0749e-03 | validation loss: 1.7043e-03\n",
      "Epoch: 171410 | training loss: 2.0749e-03 | validation loss: 1.7036e-03\n",
      "Epoch: 171420 | training loss: 2.0749e-03 | validation loss: 1.7050e-03\n",
      "Epoch: 171430 | training loss: 2.0748e-03 | validation loss: 1.7039e-03\n",
      "Epoch: 171440 | training loss: 2.0748e-03 | validation loss: 1.7045e-03\n",
      "Epoch: 171450 | training loss: 2.0748e-03 | validation loss: 1.7043e-03\n",
      "Epoch: 171460 | training loss: 2.0748e-03 | validation loss: 1.7051e-03\n",
      "Epoch: 171470 | training loss: 2.0780e-03 | validation loss: 1.7152e-03\n",
      "Epoch: 171480 | training loss: 2.4387e-03 | validation loss: 2.0450e-03\n",
      "Epoch: 171490 | training loss: 2.3850e-03 | validation loss: 1.8795e-03\n",
      "Epoch: 171500 | training loss: 2.1308e-03 | validation loss: 1.7013e-03\n",
      "Epoch: 171510 | training loss: 2.0872e-03 | validation loss: 1.7244e-03\n",
      "Epoch: 171520 | training loss: 2.0838e-03 | validation loss: 1.7021e-03\n",
      "Epoch: 171530 | training loss: 2.0792e-03 | validation loss: 1.7156e-03\n",
      "Epoch: 171540 | training loss: 2.0767e-03 | validation loss: 1.7124e-03\n",
      "Epoch: 171550 | training loss: 2.0745e-03 | validation loss: 1.7034e-03\n",
      "Epoch: 171560 | training loss: 2.0750e-03 | validation loss: 1.7002e-03\n",
      "Epoch: 171570 | training loss: 2.0771e-03 | validation loss: 1.6972e-03\n",
      "Epoch: 171580 | training loss: 2.1432e-03 | validation loss: 1.6990e-03\n",
      "Epoch: 171590 | training loss: 3.0471e-03 | validation loss: 2.0280e-03\n",
      "Epoch: 171600 | training loss: 2.3306e-03 | validation loss: 1.8878e-03\n",
      "Epoch: 171610 | training loss: 2.0792e-03 | validation loss: 1.6964e-03\n",
      "Epoch: 171620 | training loss: 2.0904e-03 | validation loss: 1.6958e-03\n",
      "Epoch: 171630 | training loss: 2.0889e-03 | validation loss: 1.7269e-03\n",
      "Epoch: 171640 | training loss: 2.0794e-03 | validation loss: 1.6964e-03\n",
      "Epoch: 171650 | training loss: 2.0759e-03 | validation loss: 1.7089e-03\n",
      "Epoch: 171660 | training loss: 2.0750e-03 | validation loss: 1.7004e-03\n",
      "Epoch: 171670 | training loss: 2.0746e-03 | validation loss: 1.7060e-03\n",
      "Epoch: 171680 | training loss: 2.0743e-03 | validation loss: 1.7022e-03\n",
      "Epoch: 171690 | training loss: 2.0742e-03 | validation loss: 1.7028e-03\n",
      "Epoch: 171700 | training loss: 2.0742e-03 | validation loss: 1.7038e-03\n",
      "Epoch: 171710 | training loss: 2.0742e-03 | validation loss: 1.7041e-03\n",
      "Epoch: 171720 | training loss: 2.0744e-03 | validation loss: 1.7054e-03\n",
      "Epoch: 171730 | training loss: 2.0794e-03 | validation loss: 1.7152e-03\n",
      "Epoch: 171740 | training loss: 2.3222e-03 | validation loss: 1.8857e-03\n",
      "Epoch: 171750 | training loss: 2.1933e-03 | validation loss: 1.7984e-03\n",
      "Epoch: 171760 | training loss: 2.2056e-03 | validation loss: 1.8169e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 171770 | training loss: 2.1391e-03 | validation loss: 1.7054e-03\n",
      "Epoch: 171780 | training loss: 2.0813e-03 | validation loss: 1.6916e-03\n",
      "Epoch: 171790 | training loss: 2.0857e-03 | validation loss: 1.7241e-03\n",
      "Epoch: 171800 | training loss: 2.0752e-03 | validation loss: 1.7011e-03\n",
      "Epoch: 171810 | training loss: 2.0743e-03 | validation loss: 1.7000e-03\n",
      "Epoch: 171820 | training loss: 2.0743e-03 | validation loss: 1.7069e-03\n",
      "Epoch: 171830 | training loss: 2.0741e-03 | validation loss: 1.7005e-03\n",
      "Epoch: 171840 | training loss: 2.0739e-03 | validation loss: 1.7045e-03\n",
      "Epoch: 171850 | training loss: 2.0738e-03 | validation loss: 1.7023e-03\n",
      "Epoch: 171860 | training loss: 2.0738e-03 | validation loss: 1.7026e-03\n",
      "Epoch: 171870 | training loss: 2.0738e-03 | validation loss: 1.7032e-03\n",
      "Epoch: 171880 | training loss: 2.0737e-03 | validation loss: 1.7029e-03\n",
      "Epoch: 171890 | training loss: 2.0737e-03 | validation loss: 1.7027e-03\n",
      "Epoch: 171900 | training loss: 2.0737e-03 | validation loss: 1.7028e-03\n",
      "Epoch: 171910 | training loss: 2.0763e-03 | validation loss: 1.7064e-03\n",
      "Epoch: 171920 | training loss: 2.3452e-03 | validation loss: 1.9082e-03\n",
      "Epoch: 171930 | training loss: 2.5793e-03 | validation loss: 1.9046e-03\n",
      "Epoch: 171940 | training loss: 2.1371e-03 | validation loss: 1.6963e-03\n",
      "Epoch: 171950 | training loss: 2.1030e-03 | validation loss: 1.7354e-03\n",
      "Epoch: 171960 | training loss: 2.0891e-03 | validation loss: 1.6898e-03\n",
      "Epoch: 171970 | training loss: 2.0843e-03 | validation loss: 1.7232e-03\n",
      "Epoch: 171980 | training loss: 2.0786e-03 | validation loss: 1.6964e-03\n",
      "Epoch: 171990 | training loss: 2.0740e-03 | validation loss: 1.7051e-03\n",
      "Epoch: 172000 | training loss: 2.0750e-03 | validation loss: 1.7097e-03\n",
      "Epoch: 172010 | training loss: 2.0741e-03 | validation loss: 1.7069e-03\n",
      "Epoch: 172020 | training loss: 2.0750e-03 | validation loss: 1.7085e-03\n",
      "Epoch: 172030 | training loss: 2.0940e-03 | validation loss: 1.7329e-03\n",
      "Epoch: 172040 | training loss: 2.5890e-03 | validation loss: 2.0535e-03\n",
      "Epoch: 172050 | training loss: 2.1336e-03 | validation loss: 1.6941e-03\n",
      "Epoch: 172060 | training loss: 2.1691e-03 | validation loss: 1.7876e-03\n",
      "Epoch: 172070 | training loss: 2.1247e-03 | validation loss: 1.6950e-03\n",
      "Epoch: 172080 | training loss: 2.0883e-03 | validation loss: 1.7262e-03\n",
      "Epoch: 172090 | training loss: 2.0774e-03 | validation loss: 1.6958e-03\n",
      "Epoch: 172100 | training loss: 2.0750e-03 | validation loss: 1.7086e-03\n",
      "Epoch: 172110 | training loss: 2.0742e-03 | validation loss: 1.6980e-03\n",
      "Epoch: 172120 | training loss: 2.0735e-03 | validation loss: 1.7047e-03\n",
      "Epoch: 172130 | training loss: 2.0731e-03 | validation loss: 1.7024e-03\n",
      "Epoch: 172140 | training loss: 2.0732e-03 | validation loss: 1.7006e-03\n",
      "Epoch: 172150 | training loss: 2.0732e-03 | validation loss: 1.7004e-03\n",
      "Epoch: 172160 | training loss: 2.0736e-03 | validation loss: 1.6990e-03\n",
      "Epoch: 172170 | training loss: 2.0829e-03 | validation loss: 1.6932e-03\n",
      "Epoch: 172180 | training loss: 2.4485e-03 | validation loss: 1.7985e-03\n",
      "Epoch: 172190 | training loss: 2.0776e-03 | validation loss: 1.7132e-03\n",
      "Epoch: 172200 | training loss: 2.2119e-03 | validation loss: 1.7168e-03\n",
      "Epoch: 172210 | training loss: 2.1318e-03 | validation loss: 1.7541e-03\n",
      "Epoch: 172220 | training loss: 2.0749e-03 | validation loss: 1.7073e-03\n",
      "Epoch: 172230 | training loss: 2.0775e-03 | validation loss: 1.6948e-03\n",
      "Epoch: 172240 | training loss: 2.0763e-03 | validation loss: 1.7090e-03\n",
      "Epoch: 172250 | training loss: 2.0743e-03 | validation loss: 1.6997e-03\n",
      "Epoch: 172260 | training loss: 2.0734e-03 | validation loss: 1.7035e-03\n",
      "Epoch: 172270 | training loss: 2.0730e-03 | validation loss: 1.7009e-03\n",
      "Epoch: 172280 | training loss: 2.0728e-03 | validation loss: 1.7020e-03\n",
      "Epoch: 172290 | training loss: 2.0728e-03 | validation loss: 1.7019e-03\n",
      "Epoch: 172300 | training loss: 2.0727e-03 | validation loss: 1.7013e-03\n",
      "Epoch: 172310 | training loss: 2.0727e-03 | validation loss: 1.7011e-03\n",
      "Epoch: 172320 | training loss: 2.0727e-03 | validation loss: 1.7008e-03\n",
      "Epoch: 172330 | training loss: 2.0732e-03 | validation loss: 1.6992e-03\n",
      "Epoch: 172340 | training loss: 2.0950e-03 | validation loss: 1.6973e-03\n",
      "Epoch: 172350 | training loss: 3.1566e-03 | validation loss: 2.1177e-03\n",
      "Epoch: 172360 | training loss: 2.5763e-03 | validation loss: 2.0157e-03\n",
      "Epoch: 172370 | training loss: 2.1251e-03 | validation loss: 1.7017e-03\n",
      "Epoch: 172380 | training loss: 2.0996e-03 | validation loss: 1.7226e-03\n",
      "Epoch: 172390 | training loss: 2.0935e-03 | validation loss: 1.7255e-03\n",
      "Epoch: 172400 | training loss: 2.0739e-03 | validation loss: 1.6963e-03\n",
      "Epoch: 172410 | training loss: 2.0753e-03 | validation loss: 1.7053e-03\n",
      "Epoch: 172420 | training loss: 2.0728e-03 | validation loss: 1.6988e-03\n",
      "Epoch: 172430 | training loss: 2.0728e-03 | validation loss: 1.7035e-03\n",
      "Epoch: 172440 | training loss: 2.0724e-03 | validation loss: 1.7005e-03\n",
      "Epoch: 172450 | training loss: 2.0724e-03 | validation loss: 1.7008e-03\n",
      "Epoch: 172460 | training loss: 2.0724e-03 | validation loss: 1.7016e-03\n",
      "Epoch: 172470 | training loss: 2.0723e-03 | validation loss: 1.7014e-03\n",
      "Epoch: 172480 | training loss: 2.0723e-03 | validation loss: 1.7016e-03\n",
      "Epoch: 172490 | training loss: 2.0724e-03 | validation loss: 1.7028e-03\n",
      "Epoch: 172500 | training loss: 2.0760e-03 | validation loss: 1.7137e-03\n",
      "Epoch: 172510 | training loss: 2.3422e-03 | validation loss: 1.9366e-03\n",
      "Epoch: 172520 | training loss: 2.1579e-03 | validation loss: 1.7386e-03\n",
      "Epoch: 172530 | training loss: 2.1875e-03 | validation loss: 1.8284e-03\n",
      "Epoch: 172540 | training loss: 2.1437e-03 | validation loss: 1.7686e-03\n",
      "Epoch: 172550 | training loss: 2.0801e-03 | validation loss: 1.7192e-03\n",
      "Epoch: 172560 | training loss: 2.0756e-03 | validation loss: 1.6924e-03\n",
      "Epoch: 172570 | training loss: 2.0750e-03 | validation loss: 1.6965e-03\n",
      "Epoch: 172580 | training loss: 2.0721e-03 | validation loss: 1.6995e-03\n",
      "Epoch: 172590 | training loss: 2.0725e-03 | validation loss: 1.7043e-03\n",
      "Epoch: 172600 | training loss: 2.0721e-03 | validation loss: 1.6997e-03\n",
      "Epoch: 172610 | training loss: 2.0720e-03 | validation loss: 1.6998e-03\n",
      "Epoch: 172620 | training loss: 2.0720e-03 | validation loss: 1.7013e-03\n",
      "Epoch: 172630 | training loss: 2.0719e-03 | validation loss: 1.7006e-03\n",
      "Epoch: 172640 | training loss: 2.0719e-03 | validation loss: 1.7008e-03\n",
      "Epoch: 172650 | training loss: 2.0722e-03 | validation loss: 1.7023e-03\n",
      "Epoch: 172660 | training loss: 2.0841e-03 | validation loss: 1.7180e-03\n",
      "Epoch: 172670 | training loss: 2.9371e-03 | validation loss: 2.2206e-03\n",
      "Epoch: 172680 | training loss: 2.6311e-03 | validation loss: 1.8855e-03\n",
      "Epoch: 172690 | training loss: 2.1082e-03 | validation loss: 1.7532e-03\n",
      "Epoch: 172700 | training loss: 2.1507e-03 | validation loss: 1.7770e-03\n",
      "Epoch: 172710 | training loss: 2.0740e-03 | validation loss: 1.7005e-03\n",
      "Epoch: 172720 | training loss: 2.0822e-03 | validation loss: 1.6891e-03\n",
      "Epoch: 172730 | training loss: 2.0718e-03 | validation loss: 1.7005e-03\n",
      "Epoch: 172740 | training loss: 2.0731e-03 | validation loss: 1.7069e-03\n",
      "Epoch: 172750 | training loss: 2.0720e-03 | validation loss: 1.6976e-03\n",
      "Epoch: 172760 | training loss: 2.0716e-03 | validation loss: 1.7002e-03\n",
      "Epoch: 172770 | training loss: 2.0716e-03 | validation loss: 1.7008e-03\n",
      "Epoch: 172780 | training loss: 2.0716e-03 | validation loss: 1.6995e-03\n",
      "Epoch: 172790 | training loss: 2.0716e-03 | validation loss: 1.7003e-03\n",
      "Epoch: 172800 | training loss: 2.0715e-03 | validation loss: 1.7000e-03\n",
      "Epoch: 172810 | training loss: 2.0715e-03 | validation loss: 1.6999e-03\n",
      "Epoch: 172820 | training loss: 2.0715e-03 | validation loss: 1.7000e-03\n",
      "Epoch: 172830 | training loss: 2.0715e-03 | validation loss: 1.7000e-03\n",
      "Epoch: 172840 | training loss: 2.0714e-03 | validation loss: 1.7000e-03\n",
      "Epoch: 172850 | training loss: 2.0714e-03 | validation loss: 1.7000e-03\n",
      "Epoch: 172860 | training loss: 2.0714e-03 | validation loss: 1.7004e-03\n",
      "Epoch: 172870 | training loss: 2.0722e-03 | validation loss: 1.7041e-03\n",
      "Epoch: 172880 | training loss: 2.1683e-03 | validation loss: 1.7881e-03\n",
      "Epoch: 172890 | training loss: 3.1611e-03 | validation loss: 2.3455e-03\n",
      "Epoch: 172900 | training loss: 2.5181e-03 | validation loss: 2.0423e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 172910 | training loss: 2.1446e-03 | validation loss: 1.7821e-03\n",
      "Epoch: 172920 | training loss: 2.0779e-03 | validation loss: 1.7116e-03\n",
      "Epoch: 172930 | training loss: 2.0722e-03 | validation loss: 1.6976e-03\n",
      "Epoch: 172940 | training loss: 2.0752e-03 | validation loss: 1.6910e-03\n",
      "Epoch: 172950 | training loss: 2.0742e-03 | validation loss: 1.6923e-03\n",
      "Epoch: 172960 | training loss: 2.0716e-03 | validation loss: 1.6962e-03\n",
      "Epoch: 172970 | training loss: 2.0712e-03 | validation loss: 1.7011e-03\n",
      "Epoch: 172980 | training loss: 2.0712e-03 | validation loss: 1.7013e-03\n",
      "Epoch: 172990 | training loss: 2.0711e-03 | validation loss: 1.6993e-03\n",
      "Epoch: 173000 | training loss: 2.0711e-03 | validation loss: 1.6988e-03\n",
      "Epoch: 173010 | training loss: 2.0710e-03 | validation loss: 1.6997e-03\n",
      "Epoch: 173020 | training loss: 2.0710e-03 | validation loss: 1.6996e-03\n",
      "Epoch: 173030 | training loss: 2.0710e-03 | validation loss: 1.6998e-03\n",
      "Epoch: 173040 | training loss: 2.0735e-03 | validation loss: 1.7049e-03\n",
      "Epoch: 173050 | training loss: 2.2848e-03 | validation loss: 1.8772e-03\n",
      "Epoch: 173060 | training loss: 2.2129e-03 | validation loss: 1.7596e-03\n",
      "Epoch: 173070 | training loss: 2.0846e-03 | validation loss: 1.7173e-03\n",
      "Epoch: 173080 | training loss: 2.0986e-03 | validation loss: 1.7362e-03\n",
      "Epoch: 173090 | training loss: 2.0717e-03 | validation loss: 1.7017e-03\n",
      "Epoch: 173100 | training loss: 2.0738e-03 | validation loss: 1.6944e-03\n",
      "Epoch: 173110 | training loss: 2.0713e-03 | validation loss: 1.6998e-03\n",
      "Epoch: 173120 | training loss: 2.0712e-03 | validation loss: 1.7008e-03\n",
      "Epoch: 173130 | training loss: 2.0708e-03 | validation loss: 1.6975e-03\n",
      "Epoch: 173140 | training loss: 2.0709e-03 | validation loss: 1.6969e-03\n",
      "Epoch: 173150 | training loss: 2.0716e-03 | validation loss: 1.6954e-03\n",
      "Epoch: 173160 | training loss: 2.0948e-03 | validation loss: 1.6880e-03\n",
      "Epoch: 173170 | training loss: 2.9338e-03 | validation loss: 1.9786e-03\n",
      "Epoch: 173180 | training loss: 2.4397e-03 | validation loss: 1.9585e-03\n",
      "Epoch: 173190 | training loss: 2.1796e-03 | validation loss: 1.7231e-03\n",
      "Epoch: 173200 | training loss: 2.0761e-03 | validation loss: 1.6890e-03\n",
      "Epoch: 173210 | training loss: 2.0870e-03 | validation loss: 1.7187e-03\n",
      "Epoch: 173220 | training loss: 2.0776e-03 | validation loss: 1.6976e-03\n",
      "Epoch: 173230 | training loss: 2.0715e-03 | validation loss: 1.7015e-03\n",
      "Epoch: 173240 | training loss: 2.0706e-03 | validation loss: 1.6969e-03\n",
      "Epoch: 173250 | training loss: 2.0705e-03 | validation loss: 1.7002e-03\n",
      "Epoch: 173260 | training loss: 2.0705e-03 | validation loss: 1.6972e-03\n",
      "Epoch: 173270 | training loss: 2.0705e-03 | validation loss: 1.6998e-03\n",
      "Epoch: 173280 | training loss: 2.0704e-03 | validation loss: 1.6980e-03\n",
      "Epoch: 173290 | training loss: 2.0704e-03 | validation loss: 1.6981e-03\n",
      "Epoch: 173300 | training loss: 2.0703e-03 | validation loss: 1.6983e-03\n",
      "Epoch: 173310 | training loss: 2.0703e-03 | validation loss: 1.6981e-03\n",
      "Epoch: 173320 | training loss: 2.0704e-03 | validation loss: 1.6974e-03\n",
      "Epoch: 173330 | training loss: 2.0734e-03 | validation loss: 1.6935e-03\n",
      "Epoch: 173340 | training loss: 2.3465e-03 | validation loss: 1.7723e-03\n",
      "Epoch: 173350 | training loss: 2.0911e-03 | validation loss: 1.6859e-03\n",
      "Epoch: 173360 | training loss: 2.4067e-03 | validation loss: 1.8037e-03\n",
      "Epoch: 173370 | training loss: 2.1425e-03 | validation loss: 1.7321e-03\n",
      "Epoch: 173380 | training loss: 2.0709e-03 | validation loss: 1.7031e-03\n",
      "Epoch: 173390 | training loss: 2.0820e-03 | validation loss: 1.7065e-03\n",
      "Epoch: 173400 | training loss: 2.0741e-03 | validation loss: 1.7041e-03\n",
      "Epoch: 173410 | training loss: 2.0703e-03 | validation loss: 1.6979e-03\n",
      "Epoch: 173420 | training loss: 2.0708e-03 | validation loss: 1.6970e-03\n",
      "Epoch: 173430 | training loss: 2.0701e-03 | validation loss: 1.6988e-03\n",
      "Epoch: 173440 | training loss: 2.0701e-03 | validation loss: 1.6984e-03\n",
      "Epoch: 173450 | training loss: 2.0700e-03 | validation loss: 1.6978e-03\n",
      "Epoch: 173460 | training loss: 2.0700e-03 | validation loss: 1.6983e-03\n",
      "Epoch: 173470 | training loss: 2.0699e-03 | validation loss: 1.6980e-03\n",
      "Epoch: 173480 | training loss: 2.0699e-03 | validation loss: 1.6979e-03\n",
      "Epoch: 173490 | training loss: 2.0699e-03 | validation loss: 1.6978e-03\n",
      "Epoch: 173500 | training loss: 2.0699e-03 | validation loss: 1.6976e-03\n",
      "Epoch: 173510 | training loss: 2.0700e-03 | validation loss: 1.6957e-03\n",
      "Epoch: 173520 | training loss: 2.0833e-03 | validation loss: 1.6866e-03\n",
      "Epoch: 173530 | training loss: 2.9569e-03 | validation loss: 2.0745e-03\n",
      "Epoch: 173540 | training loss: 2.1928e-03 | validation loss: 1.8224e-03\n",
      "Epoch: 173550 | training loss: 2.1821e-03 | validation loss: 1.7689e-03\n",
      "Epoch: 173560 | training loss: 2.1160e-03 | validation loss: 1.7613e-03\n",
      "Epoch: 173570 | training loss: 2.0824e-03 | validation loss: 1.7033e-03\n",
      "Epoch: 173580 | training loss: 2.0721e-03 | validation loss: 1.7065e-03\n",
      "Epoch: 173590 | training loss: 2.0708e-03 | validation loss: 1.6922e-03\n",
      "Epoch: 173600 | training loss: 2.0702e-03 | validation loss: 1.6958e-03\n",
      "Epoch: 173610 | training loss: 2.0698e-03 | validation loss: 1.6987e-03\n",
      "Epoch: 173620 | training loss: 2.0696e-03 | validation loss: 1.6982e-03\n",
      "Epoch: 173630 | training loss: 2.0696e-03 | validation loss: 1.6968e-03\n",
      "Epoch: 173640 | training loss: 2.0698e-03 | validation loss: 1.6948e-03\n",
      "Epoch: 173650 | training loss: 2.0767e-03 | validation loss: 1.6893e-03\n",
      "Epoch: 173660 | training loss: 2.4250e-03 | validation loss: 1.7856e-03\n",
      "Epoch: 173670 | training loss: 2.0724e-03 | validation loss: 1.7069e-03\n",
      "Epoch: 173680 | training loss: 2.2875e-03 | validation loss: 1.7437e-03\n",
      "Epoch: 173690 | training loss: 2.0894e-03 | validation loss: 1.7211e-03\n",
      "Epoch: 173700 | training loss: 2.0885e-03 | validation loss: 1.7209e-03\n",
      "Epoch: 173710 | training loss: 2.0784e-03 | validation loss: 1.6893e-03\n",
      "Epoch: 173720 | training loss: 2.0694e-03 | validation loss: 1.6988e-03\n",
      "Epoch: 173730 | training loss: 2.0699e-03 | validation loss: 1.7007e-03\n",
      "Epoch: 173740 | training loss: 2.0698e-03 | validation loss: 1.6943e-03\n",
      "Epoch: 173750 | training loss: 2.0695e-03 | validation loss: 1.6991e-03\n",
      "Epoch: 173760 | training loss: 2.0693e-03 | validation loss: 1.6961e-03\n",
      "Epoch: 173770 | training loss: 2.0692e-03 | validation loss: 1.6975e-03\n",
      "Epoch: 173780 | training loss: 2.0692e-03 | validation loss: 1.6971e-03\n",
      "Epoch: 173790 | training loss: 2.0692e-03 | validation loss: 1.6966e-03\n",
      "Epoch: 173800 | training loss: 2.0691e-03 | validation loss: 1.6969e-03\n",
      "Epoch: 173810 | training loss: 2.0691e-03 | validation loss: 1.6970e-03\n",
      "Epoch: 173820 | training loss: 2.0691e-03 | validation loss: 1.6972e-03\n",
      "Epoch: 173830 | training loss: 2.0692e-03 | validation loss: 1.6982e-03\n",
      "Epoch: 173840 | training loss: 2.0753e-03 | validation loss: 1.7095e-03\n",
      "Epoch: 173850 | training loss: 2.6873e-03 | validation loss: 2.0891e-03\n",
      "Epoch: 173860 | training loss: 2.5507e-03 | validation loss: 1.8386e-03\n",
      "Epoch: 173870 | training loss: 2.0991e-03 | validation loss: 1.7292e-03\n",
      "Epoch: 173880 | training loss: 2.1627e-03 | validation loss: 1.7804e-03\n",
      "Epoch: 173890 | training loss: 2.1000e-03 | validation loss: 1.7354e-03\n",
      "Epoch: 173900 | training loss: 2.0690e-03 | validation loss: 1.6976e-03\n",
      "Epoch: 173910 | training loss: 2.0731e-03 | validation loss: 1.6903e-03\n",
      "Epoch: 173920 | training loss: 2.0695e-03 | validation loss: 1.6938e-03\n",
      "Epoch: 173930 | training loss: 2.0693e-03 | validation loss: 1.6996e-03\n",
      "Epoch: 173940 | training loss: 2.0689e-03 | validation loss: 1.6976e-03\n",
      "Epoch: 173950 | training loss: 2.0689e-03 | validation loss: 1.6954e-03\n",
      "Epoch: 173960 | training loss: 2.0688e-03 | validation loss: 1.6971e-03\n",
      "Epoch: 173970 | training loss: 2.0687e-03 | validation loss: 1.6965e-03\n",
      "Epoch: 173980 | training loss: 2.0687e-03 | validation loss: 1.6964e-03\n",
      "Epoch: 173990 | training loss: 2.0687e-03 | validation loss: 1.6965e-03\n",
      "Epoch: 174000 | training loss: 2.0687e-03 | validation loss: 1.6964e-03\n",
      "Epoch: 174010 | training loss: 2.0686e-03 | validation loss: 1.6963e-03\n",
      "Epoch: 174020 | training loss: 2.0686e-03 | validation loss: 1.6958e-03\n",
      "Epoch: 174030 | training loss: 2.0714e-03 | validation loss: 1.6920e-03\n",
      "Epoch: 174040 | training loss: 2.4287e-03 | validation loss: 1.8755e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 174050 | training loss: 2.3638e-03 | validation loss: 1.9141e-03\n",
      "Epoch: 174060 | training loss: 2.1281e-03 | validation loss: 1.7683e-03\n",
      "Epoch: 174070 | training loss: 2.0705e-03 | validation loss: 1.7030e-03\n",
      "Epoch: 174080 | training loss: 2.0723e-03 | validation loss: 1.6879e-03\n",
      "Epoch: 174090 | training loss: 2.0751e-03 | validation loss: 1.6871e-03\n",
      "Epoch: 174100 | training loss: 2.0823e-03 | validation loss: 1.6836e-03\n",
      "Epoch: 174110 | training loss: 2.2396e-03 | validation loss: 1.7168e-03\n",
      "Epoch: 174120 | training loss: 2.3482e-03 | validation loss: 1.7556e-03\n",
      "Epoch: 174130 | training loss: 2.1779e-03 | validation loss: 1.7948e-03\n",
      "Epoch: 174140 | training loss: 2.1100e-03 | validation loss: 1.6866e-03\n",
      "Epoch: 174150 | training loss: 2.0873e-03 | validation loss: 1.7233e-03\n",
      "Epoch: 174160 | training loss: 2.0763e-03 | validation loss: 1.6870e-03\n",
      "Epoch: 174170 | training loss: 2.0691e-03 | validation loss: 1.7004e-03\n",
      "Epoch: 174180 | training loss: 2.0691e-03 | validation loss: 1.7003e-03\n",
      "Epoch: 174190 | training loss: 2.0684e-03 | validation loss: 1.6940e-03\n",
      "Epoch: 174200 | training loss: 2.0691e-03 | validation loss: 1.6920e-03\n",
      "Epoch: 174210 | training loss: 2.0738e-03 | validation loss: 1.6879e-03\n",
      "Epoch: 174220 | training loss: 2.1749e-03 | validation loss: 1.7011e-03\n",
      "Epoch: 174230 | training loss: 2.7542e-03 | validation loss: 1.9125e-03\n",
      "Epoch: 174240 | training loss: 2.2618e-03 | validation loss: 1.8402e-03\n",
      "Epoch: 174250 | training loss: 2.0986e-03 | validation loss: 1.6916e-03\n",
      "Epoch: 174260 | training loss: 2.0697e-03 | validation loss: 1.7014e-03\n",
      "Epoch: 174270 | training loss: 2.0682e-03 | validation loss: 1.6938e-03\n",
      "Epoch: 174280 | training loss: 2.0681e-03 | validation loss: 1.6972e-03\n",
      "Epoch: 174290 | training loss: 2.0684e-03 | validation loss: 1.6930e-03\n",
      "Epoch: 174300 | training loss: 2.0687e-03 | validation loss: 1.6991e-03\n",
      "Epoch: 174310 | training loss: 2.0680e-03 | validation loss: 1.6944e-03\n",
      "Epoch: 174320 | training loss: 2.0681e-03 | validation loss: 1.6938e-03\n",
      "Epoch: 174330 | training loss: 2.0680e-03 | validation loss: 1.6942e-03\n",
      "Epoch: 174340 | training loss: 2.0682e-03 | validation loss: 1.6933e-03\n",
      "Epoch: 174350 | training loss: 2.0741e-03 | validation loss: 1.6884e-03\n",
      "Epoch: 174360 | training loss: 2.3384e-03 | validation loss: 1.7597e-03\n",
      "Epoch: 174370 | training loss: 2.1466e-03 | validation loss: 1.7024e-03\n",
      "Epoch: 174380 | training loss: 2.1891e-03 | validation loss: 1.7061e-03\n",
      "Epoch: 174390 | training loss: 2.1382e-03 | validation loss: 1.7617e-03\n",
      "Epoch: 174400 | training loss: 2.0692e-03 | validation loss: 1.7023e-03\n",
      "Epoch: 174410 | training loss: 2.0791e-03 | validation loss: 1.6853e-03\n",
      "Epoch: 174420 | training loss: 2.0709e-03 | validation loss: 1.7050e-03\n",
      "Epoch: 174430 | training loss: 2.0679e-03 | validation loss: 1.6930e-03\n",
      "Epoch: 174440 | training loss: 2.0676e-03 | validation loss: 1.6954e-03\n",
      "Epoch: 174450 | training loss: 2.0676e-03 | validation loss: 1.6953e-03\n",
      "Epoch: 174460 | training loss: 2.0676e-03 | validation loss: 1.6952e-03\n",
      "Epoch: 174470 | training loss: 2.0676e-03 | validation loss: 1.6944e-03\n",
      "Epoch: 174480 | training loss: 2.0675e-03 | validation loss: 1.6955e-03\n",
      "Epoch: 174490 | training loss: 2.0675e-03 | validation loss: 1.6948e-03\n",
      "Epoch: 174500 | training loss: 2.0679e-03 | validation loss: 1.6928e-03\n",
      "Epoch: 174510 | training loss: 2.1000e-03 | validation loss: 1.6968e-03\n",
      "Epoch: 174520 | training loss: 2.3985e-03 | validation loss: 1.8475e-03\n",
      "Epoch: 174530 | training loss: 2.1469e-03 | validation loss: 1.6983e-03\n",
      "Epoch: 174540 | training loss: 2.0907e-03 | validation loss: 1.7009e-03\n",
      "Epoch: 174550 | training loss: 2.0878e-03 | validation loss: 1.7115e-03\n",
      "Epoch: 174560 | training loss: 2.0733e-03 | validation loss: 1.6909e-03\n",
      "Epoch: 174570 | training loss: 2.1295e-03 | validation loss: 1.6859e-03\n",
      "Epoch: 174580 | training loss: 2.7849e-03 | validation loss: 1.9132e-03\n",
      "Epoch: 174590 | training loss: 2.2976e-03 | validation loss: 1.8678e-03\n",
      "Epoch: 174600 | training loss: 2.1571e-03 | validation loss: 1.6977e-03\n",
      "Epoch: 174610 | training loss: 2.0971e-03 | validation loss: 1.7336e-03\n",
      "Epoch: 174620 | training loss: 2.0779e-03 | validation loss: 1.6859e-03\n",
      "Epoch: 174630 | training loss: 2.0720e-03 | validation loss: 1.7056e-03\n",
      "Epoch: 174640 | training loss: 2.0689e-03 | validation loss: 1.6893e-03\n",
      "Epoch: 174650 | training loss: 2.0672e-03 | validation loss: 1.6955e-03\n",
      "Epoch: 174660 | training loss: 2.0675e-03 | validation loss: 1.6973e-03\n",
      "Epoch: 174670 | training loss: 2.0671e-03 | validation loss: 1.6948e-03\n",
      "Epoch: 174680 | training loss: 2.0671e-03 | validation loss: 1.6940e-03\n",
      "Epoch: 174690 | training loss: 2.0671e-03 | validation loss: 1.6928e-03\n",
      "Epoch: 174700 | training loss: 2.0706e-03 | validation loss: 1.6879e-03\n",
      "Epoch: 174710 | training loss: 2.2966e-03 | validation loss: 1.7389e-03\n",
      "Epoch: 174720 | training loss: 2.1952e-03 | validation loss: 1.7221e-03\n",
      "Epoch: 174730 | training loss: 2.3045e-03 | validation loss: 1.7400e-03\n",
      "Epoch: 174740 | training loss: 2.0888e-03 | validation loss: 1.7079e-03\n",
      "Epoch: 174750 | training loss: 2.1000e-03 | validation loss: 1.7332e-03\n",
      "Epoch: 174760 | training loss: 2.0709e-03 | validation loss: 1.7009e-03\n",
      "Epoch: 174770 | training loss: 2.0700e-03 | validation loss: 1.6877e-03\n",
      "Epoch: 174780 | training loss: 2.0681e-03 | validation loss: 1.6953e-03\n",
      "Epoch: 174790 | training loss: 2.0671e-03 | validation loss: 1.6965e-03\n",
      "Epoch: 174800 | training loss: 2.0668e-03 | validation loss: 1.6927e-03\n",
      "Epoch: 174810 | training loss: 2.0668e-03 | validation loss: 1.6946e-03\n",
      "Epoch: 174820 | training loss: 2.0667e-03 | validation loss: 1.6938e-03\n",
      "Epoch: 174830 | training loss: 2.0667e-03 | validation loss: 1.6941e-03\n",
      "Epoch: 174840 | training loss: 2.0667e-03 | validation loss: 1.6939e-03\n",
      "Epoch: 174850 | training loss: 2.0667e-03 | validation loss: 1.6940e-03\n",
      "Epoch: 174860 | training loss: 2.0666e-03 | validation loss: 1.6938e-03\n",
      "Epoch: 174870 | training loss: 2.0666e-03 | validation loss: 1.6938e-03\n",
      "Epoch: 174880 | training loss: 2.0666e-03 | validation loss: 1.6939e-03\n",
      "Epoch: 174890 | training loss: 2.0666e-03 | validation loss: 1.6940e-03\n",
      "Epoch: 174900 | training loss: 2.0674e-03 | validation loss: 1.6953e-03\n",
      "Epoch: 174910 | training loss: 2.1451e-03 | validation loss: 1.7475e-03\n",
      "Epoch: 174920 | training loss: 3.1816e-03 | validation loss: 2.3718e-03\n",
      "Epoch: 174930 | training loss: 2.4268e-03 | validation loss: 1.8119e-03\n",
      "Epoch: 174940 | training loss: 2.1505e-03 | validation loss: 1.7032e-03\n",
      "Epoch: 174950 | training loss: 2.1044e-03 | validation loss: 1.7233e-03\n",
      "Epoch: 174960 | training loss: 2.0695e-03 | validation loss: 1.6925e-03\n",
      "Epoch: 174970 | training loss: 2.0701e-03 | validation loss: 1.6850e-03\n",
      "Epoch: 174980 | training loss: 2.0675e-03 | validation loss: 1.6965e-03\n",
      "Epoch: 174990 | training loss: 2.0665e-03 | validation loss: 1.6913e-03\n",
      "Epoch: 175000 | training loss: 2.0664e-03 | validation loss: 1.6924e-03\n",
      "Epoch: 175010 | training loss: 2.0664e-03 | validation loss: 1.6927e-03\n",
      "Epoch: 175020 | training loss: 2.0663e-03 | validation loss: 1.6930e-03\n",
      "Epoch: 175030 | training loss: 2.0662e-03 | validation loss: 1.6928e-03\n",
      "Epoch: 175040 | training loss: 2.0662e-03 | validation loss: 1.6936e-03\n",
      "Epoch: 175050 | training loss: 2.0662e-03 | validation loss: 1.6933e-03\n",
      "Epoch: 175060 | training loss: 2.0662e-03 | validation loss: 1.6930e-03\n",
      "Epoch: 175070 | training loss: 2.0661e-03 | validation loss: 1.6929e-03\n",
      "Epoch: 175080 | training loss: 2.0661e-03 | validation loss: 1.6923e-03\n",
      "Epoch: 175090 | training loss: 2.0673e-03 | validation loss: 1.6888e-03\n",
      "Epoch: 175100 | training loss: 2.1560e-03 | validation loss: 1.6937e-03\n",
      "Epoch: 175110 | training loss: 3.2025e-03 | validation loss: 2.0954e-03\n",
      "Epoch: 175120 | training loss: 2.1326e-03 | validation loss: 1.6835e-03\n",
      "Epoch: 175130 | training loss: 2.1521e-03 | validation loss: 1.7589e-03\n",
      "Epoch: 175140 | training loss: 2.1102e-03 | validation loss: 1.7353e-03\n",
      "Epoch: 175150 | training loss: 2.0662e-03 | validation loss: 1.6929e-03\n",
      "Epoch: 175160 | training loss: 2.0731e-03 | validation loss: 1.6895e-03\n",
      "Epoch: 175170 | training loss: 2.0660e-03 | validation loss: 1.6945e-03\n",
      "Epoch: 175180 | training loss: 2.0667e-03 | validation loss: 1.6965e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 175190 | training loss: 2.0660e-03 | validation loss: 1.6909e-03\n",
      "Epoch: 175200 | training loss: 2.0658e-03 | validation loss: 1.6930e-03\n",
      "Epoch: 175210 | training loss: 2.0658e-03 | validation loss: 1.6936e-03\n",
      "Epoch: 175220 | training loss: 2.0658e-03 | validation loss: 1.6923e-03\n",
      "Epoch: 175230 | training loss: 2.0658e-03 | validation loss: 1.6931e-03\n",
      "Epoch: 175240 | training loss: 2.0657e-03 | validation loss: 1.6927e-03\n",
      "Epoch: 175250 | training loss: 2.0657e-03 | validation loss: 1.6927e-03\n",
      "Epoch: 175260 | training loss: 2.0657e-03 | validation loss: 1.6928e-03\n",
      "Epoch: 175270 | training loss: 2.0657e-03 | validation loss: 1.6927e-03\n",
      "Epoch: 175280 | training loss: 2.0656e-03 | validation loss: 1.6926e-03\n",
      "Epoch: 175290 | training loss: 2.0656e-03 | validation loss: 1.6926e-03\n",
      "Epoch: 175300 | training loss: 2.0656e-03 | validation loss: 1.6923e-03\n",
      "Epoch: 175310 | training loss: 2.0660e-03 | validation loss: 1.6903e-03\n",
      "Epoch: 175320 | training loss: 2.1086e-03 | validation loss: 1.6883e-03\n",
      "Epoch: 175330 | training loss: 4.0858e-03 | validation loss: 2.4883e-03\n",
      "Epoch: 175340 | training loss: 2.0661e-03 | validation loss: 1.6940e-03\n",
      "Epoch: 175350 | training loss: 2.1625e-03 | validation loss: 1.7755e-03\n",
      "Epoch: 175360 | training loss: 2.1389e-03 | validation loss: 1.7569e-03\n",
      "Epoch: 175370 | training loss: 2.0848e-03 | validation loss: 1.7173e-03\n",
      "Epoch: 175380 | training loss: 2.0656e-03 | validation loss: 1.6944e-03\n",
      "Epoch: 175390 | training loss: 2.0676e-03 | validation loss: 1.6881e-03\n",
      "Epoch: 175400 | training loss: 2.0663e-03 | validation loss: 1.6896e-03\n",
      "Epoch: 175410 | training loss: 2.0654e-03 | validation loss: 1.6936e-03\n",
      "Epoch: 175420 | training loss: 2.0655e-03 | validation loss: 1.6938e-03\n",
      "Epoch: 175430 | training loss: 2.0653e-03 | validation loss: 1.6917e-03\n",
      "Epoch: 175440 | training loss: 2.0653e-03 | validation loss: 1.6922e-03\n",
      "Epoch: 175450 | training loss: 2.0653e-03 | validation loss: 1.6926e-03\n",
      "Epoch: 175460 | training loss: 2.0652e-03 | validation loss: 1.6921e-03\n",
      "Epoch: 175470 | training loss: 2.0652e-03 | validation loss: 1.6923e-03\n",
      "Epoch: 175480 | training loss: 2.0652e-03 | validation loss: 1.6921e-03\n",
      "Epoch: 175490 | training loss: 2.0651e-03 | validation loss: 1.6922e-03\n",
      "Epoch: 175500 | training loss: 2.0651e-03 | validation loss: 1.6919e-03\n",
      "Epoch: 175510 | training loss: 2.0653e-03 | validation loss: 1.6903e-03\n",
      "Epoch: 175520 | training loss: 2.1039e-03 | validation loss: 1.6921e-03\n",
      "Epoch: 175530 | training loss: 2.3667e-03 | validation loss: 1.7791e-03\n",
      "Epoch: 175540 | training loss: 2.2632e-03 | validation loss: 1.8104e-03\n",
      "Epoch: 175550 | training loss: 2.1336e-03 | validation loss: 1.7032e-03\n",
      "Epoch: 175560 | training loss: 2.0872e-03 | validation loss: 1.6957e-03\n",
      "Epoch: 175570 | training loss: 2.0742e-03 | validation loss: 1.6944e-03\n",
      "Epoch: 175580 | training loss: 2.0689e-03 | validation loss: 1.6852e-03\n",
      "Epoch: 175590 | training loss: 2.0662e-03 | validation loss: 1.6870e-03\n",
      "Epoch: 175600 | training loss: 2.0651e-03 | validation loss: 1.6892e-03\n",
      "Epoch: 175610 | training loss: 2.0653e-03 | validation loss: 1.6888e-03\n",
      "Epoch: 175620 | training loss: 2.0748e-03 | validation loss: 1.6822e-03\n",
      "Epoch: 175630 | training loss: 2.4985e-03 | validation loss: 1.8024e-03\n",
      "Epoch: 175640 | training loss: 2.1039e-03 | validation loss: 1.7421e-03\n",
      "Epoch: 175650 | training loss: 2.2565e-03 | validation loss: 1.7263e-03\n",
      "Epoch: 175660 | training loss: 2.0878e-03 | validation loss: 1.7239e-03\n",
      "Epoch: 175670 | training loss: 2.0718e-03 | validation loss: 1.7063e-03\n",
      "Epoch: 175680 | training loss: 2.0745e-03 | validation loss: 1.6828e-03\n",
      "Epoch: 175690 | training loss: 2.0671e-03 | validation loss: 1.6987e-03\n",
      "Epoch: 175700 | training loss: 2.0650e-03 | validation loss: 1.6890e-03\n",
      "Epoch: 175710 | training loss: 2.0647e-03 | validation loss: 1.6922e-03\n",
      "Epoch: 175720 | training loss: 2.0647e-03 | validation loss: 1.6906e-03\n",
      "Epoch: 175730 | training loss: 2.0646e-03 | validation loss: 1.6923e-03\n",
      "Epoch: 175740 | training loss: 2.0646e-03 | validation loss: 1.6905e-03\n",
      "Epoch: 175750 | training loss: 2.0645e-03 | validation loss: 1.6912e-03\n",
      "Epoch: 175760 | training loss: 2.0645e-03 | validation loss: 1.6916e-03\n",
      "Epoch: 175770 | training loss: 2.0645e-03 | validation loss: 1.6917e-03\n",
      "Epoch: 175780 | training loss: 2.0646e-03 | validation loss: 1.6926e-03\n",
      "Epoch: 175790 | training loss: 2.0672e-03 | validation loss: 1.6994e-03\n",
      "Epoch: 175800 | training loss: 2.2163e-03 | validation loss: 1.8152e-03\n",
      "Epoch: 175810 | training loss: 2.6050e-03 | validation loss: 2.0391e-03\n",
      "Epoch: 175820 | training loss: 2.1493e-03 | validation loss: 1.7771e-03\n",
      "Epoch: 175830 | training loss: 2.1479e-03 | validation loss: 1.7026e-03\n",
      "Epoch: 175840 | training loss: 2.0808e-03 | validation loss: 1.6782e-03\n",
      "Epoch: 175850 | training loss: 2.0753e-03 | validation loss: 1.7048e-03\n",
      "Epoch: 175860 | training loss: 2.0655e-03 | validation loss: 1.6978e-03\n",
      "Epoch: 175870 | training loss: 2.0659e-03 | validation loss: 1.6873e-03\n",
      "Epoch: 175880 | training loss: 2.0648e-03 | validation loss: 1.6920e-03\n",
      "Epoch: 175890 | training loss: 2.0644e-03 | validation loss: 1.6918e-03\n",
      "Epoch: 175900 | training loss: 2.0642e-03 | validation loss: 1.6903e-03\n",
      "Epoch: 175910 | training loss: 2.0642e-03 | validation loss: 1.6913e-03\n",
      "Epoch: 175920 | training loss: 2.0642e-03 | validation loss: 1.6907e-03\n",
      "Epoch: 175930 | training loss: 2.0641e-03 | validation loss: 1.6909e-03\n",
      "Epoch: 175940 | training loss: 2.0641e-03 | validation loss: 1.6908e-03\n",
      "Epoch: 175950 | training loss: 2.0641e-03 | validation loss: 1.6907e-03\n",
      "Epoch: 175960 | training loss: 2.0640e-03 | validation loss: 1.6908e-03\n",
      "Epoch: 175970 | training loss: 2.0640e-03 | validation loss: 1.6908e-03\n",
      "Epoch: 175980 | training loss: 2.0640e-03 | validation loss: 1.6911e-03\n",
      "Epoch: 175990 | training loss: 2.0659e-03 | validation loss: 1.6942e-03\n",
      "Epoch: 176000 | training loss: 2.2423e-03 | validation loss: 1.8094e-03\n",
      "Epoch: 176010 | training loss: 2.8088e-03 | validation loss: 2.2564e-03\n",
      "Epoch: 176020 | training loss: 2.0789e-03 | validation loss: 1.6992e-03\n",
      "Epoch: 176030 | training loss: 2.1239e-03 | validation loss: 1.6810e-03\n",
      "Epoch: 176040 | training loss: 2.0648e-03 | validation loss: 1.6889e-03\n",
      "Epoch: 176050 | training loss: 2.0726e-03 | validation loss: 1.7105e-03\n",
      "Epoch: 176060 | training loss: 2.0649e-03 | validation loss: 1.6903e-03\n",
      "Epoch: 176070 | training loss: 2.0643e-03 | validation loss: 1.6892e-03\n",
      "Epoch: 176080 | training loss: 2.0642e-03 | validation loss: 1.6938e-03\n",
      "Epoch: 176090 | training loss: 2.0639e-03 | validation loss: 1.6885e-03\n",
      "Epoch: 176100 | training loss: 2.0638e-03 | validation loss: 1.6908e-03\n",
      "Epoch: 176110 | training loss: 2.0637e-03 | validation loss: 1.6892e-03\n",
      "Epoch: 176120 | training loss: 2.0637e-03 | validation loss: 1.6904e-03\n",
      "Epoch: 176130 | training loss: 2.0637e-03 | validation loss: 1.6900e-03\n",
      "Epoch: 176140 | training loss: 2.0636e-03 | validation loss: 1.6901e-03\n",
      "Epoch: 176150 | training loss: 2.0636e-03 | validation loss: 1.6902e-03\n",
      "Epoch: 176160 | training loss: 2.0636e-03 | validation loss: 1.6902e-03\n",
      "Epoch: 176170 | training loss: 2.0636e-03 | validation loss: 1.6904e-03\n",
      "Epoch: 176180 | training loss: 2.0637e-03 | validation loss: 1.6919e-03\n",
      "Epoch: 176190 | training loss: 2.0722e-03 | validation loss: 1.7078e-03\n",
      "Epoch: 176200 | training loss: 2.8568e-03 | validation loss: 2.1989e-03\n",
      "Epoch: 176210 | training loss: 2.6623e-03 | validation loss: 1.8648e-03\n",
      "Epoch: 176220 | training loss: 2.1092e-03 | validation loss: 1.7075e-03\n",
      "Epoch: 176230 | training loss: 2.1413e-03 | validation loss: 1.7494e-03\n",
      "Epoch: 176240 | training loss: 2.0716e-03 | validation loss: 1.7053e-03\n",
      "Epoch: 176250 | training loss: 2.0697e-03 | validation loss: 1.6922e-03\n",
      "Epoch: 176260 | training loss: 2.0663e-03 | validation loss: 1.6906e-03\n",
      "Epoch: 176270 | training loss: 2.0640e-03 | validation loss: 1.6933e-03\n",
      "Epoch: 176280 | training loss: 2.0635e-03 | validation loss: 1.6894e-03\n",
      "Epoch: 176290 | training loss: 2.0635e-03 | validation loss: 1.6882e-03\n",
      "Epoch: 176300 | training loss: 2.0633e-03 | validation loss: 1.6911e-03\n",
      "Epoch: 176310 | training loss: 2.0633e-03 | validation loss: 1.6894e-03\n",
      "Epoch: 176320 | training loss: 2.0632e-03 | validation loss: 1.6897e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 176330 | training loss: 2.0632e-03 | validation loss: 1.6897e-03\n",
      "Epoch: 176340 | training loss: 2.0632e-03 | validation loss: 1.6897e-03\n",
      "Epoch: 176350 | training loss: 2.0631e-03 | validation loss: 1.6896e-03\n",
      "Epoch: 176360 | training loss: 2.0631e-03 | validation loss: 1.6896e-03\n",
      "Epoch: 176370 | training loss: 2.0631e-03 | validation loss: 1.6896e-03\n",
      "Epoch: 176380 | training loss: 2.0631e-03 | validation loss: 1.6896e-03\n",
      "Epoch: 176390 | training loss: 2.0631e-03 | validation loss: 1.6897e-03\n",
      "Epoch: 176400 | training loss: 2.0631e-03 | validation loss: 1.6903e-03\n",
      "Epoch: 176410 | training loss: 2.0660e-03 | validation loss: 1.6969e-03\n",
      "Epoch: 176420 | training loss: 2.4063e-03 | validation loss: 1.9201e-03\n",
      "Epoch: 176430 | training loss: 2.1458e-03 | validation loss: 1.7281e-03\n",
      "Epoch: 176440 | training loss: 2.2480e-03 | validation loss: 1.8340e-03\n",
      "Epoch: 176450 | training loss: 2.1744e-03 | validation loss: 1.7670e-03\n",
      "Epoch: 176460 | training loss: 2.1077e-03 | validation loss: 1.7132e-03\n",
      "Epoch: 176470 | training loss: 2.0717e-03 | validation loss: 1.6928e-03\n",
      "Epoch: 176480 | training loss: 2.0632e-03 | validation loss: 1.6928e-03\n",
      "Epoch: 176490 | training loss: 2.0639e-03 | validation loss: 1.6897e-03\n",
      "Epoch: 176500 | training loss: 2.0636e-03 | validation loss: 1.6881e-03\n",
      "Epoch: 176510 | training loss: 2.0628e-03 | validation loss: 1.6901e-03\n",
      "Epoch: 176520 | training loss: 2.0629e-03 | validation loss: 1.6892e-03\n",
      "Epoch: 176530 | training loss: 2.0627e-03 | validation loss: 1.6894e-03\n",
      "Epoch: 176540 | training loss: 2.0627e-03 | validation loss: 1.6890e-03\n",
      "Epoch: 176550 | training loss: 2.0627e-03 | validation loss: 1.6893e-03\n",
      "Epoch: 176560 | training loss: 2.0627e-03 | validation loss: 1.6891e-03\n",
      "Epoch: 176570 | training loss: 2.0626e-03 | validation loss: 1.6891e-03\n",
      "Epoch: 176580 | training loss: 2.0626e-03 | validation loss: 1.6891e-03\n",
      "Epoch: 176590 | training loss: 2.0626e-03 | validation loss: 1.6891e-03\n",
      "Epoch: 176600 | training loss: 2.0626e-03 | validation loss: 1.6892e-03\n",
      "Epoch: 176610 | training loss: 2.0626e-03 | validation loss: 1.6897e-03\n",
      "Epoch: 176620 | training loss: 2.0634e-03 | validation loss: 1.6948e-03\n",
      "Epoch: 176630 | training loss: 2.1779e-03 | validation loss: 1.8179e-03\n",
      "Epoch: 176640 | training loss: 2.3459e-03 | validation loss: 1.8905e-03\n",
      "Epoch: 176650 | training loss: 2.1927e-03 | validation loss: 1.8121e-03\n",
      "Epoch: 176660 | training loss: 2.1298e-03 | validation loss: 1.7494e-03\n",
      "Epoch: 176670 | training loss: 2.0937e-03 | validation loss: 1.7352e-03\n",
      "Epoch: 176680 | training loss: 2.0743e-03 | validation loss: 1.6993e-03\n",
      "Epoch: 176690 | training loss: 2.0666e-03 | validation loss: 1.7014e-03\n",
      "Epoch: 176700 | training loss: 2.0637e-03 | validation loss: 1.6907e-03\n",
      "Epoch: 176710 | training loss: 2.0626e-03 | validation loss: 1.6914e-03\n",
      "Epoch: 176720 | training loss: 2.0624e-03 | validation loss: 1.6871e-03\n",
      "Epoch: 176730 | training loss: 2.0623e-03 | validation loss: 1.6876e-03\n",
      "Epoch: 176740 | training loss: 2.0623e-03 | validation loss: 1.6885e-03\n",
      "Epoch: 176750 | training loss: 2.0622e-03 | validation loss: 1.6889e-03\n",
      "Epoch: 176760 | training loss: 2.0622e-03 | validation loss: 1.6891e-03\n",
      "Epoch: 176770 | training loss: 2.0625e-03 | validation loss: 1.6908e-03\n",
      "Epoch: 176780 | training loss: 2.0773e-03 | validation loss: 1.7110e-03\n",
      "Epoch: 176790 | training loss: 2.9894e-03 | validation loss: 2.2517e-03\n",
      "Epoch: 176800 | training loss: 2.6342e-03 | validation loss: 1.8669e-03\n",
      "Epoch: 176810 | training loss: 2.0787e-03 | validation loss: 1.7100e-03\n",
      "Epoch: 176820 | training loss: 2.1414e-03 | validation loss: 1.7592e-03\n",
      "Epoch: 176830 | training loss: 2.0623e-03 | validation loss: 1.6894e-03\n",
      "Epoch: 176840 | training loss: 2.0724e-03 | validation loss: 1.6805e-03\n",
      "Epoch: 176850 | training loss: 2.0621e-03 | validation loss: 1.6894e-03\n",
      "Epoch: 176860 | training loss: 2.0630e-03 | validation loss: 1.6926e-03\n",
      "Epoch: 176870 | training loss: 2.0624e-03 | validation loss: 1.6858e-03\n",
      "Epoch: 176880 | training loss: 2.0620e-03 | validation loss: 1.6890e-03\n",
      "Epoch: 176890 | training loss: 2.0619e-03 | validation loss: 1.6879e-03\n",
      "Epoch: 176900 | training loss: 2.0619e-03 | validation loss: 1.6881e-03\n",
      "Epoch: 176910 | training loss: 2.0619e-03 | validation loss: 1.6879e-03\n",
      "Epoch: 176920 | training loss: 2.0618e-03 | validation loss: 1.6882e-03\n",
      "Epoch: 176930 | training loss: 2.0618e-03 | validation loss: 1.6878e-03\n",
      "Epoch: 176940 | training loss: 2.0618e-03 | validation loss: 1.6879e-03\n",
      "Epoch: 176950 | training loss: 2.0618e-03 | validation loss: 1.6880e-03\n",
      "Epoch: 176960 | training loss: 2.0617e-03 | validation loss: 1.6880e-03\n",
      "Epoch: 176970 | training loss: 2.0617e-03 | validation loss: 1.6883e-03\n",
      "Epoch: 176980 | training loss: 2.0620e-03 | validation loss: 1.6903e-03\n",
      "Epoch: 176990 | training loss: 2.0893e-03 | validation loss: 1.7213e-03\n",
      "Epoch: 177000 | training loss: 3.8768e-03 | validation loss: 2.7204e-03\n",
      "Epoch: 177010 | training loss: 2.3053e-03 | validation loss: 1.7454e-03\n",
      "Epoch: 177020 | training loss: 2.2857e-03 | validation loss: 1.7358e-03\n",
      "Epoch: 177030 | training loss: 2.0971e-03 | validation loss: 1.6798e-03\n",
      "Epoch: 177040 | training loss: 2.0623e-03 | validation loss: 1.6911e-03\n",
      "Epoch: 177050 | training loss: 2.0705e-03 | validation loss: 1.7040e-03\n",
      "Epoch: 177060 | training loss: 2.0632e-03 | validation loss: 1.6936e-03\n",
      "Epoch: 177070 | training loss: 2.0620e-03 | validation loss: 1.6852e-03\n",
      "Epoch: 177080 | training loss: 2.0617e-03 | validation loss: 1.6858e-03\n",
      "Epoch: 177090 | training loss: 2.0616e-03 | validation loss: 1.6891e-03\n",
      "Epoch: 177100 | training loss: 2.0614e-03 | validation loss: 1.6878e-03\n",
      "Epoch: 177110 | training loss: 2.0614e-03 | validation loss: 1.6871e-03\n",
      "Epoch: 177120 | training loss: 2.0614e-03 | validation loss: 1.6879e-03\n",
      "Epoch: 177130 | training loss: 2.0613e-03 | validation loss: 1.6873e-03\n",
      "Epoch: 177140 | training loss: 2.0613e-03 | validation loss: 1.6876e-03\n",
      "Epoch: 177150 | training loss: 2.0613e-03 | validation loss: 1.6874e-03\n",
      "Epoch: 177160 | training loss: 2.0613e-03 | validation loss: 1.6875e-03\n",
      "Epoch: 177170 | training loss: 2.0612e-03 | validation loss: 1.6874e-03\n",
      "Epoch: 177180 | training loss: 2.0612e-03 | validation loss: 1.6875e-03\n",
      "Epoch: 177190 | training loss: 2.0615e-03 | validation loss: 1.6897e-03\n",
      "Epoch: 177200 | training loss: 2.1332e-03 | validation loss: 1.7717e-03\n",
      "Epoch: 177210 | training loss: 2.1842e-03 | validation loss: 1.7890e-03\n",
      "Epoch: 177220 | training loss: 2.0815e-03 | validation loss: 1.7056e-03\n",
      "Epoch: 177230 | training loss: 2.0723e-03 | validation loss: 1.6863e-03\n",
      "Epoch: 177240 | training loss: 2.0661e-03 | validation loss: 1.7005e-03\n",
      "Epoch: 177250 | training loss: 2.0630e-03 | validation loss: 1.6947e-03\n",
      "Epoch: 177260 | training loss: 2.0613e-03 | validation loss: 1.6893e-03\n",
      "Epoch: 177270 | training loss: 2.0612e-03 | validation loss: 1.6887e-03\n",
      "Epoch: 177280 | training loss: 2.0615e-03 | validation loss: 1.6906e-03\n",
      "Epoch: 177290 | training loss: 2.0736e-03 | validation loss: 1.7100e-03\n",
      "Epoch: 177300 | training loss: 2.9292e-03 | validation loss: 2.2353e-03\n",
      "Epoch: 177310 | training loss: 2.6232e-03 | validation loss: 1.8518e-03\n",
      "Epoch: 177320 | training loss: 2.0738e-03 | validation loss: 1.7079e-03\n",
      "Epoch: 177330 | training loss: 2.1397e-03 | validation loss: 1.7621e-03\n",
      "Epoch: 177340 | training loss: 2.0622e-03 | validation loss: 1.6924e-03\n",
      "Epoch: 177350 | training loss: 2.0706e-03 | validation loss: 1.6787e-03\n",
      "Epoch: 177360 | training loss: 2.0608e-03 | validation loss: 1.6863e-03\n",
      "Epoch: 177370 | training loss: 2.0621e-03 | validation loss: 1.6920e-03\n",
      "Epoch: 177380 | training loss: 2.0611e-03 | validation loss: 1.6843e-03\n",
      "Epoch: 177390 | training loss: 2.0608e-03 | validation loss: 1.6869e-03\n",
      "Epoch: 177400 | training loss: 2.0607e-03 | validation loss: 1.6868e-03\n",
      "Epoch: 177410 | training loss: 2.0607e-03 | validation loss: 1.6863e-03\n",
      "Epoch: 177420 | training loss: 2.0607e-03 | validation loss: 1.6866e-03\n",
      "Epoch: 177430 | training loss: 2.0606e-03 | validation loss: 1.6866e-03\n",
      "Epoch: 177440 | training loss: 2.0606e-03 | validation loss: 1.6863e-03\n",
      "Epoch: 177450 | training loss: 2.0606e-03 | validation loss: 1.6865e-03\n",
      "Epoch: 177460 | training loss: 2.0606e-03 | validation loss: 1.6865e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 177470 | training loss: 2.0606e-03 | validation loss: 1.6865e-03\n",
      "Epoch: 177480 | training loss: 2.0605e-03 | validation loss: 1.6867e-03\n",
      "Epoch: 177490 | training loss: 2.0607e-03 | validation loss: 1.6880e-03\n",
      "Epoch: 177500 | training loss: 2.0698e-03 | validation loss: 1.7036e-03\n",
      "Epoch: 177510 | training loss: 2.9859e-03 | validation loss: 2.2540e-03\n",
      "Epoch: 177520 | training loss: 2.8257e-03 | validation loss: 1.9379e-03\n",
      "Epoch: 177530 | training loss: 2.0878e-03 | validation loss: 1.6714e-03\n",
      "Epoch: 177540 | training loss: 2.0956e-03 | validation loss: 1.7157e-03\n",
      "Epoch: 177550 | training loss: 2.0909e-03 | validation loss: 1.7224e-03\n",
      "Epoch: 177560 | training loss: 2.0660e-03 | validation loss: 1.7027e-03\n",
      "Epoch: 177570 | training loss: 2.0621e-03 | validation loss: 1.6869e-03\n",
      "Epoch: 177580 | training loss: 2.0614e-03 | validation loss: 1.6812e-03\n",
      "Epoch: 177590 | training loss: 2.0605e-03 | validation loss: 1.6855e-03\n",
      "Epoch: 177600 | training loss: 2.0604e-03 | validation loss: 1.6885e-03\n",
      "Epoch: 177610 | training loss: 2.0603e-03 | validation loss: 1.6857e-03\n",
      "Epoch: 177620 | training loss: 2.0602e-03 | validation loss: 1.6856e-03\n",
      "Epoch: 177630 | training loss: 2.0602e-03 | validation loss: 1.6866e-03\n",
      "Epoch: 177640 | training loss: 2.0602e-03 | validation loss: 1.6857e-03\n",
      "Epoch: 177650 | training loss: 2.0601e-03 | validation loss: 1.6862e-03\n",
      "Epoch: 177660 | training loss: 2.0601e-03 | validation loss: 1.6859e-03\n",
      "Epoch: 177670 | training loss: 2.0601e-03 | validation loss: 1.6860e-03\n",
      "Epoch: 177680 | training loss: 2.0601e-03 | validation loss: 1.6859e-03\n",
      "Epoch: 177690 | training loss: 2.0600e-03 | validation loss: 1.6859e-03\n",
      "Epoch: 177700 | training loss: 2.0600e-03 | validation loss: 1.6859e-03\n",
      "Epoch: 177710 | training loss: 2.0600e-03 | validation loss: 1.6859e-03\n",
      "Epoch: 177720 | training loss: 2.0600e-03 | validation loss: 1.6859e-03\n",
      "Epoch: 177730 | training loss: 2.0600e-03 | validation loss: 1.6862e-03\n",
      "Epoch: 177740 | training loss: 2.0651e-03 | validation loss: 1.6924e-03\n",
      "Epoch: 177750 | training loss: 2.8328e-03 | validation loss: 2.1948e-03\n",
      "Epoch: 177760 | training loss: 2.6574e-03 | validation loss: 1.8557e-03\n",
      "Epoch: 177770 | training loss: 2.1040e-03 | validation loss: 1.7457e-03\n",
      "Epoch: 177780 | training loss: 2.1593e-03 | validation loss: 1.7961e-03\n",
      "Epoch: 177790 | training loss: 2.0875e-03 | validation loss: 1.6856e-03\n",
      "Epoch: 177800 | training loss: 2.0613e-03 | validation loss: 1.6860e-03\n",
      "Epoch: 177810 | training loss: 2.0631e-03 | validation loss: 1.6871e-03\n",
      "Epoch: 177820 | training loss: 2.0614e-03 | validation loss: 1.6798e-03\n",
      "Epoch: 177830 | training loss: 2.0602e-03 | validation loss: 1.6877e-03\n",
      "Epoch: 177840 | training loss: 2.0598e-03 | validation loss: 1.6838e-03\n",
      "Epoch: 177850 | training loss: 2.0597e-03 | validation loss: 1.6861e-03\n",
      "Epoch: 177860 | training loss: 2.0597e-03 | validation loss: 1.6857e-03\n",
      "Epoch: 177870 | training loss: 2.0597e-03 | validation loss: 1.6848e-03\n",
      "Epoch: 177880 | training loss: 2.0596e-03 | validation loss: 1.6850e-03\n",
      "Epoch: 177890 | training loss: 2.0596e-03 | validation loss: 1.6851e-03\n",
      "Epoch: 177900 | training loss: 2.0596e-03 | validation loss: 1.6851e-03\n",
      "Epoch: 177910 | training loss: 2.0595e-03 | validation loss: 1.6851e-03\n",
      "Epoch: 177920 | training loss: 2.0595e-03 | validation loss: 1.6851e-03\n",
      "Epoch: 177930 | training loss: 2.0595e-03 | validation loss: 1.6854e-03\n",
      "Epoch: 177940 | training loss: 2.0613e-03 | validation loss: 1.6919e-03\n",
      "Epoch: 177950 | training loss: 2.7653e-03 | validation loss: 2.1410e-03\n",
      "Epoch: 177960 | training loss: 3.5547e-03 | validation loss: 2.2338e-03\n",
      "Epoch: 177970 | training loss: 2.5345e-03 | validation loss: 1.8396e-03\n",
      "Epoch: 177980 | training loss: 2.1811e-03 | validation loss: 1.7266e-03\n",
      "Epoch: 177990 | training loss: 2.0672e-03 | validation loss: 1.6843e-03\n",
      "Epoch: 178000 | training loss: 2.0610e-03 | validation loss: 1.6852e-03\n",
      "Epoch: 178010 | training loss: 2.0624e-03 | validation loss: 1.6885e-03\n",
      "Epoch: 178020 | training loss: 2.0608e-03 | validation loss: 1.6888e-03\n",
      "Epoch: 178030 | training loss: 2.0598e-03 | validation loss: 1.6883e-03\n",
      "Epoch: 178040 | training loss: 2.0595e-03 | validation loss: 1.6873e-03\n",
      "Epoch: 178050 | training loss: 2.0593e-03 | validation loss: 1.6862e-03\n",
      "Epoch: 178060 | training loss: 2.0592e-03 | validation loss: 1.6854e-03\n",
      "Epoch: 178070 | training loss: 2.0592e-03 | validation loss: 1.6849e-03\n",
      "Epoch: 178080 | training loss: 2.0592e-03 | validation loss: 1.6847e-03\n",
      "Epoch: 178090 | training loss: 2.0592e-03 | validation loss: 1.6847e-03\n",
      "Epoch: 178100 | training loss: 2.0591e-03 | validation loss: 1.6848e-03\n",
      "Epoch: 178110 | training loss: 2.0591e-03 | validation loss: 1.6848e-03\n",
      "Epoch: 178120 | training loss: 2.0591e-03 | validation loss: 1.6848e-03\n",
      "Epoch: 178130 | training loss: 2.0591e-03 | validation loss: 1.6847e-03\n",
      "Epoch: 178140 | training loss: 2.0590e-03 | validation loss: 1.6847e-03\n",
      "Epoch: 178150 | training loss: 2.0590e-03 | validation loss: 1.6847e-03\n",
      "Epoch: 178160 | training loss: 2.0590e-03 | validation loss: 1.6847e-03\n",
      "Epoch: 178170 | training loss: 2.0590e-03 | validation loss: 1.6846e-03\n",
      "Epoch: 178180 | training loss: 2.0590e-03 | validation loss: 1.6846e-03\n",
      "Epoch: 178190 | training loss: 2.0589e-03 | validation loss: 1.6846e-03\n",
      "Epoch: 178200 | training loss: 2.0589e-03 | validation loss: 1.6846e-03\n",
      "Epoch: 178210 | training loss: 2.0589e-03 | validation loss: 1.6845e-03\n",
      "Epoch: 178220 | training loss: 2.0589e-03 | validation loss: 1.6845e-03\n",
      "Epoch: 178230 | training loss: 2.0588e-03 | validation loss: 1.6845e-03\n",
      "Epoch: 178240 | training loss: 2.0588e-03 | validation loss: 1.6844e-03\n",
      "Epoch: 178250 | training loss: 2.0588e-03 | validation loss: 1.6844e-03\n",
      "Epoch: 178260 | training loss: 2.0588e-03 | validation loss: 1.6844e-03\n",
      "Epoch: 178270 | training loss: 2.0587e-03 | validation loss: 1.6843e-03\n",
      "Epoch: 178280 | training loss: 2.0587e-03 | validation loss: 1.6841e-03\n",
      "Epoch: 178290 | training loss: 2.0596e-03 | validation loss: 1.6809e-03\n",
      "Epoch: 178300 | training loss: 2.4187e-03 | validation loss: 1.7809e-03\n",
      "Epoch: 178310 | training loss: 3.0143e-03 | validation loss: 2.2772e-03\n",
      "Epoch: 178320 | training loss: 2.5268e-03 | validation loss: 2.0111e-03\n",
      "Epoch: 178330 | training loss: 2.2652e-03 | validation loss: 1.8525e-03\n",
      "Epoch: 178340 | training loss: 2.0781e-03 | validation loss: 1.7149e-03\n",
      "Epoch: 178350 | training loss: 2.0601e-03 | validation loss: 1.6802e-03\n",
      "Epoch: 178360 | training loss: 2.0643e-03 | validation loss: 1.6761e-03\n",
      "Epoch: 178370 | training loss: 2.0619e-03 | validation loss: 1.6771e-03\n",
      "Epoch: 178380 | training loss: 2.0597e-03 | validation loss: 1.6797e-03\n",
      "Epoch: 178390 | training loss: 2.0588e-03 | validation loss: 1.6818e-03\n",
      "Epoch: 178400 | training loss: 2.0586e-03 | validation loss: 1.6826e-03\n",
      "Epoch: 178410 | training loss: 2.0585e-03 | validation loss: 1.6830e-03\n",
      "Epoch: 178420 | training loss: 2.0584e-03 | validation loss: 1.6834e-03\n",
      "Epoch: 178430 | training loss: 2.0584e-03 | validation loss: 1.6836e-03\n",
      "Epoch: 178440 | training loss: 2.0584e-03 | validation loss: 1.6838e-03\n",
      "Epoch: 178450 | training loss: 2.0583e-03 | validation loss: 1.6839e-03\n",
      "Epoch: 178460 | training loss: 2.0583e-03 | validation loss: 1.6840e-03\n",
      "Epoch: 178470 | training loss: 2.0583e-03 | validation loss: 1.6839e-03\n",
      "Epoch: 178480 | training loss: 2.0583e-03 | validation loss: 1.6839e-03\n",
      "Epoch: 178490 | training loss: 2.0582e-03 | validation loss: 1.6838e-03\n",
      "Epoch: 178500 | training loss: 2.0582e-03 | validation loss: 1.6838e-03\n",
      "Epoch: 178510 | training loss: 2.0582e-03 | validation loss: 1.6838e-03\n",
      "Epoch: 178520 | training loss: 2.0582e-03 | validation loss: 1.6837e-03\n",
      "Epoch: 178530 | training loss: 2.0581e-03 | validation loss: 1.6837e-03\n",
      "Epoch: 178540 | training loss: 2.0581e-03 | validation loss: 1.6834e-03\n",
      "Epoch: 178550 | training loss: 2.0596e-03 | validation loss: 1.6806e-03\n",
      "Epoch: 178560 | training loss: 2.4067e-03 | validation loss: 1.8655e-03\n",
      "Epoch: 178570 | training loss: 2.3857e-03 | validation loss: 1.9600e-03\n",
      "Epoch: 178580 | training loss: 2.1740e-03 | validation loss: 1.7846e-03\n",
      "Epoch: 178590 | training loss: 2.0984e-03 | validation loss: 1.7290e-03\n",
      "Epoch: 178600 | training loss: 2.0720e-03 | validation loss: 1.6995e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 178610 | training loss: 2.0624e-03 | validation loss: 1.6921e-03\n",
      "Epoch: 178620 | training loss: 2.0590e-03 | validation loss: 1.6866e-03\n",
      "Epoch: 178630 | training loss: 2.0580e-03 | validation loss: 1.6834e-03\n",
      "Epoch: 178640 | training loss: 2.0579e-03 | validation loss: 1.6828e-03\n",
      "Epoch: 178650 | training loss: 2.0580e-03 | validation loss: 1.6828e-03\n",
      "Epoch: 178660 | training loss: 2.0579e-03 | validation loss: 1.6834e-03\n",
      "Epoch: 178670 | training loss: 2.0581e-03 | validation loss: 1.6857e-03\n",
      "Epoch: 178680 | training loss: 2.0702e-03 | validation loss: 1.7055e-03\n",
      "Epoch: 178690 | training loss: 2.8768e-03 | validation loss: 2.2080e-03\n",
      "Epoch: 178700 | training loss: 2.5578e-03 | validation loss: 1.8211e-03\n",
      "Epoch: 178710 | training loss: 2.1040e-03 | validation loss: 1.7227e-03\n",
      "Epoch: 178720 | training loss: 2.1282e-03 | validation loss: 1.7447e-03\n",
      "Epoch: 178730 | training loss: 2.0597e-03 | validation loss: 1.6760e-03\n",
      "Epoch: 178740 | training loss: 2.0664e-03 | validation loss: 1.6747e-03\n",
      "Epoch: 178750 | training loss: 2.0590e-03 | validation loss: 1.6891e-03\n",
      "Epoch: 178760 | training loss: 2.0579e-03 | validation loss: 1.6855e-03\n",
      "Epoch: 178770 | training loss: 2.0580e-03 | validation loss: 1.6804e-03\n",
      "Epoch: 178780 | training loss: 2.0578e-03 | validation loss: 1.6844e-03\n",
      "Epoch: 178790 | training loss: 2.0576e-03 | validation loss: 1.6818e-03\n",
      "Epoch: 178800 | training loss: 2.0576e-03 | validation loss: 1.6835e-03\n",
      "Epoch: 178810 | training loss: 2.0575e-03 | validation loss: 1.6823e-03\n",
      "Epoch: 178820 | training loss: 2.0575e-03 | validation loss: 1.6829e-03\n",
      "Epoch: 178830 | training loss: 2.0575e-03 | validation loss: 1.6828e-03\n",
      "Epoch: 178840 | training loss: 2.0574e-03 | validation loss: 1.6825e-03\n",
      "Epoch: 178850 | training loss: 2.0574e-03 | validation loss: 1.6825e-03\n",
      "Epoch: 178860 | training loss: 2.0574e-03 | validation loss: 1.6823e-03\n",
      "Epoch: 178870 | training loss: 2.0575e-03 | validation loss: 1.6814e-03\n",
      "Epoch: 178880 | training loss: 2.0618e-03 | validation loss: 1.6761e-03\n",
      "Epoch: 178890 | training loss: 2.4634e-03 | validation loss: 1.7945e-03\n",
      "Epoch: 178900 | training loss: 2.1603e-03 | validation loss: 1.7823e-03\n",
      "Epoch: 178910 | training loss: 2.2741e-03 | validation loss: 1.7303e-03\n",
      "Epoch: 178920 | training loss: 2.1583e-03 | validation loss: 1.6817e-03\n",
      "Epoch: 178930 | training loss: 2.0616e-03 | validation loss: 1.6729e-03\n",
      "Epoch: 178940 | training loss: 2.0654e-03 | validation loss: 1.7017e-03\n",
      "Epoch: 178950 | training loss: 2.0611e-03 | validation loss: 1.6942e-03\n",
      "Epoch: 178960 | training loss: 2.0575e-03 | validation loss: 1.6793e-03\n",
      "Epoch: 178970 | training loss: 2.0577e-03 | validation loss: 1.6797e-03\n",
      "Epoch: 178980 | training loss: 2.0573e-03 | validation loss: 1.6840e-03\n",
      "Epoch: 178990 | training loss: 2.0571e-03 | validation loss: 1.6828e-03\n",
      "Epoch: 179000 | training loss: 2.0571e-03 | validation loss: 1.6815e-03\n",
      "Epoch: 179010 | training loss: 2.0571e-03 | validation loss: 1.6828e-03\n",
      "Epoch: 179020 | training loss: 2.0570e-03 | validation loss: 1.6820e-03\n",
      "Epoch: 179030 | training loss: 2.0570e-03 | validation loss: 1.6824e-03\n",
      "Epoch: 179040 | training loss: 2.0570e-03 | validation loss: 1.6820e-03\n",
      "Epoch: 179050 | training loss: 2.0570e-03 | validation loss: 1.6822e-03\n",
      "Epoch: 179060 | training loss: 2.0569e-03 | validation loss: 1.6819e-03\n",
      "Epoch: 179070 | training loss: 2.0574e-03 | validation loss: 1.6804e-03\n",
      "Epoch: 179080 | training loss: 2.1198e-03 | validation loss: 1.7033e-03\n",
      "Epoch: 179090 | training loss: 2.0852e-03 | validation loss: 1.6947e-03\n",
      "Epoch: 179100 | training loss: 2.1900e-03 | validation loss: 1.7474e-03\n",
      "Epoch: 179110 | training loss: 2.0885e-03 | validation loss: 1.6907e-03\n",
      "Epoch: 179120 | training loss: 2.0576e-03 | validation loss: 1.6817e-03\n",
      "Epoch: 179130 | training loss: 2.0609e-03 | validation loss: 1.6942e-03\n",
      "Epoch: 179140 | training loss: 2.1098e-03 | validation loss: 1.7451e-03\n",
      "Epoch: 179150 | training loss: 3.0914e-03 | validation loss: 2.3255e-03\n",
      "Epoch: 179160 | training loss: 2.3679e-03 | validation loss: 1.7565e-03\n",
      "Epoch: 179170 | training loss: 2.0627e-03 | validation loss: 1.6947e-03\n",
      "Epoch: 179180 | training loss: 2.0796e-03 | validation loss: 1.7116e-03\n",
      "Epoch: 179190 | training loss: 2.0727e-03 | validation loss: 1.6710e-03\n",
      "Epoch: 179200 | training loss: 2.0598e-03 | validation loss: 1.6899e-03\n",
      "Epoch: 179210 | training loss: 2.0570e-03 | validation loss: 1.6791e-03\n",
      "Epoch: 179220 | training loss: 2.0567e-03 | validation loss: 1.6830e-03\n",
      "Epoch: 179230 | training loss: 2.0566e-03 | validation loss: 1.6805e-03\n",
      "Epoch: 179240 | training loss: 2.0566e-03 | validation loss: 1.6829e-03\n",
      "Epoch: 179250 | training loss: 2.0565e-03 | validation loss: 1.6805e-03\n",
      "Epoch: 179260 | training loss: 2.0565e-03 | validation loss: 1.6812e-03\n",
      "Epoch: 179270 | training loss: 2.0565e-03 | validation loss: 1.6818e-03\n",
      "Epoch: 179280 | training loss: 2.0565e-03 | validation loss: 1.6821e-03\n",
      "Epoch: 179290 | training loss: 2.0567e-03 | validation loss: 1.6838e-03\n",
      "Epoch: 179300 | training loss: 2.0646e-03 | validation loss: 1.6974e-03\n",
      "Epoch: 179310 | training loss: 2.5284e-03 | validation loss: 1.9979e-03\n",
      "Epoch: 179320 | training loss: 2.1474e-03 | validation loss: 1.6793e-03\n",
      "Epoch: 179330 | training loss: 2.2911e-03 | validation loss: 1.8508e-03\n",
      "Epoch: 179340 | training loss: 2.0657e-03 | validation loss: 1.7031e-03\n",
      "Epoch: 179350 | training loss: 2.0841e-03 | validation loss: 1.6764e-03\n",
      "Epoch: 179360 | training loss: 2.0587e-03 | validation loss: 1.6785e-03\n",
      "Epoch: 179370 | training loss: 2.0586e-03 | validation loss: 1.6892e-03\n",
      "Epoch: 179380 | training loss: 2.0574e-03 | validation loss: 1.6801e-03\n",
      "Epoch: 179390 | training loss: 2.0565e-03 | validation loss: 1.6809e-03\n",
      "Epoch: 179400 | training loss: 2.0563e-03 | validation loss: 1.6822e-03\n",
      "Epoch: 179410 | training loss: 2.0562e-03 | validation loss: 1.6807e-03\n",
      "Epoch: 179420 | training loss: 2.0561e-03 | validation loss: 1.6814e-03\n",
      "Epoch: 179430 | training loss: 2.0561e-03 | validation loss: 1.6810e-03\n",
      "Epoch: 179440 | training loss: 2.0561e-03 | validation loss: 1.6811e-03\n",
      "Epoch: 179450 | training loss: 2.0560e-03 | validation loss: 1.6810e-03\n",
      "Epoch: 179460 | training loss: 2.0560e-03 | validation loss: 1.6809e-03\n",
      "Epoch: 179470 | training loss: 2.0560e-03 | validation loss: 1.6809e-03\n",
      "Epoch: 179480 | training loss: 2.0560e-03 | validation loss: 1.6809e-03\n",
      "Epoch: 179490 | training loss: 2.0560e-03 | validation loss: 1.6806e-03\n",
      "Epoch: 179500 | training loss: 2.0582e-03 | validation loss: 1.6808e-03\n",
      "Epoch: 179510 | training loss: 2.3449e-03 | validation loss: 1.8526e-03\n",
      "Epoch: 179520 | training loss: 3.5611e-03 | validation loss: 2.7722e-03\n",
      "Epoch: 179530 | training loss: 2.3080e-03 | validation loss: 1.9024e-03\n",
      "Epoch: 179540 | training loss: 2.1753e-03 | validation loss: 1.7325e-03\n",
      "Epoch: 179550 | training loss: 2.0570e-03 | validation loss: 1.6749e-03\n",
      "Epoch: 179560 | training loss: 2.0734e-03 | validation loss: 1.6937e-03\n",
      "Epoch: 179570 | training loss: 2.0597e-03 | validation loss: 1.6762e-03\n",
      "Epoch: 179580 | training loss: 2.0572e-03 | validation loss: 1.6749e-03\n",
      "Epoch: 179590 | training loss: 2.0561e-03 | validation loss: 1.6830e-03\n",
      "Epoch: 179600 | training loss: 2.0558e-03 | validation loss: 1.6800e-03\n",
      "Epoch: 179610 | training loss: 2.0557e-03 | validation loss: 1.6811e-03\n",
      "Epoch: 179620 | training loss: 2.0557e-03 | validation loss: 1.6806e-03\n",
      "Epoch: 179630 | training loss: 2.0556e-03 | validation loss: 1.6806e-03\n",
      "Epoch: 179640 | training loss: 2.0556e-03 | validation loss: 1.6802e-03\n",
      "Epoch: 179650 | training loss: 2.0556e-03 | validation loss: 1.6804e-03\n",
      "Epoch: 179660 | training loss: 2.0556e-03 | validation loss: 1.6803e-03\n",
      "Epoch: 179670 | training loss: 2.0555e-03 | validation loss: 1.6801e-03\n",
      "Epoch: 179680 | training loss: 2.0555e-03 | validation loss: 1.6801e-03\n",
      "Epoch: 179690 | training loss: 2.0555e-03 | validation loss: 1.6798e-03\n",
      "Epoch: 179700 | training loss: 2.0558e-03 | validation loss: 1.6779e-03\n",
      "Epoch: 179710 | training loss: 2.0798e-03 | validation loss: 1.6706e-03\n",
      "Epoch: 179720 | training loss: 3.7759e-03 | validation loss: 2.3231e-03\n",
      "Epoch: 179730 | training loss: 2.3733e-03 | validation loss: 1.8974e-03\n",
      "Epoch: 179740 | training loss: 2.2734e-03 | validation loss: 1.8382e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 179750 | training loss: 2.0787e-03 | validation loss: 1.7066e-03\n",
      "Epoch: 179760 | training loss: 2.0590e-03 | validation loss: 1.6725e-03\n",
      "Epoch: 179770 | training loss: 2.0654e-03 | validation loss: 1.6715e-03\n",
      "Epoch: 179780 | training loss: 2.0561e-03 | validation loss: 1.6766e-03\n",
      "Epoch: 179790 | training loss: 2.0562e-03 | validation loss: 1.6842e-03\n",
      "Epoch: 179800 | training loss: 2.0554e-03 | validation loss: 1.6819e-03\n",
      "Epoch: 179810 | training loss: 2.0554e-03 | validation loss: 1.6788e-03\n",
      "Epoch: 179820 | training loss: 2.0552e-03 | validation loss: 1.6804e-03\n",
      "Epoch: 179830 | training loss: 2.0552e-03 | validation loss: 1.6803e-03\n",
      "Epoch: 179840 | training loss: 2.0552e-03 | validation loss: 1.6796e-03\n",
      "Epoch: 179850 | training loss: 2.0551e-03 | validation loss: 1.6802e-03\n",
      "Epoch: 179860 | training loss: 2.0551e-03 | validation loss: 1.6798e-03\n",
      "Epoch: 179870 | training loss: 2.0551e-03 | validation loss: 1.6800e-03\n",
      "Epoch: 179880 | training loss: 2.0551e-03 | validation loss: 1.6799e-03\n",
      "Epoch: 179890 | training loss: 2.0551e-03 | validation loss: 1.6798e-03\n",
      "Epoch: 179900 | training loss: 2.0550e-03 | validation loss: 1.6799e-03\n",
      "Epoch: 179910 | training loss: 2.0550e-03 | validation loss: 1.6799e-03\n",
      "Epoch: 179920 | training loss: 2.0550e-03 | validation loss: 1.6799e-03\n",
      "Epoch: 179930 | training loss: 2.0550e-03 | validation loss: 1.6800e-03\n",
      "Epoch: 179940 | training loss: 2.0551e-03 | validation loss: 1.6814e-03\n",
      "Epoch: 179950 | training loss: 2.0714e-03 | validation loss: 1.7036e-03\n",
      "Epoch: 179960 | training loss: 3.8574e-03 | validation loss: 2.7089e-03\n",
      "Epoch: 179970 | training loss: 2.4523e-03 | validation loss: 1.7971e-03\n",
      "Epoch: 179980 | training loss: 2.3196e-03 | validation loss: 1.7390e-03\n",
      "Epoch: 179990 | training loss: 2.1465e-03 | validation loss: 1.6795e-03\n",
      "Epoch: 180000 | training loss: 2.0748e-03 | validation loss: 1.6680e-03\n",
      "Epoch: 180010 | training loss: 2.0556e-03 | validation loss: 1.6766e-03\n",
      "Epoch: 180020 | training loss: 2.0559e-03 | validation loss: 1.6853e-03\n",
      "Epoch: 180030 | training loss: 2.0564e-03 | validation loss: 1.6856e-03\n",
      "Epoch: 180040 | training loss: 2.0549e-03 | validation loss: 1.6815e-03\n",
      "Epoch: 180050 | training loss: 2.0548e-03 | validation loss: 1.6784e-03\n",
      "Epoch: 180060 | training loss: 2.0547e-03 | validation loss: 1.6786e-03\n",
      "Epoch: 180070 | training loss: 2.0547e-03 | validation loss: 1.6801e-03\n",
      "Epoch: 180080 | training loss: 2.0546e-03 | validation loss: 1.6796e-03\n",
      "Epoch: 180090 | training loss: 2.0546e-03 | validation loss: 1.6792e-03\n",
      "Epoch: 180100 | training loss: 2.0546e-03 | validation loss: 1.6796e-03\n",
      "Epoch: 180110 | training loss: 2.0546e-03 | validation loss: 1.6793e-03\n",
      "Epoch: 180120 | training loss: 2.0545e-03 | validation loss: 1.6794e-03\n",
      "Epoch: 180130 | training loss: 2.0545e-03 | validation loss: 1.6793e-03\n",
      "Epoch: 180140 | training loss: 2.0545e-03 | validation loss: 1.6793e-03\n",
      "Epoch: 180150 | training loss: 2.0545e-03 | validation loss: 1.6791e-03\n",
      "Epoch: 180160 | training loss: 2.0546e-03 | validation loss: 1.6780e-03\n",
      "Epoch: 180170 | training loss: 2.0747e-03 | validation loss: 1.6778e-03\n",
      "Epoch: 180180 | training loss: 2.5810e-03 | validation loss: 1.9575e-03\n",
      "Epoch: 180190 | training loss: 2.1782e-03 | validation loss: 1.7317e-03\n",
      "Epoch: 180200 | training loss: 2.0716e-03 | validation loss: 1.6889e-03\n",
      "Epoch: 180210 | training loss: 2.0548e-03 | validation loss: 1.6802e-03\n",
      "Epoch: 180220 | training loss: 2.0556e-03 | validation loss: 1.6803e-03\n",
      "Epoch: 180230 | training loss: 2.0577e-03 | validation loss: 1.6789e-03\n",
      "Epoch: 180240 | training loss: 2.0765e-03 | validation loss: 1.6705e-03\n",
      "Epoch: 180250 | training loss: 2.6060e-03 | validation loss: 1.8346e-03\n",
      "Epoch: 180260 | training loss: 2.1498e-03 | validation loss: 1.7690e-03\n",
      "Epoch: 180270 | training loss: 2.1707e-03 | validation loss: 1.6876e-03\n",
      "Epoch: 180280 | training loss: 2.1016e-03 | validation loss: 1.7313e-03\n",
      "Epoch: 180290 | training loss: 2.0632e-03 | validation loss: 1.6699e-03\n",
      "Epoch: 180300 | training loss: 2.0556e-03 | validation loss: 1.6844e-03\n",
      "Epoch: 180310 | training loss: 2.0546e-03 | validation loss: 1.6757e-03\n",
      "Epoch: 180320 | training loss: 2.0546e-03 | validation loss: 1.6818e-03\n",
      "Epoch: 180330 | training loss: 2.0545e-03 | validation loss: 1.6760e-03\n",
      "Epoch: 180340 | training loss: 2.0541e-03 | validation loss: 1.6797e-03\n",
      "Epoch: 180350 | training loss: 2.0541e-03 | validation loss: 1.6797e-03\n",
      "Epoch: 180360 | training loss: 2.0540e-03 | validation loss: 1.6788e-03\n",
      "Epoch: 180370 | training loss: 2.0540e-03 | validation loss: 1.6785e-03\n",
      "Epoch: 180380 | training loss: 2.0539e-03 | validation loss: 1.6784e-03\n",
      "Epoch: 180390 | training loss: 2.0539e-03 | validation loss: 1.6779e-03\n",
      "Epoch: 180400 | training loss: 2.0550e-03 | validation loss: 1.6743e-03\n",
      "Epoch: 180410 | training loss: 2.2487e-03 | validation loss: 1.7111e-03\n",
      "Epoch: 180420 | training loss: 2.1123e-03 | validation loss: 1.7036e-03\n",
      "Epoch: 180430 | training loss: 2.3524e-03 | validation loss: 1.7693e-03\n",
      "Epoch: 180440 | training loss: 2.1733e-03 | validation loss: 1.6946e-03\n",
      "Epoch: 180450 | training loss: 2.0915e-03 | validation loss: 1.6680e-03\n",
      "Epoch: 180460 | training loss: 2.0665e-03 | validation loss: 1.6658e-03\n",
      "Epoch: 180470 | training loss: 2.0599e-03 | validation loss: 1.6693e-03\n",
      "Epoch: 180480 | training loss: 2.0563e-03 | validation loss: 1.6727e-03\n",
      "Epoch: 180490 | training loss: 2.0540e-03 | validation loss: 1.6760e-03\n",
      "Epoch: 180500 | training loss: 2.0537e-03 | validation loss: 1.6792e-03\n",
      "Epoch: 180510 | training loss: 2.0538e-03 | validation loss: 1.6800e-03\n",
      "Epoch: 180520 | training loss: 2.0536e-03 | validation loss: 1.6788e-03\n",
      "Epoch: 180530 | training loss: 2.0536e-03 | validation loss: 1.6777e-03\n",
      "Epoch: 180540 | training loss: 2.0536e-03 | validation loss: 1.6778e-03\n",
      "Epoch: 180550 | training loss: 2.0536e-03 | validation loss: 1.6783e-03\n",
      "Epoch: 180560 | training loss: 2.0535e-03 | validation loss: 1.6781e-03\n",
      "Epoch: 180570 | training loss: 2.0535e-03 | validation loss: 1.6780e-03\n",
      "Epoch: 180580 | training loss: 2.0535e-03 | validation loss: 1.6781e-03\n",
      "Epoch: 180590 | training loss: 2.0535e-03 | validation loss: 1.6780e-03\n",
      "Epoch: 180600 | training loss: 2.0534e-03 | validation loss: 1.6780e-03\n",
      "Epoch: 180610 | training loss: 2.0534e-03 | validation loss: 1.6779e-03\n",
      "Epoch: 180620 | training loss: 2.0534e-03 | validation loss: 1.6779e-03\n",
      "Epoch: 180630 | training loss: 2.0534e-03 | validation loss: 1.6779e-03\n",
      "Epoch: 180640 | training loss: 2.0533e-03 | validation loss: 1.6779e-03\n",
      "Epoch: 180650 | training loss: 2.0533e-03 | validation loss: 1.6778e-03\n",
      "Epoch: 180660 | training loss: 2.0533e-03 | validation loss: 1.6778e-03\n",
      "Epoch: 180670 | training loss: 2.0579e-03 | validation loss: 1.6803e-03\n",
      "Epoch: 180680 | training loss: 2.7946e-03 | validation loss: 2.1212e-03\n",
      "Epoch: 180690 | training loss: 2.1726e-03 | validation loss: 1.8045e-03\n",
      "Epoch: 180700 | training loss: 2.1039e-03 | validation loss: 1.7341e-03\n",
      "Epoch: 180710 | training loss: 2.1191e-03 | validation loss: 1.6721e-03\n",
      "Epoch: 180720 | training loss: 2.0744e-03 | validation loss: 1.6826e-03\n",
      "Epoch: 180730 | training loss: 2.0602e-03 | validation loss: 1.6826e-03\n",
      "Epoch: 180740 | training loss: 2.0555e-03 | validation loss: 1.6707e-03\n",
      "Epoch: 180750 | training loss: 2.0539e-03 | validation loss: 1.6822e-03\n",
      "Epoch: 180760 | training loss: 2.0534e-03 | validation loss: 1.6759e-03\n",
      "Epoch: 180770 | training loss: 2.0532e-03 | validation loss: 1.6796e-03\n",
      "Epoch: 180780 | training loss: 2.0531e-03 | validation loss: 1.6768e-03\n",
      "Epoch: 180790 | training loss: 2.0530e-03 | validation loss: 1.6775e-03\n",
      "Epoch: 180800 | training loss: 2.0530e-03 | validation loss: 1.6777e-03\n",
      "Epoch: 180810 | training loss: 2.0530e-03 | validation loss: 1.6773e-03\n",
      "Epoch: 180820 | training loss: 2.0529e-03 | validation loss: 1.6771e-03\n",
      "Epoch: 180830 | training loss: 2.0529e-03 | validation loss: 1.6770e-03\n",
      "Epoch: 180840 | training loss: 2.0529e-03 | validation loss: 1.6765e-03\n",
      "Epoch: 180850 | training loss: 2.0543e-03 | validation loss: 1.6725e-03\n",
      "Epoch: 180860 | training loss: 2.2449e-03 | validation loss: 1.7065e-03\n",
      "Epoch: 180870 | training loss: 2.1305e-03 | validation loss: 1.6931e-03\n",
      "Epoch: 180880 | training loss: 2.4460e-03 | validation loss: 1.7983e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 180890 | training loss: 2.2017e-03 | validation loss: 1.7081e-03\n",
      "Epoch: 180900 | training loss: 2.0832e-03 | validation loss: 1.6753e-03\n",
      "Epoch: 180910 | training loss: 2.0541e-03 | validation loss: 1.6782e-03\n",
      "Epoch: 180920 | training loss: 2.0554e-03 | validation loss: 1.6867e-03\n",
      "Epoch: 180930 | training loss: 2.0553e-03 | validation loss: 1.6859e-03\n",
      "Epoch: 180940 | training loss: 2.0529e-03 | validation loss: 1.6793e-03\n",
      "Epoch: 180950 | training loss: 2.0529e-03 | validation loss: 1.6756e-03\n",
      "Epoch: 180960 | training loss: 2.0527e-03 | validation loss: 1.6763e-03\n",
      "Epoch: 180970 | training loss: 2.0527e-03 | validation loss: 1.6777e-03\n",
      "Epoch: 180980 | training loss: 2.0526e-03 | validation loss: 1.6769e-03\n",
      "Epoch: 180990 | training loss: 2.0526e-03 | validation loss: 1.6766e-03\n",
      "Epoch: 181000 | training loss: 2.0526e-03 | validation loss: 1.6771e-03\n",
      "Epoch: 181010 | training loss: 2.0525e-03 | validation loss: 1.6768e-03\n",
      "Epoch: 181020 | training loss: 2.0525e-03 | validation loss: 1.6769e-03\n",
      "Epoch: 181030 | training loss: 2.0525e-03 | validation loss: 1.6768e-03\n",
      "Epoch: 181040 | training loss: 2.0525e-03 | validation loss: 1.6768e-03\n",
      "Epoch: 181050 | training loss: 2.0524e-03 | validation loss: 1.6768e-03\n",
      "Epoch: 181060 | training loss: 2.0524e-03 | validation loss: 1.6767e-03\n",
      "Epoch: 181070 | training loss: 2.0524e-03 | validation loss: 1.6767e-03\n",
      "Epoch: 181080 | training loss: 2.0524e-03 | validation loss: 1.6767e-03\n",
      "Epoch: 181090 | training loss: 2.0523e-03 | validation loss: 1.6767e-03\n",
      "Epoch: 181100 | training loss: 2.0523e-03 | validation loss: 1.6767e-03\n",
      "Epoch: 181110 | training loss: 2.0523e-03 | validation loss: 1.6771e-03\n",
      "Epoch: 181120 | training loss: 2.0542e-03 | validation loss: 1.6830e-03\n",
      "Epoch: 181130 | training loss: 2.5465e-03 | validation loss: 2.0017e-03\n",
      "Epoch: 181140 | training loss: 3.0060e-03 | validation loss: 2.0026e-03\n",
      "Epoch: 181150 | training loss: 2.3035e-03 | validation loss: 1.7216e-03\n",
      "Epoch: 181160 | training loss: 2.1859e-03 | validation loss: 1.6838e-03\n",
      "Epoch: 181170 | training loss: 2.1116e-03 | validation loss: 1.6672e-03\n",
      "Epoch: 181180 | training loss: 2.0740e-03 | validation loss: 1.6657e-03\n",
      "Epoch: 181190 | training loss: 2.0599e-03 | validation loss: 1.6685e-03\n",
      "Epoch: 181200 | training loss: 2.0549e-03 | validation loss: 1.6704e-03\n",
      "Epoch: 181210 | training loss: 2.0531e-03 | validation loss: 1.6721e-03\n",
      "Epoch: 181220 | training loss: 2.0524e-03 | validation loss: 1.6742e-03\n",
      "Epoch: 181230 | training loss: 2.0521e-03 | validation loss: 1.6757e-03\n",
      "Epoch: 181240 | training loss: 2.0520e-03 | validation loss: 1.6764e-03\n",
      "Epoch: 181250 | training loss: 2.0520e-03 | validation loss: 1.6769e-03\n",
      "Epoch: 181260 | training loss: 2.0520e-03 | validation loss: 1.6766e-03\n",
      "Epoch: 181270 | training loss: 2.0519e-03 | validation loss: 1.6763e-03\n",
      "Epoch: 181280 | training loss: 2.0519e-03 | validation loss: 1.6761e-03\n",
      "Epoch: 181290 | training loss: 2.0519e-03 | validation loss: 1.6762e-03\n",
      "Epoch: 181300 | training loss: 2.0519e-03 | validation loss: 1.6762e-03\n",
      "Epoch: 181310 | training loss: 2.0519e-03 | validation loss: 1.6761e-03\n",
      "Epoch: 181320 | training loss: 2.0518e-03 | validation loss: 1.6761e-03\n",
      "Epoch: 181330 | training loss: 2.0518e-03 | validation loss: 1.6761e-03\n",
      "Epoch: 181340 | training loss: 2.0518e-03 | validation loss: 1.6761e-03\n",
      "Epoch: 181350 | training loss: 2.0518e-03 | validation loss: 1.6759e-03\n",
      "Epoch: 181360 | training loss: 2.0521e-03 | validation loss: 1.6743e-03\n",
      "Epoch: 181370 | training loss: 2.1269e-03 | validation loss: 1.7009e-03\n",
      "Epoch: 181380 | training loss: 2.0597e-03 | validation loss: 1.6835e-03\n",
      "Epoch: 181390 | training loss: 2.0945e-03 | validation loss: 1.6913e-03\n",
      "Epoch: 181400 | training loss: 2.0801e-03 | validation loss: 1.6794e-03\n",
      "Epoch: 181410 | training loss: 2.0659e-03 | validation loss: 1.6762e-03\n",
      "Epoch: 181420 | training loss: 2.0582e-03 | validation loss: 1.6739e-03\n",
      "Epoch: 181430 | training loss: 2.0538e-03 | validation loss: 1.6729e-03\n",
      "Epoch: 181440 | training loss: 2.0519e-03 | validation loss: 1.6738e-03\n",
      "Epoch: 181450 | training loss: 2.0516e-03 | validation loss: 1.6747e-03\n",
      "Epoch: 181460 | training loss: 2.0525e-03 | validation loss: 1.6724e-03\n",
      "Epoch: 181470 | training loss: 2.0858e-03 | validation loss: 1.6651e-03\n",
      "Epoch: 181480 | training loss: 3.2830e-03 | validation loss: 2.1035e-03\n",
      "Epoch: 181490 | training loss: 2.4414e-03 | validation loss: 1.9398e-03\n",
      "Epoch: 181500 | training loss: 2.0592e-03 | validation loss: 1.6899e-03\n",
      "Epoch: 181510 | training loss: 2.1061e-03 | validation loss: 1.6708e-03\n",
      "Epoch: 181520 | training loss: 2.0516e-03 | validation loss: 1.6780e-03\n",
      "Epoch: 181530 | training loss: 2.0573e-03 | validation loss: 1.6893e-03\n",
      "Epoch: 181540 | training loss: 2.0536e-03 | validation loss: 1.6707e-03\n",
      "Epoch: 181550 | training loss: 2.0515e-03 | validation loss: 1.6770e-03\n",
      "Epoch: 181560 | training loss: 2.0513e-03 | validation loss: 1.6752e-03\n",
      "Epoch: 181570 | training loss: 2.0513e-03 | validation loss: 1.6750e-03\n",
      "Epoch: 181580 | training loss: 2.0512e-03 | validation loss: 1.6753e-03\n",
      "Epoch: 181590 | training loss: 2.0512e-03 | validation loss: 1.6755e-03\n",
      "Epoch: 181600 | training loss: 2.0512e-03 | validation loss: 1.6748e-03\n",
      "Epoch: 181610 | training loss: 2.0512e-03 | validation loss: 1.6753e-03\n",
      "Epoch: 181620 | training loss: 2.0512e-03 | validation loss: 1.6754e-03\n",
      "Epoch: 181630 | training loss: 2.0511e-03 | validation loss: 1.6754e-03\n",
      "Epoch: 181640 | training loss: 2.0511e-03 | validation loss: 1.6760e-03\n",
      "Epoch: 181650 | training loss: 2.0522e-03 | validation loss: 1.6799e-03\n",
      "Epoch: 181660 | training loss: 2.1193e-03 | validation loss: 1.7418e-03\n",
      "Epoch: 181670 | training loss: 3.5185e-03 | validation loss: 2.5297e-03\n",
      "Epoch: 181680 | training loss: 2.0664e-03 | validation loss: 1.6919e-03\n",
      "Epoch: 181690 | training loss: 2.1929e-03 | validation loss: 1.6943e-03\n",
      "Epoch: 181700 | training loss: 2.0765e-03 | validation loss: 1.6603e-03\n",
      "Epoch: 181710 | training loss: 2.0571e-03 | validation loss: 1.6859e-03\n",
      "Epoch: 181720 | training loss: 2.0574e-03 | validation loss: 1.6928e-03\n",
      "Epoch: 181730 | training loss: 2.0513e-03 | validation loss: 1.6719e-03\n",
      "Epoch: 181740 | training loss: 2.0515e-03 | validation loss: 1.6711e-03\n",
      "Epoch: 181750 | training loss: 2.0512e-03 | validation loss: 1.6783e-03\n",
      "Epoch: 181760 | training loss: 2.0509e-03 | validation loss: 1.6741e-03\n",
      "Epoch: 181770 | training loss: 2.0508e-03 | validation loss: 1.6748e-03\n",
      "Epoch: 181780 | training loss: 2.0508e-03 | validation loss: 1.6751e-03\n",
      "Epoch: 181790 | training loss: 2.0508e-03 | validation loss: 1.6746e-03\n",
      "Epoch: 181800 | training loss: 2.0507e-03 | validation loss: 1.6749e-03\n",
      "Epoch: 181810 | training loss: 2.0507e-03 | validation loss: 1.6748e-03\n",
      "Epoch: 181820 | training loss: 2.0507e-03 | validation loss: 1.6746e-03\n",
      "Epoch: 181830 | training loss: 2.0507e-03 | validation loss: 1.6746e-03\n",
      "Epoch: 181840 | training loss: 2.0507e-03 | validation loss: 1.6745e-03\n",
      "Epoch: 181850 | training loss: 2.0513e-03 | validation loss: 1.6736e-03\n",
      "Epoch: 181860 | training loss: 2.1251e-03 | validation loss: 1.7076e-03\n",
      "Epoch: 181870 | training loss: 2.1477e-03 | validation loss: 1.7607e-03\n",
      "Epoch: 181880 | training loss: 2.3631e-03 | validation loss: 1.8736e-03\n",
      "Epoch: 181890 | training loss: 2.1398e-03 | validation loss: 1.6832e-03\n",
      "Epoch: 181900 | training loss: 2.0689e-03 | validation loss: 1.6697e-03\n",
      "Epoch: 181910 | training loss: 2.0661e-03 | validation loss: 1.6932e-03\n",
      "Epoch: 181920 | training loss: 2.0519e-03 | validation loss: 1.6797e-03\n",
      "Epoch: 181930 | training loss: 2.0508e-03 | validation loss: 1.6742e-03\n",
      "Epoch: 181940 | training loss: 2.0529e-03 | validation loss: 1.6702e-03\n",
      "Epoch: 181950 | training loss: 2.0932e-03 | validation loss: 1.6651e-03\n",
      "Epoch: 181960 | training loss: 2.8855e-03 | validation loss: 1.9420e-03\n",
      "Epoch: 181970 | training loss: 2.3513e-03 | validation loss: 1.8904e-03\n",
      "Epoch: 181980 | training loss: 2.1449e-03 | validation loss: 1.6774e-03\n",
      "Epoch: 181990 | training loss: 2.0582e-03 | validation loss: 1.6901e-03\n",
      "Epoch: 182000 | training loss: 2.0504e-03 | validation loss: 1.6759e-03\n",
      "Epoch: 182010 | training loss: 2.0510e-03 | validation loss: 1.6711e-03\n",
      "Epoch: 182020 | training loss: 2.0505e-03 | validation loss: 1.6761e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 182030 | training loss: 2.0502e-03 | validation loss: 1.6740e-03\n",
      "Epoch: 182040 | training loss: 2.0504e-03 | validation loss: 1.6724e-03\n",
      "Epoch: 182050 | training loss: 2.0503e-03 | validation loss: 1.6758e-03\n",
      "Epoch: 182060 | training loss: 2.0502e-03 | validation loss: 1.6746e-03\n",
      "Epoch: 182070 | training loss: 2.0501e-03 | validation loss: 1.6736e-03\n",
      "Epoch: 182080 | training loss: 2.0502e-03 | validation loss: 1.6730e-03\n",
      "Epoch: 182090 | training loss: 2.0509e-03 | validation loss: 1.6704e-03\n",
      "Epoch: 182100 | training loss: 2.0859e-03 | validation loss: 1.6656e-03\n",
      "Epoch: 182110 | training loss: 3.3917e-03 | validation loss: 2.1624e-03\n",
      "Epoch: 182120 | training loss: 2.4592e-03 | validation loss: 1.9457e-03\n",
      "Epoch: 182130 | training loss: 2.0783e-03 | validation loss: 1.7169e-03\n",
      "Epoch: 182140 | training loss: 2.0993e-03 | validation loss: 1.6761e-03\n",
      "Epoch: 182150 | training loss: 2.0538e-03 | validation loss: 1.6653e-03\n",
      "Epoch: 182160 | training loss: 2.0566e-03 | validation loss: 1.6851e-03\n",
      "Epoch: 182170 | training loss: 2.0513e-03 | validation loss: 1.6753e-03\n",
      "Epoch: 182180 | training loss: 2.0501e-03 | validation loss: 1.6716e-03\n",
      "Epoch: 182190 | training loss: 2.0500e-03 | validation loss: 1.6752e-03\n",
      "Epoch: 182200 | training loss: 2.0499e-03 | validation loss: 1.6733e-03\n",
      "Epoch: 182210 | training loss: 2.0499e-03 | validation loss: 1.6739e-03\n",
      "Epoch: 182220 | training loss: 2.0498e-03 | validation loss: 1.6736e-03\n",
      "Epoch: 182230 | training loss: 2.0498e-03 | validation loss: 1.6736e-03\n",
      "Epoch: 182240 | training loss: 2.0498e-03 | validation loss: 1.6736e-03\n",
      "Epoch: 182250 | training loss: 2.0497e-03 | validation loss: 1.6737e-03\n",
      "Epoch: 182260 | training loss: 2.0497e-03 | validation loss: 1.6735e-03\n",
      "Epoch: 182270 | training loss: 2.0497e-03 | validation loss: 1.6734e-03\n",
      "Epoch: 182280 | training loss: 2.0497e-03 | validation loss: 1.6730e-03\n",
      "Epoch: 182290 | training loss: 2.0531e-03 | validation loss: 1.6713e-03\n",
      "Epoch: 182300 | training loss: 2.3569e-03 | validation loss: 1.7930e-03\n",
      "Epoch: 182310 | training loss: 2.4399e-03 | validation loss: 1.8390e-03\n",
      "Epoch: 182320 | training loss: 2.1330e-03 | validation loss: 1.6674e-03\n",
      "Epoch: 182330 | training loss: 2.0751e-03 | validation loss: 1.7100e-03\n",
      "Epoch: 182340 | training loss: 2.0575e-03 | validation loss: 1.6906e-03\n",
      "Epoch: 182350 | training loss: 2.0544e-03 | validation loss: 1.6656e-03\n",
      "Epoch: 182360 | training loss: 2.0496e-03 | validation loss: 1.6733e-03\n",
      "Epoch: 182370 | training loss: 2.0504e-03 | validation loss: 1.6787e-03\n",
      "Epoch: 182380 | training loss: 2.0500e-03 | validation loss: 1.6722e-03\n",
      "Epoch: 182390 | training loss: 2.0497e-03 | validation loss: 1.6757e-03\n",
      "Epoch: 182400 | training loss: 2.0495e-03 | validation loss: 1.6727e-03\n",
      "Epoch: 182410 | training loss: 2.0494e-03 | validation loss: 1.6736e-03\n",
      "Epoch: 182420 | training loss: 2.0494e-03 | validation loss: 1.6724e-03\n",
      "Epoch: 182430 | training loss: 2.0493e-03 | validation loss: 1.6731e-03\n",
      "Epoch: 182440 | training loss: 2.0493e-03 | validation loss: 1.6732e-03\n",
      "Epoch: 182450 | training loss: 2.0493e-03 | validation loss: 1.6730e-03\n",
      "Epoch: 182460 | training loss: 2.0493e-03 | validation loss: 1.6728e-03\n",
      "Epoch: 182470 | training loss: 2.0492e-03 | validation loss: 1.6727e-03\n",
      "Epoch: 182480 | training loss: 2.0493e-03 | validation loss: 1.6714e-03\n",
      "Epoch: 182490 | training loss: 2.0567e-03 | validation loss: 1.6636e-03\n",
      "Epoch: 182500 | training loss: 2.8739e-03 | validation loss: 1.9367e-03\n",
      "Epoch: 182510 | training loss: 2.7293e-03 | validation loss: 2.1278e-03\n",
      "Epoch: 182520 | training loss: 2.1418e-03 | validation loss: 1.7753e-03\n",
      "Epoch: 182530 | training loss: 2.1092e-03 | validation loss: 1.7092e-03\n",
      "Epoch: 182540 | training loss: 2.0554e-03 | validation loss: 1.6701e-03\n",
      "Epoch: 182550 | training loss: 2.0528e-03 | validation loss: 1.6724e-03\n",
      "Epoch: 182560 | training loss: 2.0529e-03 | validation loss: 1.6715e-03\n",
      "Epoch: 182570 | training loss: 2.0499e-03 | validation loss: 1.6680e-03\n",
      "Epoch: 182580 | training loss: 2.0491e-03 | validation loss: 1.6720e-03\n",
      "Epoch: 182590 | training loss: 2.0492e-03 | validation loss: 1.6752e-03\n",
      "Epoch: 182600 | training loss: 2.0490e-03 | validation loss: 1.6727e-03\n",
      "Epoch: 182610 | training loss: 2.0489e-03 | validation loss: 1.6720e-03\n",
      "Epoch: 182620 | training loss: 2.0489e-03 | validation loss: 1.6726e-03\n",
      "Epoch: 182630 | training loss: 2.0489e-03 | validation loss: 1.6727e-03\n",
      "Epoch: 182640 | training loss: 2.0489e-03 | validation loss: 1.6723e-03\n",
      "Epoch: 182650 | training loss: 2.0488e-03 | validation loss: 1.6726e-03\n",
      "Epoch: 182660 | training loss: 2.0488e-03 | validation loss: 1.6724e-03\n",
      "Epoch: 182670 | training loss: 2.0488e-03 | validation loss: 1.6724e-03\n",
      "Epoch: 182680 | training loss: 2.0488e-03 | validation loss: 1.6724e-03\n",
      "Epoch: 182690 | training loss: 2.0487e-03 | validation loss: 1.6723e-03\n",
      "Epoch: 182700 | training loss: 2.0487e-03 | validation loss: 1.6722e-03\n",
      "Epoch: 182710 | training loss: 2.0487e-03 | validation loss: 1.6717e-03\n",
      "Epoch: 182720 | training loss: 2.0545e-03 | validation loss: 1.6684e-03\n",
      "Epoch: 182730 | training loss: 3.0149e-03 | validation loss: 2.0612e-03\n",
      "Epoch: 182740 | training loss: 2.8956e-03 | validation loss: 2.1830e-03\n",
      "Epoch: 182750 | training loss: 2.1988e-03 | validation loss: 1.7567e-03\n",
      "Epoch: 182760 | training loss: 2.1000e-03 | validation loss: 1.6740e-03\n",
      "Epoch: 182770 | training loss: 2.0712e-03 | validation loss: 1.6593e-03\n",
      "Epoch: 182780 | training loss: 2.0532e-03 | validation loss: 1.6635e-03\n",
      "Epoch: 182790 | training loss: 2.0486e-03 | validation loss: 1.6735e-03\n",
      "Epoch: 182800 | training loss: 2.0494e-03 | validation loss: 1.6775e-03\n",
      "Epoch: 182810 | training loss: 2.0485e-03 | validation loss: 1.6732e-03\n",
      "Epoch: 182820 | training loss: 2.0486e-03 | validation loss: 1.6705e-03\n",
      "Epoch: 182830 | training loss: 2.0485e-03 | validation loss: 1.6719e-03\n",
      "Epoch: 182840 | training loss: 2.0484e-03 | validation loss: 1.6723e-03\n",
      "Epoch: 182850 | training loss: 2.0484e-03 | validation loss: 1.6714e-03\n",
      "Epoch: 182860 | training loss: 2.0484e-03 | validation loss: 1.6721e-03\n",
      "Epoch: 182870 | training loss: 2.0484e-03 | validation loss: 1.6719e-03\n",
      "Epoch: 182880 | training loss: 2.0483e-03 | validation loss: 1.6720e-03\n",
      "Epoch: 182890 | training loss: 2.0483e-03 | validation loss: 1.6718e-03\n",
      "Epoch: 182900 | training loss: 2.0483e-03 | validation loss: 1.6719e-03\n",
      "Epoch: 182910 | training loss: 2.0483e-03 | validation loss: 1.6718e-03\n",
      "Epoch: 182920 | training loss: 2.0482e-03 | validation loss: 1.6718e-03\n",
      "Epoch: 182930 | training loss: 2.0482e-03 | validation loss: 1.6718e-03\n",
      "Epoch: 182940 | training loss: 2.0482e-03 | validation loss: 1.6717e-03\n",
      "Epoch: 182950 | training loss: 2.0482e-03 | validation loss: 1.6716e-03\n",
      "Epoch: 182960 | training loss: 2.0482e-03 | validation loss: 1.6706e-03\n",
      "Epoch: 182970 | training loss: 2.0536e-03 | validation loss: 1.6629e-03\n",
      "Epoch: 182980 | training loss: 2.9435e-03 | validation loss: 1.9655e-03\n",
      "Epoch: 182990 | training loss: 2.8462e-03 | validation loss: 2.2135e-03\n",
      "Epoch: 183000 | training loss: 2.2941e-03 | validation loss: 1.9097e-03\n",
      "Epoch: 183010 | training loss: 2.1560e-03 | validation loss: 1.7845e-03\n",
      "Epoch: 183020 | training loss: 2.0824e-03 | validation loss: 1.7185e-03\n",
      "Epoch: 183030 | training loss: 2.0594e-03 | validation loss: 1.6957e-03\n",
      "Epoch: 183040 | training loss: 2.0525e-03 | validation loss: 1.6854e-03\n",
      "Epoch: 183050 | training loss: 2.0495e-03 | validation loss: 1.6778e-03\n",
      "Epoch: 183060 | training loss: 2.0485e-03 | validation loss: 1.6746e-03\n",
      "Epoch: 183070 | training loss: 2.0480e-03 | validation loss: 1.6733e-03\n",
      "Epoch: 183080 | training loss: 2.0479e-03 | validation loss: 1.6714e-03\n",
      "Epoch: 183090 | training loss: 2.0479e-03 | validation loss: 1.6706e-03\n",
      "Epoch: 183100 | training loss: 2.0479e-03 | validation loss: 1.6708e-03\n",
      "Epoch: 183110 | training loss: 2.0478e-03 | validation loss: 1.6710e-03\n",
      "Epoch: 183120 | training loss: 2.0478e-03 | validation loss: 1.6713e-03\n",
      "Epoch: 183130 | training loss: 2.0478e-03 | validation loss: 1.6711e-03\n",
      "Epoch: 183140 | training loss: 2.0478e-03 | validation loss: 1.6710e-03\n",
      "Epoch: 183150 | training loss: 2.0477e-03 | validation loss: 1.6711e-03\n",
      "Epoch: 183160 | training loss: 2.0477e-03 | validation loss: 1.6710e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 183170 | training loss: 2.0477e-03 | validation loss: 1.6710e-03\n",
      "Epoch: 183180 | training loss: 2.0477e-03 | validation loss: 1.6711e-03\n",
      "Epoch: 183190 | training loss: 2.0477e-03 | validation loss: 1.6714e-03\n",
      "Epoch: 183200 | training loss: 2.0503e-03 | validation loss: 1.6763e-03\n",
      "Epoch: 183210 | training loss: 2.4729e-03 | validation loss: 1.9397e-03\n",
      "Epoch: 183220 | training loss: 2.3425e-03 | validation loss: 1.8273e-03\n",
      "Epoch: 183230 | training loss: 2.2602e-03 | validation loss: 1.8723e-03\n",
      "Epoch: 183240 | training loss: 2.1756e-03 | validation loss: 1.7999e-03\n",
      "Epoch: 183250 | training loss: 2.0752e-03 | validation loss: 1.7149e-03\n",
      "Epoch: 183260 | training loss: 2.0530e-03 | validation loss: 1.6794e-03\n",
      "Epoch: 183270 | training loss: 2.0530e-03 | validation loss: 1.6704e-03\n",
      "Epoch: 183280 | training loss: 2.0481e-03 | validation loss: 1.6712e-03\n",
      "Epoch: 183290 | training loss: 2.0479e-03 | validation loss: 1.6742e-03\n",
      "Epoch: 183300 | training loss: 2.0474e-03 | validation loss: 1.6711e-03\n",
      "Epoch: 183310 | training loss: 2.0475e-03 | validation loss: 1.6691e-03\n",
      "Epoch: 183320 | training loss: 2.0474e-03 | validation loss: 1.6708e-03\n",
      "Epoch: 183330 | training loss: 2.0473e-03 | validation loss: 1.6707e-03\n",
      "Epoch: 183340 | training loss: 2.0473e-03 | validation loss: 1.6706e-03\n",
      "Epoch: 183350 | training loss: 2.0473e-03 | validation loss: 1.6707e-03\n",
      "Epoch: 183360 | training loss: 2.0473e-03 | validation loss: 1.6705e-03\n",
      "Epoch: 183370 | training loss: 2.0472e-03 | validation loss: 1.6706e-03\n",
      "Epoch: 183380 | training loss: 2.0472e-03 | validation loss: 1.6706e-03\n",
      "Epoch: 183390 | training loss: 2.0472e-03 | validation loss: 1.6705e-03\n",
      "Epoch: 183400 | training loss: 2.0472e-03 | validation loss: 1.6705e-03\n",
      "Epoch: 183410 | training loss: 2.0472e-03 | validation loss: 1.6705e-03\n",
      "Epoch: 183420 | training loss: 2.0471e-03 | validation loss: 1.6704e-03\n",
      "Epoch: 183430 | training loss: 2.0471e-03 | validation loss: 1.6701e-03\n",
      "Epoch: 183440 | training loss: 2.0476e-03 | validation loss: 1.6678e-03\n",
      "Epoch: 183450 | training loss: 2.1165e-03 | validation loss: 1.6701e-03\n",
      "Epoch: 183460 | training loss: 3.6964e-03 | validation loss: 2.3051e-03\n",
      "Epoch: 183470 | training loss: 2.4582e-03 | validation loss: 1.7777e-03\n",
      "Epoch: 183480 | training loss: 2.1166e-03 | validation loss: 1.6611e-03\n",
      "Epoch: 183490 | training loss: 2.0553e-03 | validation loss: 1.6578e-03\n",
      "Epoch: 183500 | training loss: 2.0471e-03 | validation loss: 1.6721e-03\n",
      "Epoch: 183510 | training loss: 2.0496e-03 | validation loss: 1.6804e-03\n",
      "Epoch: 183520 | training loss: 2.0499e-03 | validation loss: 1.6793e-03\n",
      "Epoch: 183530 | training loss: 2.0479e-03 | validation loss: 1.6753e-03\n",
      "Epoch: 183540 | training loss: 2.0469e-03 | validation loss: 1.6711e-03\n",
      "Epoch: 183550 | training loss: 2.0470e-03 | validation loss: 1.6686e-03\n",
      "Epoch: 183560 | training loss: 2.0469e-03 | validation loss: 1.6695e-03\n",
      "Epoch: 183570 | training loss: 2.0468e-03 | validation loss: 1.6707e-03\n",
      "Epoch: 183580 | training loss: 2.0468e-03 | validation loss: 1.6704e-03\n",
      "Epoch: 183590 | training loss: 2.0468e-03 | validation loss: 1.6698e-03\n",
      "Epoch: 183600 | training loss: 2.0467e-03 | validation loss: 1.6702e-03\n",
      "Epoch: 183610 | training loss: 2.0467e-03 | validation loss: 1.6701e-03\n",
      "Epoch: 183620 | training loss: 2.0467e-03 | validation loss: 1.6700e-03\n",
      "Epoch: 183630 | training loss: 2.0467e-03 | validation loss: 1.6700e-03\n",
      "Epoch: 183640 | training loss: 2.0466e-03 | validation loss: 1.6699e-03\n",
      "Epoch: 183650 | training loss: 2.0466e-03 | validation loss: 1.6698e-03\n",
      "Epoch: 183660 | training loss: 2.0468e-03 | validation loss: 1.6687e-03\n",
      "Epoch: 183670 | training loss: 2.0816e-03 | validation loss: 1.6767e-03\n",
      "Epoch: 183680 | training loss: 2.2567e-03 | validation loss: 1.7754e-03\n",
      "Epoch: 183690 | training loss: 2.2132e-03 | validation loss: 1.7513e-03\n",
      "Epoch: 183700 | training loss: 2.0872e-03 | validation loss: 1.6803e-03\n",
      "Epoch: 183710 | training loss: 2.0521e-03 | validation loss: 1.6664e-03\n",
      "Epoch: 183720 | training loss: 2.0465e-03 | validation loss: 1.6692e-03\n",
      "Epoch: 183730 | training loss: 2.0475e-03 | validation loss: 1.6726e-03\n",
      "Epoch: 183740 | training loss: 2.0474e-03 | validation loss: 1.6723e-03\n",
      "Epoch: 183750 | training loss: 2.0466e-03 | validation loss: 1.6690e-03\n",
      "Epoch: 183760 | training loss: 2.0549e-03 | validation loss: 1.6597e-03\n",
      "Epoch: 183770 | training loss: 3.1374e-03 | validation loss: 2.0395e-03\n",
      "Epoch: 183780 | training loss: 2.8661e-03 | validation loss: 2.1772e-03\n",
      "Epoch: 183790 | training loss: 2.1802e-03 | validation loss: 1.7815e-03\n",
      "Epoch: 183800 | training loss: 2.0478e-03 | validation loss: 1.6748e-03\n",
      "Epoch: 183810 | training loss: 2.0562e-03 | validation loss: 1.6603e-03\n",
      "Epoch: 183820 | training loss: 2.0581e-03 | validation loss: 1.6605e-03\n",
      "Epoch: 183830 | training loss: 2.0483e-03 | validation loss: 1.6646e-03\n",
      "Epoch: 183840 | training loss: 2.0465e-03 | validation loss: 1.6719e-03\n",
      "Epoch: 183850 | training loss: 2.0468e-03 | validation loss: 1.6730e-03\n",
      "Epoch: 183860 | training loss: 2.0462e-03 | validation loss: 1.6693e-03\n",
      "Epoch: 183870 | training loss: 2.0462e-03 | validation loss: 1.6685e-03\n",
      "Epoch: 183880 | training loss: 2.0461e-03 | validation loss: 1.6698e-03\n",
      "Epoch: 183890 | training loss: 2.0461e-03 | validation loss: 1.6691e-03\n",
      "Epoch: 183900 | training loss: 2.0461e-03 | validation loss: 1.6690e-03\n",
      "Epoch: 183910 | training loss: 2.0461e-03 | validation loss: 1.6693e-03\n",
      "Epoch: 183920 | training loss: 2.0460e-03 | validation loss: 1.6690e-03\n",
      "Epoch: 183930 | training loss: 2.0460e-03 | validation loss: 1.6691e-03\n",
      "Epoch: 183940 | training loss: 2.0460e-03 | validation loss: 1.6691e-03\n",
      "Epoch: 183950 | training loss: 2.0460e-03 | validation loss: 1.6690e-03\n",
      "Epoch: 183960 | training loss: 2.0459e-03 | validation loss: 1.6690e-03\n",
      "Epoch: 183970 | training loss: 2.0459e-03 | validation loss: 1.6690e-03\n",
      "Epoch: 183980 | training loss: 2.0459e-03 | validation loss: 1.6690e-03\n",
      "Epoch: 183990 | training loss: 2.0459e-03 | validation loss: 1.6690e-03\n",
      "Epoch: 184000 | training loss: 2.0459e-03 | validation loss: 1.6694e-03\n",
      "Epoch: 184010 | training loss: 2.0470e-03 | validation loss: 1.6738e-03\n",
      "Epoch: 184020 | training loss: 2.2259e-03 | validation loss: 1.8088e-03\n",
      "Epoch: 184030 | training loss: 2.1609e-03 | validation loss: 1.7573e-03\n",
      "Epoch: 184040 | training loss: 2.4249e-03 | validation loss: 1.9262e-03\n",
      "Epoch: 184050 | training loss: 2.2089e-03 | validation loss: 1.8069e-03\n",
      "Epoch: 184060 | training loss: 2.1089e-03 | validation loss: 1.7401e-03\n",
      "Epoch: 184070 | training loss: 2.0662e-03 | validation loss: 1.7012e-03\n",
      "Epoch: 184080 | training loss: 2.0501e-03 | validation loss: 1.6798e-03\n",
      "Epoch: 184090 | training loss: 2.0458e-03 | validation loss: 1.6705e-03\n",
      "Epoch: 184100 | training loss: 2.0460e-03 | validation loss: 1.6668e-03\n",
      "Epoch: 184110 | training loss: 2.0460e-03 | validation loss: 1.6659e-03\n",
      "Epoch: 184120 | training loss: 2.0456e-03 | validation loss: 1.6681e-03\n",
      "Epoch: 184130 | training loss: 2.0456e-03 | validation loss: 1.6695e-03\n",
      "Epoch: 184140 | training loss: 2.0456e-03 | validation loss: 1.6691e-03\n",
      "Epoch: 184150 | training loss: 2.0455e-03 | validation loss: 1.6683e-03\n",
      "Epoch: 184160 | training loss: 2.0455e-03 | validation loss: 1.6686e-03\n",
      "Epoch: 184170 | training loss: 2.0455e-03 | validation loss: 1.6687e-03\n",
      "Epoch: 184180 | training loss: 2.0455e-03 | validation loss: 1.6684e-03\n",
      "Epoch: 184190 | training loss: 2.0454e-03 | validation loss: 1.6686e-03\n",
      "Epoch: 184200 | training loss: 2.0454e-03 | validation loss: 1.6685e-03\n",
      "Epoch: 184210 | training loss: 2.0454e-03 | validation loss: 1.6685e-03\n",
      "Epoch: 184220 | training loss: 2.0454e-03 | validation loss: 1.6685e-03\n",
      "Epoch: 184230 | training loss: 2.0454e-03 | validation loss: 1.6689e-03\n",
      "Epoch: 184240 | training loss: 2.0503e-03 | validation loss: 1.6785e-03\n",
      "Epoch: 184250 | training loss: 2.8324e-03 | validation loss: 2.2867e-03\n",
      "Epoch: 184260 | training loss: 2.0462e-03 | validation loss: 1.6679e-03\n",
      "Epoch: 184270 | training loss: 2.0497e-03 | validation loss: 1.6668e-03\n",
      "Epoch: 184280 | training loss: 2.0467e-03 | validation loss: 1.6656e-03\n",
      "Epoch: 184290 | training loss: 2.0459e-03 | validation loss: 1.6656e-03\n",
      "Epoch: 184300 | training loss: 2.0459e-03 | validation loss: 1.6659e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 184310 | training loss: 2.0459e-03 | validation loss: 1.6652e-03\n",
      "Epoch: 184320 | training loss: 2.0471e-03 | validation loss: 1.6617e-03\n",
      "Epoch: 184330 | training loss: 2.1188e-03 | validation loss: 1.6629e-03\n",
      "Epoch: 184340 | training loss: 3.1713e-03 | validation loss: 2.0546e-03\n",
      "Epoch: 184350 | training loss: 2.0822e-03 | validation loss: 1.7069e-03\n",
      "Epoch: 184360 | training loss: 2.1580e-03 | validation loss: 1.7647e-03\n",
      "Epoch: 184370 | training loss: 2.0529e-03 | validation loss: 1.6581e-03\n",
      "Epoch: 184380 | training loss: 2.0566e-03 | validation loss: 1.6580e-03\n",
      "Epoch: 184390 | training loss: 2.0493e-03 | validation loss: 1.6781e-03\n",
      "Epoch: 184400 | training loss: 2.0450e-03 | validation loss: 1.6674e-03\n",
      "Epoch: 184410 | training loss: 2.0453e-03 | validation loss: 1.6652e-03\n",
      "Epoch: 184420 | training loss: 2.0452e-03 | validation loss: 1.6701e-03\n",
      "Epoch: 184430 | training loss: 2.0450e-03 | validation loss: 1.6664e-03\n",
      "Epoch: 184440 | training loss: 2.0449e-03 | validation loss: 1.6686e-03\n",
      "Epoch: 184450 | training loss: 2.0449e-03 | validation loss: 1.6673e-03\n",
      "Epoch: 184460 | training loss: 2.0448e-03 | validation loss: 1.6676e-03\n",
      "Epoch: 184470 | training loss: 2.0448e-03 | validation loss: 1.6679e-03\n",
      "Epoch: 184480 | training loss: 2.0448e-03 | validation loss: 1.6677e-03\n",
      "Epoch: 184490 | training loss: 2.0448e-03 | validation loss: 1.6675e-03\n",
      "Epoch: 184500 | training loss: 2.0447e-03 | validation loss: 1.6674e-03\n",
      "Epoch: 184510 | training loss: 2.0447e-03 | validation loss: 1.6669e-03\n",
      "Epoch: 184520 | training loss: 2.0463e-03 | validation loss: 1.6630e-03\n",
      "Epoch: 184530 | training loss: 2.2327e-03 | validation loss: 1.6994e-03\n",
      "Epoch: 184540 | training loss: 2.1946e-03 | validation loss: 1.7064e-03\n",
      "Epoch: 184550 | training loss: 2.4603e-03 | validation loss: 1.7874e-03\n",
      "Epoch: 184560 | training loss: 2.1568e-03 | validation loss: 1.6703e-03\n",
      "Epoch: 184570 | training loss: 2.0649e-03 | validation loss: 1.6544e-03\n",
      "Epoch: 184580 | training loss: 2.0510e-03 | validation loss: 1.6669e-03\n",
      "Epoch: 184590 | training loss: 2.0476e-03 | validation loss: 1.6753e-03\n",
      "Epoch: 184600 | training loss: 2.0465e-03 | validation loss: 1.6760e-03\n",
      "Epoch: 184610 | training loss: 2.0449e-03 | validation loss: 1.6697e-03\n",
      "Epoch: 184620 | training loss: 2.0447e-03 | validation loss: 1.6654e-03\n",
      "Epoch: 184630 | training loss: 2.0445e-03 | validation loss: 1.6664e-03\n",
      "Epoch: 184640 | training loss: 2.0445e-03 | validation loss: 1.6683e-03\n",
      "Epoch: 184650 | training loss: 2.0444e-03 | validation loss: 1.6673e-03\n",
      "Epoch: 184660 | training loss: 2.0444e-03 | validation loss: 1.6670e-03\n",
      "Epoch: 184670 | training loss: 2.0444e-03 | validation loss: 1.6674e-03\n",
      "Epoch: 184680 | training loss: 2.0444e-03 | validation loss: 1.6671e-03\n",
      "Epoch: 184690 | training loss: 2.0443e-03 | validation loss: 1.6672e-03\n",
      "Epoch: 184700 | training loss: 2.0443e-03 | validation loss: 1.6671e-03\n",
      "Epoch: 184710 | training loss: 2.0443e-03 | validation loss: 1.6672e-03\n",
      "Epoch: 184720 | training loss: 2.0443e-03 | validation loss: 1.6671e-03\n",
      "Epoch: 184730 | training loss: 2.0442e-03 | validation loss: 1.6671e-03\n",
      "Epoch: 184740 | training loss: 2.0442e-03 | validation loss: 1.6670e-03\n",
      "Epoch: 184750 | training loss: 2.0442e-03 | validation loss: 1.6670e-03\n",
      "Epoch: 184760 | training loss: 2.0443e-03 | validation loss: 1.6669e-03\n",
      "Epoch: 184770 | training loss: 2.0635e-03 | validation loss: 1.6767e-03\n",
      "Epoch: 184780 | training loss: 2.9794e-03 | validation loss: 2.2201e-03\n",
      "Epoch: 184790 | training loss: 2.2762e-03 | validation loss: 1.7782e-03\n",
      "Epoch: 184800 | training loss: 2.2188e-03 | validation loss: 1.7738e-03\n",
      "Epoch: 184810 | training loss: 2.1170e-03 | validation loss: 1.6659e-03\n",
      "Epoch: 184820 | training loss: 2.0536e-03 | validation loss: 1.6743e-03\n",
      "Epoch: 184830 | training loss: 2.0441e-03 | validation loss: 1.6652e-03\n",
      "Epoch: 184840 | training loss: 2.0442e-03 | validation loss: 1.6663e-03\n",
      "Epoch: 184850 | training loss: 2.0442e-03 | validation loss: 1.6686e-03\n",
      "Epoch: 184860 | training loss: 2.0440e-03 | validation loss: 1.6676e-03\n",
      "Epoch: 184870 | training loss: 2.0440e-03 | validation loss: 1.6658e-03\n",
      "Epoch: 184880 | training loss: 2.0440e-03 | validation loss: 1.6676e-03\n",
      "Epoch: 184890 | training loss: 2.0439e-03 | validation loss: 1.6668e-03\n",
      "Epoch: 184900 | training loss: 2.0439e-03 | validation loss: 1.6662e-03\n",
      "Epoch: 184910 | training loss: 2.0439e-03 | validation loss: 1.6658e-03\n",
      "Epoch: 184920 | training loss: 2.0440e-03 | validation loss: 1.6645e-03\n",
      "Epoch: 184930 | training loss: 2.0514e-03 | validation loss: 1.6577e-03\n",
      "Epoch: 184940 | training loss: 2.6190e-03 | validation loss: 1.8338e-03\n",
      "Epoch: 184950 | training loss: 2.3449e-03 | validation loss: 1.8889e-03\n",
      "Epoch: 184960 | training loss: 2.1805e-03 | validation loss: 1.6906e-03\n",
      "Epoch: 184970 | training loss: 2.1139e-03 | validation loss: 1.6694e-03\n",
      "Epoch: 184980 | training loss: 2.0466e-03 | validation loss: 1.6760e-03\n",
      "Epoch: 184990 | training loss: 2.0553e-03 | validation loss: 1.6868e-03\n",
      "Epoch: 185000 | training loss: 2.0441e-03 | validation loss: 1.6646e-03\n",
      "Epoch: 185010 | training loss: 2.0447e-03 | validation loss: 1.6628e-03\n",
      "Epoch: 185020 | training loss: 2.0441e-03 | validation loss: 1.6692e-03\n",
      "Epoch: 185030 | training loss: 2.0436e-03 | validation loss: 1.6654e-03\n",
      "Epoch: 185040 | training loss: 2.0436e-03 | validation loss: 1.6661e-03\n",
      "Epoch: 185050 | training loss: 2.0435e-03 | validation loss: 1.6665e-03\n",
      "Epoch: 185060 | training loss: 2.0435e-03 | validation loss: 1.6661e-03\n",
      "Epoch: 185070 | training loss: 2.0435e-03 | validation loss: 1.6661e-03\n",
      "Epoch: 185080 | training loss: 2.0435e-03 | validation loss: 1.6663e-03\n",
      "Epoch: 185090 | training loss: 2.0434e-03 | validation loss: 1.6660e-03\n",
      "Epoch: 185100 | training loss: 2.0434e-03 | validation loss: 1.6660e-03\n",
      "Epoch: 185110 | training loss: 2.0434e-03 | validation loss: 1.6660e-03\n",
      "Epoch: 185120 | training loss: 2.0434e-03 | validation loss: 1.6660e-03\n",
      "Epoch: 185130 | training loss: 2.0434e-03 | validation loss: 1.6658e-03\n",
      "Epoch: 185140 | training loss: 2.0434e-03 | validation loss: 1.6647e-03\n",
      "Epoch: 185150 | training loss: 2.0548e-03 | validation loss: 1.6578e-03\n",
      "Epoch: 185160 | training loss: 3.6373e-03 | validation loss: 2.2694e-03\n",
      "Epoch: 185170 | training loss: 2.6711e-03 | validation loss: 2.0620e-03\n",
      "Epoch: 185180 | training loss: 2.3403e-03 | validation loss: 1.8802e-03\n",
      "Epoch: 185190 | training loss: 2.1468e-03 | validation loss: 1.7648e-03\n",
      "Epoch: 185200 | training loss: 2.0758e-03 | validation loss: 1.7119e-03\n",
      "Epoch: 185210 | training loss: 2.0495e-03 | validation loss: 1.6822e-03\n",
      "Epoch: 185220 | training loss: 2.0434e-03 | validation loss: 1.6672e-03\n",
      "Epoch: 185230 | training loss: 2.0436e-03 | validation loss: 1.6625e-03\n",
      "Epoch: 185240 | training loss: 2.0438e-03 | validation loss: 1.6629e-03\n",
      "Epoch: 185250 | training loss: 2.0432e-03 | validation loss: 1.6646e-03\n",
      "Epoch: 185260 | training loss: 2.0431e-03 | validation loss: 1.6666e-03\n",
      "Epoch: 185270 | training loss: 2.0431e-03 | validation loss: 1.6666e-03\n",
      "Epoch: 185280 | training loss: 2.0430e-03 | validation loss: 1.6654e-03\n",
      "Epoch: 185290 | training loss: 2.0430e-03 | validation loss: 1.6656e-03\n",
      "Epoch: 185300 | training loss: 2.0430e-03 | validation loss: 1.6658e-03\n",
      "Epoch: 185310 | training loss: 2.0430e-03 | validation loss: 1.6656e-03\n",
      "Epoch: 185320 | training loss: 2.0429e-03 | validation loss: 1.6657e-03\n",
      "Epoch: 185330 | training loss: 2.0429e-03 | validation loss: 1.6656e-03\n",
      "Epoch: 185340 | training loss: 2.0429e-03 | validation loss: 1.6656e-03\n",
      "Epoch: 185350 | training loss: 2.0429e-03 | validation loss: 1.6655e-03\n",
      "Epoch: 185360 | training loss: 2.0429e-03 | validation loss: 1.6655e-03\n",
      "Epoch: 185370 | training loss: 2.0428e-03 | validation loss: 1.6651e-03\n",
      "Epoch: 185380 | training loss: 2.0451e-03 | validation loss: 1.6628e-03\n",
      "Epoch: 185390 | training loss: 2.3823e-03 | validation loss: 1.8478e-03\n",
      "Epoch: 185400 | training loss: 2.3440e-03 | validation loss: 1.9196e-03\n",
      "Epoch: 185410 | training loss: 2.1041e-03 | validation loss: 1.7272e-03\n",
      "Epoch: 185420 | training loss: 2.0481e-03 | validation loss: 1.6731e-03\n",
      "Epoch: 185430 | training loss: 2.0431e-03 | validation loss: 1.6617e-03\n",
      "Epoch: 185440 | training loss: 2.0467e-03 | validation loss: 1.6578e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 185450 | training loss: 2.0872e-03 | validation loss: 1.6540e-03\n",
      "Epoch: 185460 | training loss: 2.9914e-03 | validation loss: 1.9784e-03\n",
      "Epoch: 185470 | training loss: 2.3881e-03 | validation loss: 1.9070e-03\n",
      "Epoch: 185480 | training loss: 2.0870e-03 | validation loss: 1.6566e-03\n",
      "Epoch: 185490 | training loss: 2.0459e-03 | validation loss: 1.6586e-03\n",
      "Epoch: 185500 | training loss: 2.0548e-03 | validation loss: 1.6862e-03\n",
      "Epoch: 185510 | training loss: 2.0487e-03 | validation loss: 1.6580e-03\n",
      "Epoch: 185520 | training loss: 2.0447e-03 | validation loss: 1.6725e-03\n",
      "Epoch: 185530 | training loss: 2.0433e-03 | validation loss: 1.6614e-03\n",
      "Epoch: 185540 | training loss: 2.0428e-03 | validation loss: 1.6675e-03\n",
      "Epoch: 185550 | training loss: 2.0425e-03 | validation loss: 1.6638e-03\n",
      "Epoch: 185560 | training loss: 2.0424e-03 | validation loss: 1.6644e-03\n",
      "Epoch: 185570 | training loss: 2.0424e-03 | validation loss: 1.6655e-03\n",
      "Epoch: 185580 | training loss: 2.0424e-03 | validation loss: 1.6656e-03\n",
      "Epoch: 185590 | training loss: 2.0425e-03 | validation loss: 1.6665e-03\n",
      "Epoch: 185600 | training loss: 2.0446e-03 | validation loss: 1.6723e-03\n",
      "Epoch: 185610 | training loss: 2.1381e-03 | validation loss: 1.7521e-03\n",
      "Epoch: 185620 | training loss: 3.0507e-03 | validation loss: 2.2756e-03\n",
      "Epoch: 185630 | training loss: 2.0805e-03 | validation loss: 1.6742e-03\n",
      "Epoch: 185640 | training loss: 2.1334e-03 | validation loss: 1.6699e-03\n",
      "Epoch: 185650 | training loss: 2.0632e-03 | validation loss: 1.6845e-03\n",
      "Epoch: 185660 | training loss: 2.0457e-03 | validation loss: 1.6739e-03\n",
      "Epoch: 185670 | training loss: 2.0481e-03 | validation loss: 1.6613e-03\n",
      "Epoch: 185680 | training loss: 2.0434e-03 | validation loss: 1.6683e-03\n",
      "Epoch: 185690 | training loss: 2.0422e-03 | validation loss: 1.6634e-03\n",
      "Epoch: 185700 | training loss: 2.0421e-03 | validation loss: 1.6653e-03\n",
      "Epoch: 185710 | training loss: 2.0421e-03 | validation loss: 1.6641e-03\n",
      "Epoch: 185720 | training loss: 2.0421e-03 | validation loss: 1.6652e-03\n",
      "Epoch: 185730 | training loss: 2.0421e-03 | validation loss: 1.6639e-03\n",
      "Epoch: 185740 | training loss: 2.0420e-03 | validation loss: 1.6647e-03\n",
      "Epoch: 185750 | training loss: 2.0420e-03 | validation loss: 1.6647e-03\n",
      "Epoch: 185760 | training loss: 2.0420e-03 | validation loss: 1.6646e-03\n",
      "Epoch: 185770 | training loss: 2.0420e-03 | validation loss: 1.6648e-03\n",
      "Epoch: 185780 | training loss: 2.0421e-03 | validation loss: 1.6660e-03\n",
      "Epoch: 185790 | training loss: 2.0513e-03 | validation loss: 1.6798e-03\n",
      "Epoch: 185800 | training loss: 2.8215e-03 | validation loss: 2.1402e-03\n",
      "Epoch: 185810 | training loss: 2.6260e-03 | validation loss: 1.8703e-03\n",
      "Epoch: 185820 | training loss: 2.0588e-03 | validation loss: 1.6942e-03\n",
      "Epoch: 185830 | training loss: 2.1230e-03 | validation loss: 1.7257e-03\n",
      "Epoch: 185840 | training loss: 2.0640e-03 | validation loss: 1.6828e-03\n",
      "Epoch: 185850 | training loss: 2.0426e-03 | validation loss: 1.6650e-03\n",
      "Epoch: 185860 | training loss: 2.0462e-03 | validation loss: 1.6616e-03\n",
      "Epoch: 185870 | training loss: 2.0417e-03 | validation loss: 1.6643e-03\n",
      "Epoch: 185880 | training loss: 2.0423e-03 | validation loss: 1.6661e-03\n",
      "Epoch: 185890 | training loss: 2.0417e-03 | validation loss: 1.6640e-03\n",
      "Epoch: 185900 | training loss: 2.0417e-03 | validation loss: 1.6638e-03\n",
      "Epoch: 185910 | training loss: 2.0417e-03 | validation loss: 1.6646e-03\n",
      "Epoch: 185920 | training loss: 2.0416e-03 | validation loss: 1.6640e-03\n",
      "Epoch: 185930 | training loss: 2.0416e-03 | validation loss: 1.6642e-03\n",
      "Epoch: 185940 | training loss: 2.0416e-03 | validation loss: 1.6639e-03\n",
      "Epoch: 185950 | training loss: 2.0415e-03 | validation loss: 1.6639e-03\n",
      "Epoch: 185960 | training loss: 2.0416e-03 | validation loss: 1.6629e-03\n",
      "Epoch: 185970 | training loss: 2.0457e-03 | validation loss: 1.6563e-03\n",
      "Epoch: 185980 | training loss: 2.5372e-03 | validation loss: 1.8740e-03\n",
      "Epoch: 185990 | training loss: 2.3205e-03 | validation loss: 1.9101e-03\n",
      "Epoch: 186000 | training loss: 2.1354e-03 | validation loss: 1.7463e-03\n",
      "Epoch: 186010 | training loss: 2.0662e-03 | validation loss: 1.7019e-03\n",
      "Epoch: 186020 | training loss: 2.0458e-03 | validation loss: 1.6643e-03\n",
      "Epoch: 186030 | training loss: 2.0416e-03 | validation loss: 1.6652e-03\n",
      "Epoch: 186040 | training loss: 2.0421e-03 | validation loss: 1.6634e-03\n",
      "Epoch: 186050 | training loss: 2.0424e-03 | validation loss: 1.6589e-03\n",
      "Epoch: 186060 | training loss: 2.0414e-03 | validation loss: 1.6630e-03\n",
      "Epoch: 186070 | training loss: 2.0415e-03 | validation loss: 1.6656e-03\n",
      "Epoch: 186080 | training loss: 2.0417e-03 | validation loss: 1.6671e-03\n",
      "Epoch: 186090 | training loss: 2.0461e-03 | validation loss: 1.6752e-03\n",
      "Epoch: 186100 | training loss: 2.1836e-03 | validation loss: 1.7829e-03\n",
      "Epoch: 186110 | training loss: 2.6034e-03 | validation loss: 2.0288e-03\n",
      "Epoch: 186120 | training loss: 2.0743e-03 | validation loss: 1.6605e-03\n",
      "Epoch: 186130 | training loss: 2.0668e-03 | validation loss: 1.6571e-03\n",
      "Epoch: 186140 | training loss: 2.0736e-03 | validation loss: 1.7011e-03\n",
      "Epoch: 186150 | training loss: 2.0507e-03 | validation loss: 1.6546e-03\n",
      "Epoch: 186160 | training loss: 2.0428e-03 | validation loss: 1.6691e-03\n",
      "Epoch: 186170 | training loss: 2.0415e-03 | validation loss: 1.6612e-03\n",
      "Epoch: 186180 | training loss: 2.0413e-03 | validation loss: 1.6656e-03\n",
      "Epoch: 186190 | training loss: 2.0413e-03 | validation loss: 1.6613e-03\n",
      "Epoch: 186200 | training loss: 2.0411e-03 | validation loss: 1.6645e-03\n",
      "Epoch: 186210 | training loss: 2.0410e-03 | validation loss: 1.6636e-03\n",
      "Epoch: 186220 | training loss: 2.0410e-03 | validation loss: 1.6627e-03\n",
      "Epoch: 186230 | training loss: 2.0410e-03 | validation loss: 1.6623e-03\n",
      "Epoch: 186240 | training loss: 2.0413e-03 | validation loss: 1.6607e-03\n",
      "Epoch: 186250 | training loss: 2.0524e-03 | validation loss: 1.6551e-03\n",
      "Epoch: 186260 | training loss: 2.6030e-03 | validation loss: 1.8380e-03\n",
      "Epoch: 186270 | training loss: 2.1968e-03 | validation loss: 1.7911e-03\n",
      "Epoch: 186280 | training loss: 2.2512e-03 | validation loss: 1.7081e-03\n",
      "Epoch: 186290 | training loss: 2.0435e-03 | validation loss: 1.6554e-03\n",
      "Epoch: 186300 | training loss: 2.0683e-03 | validation loss: 1.6982e-03\n",
      "Epoch: 186310 | training loss: 2.0432e-03 | validation loss: 1.6605e-03\n",
      "Epoch: 186320 | training loss: 2.0419e-03 | validation loss: 1.6578e-03\n",
      "Epoch: 186330 | training loss: 2.0420e-03 | validation loss: 1.6695e-03\n",
      "Epoch: 186340 | training loss: 2.0412e-03 | validation loss: 1.6597e-03\n",
      "Epoch: 186350 | training loss: 2.0408e-03 | validation loss: 1.6652e-03\n",
      "Epoch: 186360 | training loss: 2.0407e-03 | validation loss: 1.6615e-03\n",
      "Epoch: 186370 | training loss: 2.0407e-03 | validation loss: 1.6638e-03\n",
      "Epoch: 186380 | training loss: 2.0406e-03 | validation loss: 1.6626e-03\n",
      "Epoch: 186390 | training loss: 2.0406e-03 | validation loss: 1.6626e-03\n",
      "Epoch: 186400 | training loss: 2.0406e-03 | validation loss: 1.6629e-03\n",
      "Epoch: 186410 | training loss: 2.0405e-03 | validation loss: 1.6629e-03\n",
      "Epoch: 186420 | training loss: 2.0407e-03 | validation loss: 1.6624e-03\n",
      "Epoch: 186430 | training loss: 2.0541e-03 | validation loss: 1.6650e-03\n",
      "Epoch: 186440 | training loss: 2.6141e-03 | validation loss: 1.9985e-03\n",
      "Epoch: 186450 | training loss: 2.2187e-03 | validation loss: 1.8264e-03\n",
      "Epoch: 186460 | training loss: 2.3169e-03 | validation loss: 1.9037e-03\n",
      "Epoch: 186470 | training loss: 2.1657e-03 | validation loss: 1.6954e-03\n",
      "Epoch: 186480 | training loss: 2.0875e-03 | validation loss: 1.7235e-03\n",
      "Epoch: 186490 | training loss: 2.0521e-03 | validation loss: 1.6555e-03\n",
      "Epoch: 186500 | training loss: 2.0405e-03 | validation loss: 1.6630e-03\n",
      "Epoch: 186510 | training loss: 2.0434e-03 | validation loss: 1.6694e-03\n",
      "Epoch: 186520 | training loss: 2.0404e-03 | validation loss: 1.6613e-03\n",
      "Epoch: 186530 | training loss: 2.0410e-03 | validation loss: 1.6591e-03\n",
      "Epoch: 186540 | training loss: 2.0453e-03 | validation loss: 1.6550e-03\n",
      "Epoch: 186550 | training loss: 2.1350e-03 | validation loss: 1.6637e-03\n",
      "Epoch: 186560 | training loss: 2.7474e-03 | validation loss: 1.8842e-03\n",
      "Epoch: 186570 | training loss: 2.2503e-03 | validation loss: 1.8198e-03\n",
      "Epoch: 186580 | training loss: 2.0782e-03 | validation loss: 1.6550e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 186590 | training loss: 2.0434e-03 | validation loss: 1.6719e-03\n",
      "Epoch: 186600 | training loss: 2.0403e-03 | validation loss: 1.6607e-03\n",
      "Epoch: 186610 | training loss: 2.0403e-03 | validation loss: 1.6636e-03\n",
      "Epoch: 186620 | training loss: 2.0407e-03 | validation loss: 1.6592e-03\n",
      "Epoch: 186630 | training loss: 2.0408e-03 | validation loss: 1.6662e-03\n",
      "Epoch: 186640 | training loss: 2.0401e-03 | validation loss: 1.6610e-03\n",
      "Epoch: 186650 | training loss: 2.0402e-03 | validation loss: 1.6604e-03\n",
      "Epoch: 186660 | training loss: 2.0401e-03 | validation loss: 1.6607e-03\n",
      "Epoch: 186670 | training loss: 2.0404e-03 | validation loss: 1.6596e-03\n",
      "Epoch: 186680 | training loss: 2.0471e-03 | validation loss: 1.6544e-03\n",
      "Epoch: 186690 | training loss: 2.3393e-03 | validation loss: 1.7343e-03\n",
      "Epoch: 186700 | training loss: 2.0805e-03 | validation loss: 1.6659e-03\n",
      "Epoch: 186710 | training loss: 2.1672e-03 | validation loss: 1.6727e-03\n",
      "Epoch: 186720 | training loss: 2.1055e-03 | validation loss: 1.7208e-03\n",
      "Epoch: 186730 | training loss: 2.0434e-03 | validation loss: 1.6729e-03\n",
      "Epoch: 186740 | training loss: 2.0482e-03 | validation loss: 1.6522e-03\n",
      "Epoch: 186750 | training loss: 2.0428e-03 | validation loss: 1.6698e-03\n",
      "Epoch: 186760 | training loss: 2.0404e-03 | validation loss: 1.6611e-03\n",
      "Epoch: 186770 | training loss: 2.0399e-03 | validation loss: 1.6617e-03\n",
      "Epoch: 186780 | training loss: 2.0398e-03 | validation loss: 1.6622e-03\n",
      "Epoch: 186790 | training loss: 2.0397e-03 | validation loss: 1.6622e-03\n",
      "Epoch: 186800 | training loss: 2.0397e-03 | validation loss: 1.6610e-03\n",
      "Epoch: 186810 | training loss: 2.0397e-03 | validation loss: 1.6623e-03\n",
      "Epoch: 186820 | training loss: 2.0396e-03 | validation loss: 1.6621e-03\n",
      "Epoch: 186830 | training loss: 2.0396e-03 | validation loss: 1.6618e-03\n",
      "Epoch: 186840 | training loss: 2.0397e-03 | validation loss: 1.6619e-03\n",
      "Epoch: 186850 | training loss: 2.0431e-03 | validation loss: 1.6651e-03\n",
      "Epoch: 186860 | training loss: 2.2610e-03 | validation loss: 1.8097e-03\n",
      "Epoch: 186870 | training loss: 2.7554e-03 | validation loss: 1.9251e-03\n",
      "Epoch: 186880 | training loss: 2.1625e-03 | validation loss: 1.7767e-03\n",
      "Epoch: 186890 | training loss: 2.1155e-03 | validation loss: 1.6756e-03\n",
      "Epoch: 186900 | training loss: 2.0751e-03 | validation loss: 1.7126e-03\n",
      "Epoch: 186910 | training loss: 2.0549e-03 | validation loss: 1.6632e-03\n",
      "Epoch: 186920 | training loss: 2.0455e-03 | validation loss: 1.6777e-03\n",
      "Epoch: 186930 | training loss: 2.0408e-03 | validation loss: 1.6597e-03\n",
      "Epoch: 186940 | training loss: 2.0394e-03 | validation loss: 1.6611e-03\n",
      "Epoch: 186950 | training loss: 2.0397e-03 | validation loss: 1.6630e-03\n",
      "Epoch: 186960 | training loss: 2.0394e-03 | validation loss: 1.6615e-03\n",
      "Epoch: 186970 | training loss: 2.0393e-03 | validation loss: 1.6613e-03\n",
      "Epoch: 186980 | training loss: 2.0393e-03 | validation loss: 1.6608e-03\n",
      "Epoch: 186990 | training loss: 2.0397e-03 | validation loss: 1.6584e-03\n",
      "Epoch: 187000 | training loss: 2.0653e-03 | validation loss: 1.6509e-03\n",
      "Epoch: 187010 | training loss: 3.6039e-03 | validation loss: 2.2363e-03\n",
      "Epoch: 187020 | training loss: 2.4204e-03 | validation loss: 1.9135e-03\n",
      "Epoch: 187030 | training loss: 2.1760e-03 | validation loss: 1.7731e-03\n",
      "Epoch: 187040 | training loss: 2.0425e-03 | validation loss: 1.6653e-03\n",
      "Epoch: 187050 | training loss: 2.0637e-03 | validation loss: 1.6605e-03\n",
      "Epoch: 187060 | training loss: 2.0416e-03 | validation loss: 1.6599e-03\n",
      "Epoch: 187070 | training loss: 2.0415e-03 | validation loss: 1.6684e-03\n",
      "Epoch: 187080 | training loss: 2.0393e-03 | validation loss: 1.6616e-03\n",
      "Epoch: 187090 | training loss: 2.0395e-03 | validation loss: 1.6582e-03\n",
      "Epoch: 187100 | training loss: 2.0391e-03 | validation loss: 1.6623e-03\n",
      "Epoch: 187110 | training loss: 2.0390e-03 | validation loss: 1.6612e-03\n",
      "Epoch: 187120 | training loss: 2.0390e-03 | validation loss: 1.6606e-03\n",
      "Epoch: 187130 | training loss: 2.0390e-03 | validation loss: 1.6611e-03\n",
      "Epoch: 187140 | training loss: 2.0389e-03 | validation loss: 1.6608e-03\n",
      "Epoch: 187150 | training loss: 2.0389e-03 | validation loss: 1.6608e-03\n",
      "Epoch: 187160 | training loss: 2.0389e-03 | validation loss: 1.6609e-03\n",
      "Epoch: 187170 | training loss: 2.0389e-03 | validation loss: 1.6608e-03\n",
      "Epoch: 187180 | training loss: 2.0388e-03 | validation loss: 1.6607e-03\n",
      "Epoch: 187190 | training loss: 2.0388e-03 | validation loss: 1.6607e-03\n",
      "Epoch: 187200 | training loss: 2.0388e-03 | validation loss: 1.6605e-03\n",
      "Epoch: 187210 | training loss: 2.0388e-03 | validation loss: 1.6597e-03\n",
      "Epoch: 187220 | training loss: 2.0438e-03 | validation loss: 1.6545e-03\n",
      "Epoch: 187230 | training loss: 2.6483e-03 | validation loss: 1.8592e-03\n",
      "Epoch: 187240 | training loss: 2.6110e-03 | validation loss: 2.0288e-03\n",
      "Epoch: 187250 | training loss: 2.0414e-03 | validation loss: 1.6562e-03\n",
      "Epoch: 187260 | training loss: 2.0932e-03 | validation loss: 1.6574e-03\n",
      "Epoch: 187270 | training loss: 2.0808e-03 | validation loss: 1.6549e-03\n",
      "Epoch: 187280 | training loss: 2.0489e-03 | validation loss: 1.6533e-03\n",
      "Epoch: 187290 | training loss: 2.0386e-03 | validation loss: 1.6608e-03\n",
      "Epoch: 187300 | training loss: 2.0403e-03 | validation loss: 1.6663e-03\n",
      "Epoch: 187310 | training loss: 2.0389e-03 | validation loss: 1.6630e-03\n",
      "Epoch: 187320 | training loss: 2.0387e-03 | validation loss: 1.6592e-03\n",
      "Epoch: 187330 | training loss: 2.0385e-03 | validation loss: 1.6599e-03\n",
      "Epoch: 187340 | training loss: 2.0385e-03 | validation loss: 1.6612e-03\n",
      "Epoch: 187350 | training loss: 2.0385e-03 | validation loss: 1.6603e-03\n",
      "Epoch: 187360 | training loss: 2.0384e-03 | validation loss: 1.6604e-03\n",
      "Epoch: 187370 | training loss: 2.0384e-03 | validation loss: 1.6605e-03\n",
      "Epoch: 187380 | training loss: 2.0384e-03 | validation loss: 1.6603e-03\n",
      "Epoch: 187390 | training loss: 2.0384e-03 | validation loss: 1.6604e-03\n",
      "Epoch: 187400 | training loss: 2.0384e-03 | validation loss: 1.6603e-03\n",
      "Epoch: 187410 | training loss: 2.0383e-03 | validation loss: 1.6604e-03\n",
      "Epoch: 187420 | training loss: 2.0385e-03 | validation loss: 1.6623e-03\n",
      "Epoch: 187430 | training loss: 2.0994e-03 | validation loss: 1.7342e-03\n",
      "Epoch: 187440 | training loss: 2.1375e-03 | validation loss: 1.7505e-03\n",
      "Epoch: 187450 | training loss: 2.0785e-03 | validation loss: 1.6725e-03\n",
      "Epoch: 187460 | training loss: 2.0423e-03 | validation loss: 1.6724e-03\n",
      "Epoch: 187470 | training loss: 2.0388e-03 | validation loss: 1.6624e-03\n",
      "Epoch: 187480 | training loss: 2.0403e-03 | validation loss: 1.6532e-03\n",
      "Epoch: 187490 | training loss: 2.0384e-03 | validation loss: 1.6576e-03\n",
      "Epoch: 187500 | training loss: 2.0383e-03 | validation loss: 1.6611e-03\n",
      "Epoch: 187510 | training loss: 2.0391e-03 | validation loss: 1.6650e-03\n",
      "Epoch: 187520 | training loss: 2.0552e-03 | validation loss: 1.6873e-03\n",
      "Epoch: 187530 | training loss: 2.5761e-03 | validation loss: 2.0228e-03\n",
      "Epoch: 187540 | training loss: 2.1345e-03 | validation loss: 1.6617e-03\n",
      "Epoch: 187550 | training loss: 2.1843e-03 | validation loss: 1.7807e-03\n",
      "Epoch: 187560 | training loss: 2.0765e-03 | validation loss: 1.6506e-03\n",
      "Epoch: 187570 | training loss: 2.0388e-03 | validation loss: 1.6633e-03\n",
      "Epoch: 187580 | training loss: 2.0389e-03 | validation loss: 1.6640e-03\n",
      "Epoch: 187590 | training loss: 2.0390e-03 | validation loss: 1.6555e-03\n",
      "Epoch: 187600 | training loss: 2.0383e-03 | validation loss: 1.6624e-03\n",
      "Epoch: 187610 | training loss: 2.0379e-03 | validation loss: 1.6587e-03\n",
      "Epoch: 187620 | training loss: 2.0379e-03 | validation loss: 1.6588e-03\n",
      "Epoch: 187630 | training loss: 2.0379e-03 | validation loss: 1.6608e-03\n",
      "Epoch: 187640 | training loss: 2.0378e-03 | validation loss: 1.6596e-03\n",
      "Epoch: 187650 | training loss: 2.0378e-03 | validation loss: 1.6590e-03\n",
      "Epoch: 187660 | training loss: 2.0379e-03 | validation loss: 1.6583e-03\n",
      "Epoch: 187670 | training loss: 2.0390e-03 | validation loss: 1.6553e-03\n",
      "Epoch: 187680 | training loss: 2.0861e-03 | validation loss: 1.6528e-03\n",
      "Epoch: 187690 | training loss: 3.3760e-03 | validation loss: 2.1461e-03\n",
      "Epoch: 187700 | training loss: 2.3182e-03 | validation loss: 1.8538e-03\n",
      "Epoch: 187710 | training loss: 2.0758e-03 | validation loss: 1.7041e-03\n",
      "Epoch: 187720 | training loss: 2.0828e-03 | validation loss: 1.6604e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 187730 | training loss: 2.0388e-03 | validation loss: 1.6569e-03\n",
      "Epoch: 187740 | training loss: 2.0447e-03 | validation loss: 1.6716e-03\n",
      "Epoch: 187750 | training loss: 2.0389e-03 | validation loss: 1.6550e-03\n",
      "Epoch: 187760 | training loss: 2.0376e-03 | validation loss: 1.6601e-03\n",
      "Epoch: 187770 | training loss: 2.0376e-03 | validation loss: 1.6603e-03\n",
      "Epoch: 187780 | training loss: 2.0376e-03 | validation loss: 1.6584e-03\n",
      "Epoch: 187790 | training loss: 2.0375e-03 | validation loss: 1.6597e-03\n",
      "Epoch: 187800 | training loss: 2.0375e-03 | validation loss: 1.6592e-03\n",
      "Epoch: 187810 | training loss: 2.0375e-03 | validation loss: 1.6589e-03\n",
      "Epoch: 187820 | training loss: 2.0374e-03 | validation loss: 1.6593e-03\n",
      "Epoch: 187830 | training loss: 2.0374e-03 | validation loss: 1.6594e-03\n",
      "Epoch: 187840 | training loss: 2.0374e-03 | validation loss: 1.6593e-03\n",
      "Epoch: 187850 | training loss: 2.0374e-03 | validation loss: 1.6597e-03\n",
      "Epoch: 187860 | training loss: 2.0379e-03 | validation loss: 1.6622e-03\n",
      "Epoch: 187870 | training loss: 2.0716e-03 | validation loss: 1.6978e-03\n",
      "Epoch: 187880 | training loss: 3.7350e-03 | validation loss: 2.6309e-03\n",
      "Epoch: 187890 | training loss: 2.2833e-03 | validation loss: 1.7186e-03\n",
      "Epoch: 187900 | training loss: 2.2206e-03 | validation loss: 1.6927e-03\n",
      "Epoch: 187910 | training loss: 2.0406e-03 | validation loss: 1.6520e-03\n",
      "Epoch: 187920 | training loss: 2.0539e-03 | validation loss: 1.6832e-03\n",
      "Epoch: 187930 | training loss: 2.0430e-03 | validation loss: 1.6711e-03\n",
      "Epoch: 187940 | training loss: 2.0382e-03 | validation loss: 1.6555e-03\n",
      "Epoch: 187950 | training loss: 2.0379e-03 | validation loss: 1.6560e-03\n",
      "Epoch: 187960 | training loss: 2.0375e-03 | validation loss: 1.6615e-03\n",
      "Epoch: 187970 | training loss: 2.0371e-03 | validation loss: 1.6588e-03\n",
      "Epoch: 187980 | training loss: 2.0371e-03 | validation loss: 1.6583e-03\n",
      "Epoch: 187990 | training loss: 2.0371e-03 | validation loss: 1.6594e-03\n",
      "Epoch: 188000 | training loss: 2.0371e-03 | validation loss: 1.6584e-03\n",
      "Epoch: 188010 | training loss: 2.0370e-03 | validation loss: 1.6590e-03\n",
      "Epoch: 188020 | training loss: 2.0370e-03 | validation loss: 1.6586e-03\n",
      "Epoch: 188030 | training loss: 2.0370e-03 | validation loss: 1.6584e-03\n",
      "Epoch: 188040 | training loss: 2.0373e-03 | validation loss: 1.6568e-03\n",
      "Epoch: 188050 | training loss: 2.0858e-03 | validation loss: 1.6659e-03\n",
      "Epoch: 188060 | training loss: 2.1905e-03 | validation loss: 1.6895e-03\n",
      "Epoch: 188070 | training loss: 2.1937e-03 | validation loss: 1.7421e-03\n",
      "Epoch: 188080 | training loss: 2.0899e-03 | validation loss: 1.6804e-03\n",
      "Epoch: 188090 | training loss: 2.0519e-03 | validation loss: 1.6500e-03\n",
      "Epoch: 188100 | training loss: 2.0381e-03 | validation loss: 1.6534e-03\n",
      "Epoch: 188110 | training loss: 2.0372e-03 | validation loss: 1.6609e-03\n",
      "Epoch: 188120 | training loss: 2.0383e-03 | validation loss: 1.6653e-03\n",
      "Epoch: 188130 | training loss: 2.0507e-03 | validation loss: 1.6830e-03\n",
      "Epoch: 188140 | training loss: 2.5052e-03 | validation loss: 1.9806e-03\n",
      "Epoch: 188150 | training loss: 2.0826e-03 | validation loss: 1.6474e-03\n",
      "Epoch: 188160 | training loss: 2.1804e-03 | validation loss: 1.7739e-03\n",
      "Epoch: 188170 | training loss: 2.0840e-03 | validation loss: 1.6502e-03\n",
      "Epoch: 188180 | training loss: 2.0378e-03 | validation loss: 1.6628e-03\n",
      "Epoch: 188190 | training loss: 2.0380e-03 | validation loss: 1.6640e-03\n",
      "Epoch: 188200 | training loss: 2.0382e-03 | validation loss: 1.6540e-03\n",
      "Epoch: 188210 | training loss: 2.0373e-03 | validation loss: 1.6619e-03\n",
      "Epoch: 188220 | training loss: 2.0367e-03 | validation loss: 1.6563e-03\n",
      "Epoch: 188230 | training loss: 2.0366e-03 | validation loss: 1.6580e-03\n",
      "Epoch: 188240 | training loss: 2.0366e-03 | validation loss: 1.6591e-03\n",
      "Epoch: 188250 | training loss: 2.0365e-03 | validation loss: 1.6575e-03\n",
      "Epoch: 188260 | training loss: 2.0365e-03 | validation loss: 1.6572e-03\n",
      "Epoch: 188270 | training loss: 2.0365e-03 | validation loss: 1.6568e-03\n",
      "Epoch: 188280 | training loss: 2.0372e-03 | validation loss: 1.6545e-03\n",
      "Epoch: 188290 | training loss: 2.0615e-03 | validation loss: 1.6489e-03\n",
      "Epoch: 188300 | training loss: 3.0609e-03 | validation loss: 2.0153e-03\n",
      "Epoch: 188310 | training loss: 2.5217e-03 | validation loss: 1.9787e-03\n",
      "Epoch: 188320 | training loss: 2.0749e-03 | validation loss: 1.6661e-03\n",
      "Epoch: 188330 | training loss: 2.0765e-03 | validation loss: 1.6491e-03\n",
      "Epoch: 188340 | training loss: 2.0508e-03 | validation loss: 1.6734e-03\n",
      "Epoch: 188350 | training loss: 2.0375e-03 | validation loss: 1.6643e-03\n",
      "Epoch: 188360 | training loss: 2.0382e-03 | validation loss: 1.6536e-03\n",
      "Epoch: 188370 | training loss: 2.0373e-03 | validation loss: 1.6604e-03\n",
      "Epoch: 188380 | training loss: 2.0366e-03 | validation loss: 1.6574e-03\n",
      "Epoch: 188390 | training loss: 2.0364e-03 | validation loss: 1.6581e-03\n",
      "Epoch: 188400 | training loss: 2.0362e-03 | validation loss: 1.6576e-03\n",
      "Epoch: 188410 | training loss: 2.0362e-03 | validation loss: 1.6578e-03\n",
      "Epoch: 188420 | training loss: 2.0361e-03 | validation loss: 1.6577e-03\n",
      "Epoch: 188430 | training loss: 2.0361e-03 | validation loss: 1.6575e-03\n",
      "Epoch: 188440 | training loss: 2.0361e-03 | validation loss: 1.6574e-03\n",
      "Epoch: 188450 | training loss: 2.0361e-03 | validation loss: 1.6573e-03\n",
      "Epoch: 188460 | training loss: 2.0361e-03 | validation loss: 1.6568e-03\n",
      "Epoch: 188470 | training loss: 2.0386e-03 | validation loss: 1.6544e-03\n",
      "Epoch: 188480 | training loss: 2.2407e-03 | validation loss: 1.7218e-03\n",
      "Epoch: 188490 | training loss: 2.3965e-03 | validation loss: 1.7513e-03\n",
      "Epoch: 188500 | training loss: 2.2208e-03 | validation loss: 1.7645e-03\n",
      "Epoch: 188510 | training loss: 2.1604e-03 | validation loss: 1.7907e-03\n",
      "Epoch: 188520 | training loss: 2.0665e-03 | validation loss: 1.6870e-03\n",
      "Epoch: 188530 | training loss: 2.0498e-03 | validation loss: 1.6610e-03\n",
      "Epoch: 188540 | training loss: 2.0395e-03 | validation loss: 1.6684e-03\n",
      "Epoch: 188550 | training loss: 2.0361e-03 | validation loss: 1.6569e-03\n",
      "Epoch: 188560 | training loss: 2.0359e-03 | validation loss: 1.6564e-03\n",
      "Epoch: 188570 | training loss: 2.0360e-03 | validation loss: 1.6572e-03\n",
      "Epoch: 188580 | training loss: 2.0358e-03 | validation loss: 1.6567e-03\n",
      "Epoch: 188590 | training loss: 2.0358e-03 | validation loss: 1.6572e-03\n",
      "Epoch: 188600 | training loss: 2.0358e-03 | validation loss: 1.6577e-03\n",
      "Epoch: 188610 | training loss: 2.0357e-03 | validation loss: 1.6569e-03\n",
      "Epoch: 188620 | training loss: 2.0357e-03 | validation loss: 1.6568e-03\n",
      "Epoch: 188630 | training loss: 2.0357e-03 | validation loss: 1.6569e-03\n",
      "Epoch: 188640 | training loss: 2.0357e-03 | validation loss: 1.6566e-03\n",
      "Epoch: 188650 | training loss: 2.0358e-03 | validation loss: 1.6551e-03\n",
      "Epoch: 188660 | training loss: 2.0453e-03 | validation loss: 1.6464e-03\n",
      "Epoch: 188670 | training loss: 2.8714e-03 | validation loss: 1.9301e-03\n",
      "Epoch: 188680 | training loss: 2.5809e-03 | validation loss: 2.0494e-03\n",
      "Epoch: 188690 | training loss: 2.1452e-03 | validation loss: 1.7535e-03\n",
      "Epoch: 188700 | training loss: 2.0551e-03 | validation loss: 1.6599e-03\n",
      "Epoch: 188710 | training loss: 2.0453e-03 | validation loss: 1.6590e-03\n",
      "Epoch: 188720 | training loss: 2.0430e-03 | validation loss: 1.6534e-03\n",
      "Epoch: 188730 | training loss: 2.0383e-03 | validation loss: 1.6493e-03\n",
      "Epoch: 188740 | training loss: 2.0356e-03 | validation loss: 1.6586e-03\n",
      "Epoch: 188750 | training loss: 2.0358e-03 | validation loss: 1.6595e-03\n",
      "Epoch: 188760 | training loss: 2.0354e-03 | validation loss: 1.6568e-03\n",
      "Epoch: 188770 | training loss: 2.0354e-03 | validation loss: 1.6560e-03\n",
      "Epoch: 188780 | training loss: 2.0354e-03 | validation loss: 1.6567e-03\n",
      "Epoch: 188790 | training loss: 2.0353e-03 | validation loss: 1.6568e-03\n",
      "Epoch: 188800 | training loss: 2.0353e-03 | validation loss: 1.6565e-03\n",
      "Epoch: 188810 | training loss: 2.0353e-03 | validation loss: 1.6567e-03\n",
      "Epoch: 188820 | training loss: 2.0353e-03 | validation loss: 1.6564e-03\n",
      "Epoch: 188830 | training loss: 2.0352e-03 | validation loss: 1.6564e-03\n",
      "Epoch: 188840 | training loss: 2.0352e-03 | validation loss: 1.6560e-03\n",
      "Epoch: 188850 | training loss: 2.0359e-03 | validation loss: 1.6543e-03\n",
      "Epoch: 188860 | training loss: 2.0896e-03 | validation loss: 1.6613e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 188870 | training loss: 3.6926e-03 | validation loss: 2.3092e-03\n",
      "Epoch: 188880 | training loss: 2.1149e-03 | validation loss: 1.7536e-03\n",
      "Epoch: 188890 | training loss: 2.1609e-03 | validation loss: 1.7735e-03\n",
      "Epoch: 188900 | training loss: 2.0511e-03 | validation loss: 1.6699e-03\n",
      "Epoch: 188910 | training loss: 2.0526e-03 | validation loss: 1.6463e-03\n",
      "Epoch: 188920 | training loss: 2.0395e-03 | validation loss: 1.6485e-03\n",
      "Epoch: 188930 | training loss: 2.0363e-03 | validation loss: 1.6625e-03\n",
      "Epoch: 188940 | training loss: 2.0355e-03 | validation loss: 1.6595e-03\n",
      "Epoch: 188950 | training loss: 2.0353e-03 | validation loss: 1.6545e-03\n",
      "Epoch: 188960 | training loss: 2.0351e-03 | validation loss: 1.6572e-03\n",
      "Epoch: 188970 | training loss: 2.0350e-03 | validation loss: 1.6558e-03\n",
      "Epoch: 188980 | training loss: 2.0349e-03 | validation loss: 1.6565e-03\n",
      "Epoch: 188990 | training loss: 2.0349e-03 | validation loss: 1.6560e-03\n",
      "Epoch: 189000 | training loss: 2.0349e-03 | validation loss: 1.6563e-03\n",
      "Epoch: 189010 | training loss: 2.0349e-03 | validation loss: 1.6560e-03\n",
      "Epoch: 189020 | training loss: 2.0348e-03 | validation loss: 1.6561e-03\n",
      "Epoch: 189030 | training loss: 2.0348e-03 | validation loss: 1.6562e-03\n",
      "Epoch: 189040 | training loss: 2.0348e-03 | validation loss: 1.6561e-03\n",
      "Epoch: 189050 | training loss: 2.0348e-03 | validation loss: 1.6561e-03\n",
      "Epoch: 189060 | training loss: 2.0348e-03 | validation loss: 1.6565e-03\n",
      "Epoch: 189070 | training loss: 2.0351e-03 | validation loss: 1.6592e-03\n",
      "Epoch: 189080 | training loss: 2.0736e-03 | validation loss: 1.7057e-03\n",
      "Epoch: 189090 | training loss: 3.8911e-03 | validation loss: 2.7599e-03\n",
      "Epoch: 189100 | training loss: 2.2189e-03 | validation loss: 1.8338e-03\n",
      "Epoch: 189110 | training loss: 2.0407e-03 | validation loss: 1.6702e-03\n",
      "Epoch: 189120 | training loss: 2.0381e-03 | validation loss: 1.6632e-03\n",
      "Epoch: 189130 | training loss: 2.0346e-03 | validation loss: 1.6551e-03\n",
      "Epoch: 189140 | training loss: 2.0353e-03 | validation loss: 1.6520e-03\n",
      "Epoch: 189150 | training loss: 2.0355e-03 | validation loss: 1.6522e-03\n",
      "Epoch: 189160 | training loss: 2.0354e-03 | validation loss: 1.6515e-03\n",
      "Epoch: 189170 | training loss: 2.0349e-03 | validation loss: 1.6526e-03\n",
      "Epoch: 189180 | training loss: 2.0345e-03 | validation loss: 1.6546e-03\n",
      "Epoch: 189190 | training loss: 2.0345e-03 | validation loss: 1.6559e-03\n",
      "Epoch: 189200 | training loss: 2.0345e-03 | validation loss: 1.6562e-03\n",
      "Epoch: 189210 | training loss: 2.0344e-03 | validation loss: 1.6555e-03\n",
      "Epoch: 189220 | training loss: 2.0344e-03 | validation loss: 1.6551e-03\n",
      "Epoch: 189230 | training loss: 2.0344e-03 | validation loss: 1.6554e-03\n",
      "Epoch: 189240 | training loss: 2.0344e-03 | validation loss: 1.6554e-03\n",
      "Epoch: 189250 | training loss: 2.0343e-03 | validation loss: 1.6553e-03\n",
      "Epoch: 189260 | training loss: 2.0343e-03 | validation loss: 1.6552e-03\n",
      "Epoch: 189270 | training loss: 2.0345e-03 | validation loss: 1.6546e-03\n",
      "Epoch: 189280 | training loss: 2.0612e-03 | validation loss: 1.6611e-03\n",
      "Epoch: 189290 | training loss: 3.6154e-03 | validation loss: 2.3458e-03\n",
      "Epoch: 189300 | training loss: 2.3405e-03 | validation loss: 1.9188e-03\n",
      "Epoch: 189310 | training loss: 2.1793e-03 | validation loss: 1.7954e-03\n",
      "Epoch: 189320 | training loss: 2.0557e-03 | validation loss: 1.6764e-03\n",
      "Epoch: 189330 | training loss: 2.0564e-03 | validation loss: 1.6605e-03\n",
      "Epoch: 189340 | training loss: 2.0361e-03 | validation loss: 1.6629e-03\n",
      "Epoch: 189350 | training loss: 2.0363e-03 | validation loss: 1.6634e-03\n",
      "Epoch: 189360 | training loss: 2.0346e-03 | validation loss: 1.6529e-03\n",
      "Epoch: 189370 | training loss: 2.0341e-03 | validation loss: 1.6544e-03\n",
      "Epoch: 189380 | training loss: 2.0342e-03 | validation loss: 1.6552e-03\n",
      "Epoch: 189390 | training loss: 2.0341e-03 | validation loss: 1.6542e-03\n",
      "Epoch: 189400 | training loss: 2.0340e-03 | validation loss: 1.6556e-03\n",
      "Epoch: 189410 | training loss: 2.0340e-03 | validation loss: 1.6549e-03\n",
      "Epoch: 189420 | training loss: 2.0340e-03 | validation loss: 1.6550e-03\n",
      "Epoch: 189430 | training loss: 2.0340e-03 | validation loss: 1.6550e-03\n",
      "Epoch: 189440 | training loss: 2.0339e-03 | validation loss: 1.6549e-03\n",
      "Epoch: 189450 | training loss: 2.0339e-03 | validation loss: 1.6548e-03\n",
      "Epoch: 189460 | training loss: 2.0339e-03 | validation loss: 1.6548e-03\n",
      "Epoch: 189470 | training loss: 2.0339e-03 | validation loss: 1.6544e-03\n",
      "Epoch: 189480 | training loss: 2.0344e-03 | validation loss: 1.6523e-03\n",
      "Epoch: 189490 | training loss: 2.0764e-03 | validation loss: 1.6499e-03\n",
      "Epoch: 189500 | training loss: 3.9398e-03 | validation loss: 2.3988e-03\n",
      "Epoch: 189510 | training loss: 2.0535e-03 | validation loss: 1.6769e-03\n",
      "Epoch: 189520 | training loss: 2.1937e-03 | validation loss: 1.7776e-03\n",
      "Epoch: 189530 | training loss: 2.0975e-03 | validation loss: 1.7169e-03\n",
      "Epoch: 189540 | training loss: 2.0369e-03 | validation loss: 1.6656e-03\n",
      "Epoch: 189550 | training loss: 2.0377e-03 | validation loss: 1.6499e-03\n",
      "Epoch: 189560 | training loss: 2.0366e-03 | validation loss: 1.6484e-03\n",
      "Epoch: 189570 | training loss: 2.0337e-03 | validation loss: 1.6554e-03\n",
      "Epoch: 189580 | training loss: 2.0341e-03 | validation loss: 1.6582e-03\n",
      "Epoch: 189590 | training loss: 2.0337e-03 | validation loss: 1.6539e-03\n",
      "Epoch: 189600 | training loss: 2.0336e-03 | validation loss: 1.6541e-03\n",
      "Epoch: 189610 | training loss: 2.0336e-03 | validation loss: 1.6554e-03\n",
      "Epoch: 189620 | training loss: 2.0336e-03 | validation loss: 1.6543e-03\n",
      "Epoch: 189630 | training loss: 2.0335e-03 | validation loss: 1.6548e-03\n",
      "Epoch: 189640 | training loss: 2.0335e-03 | validation loss: 1.6545e-03\n",
      "Epoch: 189650 | training loss: 2.0335e-03 | validation loss: 1.6547e-03\n",
      "Epoch: 189660 | training loss: 2.0335e-03 | validation loss: 1.6545e-03\n",
      "Epoch: 189670 | training loss: 2.0334e-03 | validation loss: 1.6545e-03\n",
      "Epoch: 189680 | training loss: 2.0334e-03 | validation loss: 1.6546e-03\n",
      "Epoch: 189690 | training loss: 2.0334e-03 | validation loss: 1.6545e-03\n",
      "Epoch: 189700 | training loss: 2.0334e-03 | validation loss: 1.6547e-03\n",
      "Epoch: 189710 | training loss: 2.0336e-03 | validation loss: 1.6564e-03\n",
      "Epoch: 189720 | training loss: 2.0706e-03 | validation loss: 1.7008e-03\n",
      "Epoch: 189730 | training loss: 2.2682e-03 | validation loss: 1.8705e-03\n",
      "Epoch: 189740 | training loss: 2.4571e-03 | validation loss: 2.0092e-03\n",
      "Epoch: 189750 | training loss: 2.1173e-03 | validation loss: 1.7495e-03\n",
      "Epoch: 189760 | training loss: 2.1033e-03 | validation loss: 1.6731e-03\n",
      "Epoch: 189770 | training loss: 2.0579e-03 | validation loss: 1.6930e-03\n",
      "Epoch: 189780 | training loss: 2.0370e-03 | validation loss: 1.6653e-03\n",
      "Epoch: 189790 | training loss: 2.0375e-03 | validation loss: 1.6494e-03\n",
      "Epoch: 189800 | training loss: 2.0407e-03 | validation loss: 1.6459e-03\n",
      "Epoch: 189810 | training loss: 2.0668e-03 | validation loss: 1.6439e-03\n",
      "Epoch: 189820 | training loss: 2.3427e-03 | validation loss: 1.7234e-03\n",
      "Epoch: 189830 | training loss: 2.0494e-03 | validation loss: 1.6442e-03\n",
      "Epoch: 189840 | training loss: 2.0696e-03 | validation loss: 1.6972e-03\n",
      "Epoch: 189850 | training loss: 2.0633e-03 | validation loss: 1.6446e-03\n",
      "Epoch: 189860 | training loss: 2.0443e-03 | validation loss: 1.6736e-03\n",
      "Epoch: 189870 | training loss: 2.0331e-03 | validation loss: 1.6550e-03\n",
      "Epoch: 189880 | training loss: 2.0362e-03 | validation loss: 1.6477e-03\n",
      "Epoch: 189890 | training loss: 2.0357e-03 | validation loss: 1.6480e-03\n",
      "Epoch: 189900 | training loss: 2.0408e-03 | validation loss: 1.6456e-03\n",
      "Epoch: 189910 | training loss: 2.1295e-03 | validation loss: 1.6579e-03\n",
      "Epoch: 189920 | training loss: 2.6391e-03 | validation loss: 1.8407e-03\n",
      "Epoch: 189930 | training loss: 2.2341e-03 | validation loss: 1.8058e-03\n",
      "Epoch: 189940 | training loss: 2.1070e-03 | validation loss: 1.6546e-03\n",
      "Epoch: 189950 | training loss: 2.0610e-03 | validation loss: 1.6887e-03\n",
      "Epoch: 189960 | training loss: 2.0432e-03 | validation loss: 1.6456e-03\n",
      "Epoch: 189970 | training loss: 2.0349e-03 | validation loss: 1.6603e-03\n",
      "Epoch: 189980 | training loss: 2.0329e-03 | validation loss: 1.6552e-03\n",
      "Epoch: 189990 | training loss: 2.0337e-03 | validation loss: 1.6503e-03\n",
      "Epoch: 190000 | training loss: 2.0330e-03 | validation loss: 1.6516e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190010 | training loss: 2.0329e-03 | validation loss: 1.6520e-03\n",
      "Epoch: 190020 | training loss: 2.0342e-03 | validation loss: 1.6494e-03\n",
      "Epoch: 190030 | training loss: 2.0753e-03 | validation loss: 1.6479e-03\n",
      "Epoch: 190040 | training loss: 3.1585e-03 | validation loss: 2.0605e-03\n",
      "Epoch: 190050 | training loss: 2.4397e-03 | validation loss: 1.9258e-03\n",
      "Epoch: 190060 | training loss: 2.0456e-03 | validation loss: 1.6524e-03\n",
      "Epoch: 190070 | training loss: 2.0646e-03 | validation loss: 1.6448e-03\n",
      "Epoch: 190080 | training loss: 2.0488e-03 | validation loss: 1.6746e-03\n",
      "Epoch: 190090 | training loss: 2.0337e-03 | validation loss: 1.6533e-03\n",
      "Epoch: 190100 | training loss: 2.0328e-03 | validation loss: 1.6507e-03\n",
      "Epoch: 190110 | training loss: 2.0328e-03 | validation loss: 1.6564e-03\n",
      "Epoch: 190120 | training loss: 2.0326e-03 | validation loss: 1.6518e-03\n",
      "Epoch: 190130 | training loss: 2.0325e-03 | validation loss: 1.6543e-03\n",
      "Epoch: 190140 | training loss: 2.0325e-03 | validation loss: 1.6533e-03\n",
      "Epoch: 190150 | training loss: 2.0324e-03 | validation loss: 1.6530e-03\n",
      "Epoch: 190160 | training loss: 2.0324e-03 | validation loss: 1.6535e-03\n",
      "Epoch: 190170 | training loss: 2.0324e-03 | validation loss: 1.6535e-03\n",
      "Epoch: 190180 | training loss: 2.0324e-03 | validation loss: 1.6536e-03\n",
      "Epoch: 190190 | training loss: 2.0326e-03 | validation loss: 1.6544e-03\n",
      "Epoch: 190200 | training loss: 2.0436e-03 | validation loss: 1.6654e-03\n",
      "Epoch: 190210 | training loss: 2.7133e-03 | validation loss: 2.0611e-03\n",
      "Epoch: 190220 | training loss: 2.3438e-03 | validation loss: 1.8828e-03\n",
      "Epoch: 190230 | training loss: 2.2427e-03 | validation loss: 1.8591e-03\n",
      "Epoch: 190240 | training loss: 2.0672e-03 | validation loss: 1.6700e-03\n",
      "Epoch: 190250 | training loss: 2.0393e-03 | validation loss: 1.6529e-03\n",
      "Epoch: 190260 | training loss: 2.0371e-03 | validation loss: 1.6659e-03\n",
      "Epoch: 190270 | training loss: 2.0337e-03 | validation loss: 1.6473e-03\n",
      "Epoch: 190280 | training loss: 2.0329e-03 | validation loss: 1.6524e-03\n",
      "Epoch: 190290 | training loss: 2.0324e-03 | validation loss: 1.6514e-03\n",
      "Epoch: 190300 | training loss: 2.0321e-03 | validation loss: 1.6532e-03\n",
      "Epoch: 190310 | training loss: 2.0321e-03 | validation loss: 1.6528e-03\n",
      "Epoch: 190320 | training loss: 2.0321e-03 | validation loss: 1.6536e-03\n",
      "Epoch: 190330 | training loss: 2.0320e-03 | validation loss: 1.6525e-03\n",
      "Epoch: 190340 | training loss: 2.0320e-03 | validation loss: 1.6525e-03\n",
      "Epoch: 190350 | training loss: 2.0320e-03 | validation loss: 1.6525e-03\n",
      "Epoch: 190360 | training loss: 2.0320e-03 | validation loss: 1.6520e-03\n",
      "Epoch: 190370 | training loss: 2.0325e-03 | validation loss: 1.6497e-03\n",
      "Epoch: 190380 | training loss: 2.0603e-03 | validation loss: 1.6421e-03\n",
      "Epoch: 190390 | training loss: 3.4466e-03 | validation loss: 2.1648e-03\n",
      "Epoch: 190400 | training loss: 2.4338e-03 | validation loss: 1.9192e-03\n",
      "Epoch: 190410 | training loss: 2.1039e-03 | validation loss: 1.7387e-03\n",
      "Epoch: 190420 | training loss: 2.0744e-03 | validation loss: 1.6757e-03\n",
      "Epoch: 190430 | training loss: 2.0367e-03 | validation loss: 1.6440e-03\n",
      "Epoch: 190440 | training loss: 2.0400e-03 | validation loss: 1.6553e-03\n",
      "Epoch: 190450 | training loss: 2.0318e-03 | validation loss: 1.6518e-03\n",
      "Epoch: 190460 | training loss: 2.0327e-03 | validation loss: 1.6544e-03\n",
      "Epoch: 190470 | training loss: 2.0320e-03 | validation loss: 1.6539e-03\n",
      "Epoch: 190480 | training loss: 2.0318e-03 | validation loss: 1.6508e-03\n",
      "Epoch: 190490 | training loss: 2.0317e-03 | validation loss: 1.6536e-03\n",
      "Epoch: 190500 | training loss: 2.0317e-03 | validation loss: 1.6519e-03\n",
      "Epoch: 190510 | training loss: 2.0316e-03 | validation loss: 1.6527e-03\n",
      "Epoch: 190520 | training loss: 2.0316e-03 | validation loss: 1.6523e-03\n",
      "Epoch: 190530 | training loss: 2.0316e-03 | validation loss: 1.6523e-03\n",
      "Epoch: 190540 | training loss: 2.0316e-03 | validation loss: 1.6523e-03\n",
      "Epoch: 190550 | training loss: 2.0315e-03 | validation loss: 1.6524e-03\n",
      "Epoch: 190560 | training loss: 2.0315e-03 | validation loss: 1.6525e-03\n",
      "Epoch: 190570 | training loss: 2.0317e-03 | validation loss: 1.6534e-03\n",
      "Epoch: 190580 | training loss: 2.0408e-03 | validation loss: 1.6653e-03\n",
      "Epoch: 190590 | training loss: 2.9101e-03 | validation loss: 2.1739e-03\n",
      "Epoch: 190600 | training loss: 2.6330e-03 | validation loss: 1.8959e-03\n",
      "Epoch: 190610 | training loss: 2.1886e-03 | validation loss: 1.8137e-03\n",
      "Epoch: 190620 | training loss: 2.0838e-03 | validation loss: 1.7223e-03\n",
      "Epoch: 190630 | training loss: 2.0366e-03 | validation loss: 1.6512e-03\n",
      "Epoch: 190640 | training loss: 2.0352e-03 | validation loss: 1.6447e-03\n",
      "Epoch: 190650 | training loss: 2.0325e-03 | validation loss: 1.6558e-03\n",
      "Epoch: 190660 | training loss: 2.0321e-03 | validation loss: 1.6524e-03\n",
      "Epoch: 190670 | training loss: 2.0319e-03 | validation loss: 1.6491e-03\n",
      "Epoch: 190680 | training loss: 2.0314e-03 | validation loss: 1.6533e-03\n",
      "Epoch: 190690 | training loss: 2.0313e-03 | validation loss: 1.6516e-03\n",
      "Epoch: 190700 | training loss: 2.0313e-03 | validation loss: 1.6526e-03\n",
      "Epoch: 190710 | training loss: 2.0312e-03 | validation loss: 1.6515e-03\n",
      "Epoch: 190720 | training loss: 2.0312e-03 | validation loss: 1.6520e-03\n",
      "Epoch: 190730 | training loss: 2.0312e-03 | validation loss: 1.6519e-03\n",
      "Epoch: 190740 | training loss: 2.0311e-03 | validation loss: 1.6517e-03\n",
      "Epoch: 190750 | training loss: 2.0311e-03 | validation loss: 1.6517e-03\n",
      "Epoch: 190760 | training loss: 2.0311e-03 | validation loss: 1.6517e-03\n",
      "Epoch: 190770 | training loss: 2.0311e-03 | validation loss: 1.6514e-03\n",
      "Epoch: 190780 | training loss: 2.0313e-03 | validation loss: 1.6495e-03\n",
      "Epoch: 190790 | training loss: 2.0519e-03 | validation loss: 1.6395e-03\n",
      "Epoch: 190800 | training loss: 3.6449e-03 | validation loss: 2.2552e-03\n",
      "Epoch: 190810 | training loss: 2.2632e-03 | validation loss: 1.8134e-03\n",
      "Epoch: 190820 | training loss: 2.1187e-03 | validation loss: 1.7536e-03\n",
      "Epoch: 190830 | training loss: 2.0902e-03 | validation loss: 1.7179e-03\n",
      "Epoch: 190840 | training loss: 2.0524e-03 | validation loss: 1.6862e-03\n",
      "Epoch: 190850 | training loss: 2.0398e-03 | validation loss: 1.6723e-03\n",
      "Epoch: 190860 | training loss: 2.0326e-03 | validation loss: 1.6577e-03\n",
      "Epoch: 190870 | training loss: 2.0309e-03 | validation loss: 1.6518e-03\n",
      "Epoch: 190880 | training loss: 2.0311e-03 | validation loss: 1.6503e-03\n",
      "Epoch: 190890 | training loss: 2.0310e-03 | validation loss: 1.6494e-03\n",
      "Epoch: 190900 | training loss: 2.0308e-03 | validation loss: 1.6514e-03\n",
      "Epoch: 190910 | training loss: 2.0308e-03 | validation loss: 1.6517e-03\n",
      "Epoch: 190920 | training loss: 2.0308e-03 | validation loss: 1.6514e-03\n",
      "Epoch: 190930 | training loss: 2.0307e-03 | validation loss: 1.6510e-03\n",
      "Epoch: 190940 | training loss: 2.0307e-03 | validation loss: 1.6512e-03\n",
      "Epoch: 190950 | training loss: 2.0307e-03 | validation loss: 1.6512e-03\n",
      "Epoch: 190960 | training loss: 2.0307e-03 | validation loss: 1.6511e-03\n",
      "Epoch: 190970 | training loss: 2.0307e-03 | validation loss: 1.6511e-03\n",
      "Epoch: 190980 | training loss: 2.0307e-03 | validation loss: 1.6505e-03\n",
      "Epoch: 190990 | training loss: 2.0354e-03 | validation loss: 1.6480e-03\n",
      "Epoch: 191000 | training loss: 2.6450e-03 | validation loss: 1.8980e-03\n",
      "Epoch: 191010 | training loss: 2.4896e-03 | validation loss: 1.9391e-03\n",
      "Epoch: 191020 | training loss: 2.1523e-03 | validation loss: 1.6640e-03\n",
      "Epoch: 191030 | training loss: 2.1400e-03 | validation loss: 1.6524e-03\n",
      "Epoch: 191040 | training loss: 2.0504e-03 | validation loss: 1.6380e-03\n",
      "Epoch: 191050 | training loss: 2.0350e-03 | validation loss: 1.6547e-03\n",
      "Epoch: 191060 | training loss: 2.0341e-03 | validation loss: 1.6590e-03\n",
      "Epoch: 191070 | training loss: 2.0306e-03 | validation loss: 1.6510e-03\n",
      "Epoch: 191080 | training loss: 2.0310e-03 | validation loss: 1.6501e-03\n",
      "Epoch: 191090 | training loss: 2.0305e-03 | validation loss: 1.6529e-03\n",
      "Epoch: 191100 | training loss: 2.0304e-03 | validation loss: 1.6510e-03\n",
      "Epoch: 191110 | training loss: 2.0304e-03 | validation loss: 1.6500e-03\n",
      "Epoch: 191120 | training loss: 2.0304e-03 | validation loss: 1.6513e-03\n",
      "Epoch: 191130 | training loss: 2.0303e-03 | validation loss: 1.6507e-03\n",
      "Epoch: 191140 | training loss: 2.0303e-03 | validation loss: 1.6509e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 191150 | training loss: 2.0303e-03 | validation loss: 1.6507e-03\n",
      "Epoch: 191160 | training loss: 2.0303e-03 | validation loss: 1.6508e-03\n",
      "Epoch: 191170 | training loss: 2.0302e-03 | validation loss: 1.6508e-03\n",
      "Epoch: 191180 | training loss: 2.0302e-03 | validation loss: 1.6507e-03\n",
      "Epoch: 191190 | training loss: 2.0302e-03 | validation loss: 1.6507e-03\n",
      "Epoch: 191200 | training loss: 2.0302e-03 | validation loss: 1.6507e-03\n",
      "Epoch: 191210 | training loss: 2.0301e-03 | validation loss: 1.6506e-03\n",
      "Epoch: 191220 | training loss: 2.0301e-03 | validation loss: 1.6502e-03\n",
      "Epoch: 191230 | training loss: 2.0315e-03 | validation loss: 1.6466e-03\n",
      "Epoch: 191240 | training loss: 2.3709e-03 | validation loss: 1.7403e-03\n",
      "Epoch: 191250 | training loss: 2.4311e-03 | validation loss: 1.9364e-03\n",
      "Epoch: 191260 | training loss: 2.0786e-03 | validation loss: 1.7152e-03\n",
      "Epoch: 191270 | training loss: 2.0712e-03 | validation loss: 1.7047e-03\n",
      "Epoch: 191280 | training loss: 2.0590e-03 | validation loss: 1.6892e-03\n",
      "Epoch: 191290 | training loss: 2.0446e-03 | validation loss: 1.6739e-03\n",
      "Epoch: 191300 | training loss: 2.0356e-03 | validation loss: 1.6639e-03\n",
      "Epoch: 191310 | training loss: 2.0317e-03 | validation loss: 1.6580e-03\n",
      "Epoch: 191320 | training loss: 2.0303e-03 | validation loss: 1.6536e-03\n",
      "Epoch: 191330 | training loss: 2.0299e-03 | validation loss: 1.6510e-03\n",
      "Epoch: 191340 | training loss: 2.0299e-03 | validation loss: 1.6500e-03\n",
      "Epoch: 191350 | training loss: 2.0299e-03 | validation loss: 1.6495e-03\n",
      "Epoch: 191360 | training loss: 2.0299e-03 | validation loss: 1.6499e-03\n",
      "Epoch: 191370 | training loss: 2.0298e-03 | validation loss: 1.6504e-03\n",
      "Epoch: 191380 | training loss: 2.0298e-03 | validation loss: 1.6506e-03\n",
      "Epoch: 191390 | training loss: 2.0298e-03 | validation loss: 1.6503e-03\n",
      "Epoch: 191400 | training loss: 2.0298e-03 | validation loss: 1.6502e-03\n",
      "Epoch: 191410 | training loss: 2.0297e-03 | validation loss: 1.6503e-03\n",
      "Epoch: 191420 | training loss: 2.0297e-03 | validation loss: 1.6503e-03\n",
      "Epoch: 191430 | training loss: 2.0297e-03 | validation loss: 1.6502e-03\n",
      "Epoch: 191440 | training loss: 2.0297e-03 | validation loss: 1.6502e-03\n",
      "Epoch: 191450 | training loss: 2.0296e-03 | validation loss: 1.6503e-03\n",
      "Epoch: 191460 | training loss: 2.0300e-03 | validation loss: 1.6524e-03\n",
      "Epoch: 191470 | training loss: 2.1128e-03 | validation loss: 1.7349e-03\n",
      "Epoch: 191480 | training loss: 2.0470e-03 | validation loss: 1.6520e-03\n",
      "Epoch: 191490 | training loss: 2.0637e-03 | validation loss: 1.6888e-03\n",
      "Epoch: 191500 | training loss: 2.0562e-03 | validation loss: 1.6800e-03\n",
      "Epoch: 191510 | training loss: 2.0439e-03 | validation loss: 1.6680e-03\n",
      "Epoch: 191520 | training loss: 2.0360e-03 | validation loss: 1.6602e-03\n",
      "Epoch: 191530 | training loss: 2.0314e-03 | validation loss: 1.6543e-03\n",
      "Epoch: 191540 | training loss: 2.0296e-03 | validation loss: 1.6507e-03\n",
      "Epoch: 191550 | training loss: 2.0295e-03 | validation loss: 1.6491e-03\n",
      "Epoch: 191560 | training loss: 2.0295e-03 | validation loss: 1.6487e-03\n",
      "Epoch: 191570 | training loss: 2.0295e-03 | validation loss: 1.6483e-03\n",
      "Epoch: 191580 | training loss: 2.0331e-03 | validation loss: 1.6428e-03\n",
      "Epoch: 191590 | training loss: 2.4611e-03 | validation loss: 1.7621e-03\n",
      "Epoch: 191600 | training loss: 2.2716e-03 | validation loss: 1.8472e-03\n",
      "Epoch: 191610 | training loss: 2.1662e-03 | validation loss: 1.6917e-03\n",
      "Epoch: 191620 | training loss: 2.1432e-03 | validation loss: 1.6742e-03\n",
      "Epoch: 191630 | training loss: 2.0460e-03 | validation loss: 1.6474e-03\n",
      "Epoch: 191640 | training loss: 2.0314e-03 | validation loss: 1.6579e-03\n",
      "Epoch: 191650 | training loss: 2.0342e-03 | validation loss: 1.6610e-03\n",
      "Epoch: 191660 | training loss: 2.0293e-03 | validation loss: 1.6497e-03\n",
      "Epoch: 191670 | training loss: 2.0299e-03 | validation loss: 1.6458e-03\n",
      "Epoch: 191680 | training loss: 2.0292e-03 | validation loss: 1.6493e-03\n",
      "Epoch: 191690 | training loss: 2.0292e-03 | validation loss: 1.6505e-03\n",
      "Epoch: 191700 | training loss: 2.0292e-03 | validation loss: 1.6489e-03\n",
      "Epoch: 191710 | training loss: 2.0291e-03 | validation loss: 1.6497e-03\n",
      "Epoch: 191720 | training loss: 2.0291e-03 | validation loss: 1.6492e-03\n",
      "Epoch: 191730 | training loss: 2.0291e-03 | validation loss: 1.6494e-03\n",
      "Epoch: 191740 | training loss: 2.0290e-03 | validation loss: 1.6492e-03\n",
      "Epoch: 191750 | training loss: 2.0290e-03 | validation loss: 1.6493e-03\n",
      "Epoch: 191760 | training loss: 2.0290e-03 | validation loss: 1.6492e-03\n",
      "Epoch: 191770 | training loss: 2.0290e-03 | validation loss: 1.6492e-03\n",
      "Epoch: 191780 | training loss: 2.0289e-03 | validation loss: 1.6492e-03\n",
      "Epoch: 191790 | training loss: 2.0289e-03 | validation loss: 1.6492e-03\n",
      "Epoch: 191800 | training loss: 2.0289e-03 | validation loss: 1.6492e-03\n",
      "Epoch: 191810 | training loss: 2.0289e-03 | validation loss: 1.6492e-03\n",
      "Epoch: 191820 | training loss: 2.0289e-03 | validation loss: 1.6498e-03\n",
      "Epoch: 191830 | training loss: 2.0344e-03 | validation loss: 1.6609e-03\n",
      "Epoch: 191840 | training loss: 3.4354e-03 | validation loss: 2.4687e-03\n",
      "Epoch: 191850 | training loss: 2.9297e-03 | validation loss: 1.9678e-03\n",
      "Epoch: 191860 | training loss: 2.3129e-03 | validation loss: 1.7148e-03\n",
      "Epoch: 191870 | training loss: 2.0817e-03 | validation loss: 1.6407e-03\n",
      "Epoch: 191880 | training loss: 2.0340e-03 | validation loss: 1.6397e-03\n",
      "Epoch: 191890 | training loss: 2.0289e-03 | validation loss: 1.6473e-03\n",
      "Epoch: 191900 | training loss: 2.0287e-03 | validation loss: 1.6500e-03\n",
      "Epoch: 191910 | training loss: 2.0287e-03 | validation loss: 1.6493e-03\n",
      "Epoch: 191920 | training loss: 2.0287e-03 | validation loss: 1.6482e-03\n",
      "Epoch: 191930 | training loss: 2.0287e-03 | validation loss: 1.6479e-03\n",
      "Epoch: 191940 | training loss: 2.0287e-03 | validation loss: 1.6479e-03\n",
      "Epoch: 191950 | training loss: 2.0286e-03 | validation loss: 1.6481e-03\n",
      "Epoch: 191960 | training loss: 2.0286e-03 | validation loss: 1.6487e-03\n",
      "Epoch: 191970 | training loss: 2.0286e-03 | validation loss: 1.6490e-03\n",
      "Epoch: 191980 | training loss: 2.0285e-03 | validation loss: 1.6490e-03\n",
      "Epoch: 191990 | training loss: 2.0285e-03 | validation loss: 1.6488e-03\n",
      "Epoch: 192000 | training loss: 2.0285e-03 | validation loss: 1.6487e-03\n",
      "Epoch: 192010 | training loss: 2.0285e-03 | validation loss: 1.6488e-03\n",
      "Epoch: 192020 | training loss: 2.0284e-03 | validation loss: 1.6488e-03\n",
      "Epoch: 192030 | training loss: 2.0284e-03 | validation loss: 1.6487e-03\n",
      "Epoch: 192040 | training loss: 2.0284e-03 | validation loss: 1.6487e-03\n",
      "Epoch: 192050 | training loss: 2.0284e-03 | validation loss: 1.6487e-03\n",
      "Epoch: 192060 | training loss: 2.0284e-03 | validation loss: 1.6489e-03\n",
      "Epoch: 192070 | training loss: 2.0294e-03 | validation loss: 1.6528e-03\n",
      "Epoch: 192080 | training loss: 2.3047e-03 | validation loss: 1.8974e-03\n",
      "Epoch: 192090 | training loss: 2.4137e-03 | validation loss: 1.8622e-03\n",
      "Epoch: 192100 | training loss: 2.1476e-03 | validation loss: 1.6999e-03\n",
      "Epoch: 192110 | training loss: 2.0711e-03 | validation loss: 1.6628e-03\n",
      "Epoch: 192120 | training loss: 2.0439e-03 | validation loss: 1.6467e-03\n",
      "Epoch: 192130 | training loss: 2.0336e-03 | validation loss: 1.6479e-03\n",
      "Epoch: 192140 | training loss: 2.0298e-03 | validation loss: 1.6466e-03\n",
      "Epoch: 192150 | training loss: 2.0285e-03 | validation loss: 1.6463e-03\n",
      "Epoch: 192160 | training loss: 2.0282e-03 | validation loss: 1.6467e-03\n",
      "Epoch: 192170 | training loss: 2.0287e-03 | validation loss: 1.6454e-03\n",
      "Epoch: 192180 | training loss: 2.0420e-03 | validation loss: 1.6381e-03\n",
      "Epoch: 192190 | training loss: 2.6346e-03 | validation loss: 1.8269e-03\n",
      "Epoch: 192200 | training loss: 2.2354e-03 | validation loss: 1.8126e-03\n",
      "Epoch: 192210 | training loss: 2.2026e-03 | validation loss: 1.6780e-03\n",
      "Epoch: 192220 | training loss: 2.0288e-03 | validation loss: 1.6527e-03\n",
      "Epoch: 192230 | training loss: 2.0464e-03 | validation loss: 1.6754e-03\n",
      "Epoch: 192240 | training loss: 2.0353e-03 | validation loss: 1.6404e-03\n",
      "Epoch: 192250 | training loss: 2.0285e-03 | validation loss: 1.6511e-03\n",
      "Epoch: 192260 | training loss: 2.0279e-03 | validation loss: 1.6479e-03\n",
      "Epoch: 192270 | training loss: 2.0279e-03 | validation loss: 1.6472e-03\n",
      "Epoch: 192280 | training loss: 2.0279e-03 | validation loss: 1.6481e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 192290 | training loss: 2.0279e-03 | validation loss: 1.6483e-03\n",
      "Epoch: 192300 | training loss: 2.0279e-03 | validation loss: 1.6471e-03\n",
      "Epoch: 192310 | training loss: 2.0278e-03 | validation loss: 1.6481e-03\n",
      "Epoch: 192320 | training loss: 2.0278e-03 | validation loss: 1.6482e-03\n",
      "Epoch: 192330 | training loss: 2.0278e-03 | validation loss: 1.6482e-03\n",
      "Epoch: 192340 | training loss: 2.0278e-03 | validation loss: 1.6488e-03\n",
      "Epoch: 192350 | training loss: 2.0293e-03 | validation loss: 1.6536e-03\n",
      "Epoch: 192360 | training loss: 2.1140e-03 | validation loss: 1.7276e-03\n",
      "Epoch: 192370 | training loss: 3.2307e-03 | validation loss: 2.3615e-03\n",
      "Epoch: 192380 | training loss: 2.0414e-03 | validation loss: 1.6711e-03\n",
      "Epoch: 192390 | training loss: 2.1582e-03 | validation loss: 1.6676e-03\n",
      "Epoch: 192400 | training loss: 2.0401e-03 | validation loss: 1.6353e-03\n",
      "Epoch: 192410 | training loss: 2.0405e-03 | validation loss: 1.6644e-03\n",
      "Epoch: 192420 | training loss: 2.0302e-03 | validation loss: 1.6574e-03\n",
      "Epoch: 192430 | training loss: 2.0289e-03 | validation loss: 1.6436e-03\n",
      "Epoch: 192440 | training loss: 2.0281e-03 | validation loss: 1.6471e-03\n",
      "Epoch: 192450 | training loss: 2.0278e-03 | validation loss: 1.6498e-03\n",
      "Epoch: 192460 | training loss: 2.0276e-03 | validation loss: 1.6464e-03\n",
      "Epoch: 192470 | training loss: 2.0275e-03 | validation loss: 1.6482e-03\n",
      "Epoch: 192480 | training loss: 2.0275e-03 | validation loss: 1.6471e-03\n",
      "Epoch: 192490 | training loss: 2.0274e-03 | validation loss: 1.6478e-03\n",
      "Epoch: 192500 | training loss: 2.0274e-03 | validation loss: 1.6473e-03\n",
      "Epoch: 192510 | training loss: 2.0274e-03 | validation loss: 1.6475e-03\n",
      "Epoch: 192520 | training loss: 2.0274e-03 | validation loss: 1.6475e-03\n",
      "Epoch: 192530 | training loss: 2.0274e-03 | validation loss: 1.6474e-03\n",
      "Epoch: 192540 | training loss: 2.0273e-03 | validation loss: 1.6473e-03\n",
      "Epoch: 192550 | training loss: 2.0274e-03 | validation loss: 1.6473e-03\n",
      "Epoch: 192560 | training loss: 2.0363e-03 | validation loss: 1.6517e-03\n",
      "Epoch: 192570 | training loss: 2.8302e-03 | validation loss: 2.0752e-03\n",
      "Epoch: 192580 | training loss: 2.0746e-03 | validation loss: 1.6490e-03\n",
      "Epoch: 192590 | training loss: 2.1255e-03 | validation loss: 1.6455e-03\n",
      "Epoch: 192600 | training loss: 2.0539e-03 | validation loss: 1.6867e-03\n",
      "Epoch: 192610 | training loss: 2.0399e-03 | validation loss: 1.6706e-03\n",
      "Epoch: 192620 | training loss: 2.0392e-03 | validation loss: 1.6506e-03\n",
      "Epoch: 192630 | training loss: 2.0310e-03 | validation loss: 1.6587e-03\n",
      "Epoch: 192640 | training loss: 2.0281e-03 | validation loss: 1.6489e-03\n",
      "Epoch: 192650 | training loss: 2.0274e-03 | validation loss: 1.6488e-03\n",
      "Epoch: 192660 | training loss: 2.0272e-03 | validation loss: 1.6478e-03\n",
      "Epoch: 192670 | training loss: 2.0271e-03 | validation loss: 1.6479e-03\n",
      "Epoch: 192680 | training loss: 2.0271e-03 | validation loss: 1.6466e-03\n",
      "Epoch: 192690 | training loss: 2.0270e-03 | validation loss: 1.6472e-03\n",
      "Epoch: 192700 | training loss: 2.0270e-03 | validation loss: 1.6470e-03\n",
      "Epoch: 192710 | training loss: 2.0270e-03 | validation loss: 1.6468e-03\n",
      "Epoch: 192720 | training loss: 2.0270e-03 | validation loss: 1.6468e-03\n",
      "Epoch: 192730 | training loss: 2.0269e-03 | validation loss: 1.6471e-03\n",
      "Epoch: 192740 | training loss: 2.0272e-03 | validation loss: 1.6490e-03\n",
      "Epoch: 192750 | training loss: 2.0512e-03 | validation loss: 1.6797e-03\n",
      "Epoch: 192760 | training loss: 3.9955e-03 | validation loss: 2.7781e-03\n",
      "Epoch: 192770 | training loss: 2.1790e-03 | validation loss: 1.6919e-03\n",
      "Epoch: 192780 | training loss: 2.2346e-03 | validation loss: 1.7038e-03\n",
      "Epoch: 192790 | training loss: 2.1033e-03 | validation loss: 1.6565e-03\n",
      "Epoch: 192800 | training loss: 2.0369e-03 | validation loss: 1.6430e-03\n",
      "Epoch: 192810 | training loss: 2.0274e-03 | validation loss: 1.6513e-03\n",
      "Epoch: 192820 | training loss: 2.0300e-03 | validation loss: 1.6556e-03\n",
      "Epoch: 192830 | training loss: 2.0274e-03 | validation loss: 1.6497e-03\n",
      "Epoch: 192840 | training loss: 2.0269e-03 | validation loss: 1.6447e-03\n",
      "Epoch: 192850 | training loss: 2.0268e-03 | validation loss: 1.6451e-03\n",
      "Epoch: 192860 | training loss: 2.0267e-03 | validation loss: 1.6473e-03\n",
      "Epoch: 192870 | training loss: 2.0267e-03 | validation loss: 1.6469e-03\n",
      "Epoch: 192880 | training loss: 2.0266e-03 | validation loss: 1.6463e-03\n",
      "Epoch: 192890 | training loss: 2.0266e-03 | validation loss: 1.6467e-03\n",
      "Epoch: 192900 | training loss: 2.0266e-03 | validation loss: 1.6464e-03\n",
      "Epoch: 192910 | training loss: 2.0266e-03 | validation loss: 1.6466e-03\n",
      "Epoch: 192920 | training loss: 2.0265e-03 | validation loss: 1.6464e-03\n",
      "Epoch: 192930 | training loss: 2.0265e-03 | validation loss: 1.6465e-03\n",
      "Epoch: 192940 | training loss: 2.0265e-03 | validation loss: 1.6464e-03\n",
      "Epoch: 192950 | training loss: 2.0265e-03 | validation loss: 1.6464e-03\n",
      "Epoch: 192960 | training loss: 2.0265e-03 | validation loss: 1.6464e-03\n",
      "Epoch: 192970 | training loss: 2.0264e-03 | validation loss: 1.6463e-03\n",
      "Epoch: 192980 | training loss: 2.0264e-03 | validation loss: 1.6463e-03\n",
      "Epoch: 192990 | training loss: 2.0264e-03 | validation loss: 1.6462e-03\n",
      "Epoch: 193000 | training loss: 2.0264e-03 | validation loss: 1.6456e-03\n",
      "Epoch: 193010 | training loss: 2.0320e-03 | validation loss: 1.6398e-03\n",
      "Epoch: 193020 | training loss: 3.3745e-03 | validation loss: 2.1522e-03\n",
      "Epoch: 193030 | training loss: 2.9902e-03 | validation loss: 2.2289e-03\n",
      "Epoch: 193040 | training loss: 2.3614e-03 | validation loss: 1.8807e-03\n",
      "Epoch: 193050 | training loss: 2.1141e-03 | validation loss: 1.7279e-03\n",
      "Epoch: 193060 | training loss: 2.0456e-03 | validation loss: 1.6746e-03\n",
      "Epoch: 193070 | training loss: 2.0308e-03 | validation loss: 1.6574e-03\n",
      "Epoch: 193080 | training loss: 2.0278e-03 | validation loss: 1.6520e-03\n",
      "Epoch: 193090 | training loss: 2.0271e-03 | validation loss: 1.6506e-03\n",
      "Epoch: 193100 | training loss: 2.0267e-03 | validation loss: 1.6497e-03\n",
      "Epoch: 193110 | training loss: 2.0264e-03 | validation loss: 1.6485e-03\n",
      "Epoch: 193120 | training loss: 2.0262e-03 | validation loss: 1.6474e-03\n",
      "Epoch: 193130 | training loss: 2.0261e-03 | validation loss: 1.6463e-03\n",
      "Epoch: 193140 | training loss: 2.0261e-03 | validation loss: 1.6457e-03\n",
      "Epoch: 193150 | training loss: 2.0261e-03 | validation loss: 1.6457e-03\n",
      "Epoch: 193160 | training loss: 2.0260e-03 | validation loss: 1.6460e-03\n",
      "Epoch: 193170 | training loss: 2.0260e-03 | validation loss: 1.6461e-03\n",
      "Epoch: 193180 | training loss: 2.0260e-03 | validation loss: 1.6459e-03\n",
      "Epoch: 193190 | training loss: 2.0260e-03 | validation loss: 1.6459e-03\n",
      "Epoch: 193200 | training loss: 2.0260e-03 | validation loss: 1.6459e-03\n",
      "Epoch: 193210 | training loss: 2.0259e-03 | validation loss: 1.6459e-03\n",
      "Epoch: 193220 | training loss: 2.0259e-03 | validation loss: 1.6459e-03\n",
      "Epoch: 193230 | training loss: 2.0259e-03 | validation loss: 1.6464e-03\n",
      "Epoch: 193240 | training loss: 2.0300e-03 | validation loss: 1.6561e-03\n",
      "Epoch: 193250 | training loss: 2.6660e-03 | validation loss: 2.1838e-03\n",
      "Epoch: 193260 | training loss: 2.0903e-03 | validation loss: 1.6620e-03\n",
      "Epoch: 193270 | training loss: 2.0819e-03 | validation loss: 1.6711e-03\n",
      "Epoch: 193280 | training loss: 2.0518e-03 | validation loss: 1.6473e-03\n",
      "Epoch: 193290 | training loss: 2.0368e-03 | validation loss: 1.6447e-03\n",
      "Epoch: 193300 | training loss: 2.0299e-03 | validation loss: 1.6451e-03\n",
      "Epoch: 193310 | training loss: 2.0267e-03 | validation loss: 1.6423e-03\n",
      "Epoch: 193320 | training loss: 2.0258e-03 | validation loss: 1.6440e-03\n",
      "Epoch: 193330 | training loss: 2.0258e-03 | validation loss: 1.6456e-03\n",
      "Epoch: 193340 | training loss: 2.0257e-03 | validation loss: 1.6452e-03\n",
      "Epoch: 193350 | training loss: 2.0261e-03 | validation loss: 1.6423e-03\n",
      "Epoch: 193360 | training loss: 2.0511e-03 | validation loss: 1.6342e-03\n",
      "Epoch: 193370 | training loss: 3.3535e-03 | validation loss: 2.1181e-03\n",
      "Epoch: 193380 | training loss: 2.4757e-03 | validation loss: 1.9424e-03\n",
      "Epoch: 193390 | training loss: 2.0658e-03 | validation loss: 1.6921e-03\n",
      "Epoch: 193400 | training loss: 2.0648e-03 | validation loss: 1.6432e-03\n",
      "Epoch: 193410 | training loss: 2.0376e-03 | validation loss: 1.6408e-03\n",
      "Epoch: 193420 | training loss: 2.0307e-03 | validation loss: 1.6584e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 193430 | training loss: 2.0260e-03 | validation loss: 1.6485e-03\n",
      "Epoch: 193440 | training loss: 2.0266e-03 | validation loss: 1.6409e-03\n",
      "Epoch: 193450 | training loss: 2.0257e-03 | validation loss: 1.6468e-03\n",
      "Epoch: 193460 | training loss: 2.0254e-03 | validation loss: 1.6448e-03\n",
      "Epoch: 193470 | training loss: 2.0254e-03 | validation loss: 1.6452e-03\n",
      "Epoch: 193480 | training loss: 2.0254e-03 | validation loss: 1.6449e-03\n",
      "Epoch: 193490 | training loss: 2.0253e-03 | validation loss: 1.6451e-03\n",
      "Epoch: 193500 | training loss: 2.0253e-03 | validation loss: 1.6448e-03\n",
      "Epoch: 193510 | training loss: 2.0253e-03 | validation loss: 1.6450e-03\n",
      "Epoch: 193520 | training loss: 2.0253e-03 | validation loss: 1.6450e-03\n",
      "Epoch: 193530 | training loss: 2.0253e-03 | validation loss: 1.6449e-03\n",
      "Epoch: 193540 | training loss: 2.0252e-03 | validation loss: 1.6449e-03\n",
      "Epoch: 193550 | training loss: 2.0252e-03 | validation loss: 1.6448e-03\n",
      "Epoch: 193560 | training loss: 2.0252e-03 | validation loss: 1.6447e-03\n",
      "Epoch: 193570 | training loss: 2.0253e-03 | validation loss: 1.6435e-03\n",
      "Epoch: 193580 | training loss: 2.0486e-03 | validation loss: 1.6367e-03\n",
      "Epoch: 193590 | training loss: 4.8808e-03 | validation loss: 2.8054e-03\n",
      "Epoch: 193600 | training loss: 2.1957e-03 | validation loss: 1.6644e-03\n",
      "Epoch: 193610 | training loss: 2.1100e-03 | validation loss: 1.6388e-03\n",
      "Epoch: 193620 | training loss: 2.0816e-03 | validation loss: 1.6338e-03\n",
      "Epoch: 193630 | training loss: 2.0534e-03 | validation loss: 1.6338e-03\n",
      "Epoch: 193640 | training loss: 2.0368e-03 | validation loss: 1.6359e-03\n",
      "Epoch: 193650 | training loss: 2.0292e-03 | validation loss: 1.6375e-03\n",
      "Epoch: 193660 | training loss: 2.0263e-03 | validation loss: 1.6397e-03\n",
      "Epoch: 193670 | training loss: 2.0253e-03 | validation loss: 1.6425e-03\n",
      "Epoch: 193680 | training loss: 2.0250e-03 | validation loss: 1.6443e-03\n",
      "Epoch: 193690 | training loss: 2.0249e-03 | validation loss: 1.6451e-03\n",
      "Epoch: 193700 | training loss: 2.0249e-03 | validation loss: 1.6454e-03\n",
      "Epoch: 193710 | training loss: 2.0249e-03 | validation loss: 1.6449e-03\n",
      "Epoch: 193720 | training loss: 2.0249e-03 | validation loss: 1.6445e-03\n",
      "Epoch: 193730 | training loss: 2.0249e-03 | validation loss: 1.6443e-03\n",
      "Epoch: 193740 | training loss: 2.0248e-03 | validation loss: 1.6445e-03\n",
      "Epoch: 193750 | training loss: 2.0248e-03 | validation loss: 1.6445e-03\n",
      "Epoch: 193760 | training loss: 2.0248e-03 | validation loss: 1.6444e-03\n",
      "Epoch: 193770 | training loss: 2.0248e-03 | validation loss: 1.6444e-03\n",
      "Epoch: 193780 | training loss: 2.0247e-03 | validation loss: 1.6444e-03\n",
      "Epoch: 193790 | training loss: 2.0247e-03 | validation loss: 1.6443e-03\n",
      "Epoch: 193800 | training loss: 2.0247e-03 | validation loss: 1.6440e-03\n",
      "Epoch: 193810 | training loss: 2.0265e-03 | validation loss: 1.6417e-03\n",
      "Epoch: 193820 | training loss: 2.3356e-03 | validation loss: 1.8099e-03\n",
      "Epoch: 193830 | training loss: 2.3380e-03 | validation loss: 1.9061e-03\n",
      "Epoch: 193840 | training loss: 2.0901e-03 | validation loss: 1.7073e-03\n",
      "Epoch: 193850 | training loss: 2.0334e-03 | validation loss: 1.6580e-03\n",
      "Epoch: 193860 | training loss: 2.0247e-03 | validation loss: 1.6434e-03\n",
      "Epoch: 193870 | training loss: 2.0256e-03 | validation loss: 1.6421e-03\n",
      "Epoch: 193880 | training loss: 2.0260e-03 | validation loss: 1.6424e-03\n",
      "Epoch: 193890 | training loss: 2.0250e-03 | validation loss: 1.6429e-03\n",
      "Epoch: 193900 | training loss: 2.0245e-03 | validation loss: 1.6444e-03\n",
      "Epoch: 193910 | training loss: 2.0247e-03 | validation loss: 1.6466e-03\n",
      "Epoch: 193920 | training loss: 2.0323e-03 | validation loss: 1.6605e-03\n",
      "Epoch: 193930 | training loss: 2.6101e-03 | validation loss: 2.0340e-03\n",
      "Epoch: 193940 | training loss: 2.3387e-03 | validation loss: 1.7136e-03\n",
      "Epoch: 193950 | training loss: 2.1637e-03 | validation loss: 1.7470e-03\n",
      "Epoch: 193960 | training loss: 2.0853e-03 | validation loss: 1.6986e-03\n",
      "Epoch: 193970 | training loss: 2.0296e-03 | validation loss: 1.6357e-03\n",
      "Epoch: 193980 | training loss: 2.0338e-03 | validation loss: 1.6369e-03\n",
      "Epoch: 193990 | training loss: 2.0257e-03 | validation loss: 1.6503e-03\n",
      "Epoch: 194000 | training loss: 2.0248e-03 | validation loss: 1.6474e-03\n",
      "Epoch: 194010 | training loss: 2.0248e-03 | validation loss: 1.6409e-03\n",
      "Epoch: 194020 | training loss: 2.0244e-03 | validation loss: 1.6449e-03\n",
      "Epoch: 194030 | training loss: 2.0242e-03 | validation loss: 1.6431e-03\n",
      "Epoch: 194040 | training loss: 2.0242e-03 | validation loss: 1.6440e-03\n",
      "Epoch: 194050 | training loss: 2.0242e-03 | validation loss: 1.6433e-03\n",
      "Epoch: 194060 | training loss: 2.0242e-03 | validation loss: 1.6438e-03\n",
      "Epoch: 194070 | training loss: 2.0241e-03 | validation loss: 1.6434e-03\n",
      "Epoch: 194080 | training loss: 2.0241e-03 | validation loss: 1.6435e-03\n",
      "Epoch: 194090 | training loss: 2.0241e-03 | validation loss: 1.6436e-03\n",
      "Epoch: 194100 | training loss: 2.0241e-03 | validation loss: 1.6436e-03\n",
      "Epoch: 194110 | training loss: 2.0241e-03 | validation loss: 1.6438e-03\n",
      "Epoch: 194120 | training loss: 2.0243e-03 | validation loss: 1.6455e-03\n",
      "Epoch: 194130 | training loss: 2.0404e-03 | validation loss: 1.6673e-03\n",
      "Epoch: 194140 | training loss: 3.4037e-03 | validation loss: 2.4501e-03\n",
      "Epoch: 194150 | training loss: 2.6907e-03 | validation loss: 1.8645e-03\n",
      "Epoch: 194160 | training loss: 2.1688e-03 | validation loss: 1.6565e-03\n",
      "Epoch: 194170 | training loss: 2.0263e-03 | validation loss: 1.6377e-03\n",
      "Epoch: 194180 | training loss: 2.0416e-03 | validation loss: 1.6722e-03\n",
      "Epoch: 194190 | training loss: 2.0342e-03 | validation loss: 1.6641e-03\n",
      "Epoch: 194200 | training loss: 2.0240e-03 | validation loss: 1.6431e-03\n",
      "Epoch: 194210 | training loss: 2.0253e-03 | validation loss: 1.6391e-03\n",
      "Epoch: 194220 | training loss: 2.0238e-03 | validation loss: 1.6426e-03\n",
      "Epoch: 194230 | training loss: 2.0240e-03 | validation loss: 1.6456e-03\n",
      "Epoch: 194240 | training loss: 2.0238e-03 | validation loss: 1.6426e-03\n",
      "Epoch: 194250 | training loss: 2.0238e-03 | validation loss: 1.6431e-03\n",
      "Epoch: 194260 | training loss: 2.0237e-03 | validation loss: 1.6436e-03\n",
      "Epoch: 194270 | training loss: 2.0237e-03 | validation loss: 1.6429e-03\n",
      "Epoch: 194280 | training loss: 2.0237e-03 | validation loss: 1.6433e-03\n",
      "Epoch: 194290 | training loss: 2.0237e-03 | validation loss: 1.6431e-03\n",
      "Epoch: 194300 | training loss: 2.0236e-03 | validation loss: 1.6432e-03\n",
      "Epoch: 194310 | training loss: 2.0237e-03 | validation loss: 1.6437e-03\n",
      "Epoch: 194320 | training loss: 2.0266e-03 | validation loss: 1.6502e-03\n",
      "Epoch: 194330 | training loss: 2.3466e-03 | validation loss: 1.9184e-03\n",
      "Epoch: 194340 | training loss: 2.2776e-03 | validation loss: 1.7770e-03\n",
      "Epoch: 194350 | training loss: 2.0380e-03 | validation loss: 1.6426e-03\n",
      "Epoch: 194360 | training loss: 2.0291e-03 | validation loss: 1.6510e-03\n",
      "Epoch: 194370 | training loss: 2.0342e-03 | validation loss: 1.6561e-03\n",
      "Epoch: 194380 | training loss: 2.0259e-03 | validation loss: 1.6439e-03\n",
      "Epoch: 194390 | training loss: 2.0383e-03 | validation loss: 1.6314e-03\n",
      "Epoch: 194400 | training loss: 2.6201e-03 | validation loss: 1.8180e-03\n",
      "Epoch: 194410 | training loss: 2.2021e-03 | validation loss: 1.7887e-03\n",
      "Epoch: 194420 | training loss: 2.2015e-03 | validation loss: 1.6759e-03\n",
      "Epoch: 194430 | training loss: 2.0293e-03 | validation loss: 1.6572e-03\n",
      "Epoch: 194440 | training loss: 2.0344e-03 | validation loss: 1.6620e-03\n",
      "Epoch: 194450 | training loss: 2.0321e-03 | validation loss: 1.6341e-03\n",
      "Epoch: 194460 | training loss: 2.0254e-03 | validation loss: 1.6490e-03\n",
      "Epoch: 194470 | training loss: 2.0237e-03 | validation loss: 1.6402e-03\n",
      "Epoch: 194480 | training loss: 2.0234e-03 | validation loss: 1.6442e-03\n",
      "Epoch: 194490 | training loss: 2.0233e-03 | validation loss: 1.6414e-03\n",
      "Epoch: 194500 | training loss: 2.0233e-03 | validation loss: 1.6436e-03\n",
      "Epoch: 194510 | training loss: 2.0232e-03 | validation loss: 1.6420e-03\n",
      "Epoch: 194520 | training loss: 2.0232e-03 | validation loss: 1.6420e-03\n",
      "Epoch: 194530 | training loss: 2.0232e-03 | validation loss: 1.6424e-03\n",
      "Epoch: 194540 | training loss: 2.0231e-03 | validation loss: 1.6426e-03\n",
      "Epoch: 194550 | training loss: 2.0231e-03 | validation loss: 1.6430e-03\n",
      "Epoch: 194560 | training loss: 2.0238e-03 | validation loss: 1.6461e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 194570 | training loss: 2.0786e-03 | validation loss: 1.6995e-03\n",
      "Epoch: 194580 | training loss: 3.7697e-03 | validation loss: 2.6457e-03\n",
      "Epoch: 194590 | training loss: 2.0444e-03 | validation loss: 1.6778e-03\n",
      "Epoch: 194600 | training loss: 2.1306e-03 | validation loss: 1.6572e-03\n",
      "Epoch: 194610 | training loss: 2.0810e-03 | validation loss: 1.6320e-03\n",
      "Epoch: 194620 | training loss: 2.0323e-03 | validation loss: 1.6321e-03\n",
      "Epoch: 194630 | training loss: 2.0247e-03 | validation loss: 1.6501e-03\n",
      "Epoch: 194640 | training loss: 2.0260e-03 | validation loss: 1.6534e-03\n",
      "Epoch: 194650 | training loss: 2.0230e-03 | validation loss: 1.6416e-03\n",
      "Epoch: 194660 | training loss: 2.0233e-03 | validation loss: 1.6395e-03\n",
      "Epoch: 194670 | training loss: 2.0229e-03 | validation loss: 1.6431e-03\n",
      "Epoch: 194680 | training loss: 2.0229e-03 | validation loss: 1.6428e-03\n",
      "Epoch: 194690 | training loss: 2.0229e-03 | validation loss: 1.6415e-03\n",
      "Epoch: 194700 | training loss: 2.0228e-03 | validation loss: 1.6425e-03\n",
      "Epoch: 194710 | training loss: 2.0228e-03 | validation loss: 1.6419e-03\n",
      "Epoch: 194720 | training loss: 2.0228e-03 | validation loss: 1.6422e-03\n",
      "Epoch: 194730 | training loss: 2.0227e-03 | validation loss: 1.6420e-03\n",
      "Epoch: 194740 | training loss: 2.0227e-03 | validation loss: 1.6421e-03\n",
      "Epoch: 194750 | training loss: 2.0227e-03 | validation loss: 1.6420e-03\n",
      "Epoch: 194760 | training loss: 2.0227e-03 | validation loss: 1.6421e-03\n",
      "Epoch: 194770 | training loss: 2.0230e-03 | validation loss: 1.6432e-03\n",
      "Epoch: 194780 | training loss: 2.0617e-03 | validation loss: 1.6787e-03\n",
      "Epoch: 194790 | training loss: 2.3290e-03 | validation loss: 1.8086e-03\n",
      "Epoch: 194800 | training loss: 2.2817e-03 | validation loss: 1.7793e-03\n",
      "Epoch: 194810 | training loss: 2.0858e-03 | validation loss: 1.7194e-03\n",
      "Epoch: 194820 | training loss: 2.0309e-03 | validation loss: 1.6603e-03\n",
      "Epoch: 194830 | training loss: 2.0290e-03 | validation loss: 1.6336e-03\n",
      "Epoch: 194840 | training loss: 2.0323e-03 | validation loss: 1.6310e-03\n",
      "Epoch: 194850 | training loss: 2.0469e-03 | validation loss: 1.6295e-03\n",
      "Epoch: 194860 | training loss: 2.2005e-03 | validation loss: 1.6661e-03\n",
      "Epoch: 194870 | training loss: 2.2042e-03 | validation loss: 1.6692e-03\n",
      "Epoch: 194880 | training loss: 2.1205e-03 | validation loss: 1.7320e-03\n",
      "Epoch: 194890 | training loss: 2.0445e-03 | validation loss: 1.6316e-03\n",
      "Epoch: 194900 | training loss: 2.0226e-03 | validation loss: 1.6394e-03\n",
      "Epoch: 194910 | training loss: 2.0301e-03 | validation loss: 1.6571e-03\n",
      "Epoch: 194920 | training loss: 2.0229e-03 | validation loss: 1.6450e-03\n",
      "Epoch: 194930 | training loss: 2.0225e-03 | validation loss: 1.6399e-03\n",
      "Epoch: 194940 | training loss: 2.0243e-03 | validation loss: 1.6362e-03\n",
      "Epoch: 194950 | training loss: 2.0669e-03 | validation loss: 1.6337e-03\n",
      "Epoch: 194960 | training loss: 2.9547e-03 | validation loss: 1.9571e-03\n",
      "Epoch: 194970 | training loss: 2.3646e-03 | validation loss: 1.8785e-03\n",
      "Epoch: 194980 | training loss: 2.0973e-03 | validation loss: 1.6477e-03\n",
      "Epoch: 194990 | training loss: 2.0225e-03 | validation loss: 1.6438e-03\n",
      "Epoch: 195000 | training loss: 2.0276e-03 | validation loss: 1.6516e-03\n",
      "Epoch: 195010 | training loss: 2.0267e-03 | validation loss: 1.6347e-03\n",
      "Epoch: 195020 | training loss: 2.0242e-03 | validation loss: 1.6484e-03\n",
      "Epoch: 195030 | training loss: 2.0228e-03 | validation loss: 1.6383e-03\n",
      "Epoch: 195040 | training loss: 2.0222e-03 | validation loss: 1.6424e-03\n",
      "Epoch: 195050 | training loss: 2.0221e-03 | validation loss: 1.6420e-03\n",
      "Epoch: 195060 | training loss: 2.0221e-03 | validation loss: 1.6401e-03\n",
      "Epoch: 195070 | training loss: 2.0221e-03 | validation loss: 1.6407e-03\n",
      "Epoch: 195080 | training loss: 2.0220e-03 | validation loss: 1.6408e-03\n",
      "Epoch: 195090 | training loss: 2.0220e-03 | validation loss: 1.6405e-03\n",
      "Epoch: 195100 | training loss: 2.0227e-03 | validation loss: 1.6380e-03\n",
      "Epoch: 195110 | training loss: 2.0697e-03 | validation loss: 1.6370e-03\n",
      "Epoch: 195120 | training loss: 3.6964e-03 | validation loss: 2.2880e-03\n",
      "Epoch: 195130 | training loss: 2.1110e-03 | validation loss: 1.7144e-03\n",
      "Epoch: 195140 | training loss: 2.2001e-03 | validation loss: 1.7822e-03\n",
      "Epoch: 195150 | training loss: 2.0329e-03 | validation loss: 1.6631e-03\n",
      "Epoch: 195160 | training loss: 2.0341e-03 | validation loss: 1.6310e-03\n",
      "Epoch: 195170 | training loss: 2.0273e-03 | validation loss: 1.6331e-03\n",
      "Epoch: 195180 | training loss: 2.0229e-03 | validation loss: 1.6468e-03\n",
      "Epoch: 195190 | training loss: 2.0223e-03 | validation loss: 1.6440e-03\n",
      "Epoch: 195200 | training loss: 2.0221e-03 | validation loss: 1.6388e-03\n",
      "Epoch: 195210 | training loss: 2.0218e-03 | validation loss: 1.6415e-03\n",
      "Epoch: 195220 | training loss: 2.0217e-03 | validation loss: 1.6413e-03\n",
      "Epoch: 195230 | training loss: 2.0217e-03 | validation loss: 1.6406e-03\n",
      "Epoch: 195240 | training loss: 2.0217e-03 | validation loss: 1.6411e-03\n",
      "Epoch: 195250 | training loss: 2.0217e-03 | validation loss: 1.6407e-03\n",
      "Epoch: 195260 | training loss: 2.0216e-03 | validation loss: 1.6407e-03\n",
      "Epoch: 195270 | training loss: 2.0216e-03 | validation loss: 1.6406e-03\n",
      "Epoch: 195280 | training loss: 2.0219e-03 | validation loss: 1.6392e-03\n",
      "Epoch: 195290 | training loss: 2.0482e-03 | validation loss: 1.6430e-03\n",
      "Epoch: 195300 | training loss: 2.4111e-03 | validation loss: 1.8449e-03\n",
      "Epoch: 195310 | training loss: 2.0525e-03 | validation loss: 1.6416e-03\n",
      "Epoch: 195320 | training loss: 2.0386e-03 | validation loss: 1.6626e-03\n",
      "Epoch: 195330 | training loss: 2.0406e-03 | validation loss: 1.6675e-03\n",
      "Epoch: 195340 | training loss: 2.0272e-03 | validation loss: 1.6557e-03\n",
      "Epoch: 195350 | training loss: 2.1130e-03 | validation loss: 1.7239e-03\n",
      "Epoch: 195360 | training loss: 2.7087e-03 | validation loss: 2.0820e-03\n",
      "Epoch: 195370 | training loss: 2.2354e-03 | validation loss: 1.6817e-03\n",
      "Epoch: 195380 | training loss: 2.0697e-03 | validation loss: 1.6943e-03\n",
      "Epoch: 195390 | training loss: 2.0287e-03 | validation loss: 1.6326e-03\n",
      "Epoch: 195400 | training loss: 2.0229e-03 | validation loss: 1.6458e-03\n",
      "Epoch: 195410 | training loss: 2.0224e-03 | validation loss: 1.6361e-03\n",
      "Epoch: 195420 | training loss: 2.0226e-03 | validation loss: 1.6458e-03\n",
      "Epoch: 195430 | training loss: 2.0220e-03 | validation loss: 1.6372e-03\n",
      "Epoch: 195440 | training loss: 2.0213e-03 | validation loss: 1.6399e-03\n",
      "Epoch: 195450 | training loss: 2.0214e-03 | validation loss: 1.6421e-03\n",
      "Epoch: 195460 | training loss: 2.0216e-03 | validation loss: 1.6428e-03\n",
      "Epoch: 195470 | training loss: 2.0234e-03 | validation loss: 1.6474e-03\n",
      "Epoch: 195480 | training loss: 2.0694e-03 | validation loss: 1.6924e-03\n",
      "Epoch: 195490 | training loss: 2.9928e-03 | validation loss: 2.2337e-03\n",
      "Epoch: 195500 | training loss: 2.3686e-03 | validation loss: 1.7341e-03\n",
      "Epoch: 195510 | training loss: 2.0857e-03 | validation loss: 1.6942e-03\n",
      "Epoch: 195520 | training loss: 2.0230e-03 | validation loss: 1.6472e-03\n",
      "Epoch: 195530 | training loss: 2.0280e-03 | validation loss: 1.6335e-03\n",
      "Epoch: 195540 | training loss: 2.0263e-03 | validation loss: 1.6492e-03\n",
      "Epoch: 195550 | training loss: 2.0233e-03 | validation loss: 1.6374e-03\n",
      "Epoch: 195560 | training loss: 2.0218e-03 | validation loss: 1.6427e-03\n",
      "Epoch: 195570 | training loss: 2.0212e-03 | validation loss: 1.6393e-03\n",
      "Epoch: 195580 | training loss: 2.0210e-03 | validation loss: 1.6395e-03\n",
      "Epoch: 195590 | training loss: 2.0210e-03 | validation loss: 1.6411e-03\n",
      "Epoch: 195600 | training loss: 2.0209e-03 | validation loss: 1.6403e-03\n",
      "Epoch: 195610 | training loss: 2.0209e-03 | validation loss: 1.6397e-03\n",
      "Epoch: 195620 | training loss: 2.0209e-03 | validation loss: 1.6395e-03\n",
      "Epoch: 195630 | training loss: 2.0211e-03 | validation loss: 1.6381e-03\n",
      "Epoch: 195640 | training loss: 2.0369e-03 | validation loss: 1.6334e-03\n",
      "Epoch: 195650 | training loss: 3.2704e-03 | validation loss: 2.1173e-03\n",
      "Epoch: 195660 | training loss: 2.7192e-03 | validation loss: 2.0701e-03\n",
      "Epoch: 195670 | training loss: 2.1016e-03 | validation loss: 1.6964e-03\n",
      "Epoch: 195680 | training loss: 2.0294e-03 | validation loss: 1.6328e-03\n",
      "Epoch: 195690 | training loss: 2.0492e-03 | validation loss: 1.6462e-03\n",
      "Epoch: 195700 | training loss: 2.0260e-03 | validation loss: 1.6370e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 195710 | training loss: 2.0217e-03 | validation loss: 1.6424e-03\n",
      "Epoch: 195720 | training loss: 2.0219e-03 | validation loss: 1.6429e-03\n",
      "Epoch: 195730 | training loss: 2.0208e-03 | validation loss: 1.6391e-03\n",
      "Epoch: 195740 | training loss: 2.0207e-03 | validation loss: 1.6392e-03\n",
      "Epoch: 195750 | training loss: 2.0207e-03 | validation loss: 1.6404e-03\n",
      "Epoch: 195760 | training loss: 2.0206e-03 | validation loss: 1.6394e-03\n",
      "Epoch: 195770 | training loss: 2.0206e-03 | validation loss: 1.6397e-03\n",
      "Epoch: 195780 | training loss: 2.0205e-03 | validation loss: 1.6397e-03\n",
      "Epoch: 195790 | training loss: 2.0205e-03 | validation loss: 1.6397e-03\n",
      "Epoch: 195800 | training loss: 2.0205e-03 | validation loss: 1.6399e-03\n",
      "Epoch: 195810 | training loss: 2.0206e-03 | validation loss: 1.6415e-03\n",
      "Epoch: 195820 | training loss: 2.0321e-03 | validation loss: 1.6644e-03\n",
      "Epoch: 195830 | training loss: 2.8590e-03 | validation loss: 2.3211e-03\n",
      "Epoch: 195840 | training loss: 2.1037e-03 | validation loss: 1.6375e-03\n",
      "Epoch: 195850 | training loss: 2.1065e-03 | validation loss: 1.6933e-03\n",
      "Epoch: 195860 | training loss: 2.0620e-03 | validation loss: 1.6336e-03\n",
      "Epoch: 195870 | training loss: 2.0336e-03 | validation loss: 1.6477e-03\n",
      "Epoch: 195880 | training loss: 2.0222e-03 | validation loss: 1.6331e-03\n",
      "Epoch: 195890 | training loss: 2.0205e-03 | validation loss: 1.6409e-03\n",
      "Epoch: 195900 | training loss: 2.0211e-03 | validation loss: 1.6440e-03\n",
      "Epoch: 195910 | training loss: 2.0203e-03 | validation loss: 1.6390e-03\n",
      "Epoch: 195920 | training loss: 2.0205e-03 | validation loss: 1.6368e-03\n",
      "Epoch: 195930 | training loss: 2.0210e-03 | validation loss: 1.6354e-03\n",
      "Epoch: 195940 | training loss: 2.0312e-03 | validation loss: 1.6301e-03\n",
      "Epoch: 195950 | training loss: 2.3771e-03 | validation loss: 1.7280e-03\n",
      "Epoch: 195960 | training loss: 2.0259e-03 | validation loss: 1.6409e-03\n",
      "Epoch: 195970 | training loss: 2.0988e-03 | validation loss: 1.6406e-03\n",
      "Epoch: 195980 | training loss: 2.0900e-03 | validation loss: 1.7024e-03\n",
      "Epoch: 195990 | training loss: 2.0318e-03 | validation loss: 1.6298e-03\n",
      "Epoch: 196000 | training loss: 2.0203e-03 | validation loss: 1.6415e-03\n",
      "Epoch: 196010 | training loss: 2.0202e-03 | validation loss: 1.6408e-03\n",
      "Epoch: 196020 | training loss: 2.0201e-03 | validation loss: 1.6375e-03\n",
      "Epoch: 196030 | training loss: 2.0200e-03 | validation loss: 1.6388e-03\n",
      "Epoch: 196040 | training loss: 2.0201e-03 | validation loss: 1.6402e-03\n",
      "Epoch: 196050 | training loss: 2.0201e-03 | validation loss: 1.6376e-03\n",
      "Epoch: 196060 | training loss: 2.0200e-03 | validation loss: 1.6386e-03\n",
      "Epoch: 196070 | training loss: 2.0199e-03 | validation loss: 1.6392e-03\n",
      "Epoch: 196080 | training loss: 2.0200e-03 | validation loss: 1.6398e-03\n",
      "Epoch: 196090 | training loss: 2.0208e-03 | validation loss: 1.6428e-03\n",
      "Epoch: 196100 | training loss: 2.0507e-03 | validation loss: 1.6751e-03\n",
      "Epoch: 196110 | training loss: 3.2042e-03 | validation loss: 2.3400e-03\n",
      "Epoch: 196120 | training loss: 2.5140e-03 | validation loss: 1.7909e-03\n",
      "Epoch: 196130 | training loss: 2.0232e-03 | validation loss: 1.6380e-03\n",
      "Epoch: 196140 | training loss: 2.0793e-03 | validation loss: 1.6996e-03\n",
      "Epoch: 196150 | training loss: 2.0224e-03 | validation loss: 1.6358e-03\n",
      "Epoch: 196160 | training loss: 2.0248e-03 | validation loss: 1.6301e-03\n",
      "Epoch: 196170 | training loss: 2.0225e-03 | validation loss: 1.6481e-03\n",
      "Epoch: 196180 | training loss: 2.0200e-03 | validation loss: 1.6360e-03\n",
      "Epoch: 196190 | training loss: 2.0197e-03 | validation loss: 1.6391e-03\n",
      "Epoch: 196200 | training loss: 2.0197e-03 | validation loss: 1.6387e-03\n",
      "Epoch: 196210 | training loss: 2.0196e-03 | validation loss: 1.6385e-03\n",
      "Epoch: 196220 | training loss: 2.0196e-03 | validation loss: 1.6383e-03\n",
      "Epoch: 196230 | training loss: 2.0196e-03 | validation loss: 1.6389e-03\n",
      "Epoch: 196240 | training loss: 2.0196e-03 | validation loss: 1.6383e-03\n",
      "Epoch: 196250 | training loss: 2.0196e-03 | validation loss: 1.6382e-03\n",
      "Epoch: 196260 | training loss: 2.0195e-03 | validation loss: 1.6384e-03\n",
      "Epoch: 196270 | training loss: 2.0200e-03 | validation loss: 1.6401e-03\n",
      "Epoch: 196280 | training loss: 2.0865e-03 | validation loss: 1.7069e-03\n",
      "Epoch: 196290 | training loss: 2.0297e-03 | validation loss: 1.6600e-03\n",
      "Epoch: 196300 | training loss: 2.1620e-03 | validation loss: 1.7820e-03\n",
      "Epoch: 196310 | training loss: 2.2781e-03 | validation loss: 1.8624e-03\n",
      "Epoch: 196320 | training loss: 2.1013e-03 | validation loss: 1.7298e-03\n",
      "Epoch: 196330 | training loss: 2.0879e-03 | validation loss: 1.6422e-03\n",
      "Epoch: 196340 | training loss: 2.0239e-03 | validation loss: 1.6513e-03\n",
      "Epoch: 196350 | training loss: 2.0316e-03 | validation loss: 1.6604e-03\n",
      "Epoch: 196360 | training loss: 2.0200e-03 | validation loss: 1.6418e-03\n",
      "Epoch: 196370 | training loss: 2.0195e-03 | validation loss: 1.6359e-03\n",
      "Epoch: 196380 | training loss: 2.0216e-03 | validation loss: 1.6319e-03\n",
      "Epoch: 196390 | training loss: 2.0811e-03 | validation loss: 1.6325e-03\n",
      "Epoch: 196400 | training loss: 3.0239e-03 | validation loss: 1.9809e-03\n",
      "Epoch: 196410 | training loss: 2.2950e-03 | validation loss: 1.8356e-03\n",
      "Epoch: 196420 | training loss: 2.0236e-03 | validation loss: 1.6301e-03\n",
      "Epoch: 196430 | training loss: 2.0388e-03 | validation loss: 1.6285e-03\n",
      "Epoch: 196440 | training loss: 2.0347e-03 | validation loss: 1.6617e-03\n",
      "Epoch: 196450 | training loss: 2.0235e-03 | validation loss: 1.6318e-03\n",
      "Epoch: 196460 | training loss: 2.0201e-03 | validation loss: 1.6425e-03\n",
      "Epoch: 196470 | training loss: 2.0195e-03 | validation loss: 1.6353e-03\n",
      "Epoch: 196480 | training loss: 2.0193e-03 | validation loss: 1.6399e-03\n",
      "Epoch: 196490 | training loss: 2.0192e-03 | validation loss: 1.6364e-03\n",
      "Epoch: 196500 | training loss: 2.0190e-03 | validation loss: 1.6379e-03\n",
      "Epoch: 196510 | training loss: 2.0191e-03 | validation loss: 1.6384e-03\n",
      "Epoch: 196520 | training loss: 2.0190e-03 | validation loss: 1.6382e-03\n",
      "Epoch: 196530 | training loss: 2.0190e-03 | validation loss: 1.6384e-03\n",
      "Epoch: 196540 | training loss: 2.0194e-03 | validation loss: 1.6407e-03\n",
      "Epoch: 196550 | training loss: 2.0388e-03 | validation loss: 1.6653e-03\n",
      "Epoch: 196560 | training loss: 3.1053e-03 | validation loss: 2.2916e-03\n",
      "Epoch: 196570 | training loss: 2.6016e-03 | validation loss: 1.8221e-03\n",
      "Epoch: 196580 | training loss: 2.0287e-03 | validation loss: 1.6370e-03\n",
      "Epoch: 196590 | training loss: 2.0865e-03 | validation loss: 1.6999e-03\n",
      "Epoch: 196600 | training loss: 2.0226e-03 | validation loss: 1.6492e-03\n",
      "Epoch: 196610 | training loss: 2.0267e-03 | validation loss: 1.6311e-03\n",
      "Epoch: 196620 | training loss: 2.0199e-03 | validation loss: 1.6367e-03\n",
      "Epoch: 196630 | training loss: 2.0194e-03 | validation loss: 1.6420e-03\n",
      "Epoch: 196640 | training loss: 2.0190e-03 | validation loss: 1.6357e-03\n",
      "Epoch: 196650 | training loss: 2.0188e-03 | validation loss: 1.6380e-03\n",
      "Epoch: 196660 | training loss: 2.0187e-03 | validation loss: 1.6376e-03\n",
      "Epoch: 196670 | training loss: 2.0187e-03 | validation loss: 1.6373e-03\n",
      "Epoch: 196680 | training loss: 2.0187e-03 | validation loss: 1.6374e-03\n",
      "Epoch: 196690 | training loss: 2.0186e-03 | validation loss: 1.6375e-03\n",
      "Epoch: 196700 | training loss: 2.0186e-03 | validation loss: 1.6372e-03\n",
      "Epoch: 196710 | training loss: 2.0186e-03 | validation loss: 1.6373e-03\n",
      "Epoch: 196720 | training loss: 2.0186e-03 | validation loss: 1.6374e-03\n",
      "Epoch: 196730 | training loss: 2.0186e-03 | validation loss: 1.6374e-03\n",
      "Epoch: 196740 | training loss: 2.0186e-03 | validation loss: 1.6378e-03\n",
      "Epoch: 196750 | training loss: 2.0202e-03 | validation loss: 1.6410e-03\n",
      "Epoch: 196760 | training loss: 2.2253e-03 | validation loss: 1.7728e-03\n",
      "Epoch: 196770 | training loss: 2.7340e-03 | validation loss: 2.2192e-03\n",
      "Epoch: 196780 | training loss: 2.0718e-03 | validation loss: 1.7082e-03\n",
      "Epoch: 196790 | training loss: 2.1194e-03 | validation loss: 1.6789e-03\n",
      "Epoch: 196800 | training loss: 2.0542e-03 | validation loss: 1.6632e-03\n",
      "Epoch: 196810 | training loss: 2.0320e-03 | validation loss: 1.6630e-03\n",
      "Epoch: 196820 | training loss: 2.0190e-03 | validation loss: 1.6395e-03\n",
      "Epoch: 196830 | training loss: 2.0191e-03 | validation loss: 1.6335e-03\n",
      "Epoch: 196840 | training loss: 2.0187e-03 | validation loss: 1.6394e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 196850 | training loss: 2.0184e-03 | validation loss: 1.6355e-03\n",
      "Epoch: 196860 | training loss: 2.0183e-03 | validation loss: 1.6371e-03\n",
      "Epoch: 196870 | training loss: 2.0183e-03 | validation loss: 1.6365e-03\n",
      "Epoch: 196880 | training loss: 2.0183e-03 | validation loss: 1.6372e-03\n",
      "Epoch: 196890 | training loss: 2.0182e-03 | validation loss: 1.6366e-03\n",
      "Epoch: 196900 | training loss: 2.0182e-03 | validation loss: 1.6369e-03\n",
      "Epoch: 196910 | training loss: 2.0182e-03 | validation loss: 1.6369e-03\n",
      "Epoch: 196920 | training loss: 2.0182e-03 | validation loss: 1.6368e-03\n",
      "Epoch: 196930 | training loss: 2.0182e-03 | validation loss: 1.6369e-03\n",
      "Epoch: 196940 | training loss: 2.0182e-03 | validation loss: 1.6376e-03\n",
      "Epoch: 196950 | training loss: 2.0199e-03 | validation loss: 1.6436e-03\n",
      "Epoch: 196960 | training loss: 2.1920e-03 | validation loss: 1.7805e-03\n",
      "Epoch: 196970 | training loss: 2.2360e-03 | validation loss: 1.7929e-03\n",
      "Epoch: 196980 | training loss: 2.3563e-03 | validation loss: 1.8729e-03\n",
      "Epoch: 196990 | training loss: 2.0320e-03 | validation loss: 1.6639e-03\n",
      "Epoch: 197000 | training loss: 2.0434e-03 | validation loss: 1.6401e-03\n",
      "Epoch: 197010 | training loss: 2.0338e-03 | validation loss: 1.6360e-03\n",
      "Epoch: 197020 | training loss: 2.0186e-03 | validation loss: 1.6406e-03\n",
      "Epoch: 197030 | training loss: 2.0200e-03 | validation loss: 1.6434e-03\n",
      "Epoch: 197040 | training loss: 2.0180e-03 | validation loss: 1.6352e-03\n",
      "Epoch: 197050 | training loss: 2.0181e-03 | validation loss: 1.6343e-03\n",
      "Epoch: 197060 | training loss: 2.0180e-03 | validation loss: 1.6377e-03\n",
      "Epoch: 197070 | training loss: 2.0179e-03 | validation loss: 1.6364e-03\n",
      "Epoch: 197080 | training loss: 2.0179e-03 | validation loss: 1.6364e-03\n",
      "Epoch: 197090 | training loss: 2.0178e-03 | validation loss: 1.6364e-03\n",
      "Epoch: 197100 | training loss: 2.0178e-03 | validation loss: 1.6364e-03\n",
      "Epoch: 197110 | training loss: 2.0178e-03 | validation loss: 1.6363e-03\n",
      "Epoch: 197120 | training loss: 2.0178e-03 | validation loss: 1.6364e-03\n",
      "Epoch: 197130 | training loss: 2.0177e-03 | validation loss: 1.6363e-03\n",
      "Epoch: 197140 | training loss: 2.0177e-03 | validation loss: 1.6363e-03\n",
      "Epoch: 197150 | training loss: 2.0177e-03 | validation loss: 1.6362e-03\n",
      "Epoch: 197160 | training loss: 2.0177e-03 | validation loss: 1.6361e-03\n",
      "Epoch: 197170 | training loss: 2.0177e-03 | validation loss: 1.6356e-03\n",
      "Epoch: 197180 | training loss: 2.0195e-03 | validation loss: 1.6319e-03\n",
      "Epoch: 197190 | training loss: 2.2463e-03 | validation loss: 1.6887e-03\n",
      "Epoch: 197200 | training loss: 2.0428e-03 | validation loss: 1.6324e-03\n",
      "Epoch: 197210 | training loss: 2.3843e-03 | validation loss: 1.7390e-03\n",
      "Epoch: 197220 | training loss: 2.1685e-03 | validation loss: 1.6579e-03\n",
      "Epoch: 197230 | training loss: 2.0487e-03 | validation loss: 1.6273e-03\n",
      "Epoch: 197240 | training loss: 2.0182e-03 | validation loss: 1.6337e-03\n",
      "Epoch: 197250 | training loss: 2.0200e-03 | validation loss: 1.6440e-03\n",
      "Epoch: 197260 | training loss: 2.0201e-03 | validation loss: 1.6436e-03\n",
      "Epoch: 197270 | training loss: 2.0176e-03 | validation loss: 1.6376e-03\n",
      "Epoch: 197280 | training loss: 2.0177e-03 | validation loss: 1.6343e-03\n",
      "Epoch: 197290 | training loss: 2.0174e-03 | validation loss: 1.6355e-03\n",
      "Epoch: 197300 | training loss: 2.0174e-03 | validation loss: 1.6369e-03\n",
      "Epoch: 197310 | training loss: 2.0174e-03 | validation loss: 1.6360e-03\n",
      "Epoch: 197320 | training loss: 2.0174e-03 | validation loss: 1.6358e-03\n",
      "Epoch: 197330 | training loss: 2.0173e-03 | validation loss: 1.6361e-03\n",
      "Epoch: 197340 | training loss: 2.0173e-03 | validation loss: 1.6358e-03\n",
      "Epoch: 197350 | training loss: 2.0173e-03 | validation loss: 1.6360e-03\n",
      "Epoch: 197360 | training loss: 2.0173e-03 | validation loss: 1.6359e-03\n",
      "Epoch: 197370 | training loss: 2.0172e-03 | validation loss: 1.6360e-03\n",
      "Epoch: 197380 | training loss: 2.0173e-03 | validation loss: 1.6366e-03\n",
      "Epoch: 197390 | training loss: 2.0231e-03 | validation loss: 1.6489e-03\n",
      "Epoch: 197400 | training loss: 2.7779e-03 | validation loss: 2.2664e-03\n",
      "Epoch: 197410 | training loss: 2.0245e-03 | validation loss: 1.6414e-03\n",
      "Epoch: 197420 | training loss: 2.0429e-03 | validation loss: 1.6299e-03\n",
      "Epoch: 197430 | training loss: 2.0316e-03 | validation loss: 1.6438e-03\n",
      "Epoch: 197440 | training loss: 2.0235e-03 | validation loss: 1.6352e-03\n",
      "Epoch: 197450 | training loss: 2.0212e-03 | validation loss: 1.6301e-03\n",
      "Epoch: 197460 | training loss: 2.0204e-03 | validation loss: 1.6283e-03\n",
      "Epoch: 197470 | training loss: 2.0314e-03 | validation loss: 1.6242e-03\n",
      "Epoch: 197480 | training loss: 2.2759e-03 | validation loss: 1.6872e-03\n",
      "Epoch: 197490 | training loss: 2.1036e-03 | validation loss: 1.6382e-03\n",
      "Epoch: 197500 | training loss: 2.0277e-03 | validation loss: 1.6551e-03\n",
      "Epoch: 197510 | training loss: 2.0170e-03 | validation loss: 1.6354e-03\n",
      "Epoch: 197520 | training loss: 2.0171e-03 | validation loss: 1.6332e-03\n",
      "Epoch: 197530 | training loss: 2.0169e-03 | validation loss: 1.6344e-03\n",
      "Epoch: 197540 | training loss: 2.0177e-03 | validation loss: 1.6394e-03\n",
      "Epoch: 197550 | training loss: 2.0182e-03 | validation loss: 1.6309e-03\n",
      "Epoch: 197560 | training loss: 2.0169e-03 | validation loss: 1.6365e-03\n",
      "Epoch: 197570 | training loss: 2.0172e-03 | validation loss: 1.6381e-03\n",
      "Epoch: 197580 | training loss: 2.0173e-03 | validation loss: 1.6382e-03\n",
      "Epoch: 197590 | training loss: 2.0194e-03 | validation loss: 1.6432e-03\n",
      "Epoch: 197600 | training loss: 2.0718e-03 | validation loss: 1.6931e-03\n",
      "Epoch: 197610 | training loss: 2.9626e-03 | validation loss: 2.2166e-03\n",
      "Epoch: 197620 | training loss: 2.3414e-03 | validation loss: 1.7213e-03\n",
      "Epoch: 197630 | training loss: 2.0737e-03 | validation loss: 1.6874e-03\n",
      "Epoch: 197640 | training loss: 2.0167e-03 | validation loss: 1.6337e-03\n",
      "Epoch: 197650 | training loss: 2.0216e-03 | validation loss: 1.6304e-03\n",
      "Epoch: 197660 | training loss: 2.0205e-03 | validation loss: 1.6446e-03\n",
      "Epoch: 197670 | training loss: 2.0183e-03 | validation loss: 1.6304e-03\n",
      "Epoch: 197680 | training loss: 2.0170e-03 | validation loss: 1.6380e-03\n",
      "Epoch: 197690 | training loss: 2.0166e-03 | validation loss: 1.6346e-03\n",
      "Epoch: 197700 | training loss: 2.0167e-03 | validation loss: 1.6338e-03\n",
      "Epoch: 197710 | training loss: 2.0166e-03 | validation loss: 1.6356e-03\n",
      "Epoch: 197720 | training loss: 2.0166e-03 | validation loss: 1.6359e-03\n",
      "Epoch: 197730 | training loss: 2.0166e-03 | validation loss: 1.6364e-03\n",
      "Epoch: 197740 | training loss: 2.0180e-03 | validation loss: 1.6403e-03\n",
      "Epoch: 197750 | training loss: 2.0640e-03 | validation loss: 1.6842e-03\n",
      "Epoch: 197760 | training loss: 3.2400e-03 | validation loss: 2.3563e-03\n",
      "Epoch: 197770 | training loss: 2.3840e-03 | validation loss: 1.7398e-03\n",
      "Epoch: 197780 | training loss: 2.0181e-03 | validation loss: 1.6283e-03\n",
      "Epoch: 197790 | training loss: 2.0668e-03 | validation loss: 1.6884e-03\n",
      "Epoch: 197800 | training loss: 2.0225e-03 | validation loss: 1.6281e-03\n",
      "Epoch: 197810 | training loss: 2.0173e-03 | validation loss: 1.6307e-03\n",
      "Epoch: 197820 | training loss: 2.0185e-03 | validation loss: 1.6423e-03\n",
      "Epoch: 197830 | training loss: 2.0173e-03 | validation loss: 1.6310e-03\n",
      "Epoch: 197840 | training loss: 2.0166e-03 | validation loss: 1.6374e-03\n",
      "Epoch: 197850 | training loss: 2.0164e-03 | validation loss: 1.6334e-03\n",
      "Epoch: 197860 | training loss: 2.0163e-03 | validation loss: 1.6357e-03\n",
      "Epoch: 197870 | training loss: 2.0162e-03 | validation loss: 1.6344e-03\n",
      "Epoch: 197880 | training loss: 2.0162e-03 | validation loss: 1.6341e-03\n",
      "Epoch: 197890 | training loss: 2.0162e-03 | validation loss: 1.6342e-03\n",
      "Epoch: 197900 | training loss: 2.0170e-03 | validation loss: 1.6324e-03\n",
      "Epoch: 197910 | training loss: 2.0992e-03 | validation loss: 1.6628e-03\n",
      "Epoch: 197920 | training loss: 2.0481e-03 | validation loss: 1.6231e-03\n",
      "Epoch: 197930 | training loss: 2.1949e-03 | validation loss: 1.6859e-03\n",
      "Epoch: 197940 | training loss: 2.0741e-03 | validation loss: 1.6286e-03\n",
      "Epoch: 197950 | training loss: 2.0239e-03 | validation loss: 1.6261e-03\n",
      "Epoch: 197960 | training loss: 2.0250e-03 | validation loss: 1.6350e-03\n",
      "Epoch: 197970 | training loss: 2.0466e-03 | validation loss: 1.6274e-03\n",
      "Epoch: 197980 | training loss: 2.4535e-03 | validation loss: 1.7508e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 197990 | training loss: 2.0256e-03 | validation loss: 1.6516e-03\n",
      "Epoch: 198000 | training loss: 2.0229e-03 | validation loss: 1.6258e-03\n",
      "Epoch: 198010 | training loss: 2.0197e-03 | validation loss: 1.6441e-03\n",
      "Epoch: 198020 | training loss: 2.0162e-03 | validation loss: 1.6319e-03\n",
      "Epoch: 198030 | training loss: 2.0165e-03 | validation loss: 1.6311e-03\n",
      "Epoch: 198040 | training loss: 2.0181e-03 | validation loss: 1.6413e-03\n",
      "Epoch: 198050 | training loss: 2.0163e-03 | validation loss: 1.6314e-03\n",
      "Epoch: 198060 | training loss: 2.0165e-03 | validation loss: 1.6310e-03\n",
      "Epoch: 198070 | training loss: 2.0161e-03 | validation loss: 1.6317e-03\n",
      "Epoch: 198080 | training loss: 2.0171e-03 | validation loss: 1.6297e-03\n",
      "Epoch: 198090 | training loss: 2.0421e-03 | validation loss: 1.6251e-03\n",
      "Epoch: 198100 | training loss: 2.7783e-03 | validation loss: 1.8845e-03\n",
      "Epoch: 198110 | training loss: 2.2805e-03 | validation loss: 1.8288e-03\n",
      "Epoch: 198120 | training loss: 2.1598e-03 | validation loss: 1.6617e-03\n",
      "Epoch: 198130 | training loss: 2.0280e-03 | validation loss: 1.6474e-03\n",
      "Epoch: 198140 | training loss: 2.0175e-03 | validation loss: 1.6397e-03\n",
      "Epoch: 198150 | training loss: 2.0201e-03 | validation loss: 1.6302e-03\n",
      "Epoch: 198160 | training loss: 2.0180e-03 | validation loss: 1.6398e-03\n",
      "Epoch: 198170 | training loss: 2.0165e-03 | validation loss: 1.6311e-03\n",
      "Epoch: 198180 | training loss: 2.0158e-03 | validation loss: 1.6357e-03\n",
      "Epoch: 198190 | training loss: 2.0156e-03 | validation loss: 1.6337e-03\n",
      "Epoch: 198200 | training loss: 2.0156e-03 | validation loss: 1.6329e-03\n",
      "Epoch: 198210 | training loss: 2.0155e-03 | validation loss: 1.6342e-03\n",
      "Epoch: 198220 | training loss: 2.0155e-03 | validation loss: 1.6344e-03\n",
      "Epoch: 198230 | training loss: 2.0156e-03 | validation loss: 1.6351e-03\n",
      "Epoch: 198240 | training loss: 2.0175e-03 | validation loss: 1.6399e-03\n",
      "Epoch: 198250 | training loss: 2.0949e-03 | validation loss: 1.7048e-03\n",
      "Epoch: 198260 | training loss: 3.2026e-03 | validation loss: 2.3331e-03\n",
      "Epoch: 198270 | training loss: 2.1081e-03 | validation loss: 1.6411e-03\n",
      "Epoch: 198280 | training loss: 2.0950e-03 | validation loss: 1.6408e-03\n",
      "Epoch: 198290 | training loss: 2.0427e-03 | validation loss: 1.6644e-03\n",
      "Epoch: 198300 | training loss: 2.0182e-03 | validation loss: 1.6409e-03\n",
      "Epoch: 198310 | training loss: 2.0217e-03 | validation loss: 1.6281e-03\n",
      "Epoch: 198320 | training loss: 2.0166e-03 | validation loss: 1.6382e-03\n",
      "Epoch: 198330 | training loss: 2.0153e-03 | validation loss: 1.6328e-03\n",
      "Epoch: 198340 | training loss: 2.0152e-03 | validation loss: 1.6336e-03\n",
      "Epoch: 198350 | training loss: 2.0152e-03 | validation loss: 1.6338e-03\n",
      "Epoch: 198360 | training loss: 2.0152e-03 | validation loss: 1.6337e-03\n",
      "Epoch: 198370 | training loss: 2.0152e-03 | validation loss: 1.6331e-03\n",
      "Epoch: 198380 | training loss: 2.0152e-03 | validation loss: 1.6340e-03\n",
      "Epoch: 198390 | training loss: 2.0152e-03 | validation loss: 1.6349e-03\n",
      "Epoch: 198400 | training loss: 2.0248e-03 | validation loss: 1.6530e-03\n",
      "Epoch: 198410 | training loss: 2.7484e-03 | validation loss: 2.2483e-03\n",
      "Epoch: 198420 | training loss: 2.0356e-03 | validation loss: 1.6551e-03\n",
      "Epoch: 198430 | training loss: 2.0761e-03 | validation loss: 1.6342e-03\n",
      "Epoch: 198440 | training loss: 2.0448e-03 | validation loss: 1.6480e-03\n",
      "Epoch: 198450 | training loss: 2.0269e-03 | validation loss: 1.6382e-03\n",
      "Epoch: 198460 | training loss: 2.0173e-03 | validation loss: 1.6280e-03\n",
      "Epoch: 198470 | training loss: 2.0178e-03 | validation loss: 1.6269e-03\n",
      "Epoch: 198480 | training loss: 2.0375e-03 | validation loss: 1.6243e-03\n",
      "Epoch: 198490 | training loss: 2.3648e-03 | validation loss: 1.7187e-03\n",
      "Epoch: 198500 | training loss: 2.0219e-03 | validation loss: 1.6273e-03\n",
      "Epoch: 198510 | training loss: 2.0155e-03 | validation loss: 1.6363e-03\n",
      "Epoch: 198520 | training loss: 2.0149e-03 | validation loss: 1.6327e-03\n",
      "Epoch: 198530 | training loss: 2.0151e-03 | validation loss: 1.6343e-03\n",
      "Epoch: 198540 | training loss: 2.0166e-03 | validation loss: 1.6281e-03\n",
      "Epoch: 198550 | training loss: 2.0175e-03 | validation loss: 1.6410e-03\n",
      "Epoch: 198560 | training loss: 2.0152e-03 | validation loss: 1.6304e-03\n",
      "Epoch: 198570 | training loss: 2.0154e-03 | validation loss: 1.6297e-03\n",
      "Epoch: 198580 | training loss: 2.0149e-03 | validation loss: 1.6310e-03\n",
      "Epoch: 198590 | training loss: 2.0152e-03 | validation loss: 1.6300e-03\n",
      "Epoch: 198600 | training loss: 2.0232e-03 | validation loss: 1.6246e-03\n",
      "Epoch: 198610 | training loss: 2.3391e-03 | validation loss: 1.7130e-03\n",
      "Epoch: 198620 | training loss: 2.0347e-03 | validation loss: 1.6340e-03\n",
      "Epoch: 198630 | training loss: 2.1223e-03 | validation loss: 1.6411e-03\n",
      "Epoch: 198640 | training loss: 2.0871e-03 | validation loss: 1.6950e-03\n",
      "Epoch: 198650 | training loss: 2.0162e-03 | validation loss: 1.6302e-03\n",
      "Epoch: 198660 | training loss: 2.0183e-03 | validation loss: 1.6291e-03\n",
      "Epoch: 198670 | training loss: 2.0181e-03 | validation loss: 1.6405e-03\n",
      "Epoch: 198680 | training loss: 2.0160e-03 | validation loss: 1.6287e-03\n",
      "Epoch: 198690 | training loss: 2.0151e-03 | validation loss: 1.6361e-03\n",
      "Epoch: 198700 | training loss: 2.0147e-03 | validation loss: 1.6308e-03\n",
      "Epoch: 198710 | training loss: 2.0145e-03 | validation loss: 1.6335e-03\n",
      "Epoch: 198720 | training loss: 2.0145e-03 | validation loss: 1.6328e-03\n",
      "Epoch: 198730 | training loss: 2.0144e-03 | validation loss: 1.6321e-03\n",
      "Epoch: 198740 | training loss: 2.0144e-03 | validation loss: 1.6321e-03\n",
      "Epoch: 198750 | training loss: 2.0144e-03 | validation loss: 1.6318e-03\n",
      "Epoch: 198760 | training loss: 2.0147e-03 | validation loss: 1.6304e-03\n",
      "Epoch: 198770 | training loss: 2.0273e-03 | validation loss: 1.6253e-03\n",
      "Epoch: 198780 | training loss: 2.7823e-03 | validation loss: 1.8986e-03\n",
      "Epoch: 198790 | training loss: 2.4524e-03 | validation loss: 1.9223e-03\n",
      "Epoch: 198800 | training loss: 2.1000e-03 | validation loss: 1.6383e-03\n",
      "Epoch: 198810 | training loss: 2.0771e-03 | validation loss: 1.6323e-03\n",
      "Epoch: 198820 | training loss: 2.0197e-03 | validation loss: 1.6435e-03\n",
      "Epoch: 198830 | training loss: 2.0223e-03 | validation loss: 1.6470e-03\n",
      "Epoch: 198840 | training loss: 2.0164e-03 | validation loss: 1.6280e-03\n",
      "Epoch: 198850 | training loss: 2.0143e-03 | validation loss: 1.6315e-03\n",
      "Epoch: 198860 | training loss: 2.0146e-03 | validation loss: 1.6350e-03\n",
      "Epoch: 198870 | training loss: 2.0144e-03 | validation loss: 1.6308e-03\n",
      "Epoch: 198880 | training loss: 2.0142e-03 | validation loss: 1.6334e-03\n",
      "Epoch: 198890 | training loss: 2.0141e-03 | validation loss: 1.6317e-03\n",
      "Epoch: 198900 | training loss: 2.0141e-03 | validation loss: 1.6327e-03\n",
      "Epoch: 198910 | training loss: 2.0141e-03 | validation loss: 1.6321e-03\n",
      "Epoch: 198920 | training loss: 2.0140e-03 | validation loss: 1.6321e-03\n",
      "Epoch: 198930 | training loss: 2.0140e-03 | validation loss: 1.6326e-03\n",
      "Epoch: 198940 | training loss: 2.0151e-03 | validation loss: 1.6372e-03\n",
      "Epoch: 198950 | training loss: 2.3009e-03 | validation loss: 1.8997e-03\n",
      "Epoch: 198960 | training loss: 2.5237e-03 | validation loss: 1.9388e-03\n",
      "Epoch: 198970 | training loss: 2.1760e-03 | validation loss: 1.6865e-03\n",
      "Epoch: 198980 | training loss: 2.0649e-03 | validation loss: 1.6514e-03\n",
      "Epoch: 198990 | training loss: 2.0347e-03 | validation loss: 1.6420e-03\n",
      "Epoch: 199000 | training loss: 2.0204e-03 | validation loss: 1.6267e-03\n",
      "Epoch: 199010 | training loss: 2.0183e-03 | validation loss: 1.6242e-03\n",
      "Epoch: 199020 | training loss: 2.0221e-03 | validation loss: 1.6217e-03\n",
      "Epoch: 199030 | training loss: 2.0974e-03 | validation loss: 1.6302e-03\n",
      "Epoch: 199040 | training loss: 2.5843e-03 | validation loss: 1.7993e-03\n",
      "Epoch: 199050 | training loss: 2.1911e-03 | validation loss: 1.7722e-03\n",
      "Epoch: 199060 | training loss: 2.0799e-03 | validation loss: 1.6275e-03\n",
      "Epoch: 199070 | training loss: 2.0375e-03 | validation loss: 1.6631e-03\n",
      "Epoch: 199080 | training loss: 2.0197e-03 | validation loss: 1.6237e-03\n",
      "Epoch: 199090 | training loss: 2.0138e-03 | validation loss: 1.6329e-03\n",
      "Epoch: 199100 | training loss: 2.0150e-03 | validation loss: 1.6372e-03\n",
      "Epoch: 199110 | training loss: 2.0139e-03 | validation loss: 1.6296e-03\n",
      "Epoch: 199120 | training loss: 2.0145e-03 | validation loss: 1.6280e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 199130 | training loss: 2.0165e-03 | validation loss: 1.6256e-03\n",
      "Epoch: 199140 | training loss: 2.0506e-03 | validation loss: 1.6232e-03\n",
      "Epoch: 199150 | training loss: 2.6690e-03 | validation loss: 1.8367e-03\n",
      "Epoch: 199160 | training loss: 2.1660e-03 | validation loss: 1.7551e-03\n",
      "Epoch: 199170 | training loss: 2.1111e-03 | validation loss: 1.6395e-03\n",
      "Epoch: 199180 | training loss: 2.0560e-03 | validation loss: 1.6780e-03\n",
      "Epoch: 199190 | training loss: 2.0289e-03 | validation loss: 1.6228e-03\n",
      "Epoch: 199200 | training loss: 2.0195e-03 | validation loss: 1.6438e-03\n",
      "Epoch: 199210 | training loss: 2.0158e-03 | validation loss: 1.6265e-03\n",
      "Epoch: 199220 | training loss: 2.0137e-03 | validation loss: 1.6338e-03\n",
      "Epoch: 199230 | training loss: 2.0136e-03 | validation loss: 1.6330e-03\n",
      "Epoch: 199240 | training loss: 2.0135e-03 | validation loss: 1.6300e-03\n",
      "Epoch: 199250 | training loss: 2.0136e-03 | validation loss: 1.6294e-03\n",
      "Epoch: 199260 | training loss: 2.0145e-03 | validation loss: 1.6275e-03\n",
      "Epoch: 199270 | training loss: 2.0322e-03 | validation loss: 1.6230e-03\n",
      "Epoch: 199280 | training loss: 2.5813e-03 | validation loss: 1.8094e-03\n",
      "Epoch: 199290 | training loss: 2.1075e-03 | validation loss: 1.7175e-03\n",
      "Epoch: 199300 | training loss: 2.1648e-03 | validation loss: 1.6567e-03\n",
      "Epoch: 199310 | training loss: 2.0559e-03 | validation loss: 1.6728e-03\n",
      "Epoch: 199320 | training loss: 2.0146e-03 | validation loss: 1.6301e-03\n",
      "Epoch: 199330 | training loss: 2.0141e-03 | validation loss: 1.6277e-03\n",
      "Epoch: 199340 | training loss: 2.0143e-03 | validation loss: 1.6353e-03\n",
      "Epoch: 199350 | training loss: 2.0136e-03 | validation loss: 1.6293e-03\n",
      "Epoch: 199360 | training loss: 2.0132e-03 | validation loss: 1.6317e-03\n",
      "Epoch: 199370 | training loss: 2.0132e-03 | validation loss: 1.6319e-03\n",
      "Epoch: 199380 | training loss: 2.0132e-03 | validation loss: 1.6301e-03\n",
      "Epoch: 199390 | training loss: 2.0131e-03 | validation loss: 1.6309e-03\n",
      "Epoch: 199400 | training loss: 2.0131e-03 | validation loss: 1.6315e-03\n",
      "Epoch: 199410 | training loss: 2.0131e-03 | validation loss: 1.6321e-03\n",
      "Epoch: 199420 | training loss: 2.0143e-03 | validation loss: 1.6356e-03\n",
      "Epoch: 199430 | training loss: 2.0630e-03 | validation loss: 1.6802e-03\n",
      "Epoch: 199440 | training loss: 3.3912e-03 | validation loss: 2.4298e-03\n",
      "Epoch: 199450 | training loss: 2.2841e-03 | validation loss: 1.7002e-03\n",
      "Epoch: 199460 | training loss: 2.0680e-03 | validation loss: 1.6436e-03\n",
      "Epoch: 199470 | training loss: 2.0494e-03 | validation loss: 1.6636e-03\n",
      "Epoch: 199480 | training loss: 2.0178e-03 | validation loss: 1.6391e-03\n",
      "Epoch: 199490 | training loss: 2.0199e-03 | validation loss: 1.6277e-03\n",
      "Epoch: 199500 | training loss: 2.0131e-03 | validation loss: 1.6325e-03\n",
      "Epoch: 199510 | training loss: 2.0131e-03 | validation loss: 1.6320e-03\n",
      "Epoch: 199520 | training loss: 2.0131e-03 | validation loss: 1.6299e-03\n",
      "Epoch: 199530 | training loss: 2.0129e-03 | validation loss: 1.6320e-03\n",
      "Epoch: 199540 | training loss: 2.0128e-03 | validation loss: 1.6302e-03\n",
      "Epoch: 199550 | training loss: 2.0128e-03 | validation loss: 1.6310e-03\n",
      "Epoch: 199560 | training loss: 2.0127e-03 | validation loss: 1.6303e-03\n",
      "Epoch: 199570 | training loss: 2.0130e-03 | validation loss: 1.6282e-03\n",
      "Epoch: 199580 | training loss: 2.0375e-03 | validation loss: 1.6240e-03\n",
      "Epoch: 199590 | training loss: 2.7255e-03 | validation loss: 1.9439e-03\n",
      "Epoch: 199600 | training loss: 2.0801e-03 | validation loss: 1.6805e-03\n",
      "Epoch: 199610 | training loss: 2.0310e-03 | validation loss: 1.6367e-03\n",
      "Epoch: 199620 | training loss: 2.0308e-03 | validation loss: 1.6573e-03\n",
      "Epoch: 199630 | training loss: 2.0207e-03 | validation loss: 1.6469e-03\n",
      "Epoch: 199640 | training loss: 2.0149e-03 | validation loss: 1.6268e-03\n",
      "Epoch: 199650 | training loss: 2.0130e-03 | validation loss: 1.6289e-03\n",
      "Epoch: 199660 | training loss: 2.0134e-03 | validation loss: 1.6324e-03\n",
      "Epoch: 199670 | training loss: 2.0143e-03 | validation loss: 1.6373e-03\n",
      "Epoch: 199680 | training loss: 2.0315e-03 | validation loss: 1.6589e-03\n",
      "Epoch: 199690 | training loss: 2.4306e-03 | validation loss: 1.9185e-03\n",
      "Epoch: 199700 | training loss: 2.0164e-03 | validation loss: 1.6219e-03\n",
      "Epoch: 199710 | training loss: 2.0435e-03 | validation loss: 1.6667e-03\n",
      "Epoch: 199720 | training loss: 2.0474e-03 | validation loss: 1.6245e-03\n",
      "Epoch: 199730 | training loss: 2.0303e-03 | validation loss: 1.6560e-03\n",
      "Epoch: 199740 | training loss: 2.0195e-03 | validation loss: 1.6224e-03\n",
      "Epoch: 199750 | training loss: 2.0146e-03 | validation loss: 1.6370e-03\n",
      "Epoch: 199760 | training loss: 2.0125e-03 | validation loss: 1.6285e-03\n",
      "Epoch: 199770 | training loss: 2.0125e-03 | validation loss: 1.6284e-03\n",
      "Epoch: 199780 | training loss: 2.0125e-03 | validation loss: 1.6319e-03\n",
      "Epoch: 199790 | training loss: 2.0124e-03 | validation loss: 1.6318e-03\n",
      "Epoch: 199800 | training loss: 2.0125e-03 | validation loss: 1.6323e-03\n",
      "Epoch: 199810 | training loss: 2.0151e-03 | validation loss: 1.6382e-03\n",
      "Epoch: 199820 | training loss: 2.1008e-03 | validation loss: 1.7097e-03\n",
      "Epoch: 199830 | training loss: 3.0111e-03 | validation loss: 2.2313e-03\n",
      "Epoch: 199840 | training loss: 2.1627e-03 | validation loss: 1.6616e-03\n",
      "Epoch: 199850 | training loss: 2.0275e-03 | validation loss: 1.6174e-03\n",
      "Epoch: 199860 | training loss: 2.0508e-03 | validation loss: 1.6728e-03\n",
      "Epoch: 199870 | training loss: 2.0204e-03 | validation loss: 1.6259e-03\n",
      "Epoch: 199880 | training loss: 2.0129e-03 | validation loss: 1.6288e-03\n",
      "Epoch: 199890 | training loss: 2.0126e-03 | validation loss: 1.6340e-03\n",
      "Epoch: 199900 | training loss: 2.0124e-03 | validation loss: 1.6271e-03\n",
      "Epoch: 199910 | training loss: 2.0121e-03 | validation loss: 1.6317e-03\n",
      "Epoch: 199920 | training loss: 2.0120e-03 | validation loss: 1.6295e-03\n",
      "Epoch: 199930 | training loss: 2.0120e-03 | validation loss: 1.6290e-03\n",
      "Epoch: 199940 | training loss: 2.0120e-03 | validation loss: 1.6305e-03\n",
      "Epoch: 199950 | training loss: 2.0119e-03 | validation loss: 1.6304e-03\n",
      "Epoch: 199960 | training loss: 2.0119e-03 | validation loss: 1.6304e-03\n",
      "Epoch: 199970 | training loss: 2.0124e-03 | validation loss: 1.6321e-03\n",
      "Epoch: 199980 | training loss: 2.0372e-03 | validation loss: 1.6590e-03\n",
      "Epoch: 199990 | training loss: 2.3970e-03 | validation loss: 1.9134e-03\n",
      "Epoch: 200000 | training loss: 2.3528e-03 | validation loss: 1.7099e-03\n",
      "Epoch: 200010 | training loss: 2.0777e-03 | validation loss: 1.6385e-03\n",
      "Epoch: 200020 | training loss: 2.0245e-03 | validation loss: 1.6308e-03\n",
      "Epoch: 200030 | training loss: 2.0118e-03 | validation loss: 1.6304e-03\n",
      "Epoch: 200040 | training loss: 2.0141e-03 | validation loss: 1.6361e-03\n",
      "Epoch: 200050 | training loss: 2.0135e-03 | validation loss: 1.6286e-03\n",
      "Epoch: 200060 | training loss: 2.0138e-03 | validation loss: 1.6360e-03\n",
      "Epoch: 200070 | training loss: 2.0124e-03 | validation loss: 1.6253e-03\n",
      "Epoch: 200080 | training loss: 2.0120e-03 | validation loss: 1.6272e-03\n",
      "Epoch: 200090 | training loss: 2.0117e-03 | validation loss: 1.6291e-03\n",
      "Epoch: 200100 | training loss: 2.0116e-03 | validation loss: 1.6291e-03\n",
      "Epoch: 200110 | training loss: 2.0116e-03 | validation loss: 1.6300e-03\n",
      "Epoch: 200120 | training loss: 2.0126e-03 | validation loss: 1.6341e-03\n",
      "Epoch: 200130 | training loss: 2.1085e-03 | validation loss: 1.7181e-03\n",
      "Epoch: 200140 | training loss: 3.0676e-03 | validation loss: 2.2643e-03\n",
      "Epoch: 200150 | training loss: 2.2106e-03 | validation loss: 1.8000e-03\n",
      "Epoch: 200160 | training loss: 2.0544e-03 | validation loss: 1.6648e-03\n",
      "Epoch: 200170 | training loss: 2.0496e-03 | validation loss: 1.6330e-03\n",
      "Epoch: 200180 | training loss: 2.0190e-03 | validation loss: 1.6189e-03\n",
      "Epoch: 200190 | training loss: 2.0169e-03 | validation loss: 1.6279e-03\n",
      "Epoch: 200200 | training loss: 2.0123e-03 | validation loss: 1.6305e-03\n",
      "Epoch: 200210 | training loss: 2.0121e-03 | validation loss: 1.6311e-03\n",
      "Epoch: 200220 | training loss: 2.0114e-03 | validation loss: 1.6299e-03\n",
      "Epoch: 200230 | training loss: 2.0115e-03 | validation loss: 1.6287e-03\n",
      "Epoch: 200240 | training loss: 2.0114e-03 | validation loss: 1.6287e-03\n",
      "Epoch: 200250 | training loss: 2.0113e-03 | validation loss: 1.6295e-03\n",
      "Epoch: 200260 | training loss: 2.0113e-03 | validation loss: 1.6287e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200270 | training loss: 2.0113e-03 | validation loss: 1.6291e-03\n",
      "Epoch: 200280 | training loss: 2.0113e-03 | validation loss: 1.6288e-03\n",
      "Epoch: 200290 | training loss: 2.0112e-03 | validation loss: 1.6290e-03\n",
      "Epoch: 200300 | training loss: 2.0112e-03 | validation loss: 1.6288e-03\n",
      "Epoch: 200310 | training loss: 2.0112e-03 | validation loss: 1.6289e-03\n",
      "Epoch: 200320 | training loss: 2.0112e-03 | validation loss: 1.6288e-03\n",
      "Epoch: 200330 | training loss: 2.0111e-03 | validation loss: 1.6287e-03\n",
      "Epoch: 200340 | training loss: 2.0111e-03 | validation loss: 1.6285e-03\n",
      "Epoch: 200350 | training loss: 2.0119e-03 | validation loss: 1.6274e-03\n",
      "Epoch: 200360 | training loss: 2.1014e-03 | validation loss: 1.6569e-03\n",
      "Epoch: 200370 | training loss: 3.1875e-03 | validation loss: 2.0424e-03\n",
      "Epoch: 200380 | training loss: 2.1686e-03 | validation loss: 1.7691e-03\n",
      "Epoch: 200390 | training loss: 2.1292e-03 | validation loss: 1.7542e-03\n",
      "Epoch: 200400 | training loss: 2.0234e-03 | validation loss: 1.6531e-03\n",
      "Epoch: 200410 | training loss: 2.0163e-03 | validation loss: 1.6210e-03\n",
      "Epoch: 200420 | training loss: 2.0129e-03 | validation loss: 1.6225e-03\n",
      "Epoch: 200430 | training loss: 2.0121e-03 | validation loss: 1.6322e-03\n",
      "Epoch: 200440 | training loss: 2.0110e-03 | validation loss: 1.6283e-03\n",
      "Epoch: 200450 | training loss: 2.0111e-03 | validation loss: 1.6268e-03\n",
      "Epoch: 200460 | training loss: 2.0110e-03 | validation loss: 1.6298e-03\n",
      "Epoch: 200470 | training loss: 2.0109e-03 | validation loss: 1.6281e-03\n",
      "Epoch: 200480 | training loss: 2.0109e-03 | validation loss: 1.6290e-03\n",
      "Epoch: 200490 | training loss: 2.0108e-03 | validation loss: 1.6282e-03\n",
      "Epoch: 200500 | training loss: 2.0108e-03 | validation loss: 1.6285e-03\n",
      "Epoch: 200510 | training loss: 2.0108e-03 | validation loss: 1.6282e-03\n",
      "Epoch: 200520 | training loss: 2.0108e-03 | validation loss: 1.6282e-03\n",
      "Epoch: 200530 | training loss: 2.0108e-03 | validation loss: 1.6283e-03\n",
      "Epoch: 200540 | training loss: 2.0107e-03 | validation loss: 1.6283e-03\n",
      "Epoch: 200550 | training loss: 2.0107e-03 | validation loss: 1.6283e-03\n",
      "Epoch: 200560 | training loss: 2.0107e-03 | validation loss: 1.6286e-03\n",
      "Epoch: 200570 | training loss: 2.0112e-03 | validation loss: 1.6319e-03\n",
      "Epoch: 200580 | training loss: 2.0962e-03 | validation loss: 1.7111e-03\n",
      "Epoch: 200590 | training loss: 3.1123e-03 | validation loss: 2.2907e-03\n",
      "Epoch: 200600 | training loss: 2.4296e-03 | validation loss: 1.9339e-03\n",
      "Epoch: 200610 | training loss: 2.1079e-03 | validation loss: 1.7382e-03\n",
      "Epoch: 200620 | training loss: 2.0519e-03 | validation loss: 1.6807e-03\n",
      "Epoch: 200630 | training loss: 2.0327e-03 | validation loss: 1.6541e-03\n",
      "Epoch: 200640 | training loss: 2.0187e-03 | validation loss: 1.6398e-03\n",
      "Epoch: 200650 | training loss: 2.0121e-03 | validation loss: 1.6343e-03\n",
      "Epoch: 200660 | training loss: 2.0111e-03 | validation loss: 1.6320e-03\n",
      "Epoch: 200670 | training loss: 2.0105e-03 | validation loss: 1.6287e-03\n",
      "Epoch: 200680 | training loss: 2.0105e-03 | validation loss: 1.6266e-03\n",
      "Epoch: 200690 | training loss: 2.0105e-03 | validation loss: 1.6270e-03\n",
      "Epoch: 200700 | training loss: 2.0104e-03 | validation loss: 1.6281e-03\n",
      "Epoch: 200710 | training loss: 2.0104e-03 | validation loss: 1.6281e-03\n",
      "Epoch: 200720 | training loss: 2.0104e-03 | validation loss: 1.6279e-03\n",
      "Epoch: 200730 | training loss: 2.0104e-03 | validation loss: 1.6277e-03\n",
      "Epoch: 200740 | training loss: 2.0103e-03 | validation loss: 1.6279e-03\n",
      "Epoch: 200750 | training loss: 2.0103e-03 | validation loss: 1.6278e-03\n",
      "Epoch: 200760 | training loss: 2.0103e-03 | validation loss: 1.6278e-03\n",
      "Epoch: 200770 | training loss: 2.0103e-03 | validation loss: 1.6278e-03\n",
      "Epoch: 200780 | training loss: 2.0103e-03 | validation loss: 1.6277e-03\n",
      "Epoch: 200790 | training loss: 2.0102e-03 | validation loss: 1.6277e-03\n",
      "Epoch: 200800 | training loss: 2.0102e-03 | validation loss: 1.6277e-03\n",
      "Epoch: 200810 | training loss: 2.0102e-03 | validation loss: 1.6277e-03\n",
      "Epoch: 200820 | training loss: 2.0102e-03 | validation loss: 1.6277e-03\n",
      "Epoch: 200830 | training loss: 2.0103e-03 | validation loss: 1.6287e-03\n",
      "Epoch: 200840 | training loss: 2.0371e-03 | validation loss: 1.6545e-03\n",
      "Epoch: 200850 | training loss: 4.5147e-03 | validation loss: 3.0061e-03\n",
      "Epoch: 200860 | training loss: 2.3268e-03 | validation loss: 1.7946e-03\n",
      "Epoch: 200870 | training loss: 2.1945e-03 | validation loss: 1.6854e-03\n",
      "Epoch: 200880 | training loss: 2.0573e-03 | validation loss: 1.6167e-03\n",
      "Epoch: 200890 | training loss: 2.0284e-03 | validation loss: 1.6226e-03\n",
      "Epoch: 200900 | training loss: 2.0185e-03 | validation loss: 1.6306e-03\n",
      "Epoch: 200910 | training loss: 2.0121e-03 | validation loss: 1.6326e-03\n",
      "Epoch: 200920 | training loss: 2.0109e-03 | validation loss: 1.6328e-03\n",
      "Epoch: 200930 | training loss: 2.0101e-03 | validation loss: 1.6297e-03\n",
      "Epoch: 200940 | training loss: 2.0100e-03 | validation loss: 1.6268e-03\n",
      "Epoch: 200950 | training loss: 2.0100e-03 | validation loss: 1.6266e-03\n",
      "Epoch: 200960 | training loss: 2.0099e-03 | validation loss: 1.6277e-03\n",
      "Epoch: 200970 | training loss: 2.0099e-03 | validation loss: 1.6274e-03\n",
      "Epoch: 200980 | training loss: 2.0099e-03 | validation loss: 1.6272e-03\n",
      "Epoch: 200990 | training loss: 2.0098e-03 | validation loss: 1.6274e-03\n",
      "Epoch: 201000 | training loss: 2.0098e-03 | validation loss: 1.6272e-03\n",
      "Epoch: 201010 | training loss: 2.0098e-03 | validation loss: 1.6273e-03\n",
      "Epoch: 201020 | training loss: 2.0098e-03 | validation loss: 1.6272e-03\n",
      "Epoch: 201030 | training loss: 2.0098e-03 | validation loss: 1.6272e-03\n",
      "Epoch: 201040 | training loss: 2.0097e-03 | validation loss: 1.6272e-03\n",
      "Epoch: 201050 | training loss: 2.0097e-03 | validation loss: 1.6271e-03\n",
      "Epoch: 201060 | training loss: 2.0097e-03 | validation loss: 1.6271e-03\n",
      "Epoch: 201070 | training loss: 2.0097e-03 | validation loss: 1.6271e-03\n",
      "Epoch: 201080 | training loss: 2.0097e-03 | validation loss: 1.6271e-03\n",
      "Epoch: 201090 | training loss: 2.0096e-03 | validation loss: 1.6269e-03\n",
      "Epoch: 201100 | training loss: 2.0098e-03 | validation loss: 1.6254e-03\n",
      "Epoch: 201110 | training loss: 2.0341e-03 | validation loss: 1.6161e-03\n",
      "Epoch: 201120 | training loss: 4.4910e-03 | validation loss: 2.6051e-03\n",
      "Epoch: 201130 | training loss: 2.2391e-03 | validation loss: 1.7057e-03\n",
      "Epoch: 201140 | training loss: 2.1735e-03 | validation loss: 1.6942e-03\n",
      "Epoch: 201150 | training loss: 2.0964e-03 | validation loss: 1.6557e-03\n",
      "Epoch: 201160 | training loss: 2.0406e-03 | validation loss: 1.6300e-03\n",
      "Epoch: 201170 | training loss: 2.0172e-03 | validation loss: 1.6215e-03\n",
      "Epoch: 201180 | training loss: 2.0114e-03 | validation loss: 1.6210e-03\n",
      "Epoch: 201190 | training loss: 2.0103e-03 | validation loss: 1.6225e-03\n",
      "Epoch: 201200 | training loss: 2.0097e-03 | validation loss: 1.6243e-03\n",
      "Epoch: 201210 | training loss: 2.0095e-03 | validation loss: 1.6256e-03\n",
      "Epoch: 201220 | training loss: 2.0094e-03 | validation loss: 1.6258e-03\n",
      "Epoch: 201230 | training loss: 2.0094e-03 | validation loss: 1.6259e-03\n",
      "Epoch: 201240 | training loss: 2.0093e-03 | validation loss: 1.6265e-03\n",
      "Epoch: 201250 | training loss: 2.0093e-03 | validation loss: 1.6268e-03\n",
      "Epoch: 201260 | training loss: 2.0093e-03 | validation loss: 1.6267e-03\n",
      "Epoch: 201270 | training loss: 2.0093e-03 | validation loss: 1.6266e-03\n",
      "Epoch: 201280 | training loss: 2.0093e-03 | validation loss: 1.6265e-03\n",
      "Epoch: 201290 | training loss: 2.0092e-03 | validation loss: 1.6265e-03\n",
      "Epoch: 201300 | training loss: 2.0092e-03 | validation loss: 1.6265e-03\n",
      "Epoch: 201310 | training loss: 2.0092e-03 | validation loss: 1.6265e-03\n",
      "Epoch: 201320 | training loss: 2.0092e-03 | validation loss: 1.6264e-03\n",
      "Epoch: 201330 | training loss: 2.0092e-03 | validation loss: 1.6264e-03\n",
      "Epoch: 201340 | training loss: 2.0091e-03 | validation loss: 1.6264e-03\n",
      "Epoch: 201350 | training loss: 2.0091e-03 | validation loss: 1.6264e-03\n",
      "Epoch: 201360 | training loss: 2.0091e-03 | validation loss: 1.6265e-03\n",
      "Epoch: 201370 | training loss: 2.0096e-03 | validation loss: 1.6282e-03\n",
      "Epoch: 201380 | training loss: 2.1015e-03 | validation loss: 1.6969e-03\n",
      "Epoch: 201390 | training loss: 2.9731e-03 | validation loss: 2.2300e-03\n",
      "Epoch: 201400 | training loss: 2.3469e-03 | validation loss: 1.8636e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 201410 | training loss: 2.0462e-03 | validation loss: 1.6752e-03\n",
      "Epoch: 201420 | training loss: 2.0207e-03 | validation loss: 1.6445e-03\n",
      "Epoch: 201430 | training loss: 2.0218e-03 | validation loss: 1.6338e-03\n",
      "Epoch: 201440 | training loss: 2.0176e-03 | validation loss: 1.6272e-03\n",
      "Epoch: 201450 | training loss: 2.0114e-03 | validation loss: 1.6262e-03\n",
      "Epoch: 201460 | training loss: 2.0094e-03 | validation loss: 1.6291e-03\n",
      "Epoch: 201470 | training loss: 2.0093e-03 | validation loss: 1.6298e-03\n",
      "Epoch: 201480 | training loss: 2.0089e-03 | validation loss: 1.6272e-03\n",
      "Epoch: 201490 | training loss: 2.0089e-03 | validation loss: 1.6258e-03\n",
      "Epoch: 201500 | training loss: 2.0088e-03 | validation loss: 1.6264e-03\n",
      "Epoch: 201510 | training loss: 2.0088e-03 | validation loss: 1.6262e-03\n",
      "Epoch: 201520 | training loss: 2.0088e-03 | validation loss: 1.6258e-03\n",
      "Epoch: 201530 | training loss: 2.0088e-03 | validation loss: 1.6261e-03\n",
      "Epoch: 201540 | training loss: 2.0088e-03 | validation loss: 1.6260e-03\n",
      "Epoch: 201550 | training loss: 2.0087e-03 | validation loss: 1.6260e-03\n",
      "Epoch: 201560 | training loss: 2.0087e-03 | validation loss: 1.6259e-03\n",
      "Epoch: 201570 | training loss: 2.0087e-03 | validation loss: 1.6259e-03\n",
      "Epoch: 201580 | training loss: 2.0087e-03 | validation loss: 1.6259e-03\n",
      "Epoch: 201590 | training loss: 2.0086e-03 | validation loss: 1.6259e-03\n",
      "Epoch: 201600 | training loss: 2.0086e-03 | validation loss: 1.6259e-03\n",
      "Epoch: 201610 | training loss: 2.0086e-03 | validation loss: 1.6258e-03\n",
      "Epoch: 201620 | training loss: 2.0086e-03 | validation loss: 1.6257e-03\n",
      "Epoch: 201630 | training loss: 2.0086e-03 | validation loss: 1.6249e-03\n",
      "Epoch: 201640 | training loss: 2.0138e-03 | validation loss: 1.6186e-03\n",
      "Epoch: 201650 | training loss: 2.9988e-03 | validation loss: 1.9679e-03\n",
      "Epoch: 201660 | training loss: 3.0597e-03 | validation loss: 2.2682e-03\n",
      "Epoch: 201670 | training loss: 2.3089e-03 | validation loss: 1.8737e-03\n",
      "Epoch: 201680 | training loss: 2.1254e-03 | validation loss: 1.7525e-03\n",
      "Epoch: 201690 | training loss: 2.0538e-03 | validation loss: 1.6885e-03\n",
      "Epoch: 201700 | training loss: 2.0231e-03 | validation loss: 1.6548e-03\n",
      "Epoch: 201710 | training loss: 2.0131e-03 | validation loss: 1.6390e-03\n",
      "Epoch: 201720 | training loss: 2.0096e-03 | validation loss: 1.6313e-03\n",
      "Epoch: 201730 | training loss: 2.0084e-03 | validation loss: 1.6270e-03\n",
      "Epoch: 201740 | training loss: 2.0084e-03 | validation loss: 1.6248e-03\n",
      "Epoch: 201750 | training loss: 2.0084e-03 | validation loss: 1.6241e-03\n",
      "Epoch: 201760 | training loss: 2.0083e-03 | validation loss: 1.6248e-03\n",
      "Epoch: 201770 | training loss: 2.0083e-03 | validation loss: 1.6257e-03\n",
      "Epoch: 201780 | training loss: 2.0083e-03 | validation loss: 1.6258e-03\n",
      "Epoch: 201790 | training loss: 2.0083e-03 | validation loss: 1.6254e-03\n",
      "Epoch: 201800 | training loss: 2.0082e-03 | validation loss: 1.6253e-03\n",
      "Epoch: 201810 | training loss: 2.0082e-03 | validation loss: 1.6255e-03\n",
      "Epoch: 201820 | training loss: 2.0082e-03 | validation loss: 1.6254e-03\n",
      "Epoch: 201830 | training loss: 2.0082e-03 | validation loss: 1.6254e-03\n",
      "Epoch: 201840 | training loss: 2.0081e-03 | validation loss: 1.6254e-03\n",
      "Epoch: 201850 | training loss: 2.0081e-03 | validation loss: 1.6253e-03\n",
      "Epoch: 201860 | training loss: 2.0081e-03 | validation loss: 1.6253e-03\n",
      "Epoch: 201870 | training loss: 2.0081e-03 | validation loss: 1.6253e-03\n",
      "Epoch: 201880 | training loss: 2.0082e-03 | validation loss: 1.6251e-03\n",
      "Epoch: 201890 | training loss: 2.0264e-03 | validation loss: 1.6342e-03\n",
      "Epoch: 201900 | training loss: 2.9507e-03 | validation loss: 2.1861e-03\n",
      "Epoch: 201910 | training loss: 2.2242e-03 | validation loss: 1.7045e-03\n",
      "Epoch: 201920 | training loss: 2.0909e-03 | validation loss: 1.6756e-03\n",
      "Epoch: 201930 | training loss: 2.0423e-03 | validation loss: 1.6189e-03\n",
      "Epoch: 201940 | training loss: 2.0201e-03 | validation loss: 1.6324e-03\n",
      "Epoch: 201950 | training loss: 2.0112e-03 | validation loss: 1.6204e-03\n",
      "Epoch: 201960 | training loss: 2.0090e-03 | validation loss: 1.6212e-03\n",
      "Epoch: 201970 | training loss: 2.0085e-03 | validation loss: 1.6275e-03\n",
      "Epoch: 201980 | training loss: 2.0079e-03 | validation loss: 1.6261e-03\n",
      "Epoch: 201990 | training loss: 2.0079e-03 | validation loss: 1.6250e-03\n",
      "Epoch: 202000 | training loss: 2.0079e-03 | validation loss: 1.6239e-03\n",
      "Epoch: 202010 | training loss: 2.0098e-03 | validation loss: 1.6197e-03\n",
      "Epoch: 202020 | training loss: 2.1020e-03 | validation loss: 1.6273e-03\n",
      "Epoch: 202030 | training loss: 2.9794e-03 | validation loss: 1.9588e-03\n",
      "Epoch: 202040 | training loss: 2.0167e-03 | validation loss: 1.6341e-03\n",
      "Epoch: 202050 | training loss: 2.1264e-03 | validation loss: 1.7216e-03\n",
      "Epoch: 202060 | training loss: 2.0126e-03 | validation loss: 1.6162e-03\n",
      "Epoch: 202070 | training loss: 2.0194e-03 | validation loss: 1.6159e-03\n",
      "Epoch: 202080 | training loss: 2.0119e-03 | validation loss: 1.6349e-03\n",
      "Epoch: 202090 | training loss: 2.0077e-03 | validation loss: 1.6246e-03\n",
      "Epoch: 202100 | training loss: 2.0079e-03 | validation loss: 1.6231e-03\n",
      "Epoch: 202110 | training loss: 2.0078e-03 | validation loss: 1.6267e-03\n",
      "Epoch: 202120 | training loss: 2.0077e-03 | validation loss: 1.6234e-03\n",
      "Epoch: 202130 | training loss: 2.0076e-03 | validation loss: 1.6253e-03\n",
      "Epoch: 202140 | training loss: 2.0076e-03 | validation loss: 1.6245e-03\n",
      "Epoch: 202150 | training loss: 2.0075e-03 | validation loss: 1.6244e-03\n",
      "Epoch: 202160 | training loss: 2.0075e-03 | validation loss: 1.6248e-03\n",
      "Epoch: 202170 | training loss: 2.0075e-03 | validation loss: 1.6248e-03\n",
      "Epoch: 202180 | training loss: 2.0075e-03 | validation loss: 1.6248e-03\n",
      "Epoch: 202190 | training loss: 2.0075e-03 | validation loss: 1.6254e-03\n",
      "Epoch: 202200 | training loss: 2.0087e-03 | validation loss: 1.6296e-03\n",
      "Epoch: 202210 | training loss: 2.0962e-03 | validation loss: 1.7043e-03\n",
      "Epoch: 202220 | training loss: 3.2464e-03 | validation loss: 2.3540e-03\n",
      "Epoch: 202230 | training loss: 2.0577e-03 | validation loss: 1.6854e-03\n",
      "Epoch: 202240 | training loss: 2.0914e-03 | validation loss: 1.6345e-03\n",
      "Epoch: 202250 | training loss: 2.0553e-03 | validation loss: 1.6149e-03\n",
      "Epoch: 202260 | training loss: 2.0094e-03 | validation loss: 1.6204e-03\n",
      "Epoch: 202270 | training loss: 2.0134e-03 | validation loss: 1.6398e-03\n",
      "Epoch: 202280 | training loss: 2.0077e-03 | validation loss: 1.6283e-03\n",
      "Epoch: 202290 | training loss: 2.0082e-03 | validation loss: 1.6198e-03\n",
      "Epoch: 202300 | training loss: 2.0073e-03 | validation loss: 1.6255e-03\n",
      "Epoch: 202310 | training loss: 2.0073e-03 | validation loss: 1.6254e-03\n",
      "Epoch: 202320 | training loss: 2.0072e-03 | validation loss: 1.6235e-03\n",
      "Epoch: 202330 | training loss: 2.0072e-03 | validation loss: 1.6249e-03\n",
      "Epoch: 202340 | training loss: 2.0072e-03 | validation loss: 1.6240e-03\n",
      "Epoch: 202350 | training loss: 2.0071e-03 | validation loss: 1.6245e-03\n",
      "Epoch: 202360 | training loss: 2.0071e-03 | validation loss: 1.6241e-03\n",
      "Epoch: 202370 | training loss: 2.0071e-03 | validation loss: 1.6242e-03\n",
      "Epoch: 202380 | training loss: 2.0071e-03 | validation loss: 1.6243e-03\n",
      "Epoch: 202390 | training loss: 2.0070e-03 | validation loss: 1.6241e-03\n",
      "Epoch: 202400 | training loss: 2.0070e-03 | validation loss: 1.6238e-03\n",
      "Epoch: 202410 | training loss: 2.0088e-03 | validation loss: 1.6221e-03\n",
      "Epoch: 202420 | training loss: 2.2744e-03 | validation loss: 1.7679e-03\n",
      "Epoch: 202430 | training loss: 2.3448e-03 | validation loss: 1.9228e-03\n",
      "Epoch: 202440 | training loss: 2.2461e-03 | validation loss: 1.8300e-03\n",
      "Epoch: 202450 | training loss: 2.0198e-03 | validation loss: 1.6489e-03\n",
      "Epoch: 202460 | training loss: 2.0482e-03 | validation loss: 1.6157e-03\n",
      "Epoch: 202470 | training loss: 2.0072e-03 | validation loss: 1.6240e-03\n",
      "Epoch: 202480 | training loss: 2.0181e-03 | validation loss: 1.6415e-03\n",
      "Epoch: 202490 | training loss: 2.0269e-03 | validation loss: 1.6508e-03\n",
      "Epoch: 202500 | training loss: 2.0938e-03 | validation loss: 1.7052e-03\n",
      "Epoch: 202510 | training loss: 2.3077e-03 | validation loss: 1.8435e-03\n",
      "Epoch: 202520 | training loss: 2.0226e-03 | validation loss: 1.6142e-03\n",
      "Epoch: 202530 | training loss: 2.0224e-03 | validation loss: 1.6143e-03\n",
      "Epoch: 202540 | training loss: 2.0252e-03 | validation loss: 1.6510e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 202550 | training loss: 2.0094e-03 | validation loss: 1.6318e-03\n",
      "Epoch: 202560 | training loss: 2.0074e-03 | validation loss: 1.6203e-03\n",
      "Epoch: 202570 | training loss: 2.0130e-03 | validation loss: 1.6158e-03\n",
      "Epoch: 202580 | training loss: 2.0899e-03 | validation loss: 1.6243e-03\n",
      "Epoch: 202590 | training loss: 2.6465e-03 | validation loss: 1.8240e-03\n",
      "Epoch: 202600 | training loss: 2.2123e-03 | validation loss: 1.7788e-03\n",
      "Epoch: 202610 | training loss: 2.0846e-03 | validation loss: 1.6257e-03\n",
      "Epoch: 202620 | training loss: 2.0362e-03 | validation loss: 1.6599e-03\n",
      "Epoch: 202630 | training loss: 2.0175e-03 | validation loss: 1.6153e-03\n",
      "Epoch: 202640 | training loss: 2.0090e-03 | validation loss: 1.6308e-03\n",
      "Epoch: 202650 | training loss: 2.0065e-03 | validation loss: 1.6242e-03\n",
      "Epoch: 202660 | training loss: 2.0074e-03 | validation loss: 1.6200e-03\n",
      "Epoch: 202670 | training loss: 2.0066e-03 | validation loss: 1.6223e-03\n",
      "Epoch: 202680 | training loss: 2.0065e-03 | validation loss: 1.6237e-03\n",
      "Epoch: 202690 | training loss: 2.0065e-03 | validation loss: 1.6246e-03\n",
      "Epoch: 202700 | training loss: 2.0086e-03 | validation loss: 1.6303e-03\n",
      "Epoch: 202710 | training loss: 2.1327e-03 | validation loss: 1.7288e-03\n",
      "Epoch: 202720 | training loss: 2.7819e-03 | validation loss: 2.1011e-03\n",
      "Epoch: 202730 | training loss: 2.0698e-03 | validation loss: 1.6935e-03\n",
      "Epoch: 202740 | training loss: 2.0966e-03 | validation loss: 1.6345e-03\n",
      "Epoch: 202750 | training loss: 2.0306e-03 | validation loss: 1.6104e-03\n",
      "Epoch: 202760 | training loss: 2.0136e-03 | validation loss: 1.6343e-03\n",
      "Epoch: 202770 | training loss: 2.0099e-03 | validation loss: 1.6355e-03\n",
      "Epoch: 202780 | training loss: 2.0077e-03 | validation loss: 1.6186e-03\n",
      "Epoch: 202790 | training loss: 2.0063e-03 | validation loss: 1.6225e-03\n",
      "Epoch: 202800 | training loss: 2.0064e-03 | validation loss: 1.6259e-03\n",
      "Epoch: 202810 | training loss: 2.0063e-03 | validation loss: 1.6215e-03\n",
      "Epoch: 202820 | training loss: 2.0062e-03 | validation loss: 1.6244e-03\n",
      "Epoch: 202830 | training loss: 2.0062e-03 | validation loss: 1.6225e-03\n",
      "Epoch: 202840 | training loss: 2.0061e-03 | validation loss: 1.6235e-03\n",
      "Epoch: 202850 | training loss: 2.0061e-03 | validation loss: 1.6231e-03\n",
      "Epoch: 202860 | training loss: 2.0061e-03 | validation loss: 1.6230e-03\n",
      "Epoch: 202870 | training loss: 2.0061e-03 | validation loss: 1.6232e-03\n",
      "Epoch: 202880 | training loss: 2.0061e-03 | validation loss: 1.6233e-03\n",
      "Epoch: 202890 | training loss: 2.0061e-03 | validation loss: 1.6237e-03\n",
      "Epoch: 202900 | training loss: 2.0081e-03 | validation loss: 1.6282e-03\n",
      "Epoch: 202910 | training loss: 2.2461e-03 | validation loss: 1.8209e-03\n",
      "Epoch: 202920 | training loss: 2.3452e-03 | validation loss: 1.7610e-03\n",
      "Epoch: 202930 | training loss: 2.3549e-03 | validation loss: 1.7037e-03\n",
      "Epoch: 202940 | training loss: 2.1211e-03 | validation loss: 1.7255e-03\n",
      "Epoch: 202950 | training loss: 2.0439e-03 | validation loss: 1.6160e-03\n",
      "Epoch: 202960 | training loss: 2.0143e-03 | validation loss: 1.6416e-03\n",
      "Epoch: 202970 | training loss: 2.0074e-03 | validation loss: 1.6282e-03\n",
      "Epoch: 202980 | training loss: 2.0097e-03 | validation loss: 1.6193e-03\n",
      "Epoch: 202990 | training loss: 2.0060e-03 | validation loss: 1.6237e-03\n",
      "Epoch: 203000 | training loss: 2.0065e-03 | validation loss: 1.6266e-03\n",
      "Epoch: 203010 | training loss: 2.0100e-03 | validation loss: 1.6331e-03\n",
      "Epoch: 203020 | training loss: 2.0723e-03 | validation loss: 1.6900e-03\n",
      "Epoch: 203030 | training loss: 2.7646e-03 | validation loss: 2.1053e-03\n",
      "Epoch: 203040 | training loss: 2.2579e-03 | validation loss: 1.6792e-03\n",
      "Epoch: 203050 | training loss: 2.0904e-03 | validation loss: 1.7004e-03\n",
      "Epoch: 203060 | training loss: 2.0273e-03 | validation loss: 1.6129e-03\n",
      "Epoch: 203070 | training loss: 2.0120e-03 | validation loss: 1.6352e-03\n",
      "Epoch: 203080 | training loss: 2.0087e-03 | validation loss: 1.6170e-03\n",
      "Epoch: 203090 | training loss: 2.0076e-03 | validation loss: 1.6292e-03\n",
      "Epoch: 203100 | training loss: 2.0062e-03 | validation loss: 1.6197e-03\n",
      "Epoch: 203110 | training loss: 2.0057e-03 | validation loss: 1.6214e-03\n",
      "Epoch: 203120 | training loss: 2.0057e-03 | validation loss: 1.6240e-03\n",
      "Epoch: 203130 | training loss: 2.0059e-03 | validation loss: 1.6248e-03\n",
      "Epoch: 203140 | training loss: 2.0075e-03 | validation loss: 1.6291e-03\n",
      "Epoch: 203150 | training loss: 2.0474e-03 | validation loss: 1.6687e-03\n",
      "Epoch: 203160 | training loss: 2.9412e-03 | validation loss: 2.1939e-03\n",
      "Epoch: 203170 | training loss: 2.3570e-03 | validation loss: 1.7186e-03\n",
      "Epoch: 203180 | training loss: 2.0924e-03 | validation loss: 1.6942e-03\n",
      "Epoch: 203190 | training loss: 2.0067e-03 | validation loss: 1.6240e-03\n",
      "Epoch: 203200 | training loss: 2.0112e-03 | validation loss: 1.6162e-03\n",
      "Epoch: 203210 | training loss: 2.0106e-03 | validation loss: 1.6319e-03\n",
      "Epoch: 203220 | training loss: 2.0077e-03 | validation loss: 1.6189e-03\n",
      "Epoch: 203230 | training loss: 2.0062e-03 | validation loss: 1.6255e-03\n",
      "Epoch: 203240 | training loss: 2.0055e-03 | validation loss: 1.6212e-03\n",
      "Epoch: 203250 | training loss: 2.0053e-03 | validation loss: 1.6219e-03\n",
      "Epoch: 203260 | training loss: 2.0054e-03 | validation loss: 1.6233e-03\n",
      "Epoch: 203270 | training loss: 2.0053e-03 | validation loss: 1.6223e-03\n",
      "Epoch: 203280 | training loss: 2.0053e-03 | validation loss: 1.6218e-03\n",
      "Epoch: 203290 | training loss: 2.0053e-03 | validation loss: 1.6212e-03\n",
      "Epoch: 203300 | training loss: 2.0067e-03 | validation loss: 1.6184e-03\n",
      "Epoch: 203310 | training loss: 2.0847e-03 | validation loss: 1.6287e-03\n",
      "Epoch: 203320 | training loss: 3.2989e-03 | validation loss: 2.1123e-03\n",
      "Epoch: 203330 | training loss: 2.0196e-03 | validation loss: 1.6480e-03\n",
      "Epoch: 203340 | training loss: 2.1521e-03 | validation loss: 1.7302e-03\n",
      "Epoch: 203350 | training loss: 2.0062e-03 | validation loss: 1.6234e-03\n",
      "Epoch: 203360 | training loss: 2.0237e-03 | validation loss: 1.6200e-03\n",
      "Epoch: 203370 | training loss: 2.0051e-03 | validation loss: 1.6211e-03\n",
      "Epoch: 203380 | training loss: 2.0074e-03 | validation loss: 1.6281e-03\n",
      "Epoch: 203390 | training loss: 2.0056e-03 | validation loss: 1.6200e-03\n",
      "Epoch: 203400 | training loss: 2.0050e-03 | validation loss: 1.6222e-03\n",
      "Epoch: 203410 | training loss: 2.0050e-03 | validation loss: 1.6226e-03\n",
      "Epoch: 203420 | training loss: 2.0050e-03 | validation loss: 1.6216e-03\n",
      "Epoch: 203430 | training loss: 2.0050e-03 | validation loss: 1.6221e-03\n",
      "Epoch: 203440 | training loss: 2.0049e-03 | validation loss: 1.6217e-03\n",
      "Epoch: 203450 | training loss: 2.0049e-03 | validation loss: 1.6212e-03\n",
      "Epoch: 203460 | training loss: 2.0056e-03 | validation loss: 1.6187e-03\n",
      "Epoch: 203470 | training loss: 2.0778e-03 | validation loss: 1.6336e-03\n",
      "Epoch: 203480 | training loss: 2.2350e-03 | validation loss: 1.6636e-03\n",
      "Epoch: 203490 | training loss: 2.1918e-03 | validation loss: 1.7360e-03\n",
      "Epoch: 203500 | training loss: 2.0583e-03 | validation loss: 1.6228e-03\n",
      "Epoch: 203510 | training loss: 2.0097e-03 | validation loss: 1.6230e-03\n",
      "Epoch: 203520 | training loss: 2.0062e-03 | validation loss: 1.6280e-03\n",
      "Epoch: 203530 | training loss: 2.0080e-03 | validation loss: 1.6218e-03\n",
      "Epoch: 203540 | training loss: 2.0059e-03 | validation loss: 1.6206e-03\n",
      "Epoch: 203550 | training loss: 2.0050e-03 | validation loss: 1.6188e-03\n",
      "Epoch: 203560 | training loss: 2.0062e-03 | validation loss: 1.6161e-03\n",
      "Epoch: 203570 | training loss: 2.0364e-03 | validation loss: 1.6121e-03\n",
      "Epoch: 203580 | training loss: 2.9033e-03 | validation loss: 1.9229e-03\n",
      "Epoch: 203590 | training loss: 2.3669e-03 | validation loss: 1.8719e-03\n",
      "Epoch: 203600 | training loss: 2.0969e-03 | validation loss: 1.6311e-03\n",
      "Epoch: 203610 | training loss: 2.0050e-03 | validation loss: 1.6211e-03\n",
      "Epoch: 203620 | training loss: 2.0162e-03 | validation loss: 1.6398e-03\n",
      "Epoch: 203630 | training loss: 2.0116e-03 | validation loss: 1.6134e-03\n",
      "Epoch: 203640 | training loss: 2.0069e-03 | validation loss: 1.6284e-03\n",
      "Epoch: 203650 | training loss: 2.0054e-03 | validation loss: 1.6183e-03\n",
      "Epoch: 203660 | training loss: 2.0049e-03 | validation loss: 1.6238e-03\n",
      "Epoch: 203670 | training loss: 2.0046e-03 | validation loss: 1.6197e-03\n",
      "Epoch: 203680 | training loss: 2.0045e-03 | validation loss: 1.6213e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 203690 | training loss: 2.0045e-03 | validation loss: 1.6218e-03\n",
      "Epoch: 203700 | training loss: 2.0044e-03 | validation loss: 1.6214e-03\n",
      "Epoch: 203710 | training loss: 2.0044e-03 | validation loss: 1.6212e-03\n",
      "Epoch: 203720 | training loss: 2.0044e-03 | validation loss: 1.6216e-03\n",
      "Epoch: 203730 | training loss: 2.0049e-03 | validation loss: 1.6242e-03\n",
      "Epoch: 203740 | training loss: 2.0453e-03 | validation loss: 1.6658e-03\n",
      "Epoch: 203750 | training loss: 3.8646e-03 | validation loss: 2.6814e-03\n",
      "Epoch: 203760 | training loss: 2.0655e-03 | validation loss: 1.6336e-03\n",
      "Epoch: 203770 | training loss: 2.1832e-03 | validation loss: 1.6518e-03\n",
      "Epoch: 203780 | training loss: 2.0549e-03 | validation loss: 1.6103e-03\n",
      "Epoch: 203790 | training loss: 2.0047e-03 | validation loss: 1.6184e-03\n",
      "Epoch: 203800 | training loss: 2.0113e-03 | validation loss: 1.6382e-03\n",
      "Epoch: 203810 | training loss: 2.0059e-03 | validation loss: 1.6276e-03\n",
      "Epoch: 203820 | training loss: 2.0046e-03 | validation loss: 1.6179e-03\n",
      "Epoch: 203830 | training loss: 2.0044e-03 | validation loss: 1.6192e-03\n",
      "Epoch: 203840 | training loss: 2.0043e-03 | validation loss: 1.6225e-03\n",
      "Epoch: 203850 | training loss: 2.0041e-03 | validation loss: 1.6209e-03\n",
      "Epoch: 203860 | training loss: 2.0041e-03 | validation loss: 1.6204e-03\n",
      "Epoch: 203870 | training loss: 2.0041e-03 | validation loss: 1.6212e-03\n",
      "Epoch: 203880 | training loss: 2.0041e-03 | validation loss: 1.6206e-03\n",
      "Epoch: 203890 | training loss: 2.0040e-03 | validation loss: 1.6209e-03\n",
      "Epoch: 203900 | training loss: 2.0040e-03 | validation loss: 1.6207e-03\n",
      "Epoch: 203910 | training loss: 2.0040e-03 | validation loss: 1.6207e-03\n",
      "Epoch: 203920 | training loss: 2.0040e-03 | validation loss: 1.6208e-03\n",
      "Epoch: 203930 | training loss: 2.0040e-03 | validation loss: 1.6215e-03\n",
      "Epoch: 203940 | training loss: 2.0147e-03 | validation loss: 1.6370e-03\n",
      "Epoch: 203950 | training loss: 2.7161e-03 | validation loss: 2.1717e-03\n",
      "Epoch: 203960 | training loss: 2.0099e-03 | validation loss: 1.6263e-03\n",
      "Epoch: 203970 | training loss: 2.0169e-03 | validation loss: 1.6174e-03\n",
      "Epoch: 203980 | training loss: 2.0202e-03 | validation loss: 1.6179e-03\n",
      "Epoch: 203990 | training loss: 2.0170e-03 | validation loss: 1.6127e-03\n",
      "Epoch: 204000 | training loss: 2.0922e-03 | validation loss: 1.6191e-03\n",
      "Epoch: 204010 | training loss: 2.7682e-03 | validation loss: 1.8652e-03\n",
      "Epoch: 204020 | training loss: 2.2092e-03 | validation loss: 1.7772e-03\n",
      "Epoch: 204030 | training loss: 2.0218e-03 | validation loss: 1.6100e-03\n",
      "Epoch: 204040 | training loss: 2.0043e-03 | validation loss: 1.6169e-03\n",
      "Epoch: 204050 | training loss: 2.0069e-03 | validation loss: 1.6294e-03\n",
      "Epoch: 204060 | training loss: 2.0055e-03 | validation loss: 1.6157e-03\n",
      "Epoch: 204070 | training loss: 2.0041e-03 | validation loss: 1.6233e-03\n",
      "Epoch: 204080 | training loss: 2.0037e-03 | validation loss: 1.6202e-03\n",
      "Epoch: 204090 | training loss: 2.0038e-03 | validation loss: 1.6183e-03\n",
      "Epoch: 204100 | training loss: 2.0037e-03 | validation loss: 1.6213e-03\n",
      "Epoch: 204110 | training loss: 2.0037e-03 | validation loss: 1.6214e-03\n",
      "Epoch: 204120 | training loss: 2.0037e-03 | validation loss: 1.6213e-03\n",
      "Epoch: 204130 | training loss: 2.0041e-03 | validation loss: 1.6235e-03\n",
      "Epoch: 204140 | training loss: 2.0184e-03 | validation loss: 1.6434e-03\n",
      "Epoch: 204150 | training loss: 2.6511e-03 | validation loss: 2.0371e-03\n",
      "Epoch: 204160 | training loss: 2.2293e-03 | validation loss: 1.6636e-03\n",
      "Epoch: 204170 | training loss: 2.1842e-03 | validation loss: 1.7527e-03\n",
      "Epoch: 204180 | training loss: 2.0043e-03 | validation loss: 1.6228e-03\n",
      "Epoch: 204190 | training loss: 2.0251e-03 | validation loss: 1.6163e-03\n",
      "Epoch: 204200 | training loss: 2.0098e-03 | validation loss: 1.6319e-03\n",
      "Epoch: 204210 | training loss: 2.0036e-03 | validation loss: 1.6175e-03\n",
      "Epoch: 204220 | training loss: 2.0035e-03 | validation loss: 1.6193e-03\n",
      "Epoch: 204230 | training loss: 2.0035e-03 | validation loss: 1.6214e-03\n",
      "Epoch: 204240 | training loss: 2.0034e-03 | validation loss: 1.6190e-03\n",
      "Epoch: 204250 | training loss: 2.0033e-03 | validation loss: 1.6201e-03\n",
      "Epoch: 204260 | training loss: 2.0033e-03 | validation loss: 1.6202e-03\n",
      "Epoch: 204270 | training loss: 2.0033e-03 | validation loss: 1.6195e-03\n",
      "Epoch: 204280 | training loss: 2.0033e-03 | validation loss: 1.6195e-03\n",
      "Epoch: 204290 | training loss: 2.0032e-03 | validation loss: 1.6197e-03\n",
      "Epoch: 204300 | training loss: 2.0032e-03 | validation loss: 1.6197e-03\n",
      "Epoch: 204310 | training loss: 2.0032e-03 | validation loss: 1.6195e-03\n",
      "Epoch: 204320 | training loss: 2.0033e-03 | validation loss: 1.6182e-03\n",
      "Epoch: 204330 | training loss: 2.0221e-03 | validation loss: 1.6126e-03\n",
      "Epoch: 204340 | training loss: 4.0452e-03 | validation loss: 2.4350e-03\n",
      "Epoch: 204350 | training loss: 2.1992e-03 | validation loss: 1.7656e-03\n",
      "Epoch: 204360 | training loss: 2.2053e-03 | validation loss: 1.7701e-03\n",
      "Epoch: 204370 | training loss: 2.0970e-03 | validation loss: 1.7015e-03\n",
      "Epoch: 204380 | training loss: 2.0368e-03 | validation loss: 1.6582e-03\n",
      "Epoch: 204390 | training loss: 2.0104e-03 | validation loss: 1.6335e-03\n",
      "Epoch: 204400 | training loss: 2.0032e-03 | validation loss: 1.6215e-03\n",
      "Epoch: 204410 | training loss: 2.0036e-03 | validation loss: 1.6171e-03\n",
      "Epoch: 204420 | training loss: 2.0036e-03 | validation loss: 1.6171e-03\n",
      "Epoch: 204430 | training loss: 2.0030e-03 | validation loss: 1.6193e-03\n",
      "Epoch: 204440 | training loss: 2.0030e-03 | validation loss: 1.6206e-03\n",
      "Epoch: 204450 | training loss: 2.0029e-03 | validation loss: 1.6198e-03\n",
      "Epoch: 204460 | training loss: 2.0029e-03 | validation loss: 1.6192e-03\n",
      "Epoch: 204470 | training loss: 2.0029e-03 | validation loss: 1.6196e-03\n",
      "Epoch: 204480 | training loss: 2.0029e-03 | validation loss: 1.6195e-03\n",
      "Epoch: 204490 | training loss: 2.0028e-03 | validation loss: 1.6194e-03\n",
      "Epoch: 204500 | training loss: 2.0028e-03 | validation loss: 1.6195e-03\n",
      "Epoch: 204510 | training loss: 2.0028e-03 | validation loss: 1.6194e-03\n",
      "Epoch: 204520 | training loss: 2.0028e-03 | validation loss: 1.6193e-03\n",
      "Epoch: 204530 | training loss: 2.0028e-03 | validation loss: 1.6184e-03\n",
      "Epoch: 204540 | training loss: 2.0182e-03 | validation loss: 1.6153e-03\n",
      "Epoch: 204550 | training loss: 2.7264e-03 | validation loss: 2.0016e-03\n",
      "Epoch: 204560 | training loss: 2.2342e-03 | validation loss: 1.7528e-03\n",
      "Epoch: 204570 | training loss: 2.0815e-03 | validation loss: 1.6434e-03\n",
      "Epoch: 204580 | training loss: 2.0309e-03 | validation loss: 1.6263e-03\n",
      "Epoch: 204590 | training loss: 2.0134e-03 | validation loss: 1.6216e-03\n",
      "Epoch: 204600 | training loss: 2.0069e-03 | validation loss: 1.6138e-03\n",
      "Epoch: 204610 | training loss: 2.0039e-03 | validation loss: 1.6156e-03\n",
      "Epoch: 204620 | training loss: 2.0029e-03 | validation loss: 1.6178e-03\n",
      "Epoch: 204630 | training loss: 2.0026e-03 | validation loss: 1.6194e-03\n",
      "Epoch: 204640 | training loss: 2.0033e-03 | validation loss: 1.6233e-03\n",
      "Epoch: 204650 | training loss: 2.0386e-03 | validation loss: 1.6633e-03\n",
      "Epoch: 204660 | training loss: 3.3671e-03 | validation loss: 2.4388e-03\n",
      "Epoch: 204670 | training loss: 2.3193e-03 | validation loss: 1.7025e-03\n",
      "Epoch: 204680 | training loss: 2.0606e-03 | validation loss: 1.6179e-03\n",
      "Epoch: 204690 | training loss: 2.0353e-03 | validation loss: 1.6590e-03\n",
      "Epoch: 204700 | training loss: 2.0111e-03 | validation loss: 1.6354e-03\n",
      "Epoch: 204710 | training loss: 2.0084e-03 | validation loss: 1.6118e-03\n",
      "Epoch: 204720 | training loss: 2.0024e-03 | validation loss: 1.6181e-03\n",
      "Epoch: 204730 | training loss: 2.0031e-03 | validation loss: 1.6222e-03\n",
      "Epoch: 204740 | training loss: 2.0027e-03 | validation loss: 1.6162e-03\n",
      "Epoch: 204750 | training loss: 2.0024e-03 | validation loss: 1.6201e-03\n",
      "Epoch: 204760 | training loss: 2.0023e-03 | validation loss: 1.6179e-03\n",
      "Epoch: 204770 | training loss: 2.0023e-03 | validation loss: 1.6192e-03\n",
      "Epoch: 204780 | training loss: 2.0023e-03 | validation loss: 1.6182e-03\n",
      "Epoch: 204790 | training loss: 2.0022e-03 | validation loss: 1.6186e-03\n",
      "Epoch: 204800 | training loss: 2.0022e-03 | validation loss: 1.6187e-03\n",
      "Epoch: 204810 | training loss: 2.0022e-03 | validation loss: 1.6185e-03\n",
      "Epoch: 204820 | training loss: 2.0022e-03 | validation loss: 1.6184e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 204830 | training loss: 2.0022e-03 | validation loss: 1.6183e-03\n",
      "Epoch: 204840 | training loss: 2.0022e-03 | validation loss: 1.6174e-03\n",
      "Epoch: 204850 | training loss: 2.0068e-03 | validation loss: 1.6119e-03\n",
      "Epoch: 204860 | training loss: 2.5866e-03 | validation loss: 1.8013e-03\n",
      "Epoch: 204870 | training loss: 2.5571e-03 | validation loss: 1.9884e-03\n",
      "Epoch: 204880 | training loss: 2.0253e-03 | validation loss: 1.6440e-03\n",
      "Epoch: 204890 | training loss: 2.0482e-03 | validation loss: 1.6217e-03\n",
      "Epoch: 204900 | training loss: 2.0337e-03 | validation loss: 1.6071e-03\n",
      "Epoch: 204910 | training loss: 2.0164e-03 | validation loss: 1.6066e-03\n",
      "Epoch: 204920 | training loss: 2.0035e-03 | validation loss: 1.6129e-03\n",
      "Epoch: 204930 | training loss: 2.0026e-03 | validation loss: 1.6227e-03\n",
      "Epoch: 204940 | training loss: 2.0026e-03 | validation loss: 1.6230e-03\n",
      "Epoch: 204950 | training loss: 2.0020e-03 | validation loss: 1.6182e-03\n",
      "Epoch: 204960 | training loss: 2.0020e-03 | validation loss: 1.6170e-03\n",
      "Epoch: 204970 | training loss: 2.0019e-03 | validation loss: 1.6185e-03\n",
      "Epoch: 204980 | training loss: 2.0019e-03 | validation loss: 1.6186e-03\n",
      "Epoch: 204990 | training loss: 2.0019e-03 | validation loss: 1.6179e-03\n",
      "Epoch: 205000 | training loss: 2.0018e-03 | validation loss: 1.6183e-03\n",
      "Epoch: 205010 | training loss: 2.0018e-03 | validation loss: 1.6181e-03\n",
      "Epoch: 205020 | training loss: 2.0018e-03 | validation loss: 1.6182e-03\n",
      "Epoch: 205030 | training loss: 2.0018e-03 | validation loss: 1.6181e-03\n",
      "Epoch: 205040 | training loss: 2.0017e-03 | validation loss: 1.6181e-03\n",
      "Epoch: 205050 | training loss: 2.0017e-03 | validation loss: 1.6181e-03\n",
      "Epoch: 205060 | training loss: 2.0017e-03 | validation loss: 1.6180e-03\n",
      "Epoch: 205070 | training loss: 2.0017e-03 | validation loss: 1.6179e-03\n",
      "Epoch: 205080 | training loss: 2.0019e-03 | validation loss: 1.6173e-03\n",
      "Epoch: 205090 | training loss: 2.0541e-03 | validation loss: 1.6404e-03\n",
      "Epoch: 205100 | training loss: 2.1826e-03 | validation loss: 1.7488e-03\n",
      "Epoch: 205110 | training loss: 2.1874e-03 | validation loss: 1.7362e-03\n",
      "Epoch: 205120 | training loss: 2.0864e-03 | validation loss: 1.6205e-03\n",
      "Epoch: 205130 | training loss: 2.0191e-03 | validation loss: 1.6345e-03\n",
      "Epoch: 205140 | training loss: 2.0044e-03 | validation loss: 1.6213e-03\n",
      "Epoch: 205150 | training loss: 2.0057e-03 | validation loss: 1.6098e-03\n",
      "Epoch: 205160 | training loss: 2.0055e-03 | validation loss: 1.6100e-03\n",
      "Epoch: 205170 | training loss: 2.0097e-03 | validation loss: 1.6081e-03\n",
      "Epoch: 205180 | training loss: 2.0803e-03 | validation loss: 1.6156e-03\n",
      "Epoch: 205190 | training loss: 2.5455e-03 | validation loss: 1.7763e-03\n",
      "Epoch: 205200 | training loss: 2.1555e-03 | validation loss: 1.7435e-03\n",
      "Epoch: 205210 | training loss: 2.0547e-03 | validation loss: 1.6110e-03\n",
      "Epoch: 205220 | training loss: 2.0171e-03 | validation loss: 1.6415e-03\n",
      "Epoch: 205230 | training loss: 2.0030e-03 | validation loss: 1.6127e-03\n",
      "Epoch: 205240 | training loss: 2.0021e-03 | validation loss: 1.6142e-03\n",
      "Epoch: 205250 | training loss: 2.0033e-03 | validation loss: 1.6245e-03\n",
      "Epoch: 205260 | training loss: 2.0015e-03 | validation loss: 1.6195e-03\n",
      "Epoch: 205270 | training loss: 2.0013e-03 | validation loss: 1.6166e-03\n",
      "Epoch: 205280 | training loss: 2.0018e-03 | validation loss: 1.6145e-03\n",
      "Epoch: 205290 | training loss: 2.0116e-03 | validation loss: 1.6086e-03\n",
      "Epoch: 205300 | training loss: 2.4111e-03 | validation loss: 1.7278e-03\n",
      "Epoch: 205310 | training loss: 2.0136e-03 | validation loss: 1.6421e-03\n",
      "Epoch: 205320 | training loss: 2.1634e-03 | validation loss: 1.6468e-03\n",
      "Epoch: 205330 | training loss: 2.0477e-03 | validation loss: 1.6643e-03\n",
      "Epoch: 205340 | training loss: 2.0018e-03 | validation loss: 1.6181e-03\n",
      "Epoch: 205350 | training loss: 2.0079e-03 | validation loss: 1.6101e-03\n",
      "Epoch: 205360 | training loss: 2.0049e-03 | validation loss: 1.6272e-03\n",
      "Epoch: 205370 | training loss: 2.0024e-03 | validation loss: 1.6138e-03\n",
      "Epoch: 205380 | training loss: 2.0016e-03 | validation loss: 1.6200e-03\n",
      "Epoch: 205390 | training loss: 2.0013e-03 | validation loss: 1.6156e-03\n",
      "Epoch: 205400 | training loss: 2.0011e-03 | validation loss: 1.6183e-03\n",
      "Epoch: 205410 | training loss: 2.0010e-03 | validation loss: 1.6172e-03\n",
      "Epoch: 205420 | training loss: 2.0010e-03 | validation loss: 1.6167e-03\n",
      "Epoch: 205430 | training loss: 2.0010e-03 | validation loss: 1.6168e-03\n",
      "Epoch: 205440 | training loss: 2.0010e-03 | validation loss: 1.6167e-03\n",
      "Epoch: 205450 | training loss: 2.0011e-03 | validation loss: 1.6156e-03\n",
      "Epoch: 205460 | training loss: 2.0068e-03 | validation loss: 1.6106e-03\n",
      "Epoch: 205470 | training loss: 2.4217e-03 | validation loss: 1.7404e-03\n",
      "Epoch: 205480 | training loss: 2.0682e-03 | validation loss: 1.6867e-03\n",
      "Epoch: 205490 | training loss: 2.2621e-03 | validation loss: 1.6811e-03\n",
      "Epoch: 205500 | training loss: 2.0396e-03 | validation loss: 1.6058e-03\n",
      "Epoch: 205510 | training loss: 2.0141e-03 | validation loss: 1.6368e-03\n",
      "Epoch: 205520 | training loss: 2.0114e-03 | validation loss: 1.6382e-03\n",
      "Epoch: 205530 | training loss: 2.0022e-03 | validation loss: 1.6122e-03\n",
      "Epoch: 205540 | training loss: 2.0016e-03 | validation loss: 1.6135e-03\n",
      "Epoch: 205550 | training loss: 2.0014e-03 | validation loss: 1.6209e-03\n",
      "Epoch: 205560 | training loss: 2.0008e-03 | validation loss: 1.6162e-03\n",
      "Epoch: 205570 | training loss: 2.0007e-03 | validation loss: 1.6168e-03\n",
      "Epoch: 205580 | training loss: 2.0007e-03 | validation loss: 1.6174e-03\n",
      "Epoch: 205590 | training loss: 2.0007e-03 | validation loss: 1.6166e-03\n",
      "Epoch: 205600 | training loss: 2.0006e-03 | validation loss: 1.6169e-03\n",
      "Epoch: 205610 | training loss: 2.0006e-03 | validation loss: 1.6170e-03\n",
      "Epoch: 205620 | training loss: 2.0006e-03 | validation loss: 1.6167e-03\n",
      "Epoch: 205630 | training loss: 2.0006e-03 | validation loss: 1.6167e-03\n",
      "Epoch: 205640 | training loss: 2.0006e-03 | validation loss: 1.6161e-03\n",
      "Epoch: 205650 | training loss: 2.0082e-03 | validation loss: 1.6139e-03\n",
      "Epoch: 205660 | training loss: 2.6495e-03 | validation loss: 1.9886e-03\n",
      "Epoch: 205670 | training loss: 2.0462e-03 | validation loss: 1.6439e-03\n",
      "Epoch: 205680 | training loss: 2.0857e-03 | validation loss: 1.6653e-03\n",
      "Epoch: 205690 | training loss: 2.0904e-03 | validation loss: 1.6395e-03\n",
      "Epoch: 205700 | training loss: 2.1878e-03 | validation loss: 1.6526e-03\n",
      "Epoch: 205710 | training loss: 2.0231e-03 | validation loss: 1.6087e-03\n",
      "Epoch: 205720 | training loss: 2.0327e-03 | validation loss: 1.6561e-03\n",
      "Epoch: 205730 | training loss: 2.0180e-03 | validation loss: 1.6413e-03\n",
      "Epoch: 205740 | training loss: 2.0032e-03 | validation loss: 1.6238e-03\n",
      "Epoch: 205750 | training loss: 2.0033e-03 | validation loss: 1.6252e-03\n",
      "Epoch: 205760 | training loss: 2.0382e-03 | validation loss: 1.6618e-03\n",
      "Epoch: 205770 | training loss: 2.6886e-03 | validation loss: 2.0608e-03\n",
      "Epoch: 205780 | training loss: 2.1884e-03 | validation loss: 1.6491e-03\n",
      "Epoch: 205790 | training loss: 2.1081e-03 | validation loss: 1.7087e-03\n",
      "Epoch: 205800 | training loss: 2.0399e-03 | validation loss: 1.6084e-03\n",
      "Epoch: 205810 | training loss: 2.0124e-03 | validation loss: 1.6361e-03\n",
      "Epoch: 205820 | training loss: 2.0049e-03 | validation loss: 1.6101e-03\n",
      "Epoch: 205830 | training loss: 2.0025e-03 | validation loss: 1.6236e-03\n",
      "Epoch: 205840 | training loss: 2.0009e-03 | validation loss: 1.6129e-03\n",
      "Epoch: 205850 | training loss: 2.0001e-03 | validation loss: 1.6161e-03\n",
      "Epoch: 205860 | training loss: 2.0004e-03 | validation loss: 1.6183e-03\n",
      "Epoch: 205870 | training loss: 2.0002e-03 | validation loss: 1.6176e-03\n",
      "Epoch: 205880 | training loss: 2.0003e-03 | validation loss: 1.6183e-03\n",
      "Epoch: 205890 | training loss: 2.0033e-03 | validation loss: 1.6250e-03\n",
      "Epoch: 205900 | training loss: 2.1168e-03 | validation loss: 1.7158e-03\n",
      "Epoch: 205910 | training loss: 2.8143e-03 | validation loss: 2.1172e-03\n",
      "Epoch: 205920 | training loss: 2.0338e-03 | validation loss: 1.6214e-03\n",
      "Epoch: 205930 | training loss: 2.0642e-03 | validation loss: 1.6134e-03\n",
      "Epoch: 205940 | training loss: 2.0317e-03 | validation loss: 1.6474e-03\n",
      "Epoch: 205950 | training loss: 2.0010e-03 | validation loss: 1.6187e-03\n",
      "Epoch: 205960 | training loss: 2.0018e-03 | validation loss: 1.6116e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 205970 | training loss: 2.0016e-03 | validation loss: 1.6206e-03\n",
      "Epoch: 205980 | training loss: 2.0006e-03 | validation loss: 1.6144e-03\n",
      "Epoch: 205990 | training loss: 2.0002e-03 | validation loss: 1.6174e-03\n",
      "Epoch: 206000 | training loss: 1.9999e-03 | validation loss: 1.6155e-03\n",
      "Epoch: 206010 | training loss: 1.9998e-03 | validation loss: 1.6159e-03\n",
      "Epoch: 206020 | training loss: 1.9998e-03 | validation loss: 1.6163e-03\n",
      "Epoch: 206030 | training loss: 1.9998e-03 | validation loss: 1.6159e-03\n",
      "Epoch: 206040 | training loss: 1.9998e-03 | validation loss: 1.6156e-03\n",
      "Epoch: 206050 | training loss: 1.9998e-03 | validation loss: 1.6151e-03\n",
      "Epoch: 206060 | training loss: 2.0006e-03 | validation loss: 1.6132e-03\n",
      "Epoch: 206070 | training loss: 2.0420e-03 | validation loss: 1.6159e-03\n",
      "Epoch: 206080 | training loss: 3.4754e-03 | validation loss: 2.1990e-03\n",
      "Epoch: 206090 | training loss: 2.2592e-03 | validation loss: 1.8043e-03\n",
      "Epoch: 206100 | training loss: 2.1290e-03 | validation loss: 1.6900e-03\n",
      "Epoch: 206110 | training loss: 2.0039e-03 | validation loss: 1.6081e-03\n",
      "Epoch: 206120 | training loss: 2.0147e-03 | validation loss: 1.6339e-03\n",
      "Epoch: 206130 | training loss: 2.0051e-03 | validation loss: 1.6159e-03\n",
      "Epoch: 206140 | training loss: 2.0000e-03 | validation loss: 1.6174e-03\n",
      "Epoch: 206150 | training loss: 2.0004e-03 | validation loss: 1.6152e-03\n",
      "Epoch: 206160 | training loss: 1.9997e-03 | validation loss: 1.6167e-03\n",
      "Epoch: 206170 | training loss: 1.9996e-03 | validation loss: 1.6151e-03\n",
      "Epoch: 206180 | training loss: 1.9995e-03 | validation loss: 1.6159e-03\n",
      "Epoch: 206190 | training loss: 1.9995e-03 | validation loss: 1.6159e-03\n",
      "Epoch: 206200 | training loss: 1.9995e-03 | validation loss: 1.6156e-03\n",
      "Epoch: 206210 | training loss: 1.9994e-03 | validation loss: 1.6155e-03\n",
      "Epoch: 206220 | training loss: 1.9994e-03 | validation loss: 1.6152e-03\n",
      "Epoch: 206230 | training loss: 1.9995e-03 | validation loss: 1.6137e-03\n",
      "Epoch: 206240 | training loss: 2.0096e-03 | validation loss: 1.6045e-03\n",
      "Epoch: 206250 | training loss: 3.0014e-03 | validation loss: 2.0095e-03\n",
      "Epoch: 206260 | training loss: 2.3559e-03 | validation loss: 1.9039e-03\n",
      "Epoch: 206270 | training loss: 2.1727e-03 | validation loss: 1.7618e-03\n",
      "Epoch: 206280 | training loss: 2.0521e-03 | validation loss: 1.6777e-03\n",
      "Epoch: 206290 | training loss: 2.0203e-03 | validation loss: 1.6502e-03\n",
      "Epoch: 206300 | training loss: 2.0056e-03 | validation loss: 1.6227e-03\n",
      "Epoch: 206310 | training loss: 2.0002e-03 | validation loss: 1.6206e-03\n",
      "Epoch: 206320 | training loss: 1.9993e-03 | validation loss: 1.6138e-03\n",
      "Epoch: 206330 | training loss: 1.9994e-03 | validation loss: 1.6142e-03\n",
      "Epoch: 206340 | training loss: 1.9993e-03 | validation loss: 1.6133e-03\n",
      "Epoch: 206350 | training loss: 1.9992e-03 | validation loss: 1.6153e-03\n",
      "Epoch: 206360 | training loss: 1.9992e-03 | validation loss: 1.6154e-03\n",
      "Epoch: 206370 | training loss: 1.9991e-03 | validation loss: 1.6148e-03\n",
      "Epoch: 206380 | training loss: 1.9991e-03 | validation loss: 1.6145e-03\n",
      "Epoch: 206390 | training loss: 1.9991e-03 | validation loss: 1.6144e-03\n",
      "Epoch: 206400 | training loss: 1.9995e-03 | validation loss: 1.6127e-03\n",
      "Epoch: 206410 | training loss: 2.0223e-03 | validation loss: 1.6092e-03\n",
      "Epoch: 206420 | training loss: 3.3370e-03 | validation loss: 2.1320e-03\n",
      "Epoch: 206430 | training loss: 2.5620e-03 | validation loss: 1.9763e-03\n",
      "Epoch: 206440 | training loss: 2.0450e-03 | validation loss: 1.6600e-03\n",
      "Epoch: 206450 | training loss: 2.0328e-03 | validation loss: 1.6064e-03\n",
      "Epoch: 206460 | training loss: 2.0199e-03 | validation loss: 1.6063e-03\n",
      "Epoch: 206470 | training loss: 2.0004e-03 | validation loss: 1.6195e-03\n",
      "Epoch: 206480 | training loss: 2.0017e-03 | validation loss: 1.6225e-03\n",
      "Epoch: 206490 | training loss: 1.9996e-03 | validation loss: 1.6126e-03\n",
      "Epoch: 206500 | training loss: 1.9989e-03 | validation loss: 1.6143e-03\n",
      "Epoch: 206510 | training loss: 1.9990e-03 | validation loss: 1.6160e-03\n",
      "Epoch: 206520 | training loss: 1.9989e-03 | validation loss: 1.6139e-03\n",
      "Epoch: 206530 | training loss: 1.9988e-03 | validation loss: 1.6153e-03\n",
      "Epoch: 206540 | training loss: 1.9988e-03 | validation loss: 1.6143e-03\n",
      "Epoch: 206550 | training loss: 1.9988e-03 | validation loss: 1.6149e-03\n",
      "Epoch: 206560 | training loss: 1.9988e-03 | validation loss: 1.6146e-03\n",
      "Epoch: 206570 | training loss: 1.9987e-03 | validation loss: 1.6146e-03\n",
      "Epoch: 206580 | training loss: 1.9987e-03 | validation loss: 1.6147e-03\n",
      "Epoch: 206590 | training loss: 1.9987e-03 | validation loss: 1.6147e-03\n",
      "Epoch: 206600 | training loss: 1.9987e-03 | validation loss: 1.6148e-03\n",
      "Epoch: 206610 | training loss: 1.9987e-03 | validation loss: 1.6153e-03\n",
      "Epoch: 206620 | training loss: 2.0009e-03 | validation loss: 1.6210e-03\n",
      "Epoch: 206630 | training loss: 2.2261e-03 | validation loss: 1.7782e-03\n",
      "Epoch: 206640 | training loss: 2.0576e-03 | validation loss: 1.6761e-03\n",
      "Epoch: 206650 | training loss: 2.3985e-03 | validation loss: 1.8767e-03\n",
      "Epoch: 206660 | training loss: 2.0903e-03 | validation loss: 1.6870e-03\n",
      "Epoch: 206670 | training loss: 1.9996e-03 | validation loss: 1.6163e-03\n",
      "Epoch: 206680 | training loss: 2.0074e-03 | validation loss: 1.6110e-03\n",
      "Epoch: 206690 | training loss: 2.0047e-03 | validation loss: 1.6099e-03\n",
      "Epoch: 206700 | training loss: 1.9985e-03 | validation loss: 1.6141e-03\n",
      "Epoch: 206710 | training loss: 1.9993e-03 | validation loss: 1.6179e-03\n",
      "Epoch: 206720 | training loss: 1.9984e-03 | validation loss: 1.6147e-03\n",
      "Epoch: 206730 | training loss: 1.9985e-03 | validation loss: 1.6136e-03\n",
      "Epoch: 206740 | training loss: 1.9984e-03 | validation loss: 1.6148e-03\n",
      "Epoch: 206750 | training loss: 1.9984e-03 | validation loss: 1.6144e-03\n",
      "Epoch: 206760 | training loss: 1.9984e-03 | validation loss: 1.6142e-03\n",
      "Epoch: 206770 | training loss: 1.9983e-03 | validation loss: 1.6145e-03\n",
      "Epoch: 206780 | training loss: 1.9983e-03 | validation loss: 1.6142e-03\n",
      "Epoch: 206790 | training loss: 1.9983e-03 | validation loss: 1.6143e-03\n",
      "Epoch: 206800 | training loss: 1.9983e-03 | validation loss: 1.6144e-03\n",
      "Epoch: 206810 | training loss: 1.9983e-03 | validation loss: 1.6155e-03\n",
      "Epoch: 206820 | training loss: 2.0081e-03 | validation loss: 1.6349e-03\n",
      "Epoch: 206830 | training loss: 2.8558e-03 | validation loss: 2.3207e-03\n",
      "Epoch: 206840 | training loss: 2.0093e-03 | validation loss: 1.6126e-03\n",
      "Epoch: 206850 | training loss: 2.0039e-03 | validation loss: 1.6140e-03\n",
      "Epoch: 206860 | training loss: 2.0050e-03 | validation loss: 1.6150e-03\n",
      "Epoch: 206870 | training loss: 2.0058e-03 | validation loss: 1.6051e-03\n",
      "Epoch: 206880 | training loss: 2.0028e-03 | validation loss: 1.6178e-03\n",
      "Epoch: 206890 | training loss: 1.9996e-03 | validation loss: 1.6137e-03\n",
      "Epoch: 206900 | training loss: 1.9985e-03 | validation loss: 1.6109e-03\n",
      "Epoch: 206910 | training loss: 1.9990e-03 | validation loss: 1.6100e-03\n",
      "Epoch: 206920 | training loss: 2.0087e-03 | validation loss: 1.6052e-03\n",
      "Epoch: 206930 | training loss: 2.2716e-03 | validation loss: 1.6732e-03\n",
      "Epoch: 206940 | training loss: 2.0719e-03 | validation loss: 1.6164e-03\n",
      "Epoch: 206950 | training loss: 2.0019e-03 | validation loss: 1.6071e-03\n",
      "Epoch: 206960 | training loss: 2.0320e-03 | validation loss: 1.6527e-03\n",
      "Epoch: 206970 | training loss: 2.0210e-03 | validation loss: 1.6042e-03\n",
      "Epoch: 206980 | training loss: 2.0071e-03 | validation loss: 1.6299e-03\n",
      "Epoch: 206990 | training loss: 2.0014e-03 | validation loss: 1.6079e-03\n",
      "Epoch: 207000 | training loss: 1.9992e-03 | validation loss: 1.6191e-03\n",
      "Epoch: 207010 | training loss: 1.9981e-03 | validation loss: 1.6116e-03\n",
      "Epoch: 207020 | training loss: 1.9979e-03 | validation loss: 1.6129e-03\n",
      "Epoch: 207030 | training loss: 1.9979e-03 | validation loss: 1.6150e-03\n",
      "Epoch: 207040 | training loss: 1.9979e-03 | validation loss: 1.6147e-03\n",
      "Epoch: 207050 | training loss: 1.9980e-03 | validation loss: 1.6153e-03\n",
      "Epoch: 207060 | training loss: 1.9998e-03 | validation loss: 1.6203e-03\n",
      "Epoch: 207070 | training loss: 2.0693e-03 | validation loss: 1.6819e-03\n",
      "Epoch: 207080 | training loss: 3.1561e-03 | validation loss: 2.3032e-03\n",
      "Epoch: 207090 | training loss: 2.1820e-03 | validation loss: 1.6586e-03\n",
      "Epoch: 207100 | training loss: 2.0239e-03 | validation loss: 1.6019e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 207110 | training loss: 2.0422e-03 | validation loss: 1.6561e-03\n",
      "Epoch: 207120 | training loss: 2.0008e-03 | validation loss: 1.6136e-03\n",
      "Epoch: 207130 | training loss: 1.9990e-03 | validation loss: 1.6093e-03\n",
      "Epoch: 207140 | training loss: 1.9995e-03 | validation loss: 1.6182e-03\n",
      "Epoch: 207150 | training loss: 1.9985e-03 | validation loss: 1.6116e-03\n",
      "Epoch: 207160 | training loss: 1.9979e-03 | validation loss: 1.6150e-03\n",
      "Epoch: 207170 | training loss: 1.9977e-03 | validation loss: 1.6126e-03\n",
      "Epoch: 207180 | training loss: 1.9976e-03 | validation loss: 1.6136e-03\n",
      "Epoch: 207190 | training loss: 1.9975e-03 | validation loss: 1.6136e-03\n",
      "Epoch: 207200 | training loss: 1.9975e-03 | validation loss: 1.6129e-03\n",
      "Epoch: 207210 | training loss: 1.9975e-03 | validation loss: 1.6129e-03\n",
      "Epoch: 207220 | training loss: 1.9975e-03 | validation loss: 1.6129e-03\n",
      "Epoch: 207230 | training loss: 1.9975e-03 | validation loss: 1.6123e-03\n",
      "Epoch: 207240 | training loss: 2.0005e-03 | validation loss: 1.6090e-03\n",
      "Epoch: 207250 | training loss: 2.2203e-03 | validation loss: 1.6737e-03\n",
      "Epoch: 207260 | training loss: 2.1656e-03 | validation loss: 1.6347e-03\n",
      "Epoch: 207270 | training loss: 2.2977e-03 | validation loss: 1.7378e-03\n",
      "Epoch: 207280 | training loss: 2.0216e-03 | validation loss: 1.6452e-03\n",
      "Epoch: 207290 | training loss: 2.0108e-03 | validation loss: 1.6201e-03\n",
      "Epoch: 207300 | training loss: 2.0123e-03 | validation loss: 1.6205e-03\n",
      "Epoch: 207310 | training loss: 1.9983e-03 | validation loss: 1.6168e-03\n",
      "Epoch: 207320 | training loss: 1.9987e-03 | validation loss: 1.6119e-03\n",
      "Epoch: 207330 | training loss: 1.9975e-03 | validation loss: 1.6144e-03\n",
      "Epoch: 207340 | training loss: 1.9974e-03 | validation loss: 1.6127e-03\n",
      "Epoch: 207350 | training loss: 1.9972e-03 | validation loss: 1.6135e-03\n",
      "Epoch: 207360 | training loss: 1.9972e-03 | validation loss: 1.6130e-03\n",
      "Epoch: 207370 | training loss: 1.9972e-03 | validation loss: 1.6128e-03\n",
      "Epoch: 207380 | training loss: 1.9971e-03 | validation loss: 1.6129e-03\n",
      "Epoch: 207390 | training loss: 1.9971e-03 | validation loss: 1.6130e-03\n",
      "Epoch: 207400 | training loss: 1.9971e-03 | validation loss: 1.6131e-03\n",
      "Epoch: 207410 | training loss: 1.9971e-03 | validation loss: 1.6138e-03\n",
      "Epoch: 207420 | training loss: 1.9989e-03 | validation loss: 1.6210e-03\n",
      "Epoch: 207430 | training loss: 2.2250e-03 | validation loss: 1.8259e-03\n",
      "Epoch: 207440 | training loss: 2.1703e-03 | validation loss: 1.7054e-03\n",
      "Epoch: 207450 | training loss: 2.0287e-03 | validation loss: 1.6561e-03\n",
      "Epoch: 207460 | training loss: 2.0312e-03 | validation loss: 1.6275e-03\n",
      "Epoch: 207470 | training loss: 2.0062e-03 | validation loss: 1.6326e-03\n",
      "Epoch: 207480 | training loss: 2.0012e-03 | validation loss: 1.6231e-03\n",
      "Epoch: 207490 | training loss: 1.9998e-03 | validation loss: 1.6174e-03\n",
      "Epoch: 207500 | training loss: 1.9983e-03 | validation loss: 1.6188e-03\n",
      "Epoch: 207510 | training loss: 1.9972e-03 | validation loss: 1.6132e-03\n",
      "Epoch: 207520 | training loss: 1.9969e-03 | validation loss: 1.6129e-03\n",
      "Epoch: 207530 | training loss: 1.9969e-03 | validation loss: 1.6120e-03\n",
      "Epoch: 207540 | training loss: 1.9969e-03 | validation loss: 1.6115e-03\n",
      "Epoch: 207550 | training loss: 1.9968e-03 | validation loss: 1.6123e-03\n",
      "Epoch: 207560 | training loss: 1.9968e-03 | validation loss: 1.6125e-03\n",
      "Epoch: 207570 | training loss: 1.9968e-03 | validation loss: 1.6124e-03\n",
      "Epoch: 207580 | training loss: 1.9968e-03 | validation loss: 1.6133e-03\n",
      "Epoch: 207590 | training loss: 2.0008e-03 | validation loss: 1.6217e-03\n",
      "Epoch: 207600 | training loss: 2.3986e-03 | validation loss: 1.8799e-03\n",
      "Epoch: 207610 | training loss: 2.1096e-03 | validation loss: 1.6275e-03\n",
      "Epoch: 207620 | training loss: 2.1952e-03 | validation loss: 1.7599e-03\n",
      "Epoch: 207630 | training loss: 2.1144e-03 | validation loss: 1.7087e-03\n",
      "Epoch: 207640 | training loss: 2.0063e-03 | validation loss: 1.6279e-03\n",
      "Epoch: 207650 | training loss: 2.0008e-03 | validation loss: 1.6068e-03\n",
      "Epoch: 207660 | training loss: 2.0021e-03 | validation loss: 1.6063e-03\n",
      "Epoch: 207670 | training loss: 1.9966e-03 | validation loss: 1.6121e-03\n",
      "Epoch: 207680 | training loss: 1.9973e-03 | validation loss: 1.6158e-03\n",
      "Epoch: 207690 | training loss: 1.9966e-03 | validation loss: 1.6118e-03\n",
      "Epoch: 207700 | training loss: 1.9966e-03 | validation loss: 1.6113e-03\n",
      "Epoch: 207710 | training loss: 1.9965e-03 | validation loss: 1.6128e-03\n",
      "Epoch: 207720 | training loss: 1.9965e-03 | validation loss: 1.6118e-03\n",
      "Epoch: 207730 | training loss: 1.9965e-03 | validation loss: 1.6122e-03\n",
      "Epoch: 207740 | training loss: 1.9964e-03 | validation loss: 1.6120e-03\n",
      "Epoch: 207750 | training loss: 1.9964e-03 | validation loss: 1.6121e-03\n",
      "Epoch: 207760 | training loss: 1.9964e-03 | validation loss: 1.6119e-03\n",
      "Epoch: 207770 | training loss: 1.9964e-03 | validation loss: 1.6120e-03\n",
      "Epoch: 207780 | training loss: 1.9964e-03 | validation loss: 1.6120e-03\n",
      "Epoch: 207790 | training loss: 1.9963e-03 | validation loss: 1.6119e-03\n",
      "Epoch: 207800 | training loss: 1.9963e-03 | validation loss: 1.6119e-03\n",
      "Epoch: 207810 | training loss: 1.9963e-03 | validation loss: 1.6119e-03\n",
      "Epoch: 207820 | training loss: 1.9963e-03 | validation loss: 1.6116e-03\n",
      "Epoch: 207830 | training loss: 1.9966e-03 | validation loss: 1.6099e-03\n",
      "Epoch: 207840 | training loss: 2.0540e-03 | validation loss: 1.6122e-03\n",
      "Epoch: 207850 | training loss: 4.0030e-03 | validation loss: 2.4132e-03\n",
      "Epoch: 207860 | training loss: 2.5471e-03 | validation loss: 1.7968e-03\n",
      "Epoch: 207870 | training loss: 2.1585e-03 | validation loss: 1.6477e-03\n",
      "Epoch: 207880 | training loss: 2.0518e-03 | validation loss: 1.6135e-03\n",
      "Epoch: 207890 | training loss: 2.0135e-03 | validation loss: 1.6061e-03\n",
      "Epoch: 207900 | training loss: 2.0001e-03 | validation loss: 1.6073e-03\n",
      "Epoch: 207910 | training loss: 1.9964e-03 | validation loss: 1.6104e-03\n",
      "Epoch: 207920 | training loss: 1.9962e-03 | validation loss: 1.6128e-03\n",
      "Epoch: 207930 | training loss: 1.9964e-03 | validation loss: 1.6140e-03\n",
      "Epoch: 207940 | training loss: 1.9962e-03 | validation loss: 1.6134e-03\n",
      "Epoch: 207950 | training loss: 1.9960e-03 | validation loss: 1.6120e-03\n",
      "Epoch: 207960 | training loss: 1.9960e-03 | validation loss: 1.6114e-03\n",
      "Epoch: 207970 | training loss: 1.9960e-03 | validation loss: 1.6115e-03\n",
      "Epoch: 207980 | training loss: 1.9960e-03 | validation loss: 1.6118e-03\n",
      "Epoch: 207990 | training loss: 1.9960e-03 | validation loss: 1.6117e-03\n",
      "Epoch: 208000 | training loss: 1.9959e-03 | validation loss: 1.6116e-03\n",
      "Epoch: 208010 | training loss: 1.9959e-03 | validation loss: 1.6116e-03\n",
      "Epoch: 208020 | training loss: 1.9959e-03 | validation loss: 1.6116e-03\n",
      "Epoch: 208030 | training loss: 1.9959e-03 | validation loss: 1.6115e-03\n",
      "Epoch: 208040 | training loss: 1.9959e-03 | validation loss: 1.6114e-03\n",
      "Epoch: 208050 | training loss: 1.9959e-03 | validation loss: 1.6102e-03\n",
      "Epoch: 208060 | training loss: 2.0215e-03 | validation loss: 1.6081e-03\n",
      "Epoch: 208070 | training loss: 2.5209e-03 | validation loss: 1.8368e-03\n",
      "Epoch: 208080 | training loss: 2.2496e-03 | validation loss: 1.7661e-03\n",
      "Epoch: 208090 | training loss: 2.0822e-03 | validation loss: 1.6302e-03\n",
      "Epoch: 208100 | training loss: 2.0259e-03 | validation loss: 1.6274e-03\n",
      "Epoch: 208110 | training loss: 2.0071e-03 | validation loss: 1.6038e-03\n",
      "Epoch: 208120 | training loss: 2.0000e-03 | validation loss: 1.6142e-03\n",
      "Epoch: 208130 | training loss: 1.9968e-03 | validation loss: 1.6083e-03\n",
      "Epoch: 208140 | training loss: 1.9961e-03 | validation loss: 1.6083e-03\n",
      "Epoch: 208150 | training loss: 1.9957e-03 | validation loss: 1.6109e-03\n",
      "Epoch: 208160 | training loss: 1.9957e-03 | validation loss: 1.6124e-03\n",
      "Epoch: 208170 | training loss: 1.9962e-03 | validation loss: 1.6148e-03\n",
      "Epoch: 208180 | training loss: 2.0095e-03 | validation loss: 1.6343e-03\n",
      "Epoch: 208190 | training loss: 2.6139e-03 | validation loss: 2.0190e-03\n",
      "Epoch: 208200 | training loss: 2.2117e-03 | validation loss: 1.6499e-03\n",
      "Epoch: 208210 | training loss: 2.1714e-03 | validation loss: 1.7444e-03\n",
      "Epoch: 208220 | training loss: 1.9960e-03 | validation loss: 1.6072e-03\n",
      "Epoch: 208230 | training loss: 2.0150e-03 | validation loss: 1.6017e-03\n",
      "Epoch: 208240 | training loss: 2.0024e-03 | validation loss: 1.6252e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 208250 | training loss: 1.9958e-03 | validation loss: 1.6092e-03\n",
      "Epoch: 208260 | training loss: 1.9955e-03 | validation loss: 1.6103e-03\n",
      "Epoch: 208270 | training loss: 1.9955e-03 | validation loss: 1.6116e-03\n",
      "Epoch: 208280 | training loss: 1.9954e-03 | validation loss: 1.6103e-03\n",
      "Epoch: 208290 | training loss: 1.9954e-03 | validation loss: 1.6106e-03\n",
      "Epoch: 208300 | training loss: 1.9954e-03 | validation loss: 1.6114e-03\n",
      "Epoch: 208310 | training loss: 1.9953e-03 | validation loss: 1.6104e-03\n",
      "Epoch: 208320 | training loss: 1.9953e-03 | validation loss: 1.6103e-03\n",
      "Epoch: 208330 | training loss: 1.9953e-03 | validation loss: 1.6103e-03\n",
      "Epoch: 208340 | training loss: 1.9953e-03 | validation loss: 1.6100e-03\n",
      "Epoch: 208350 | training loss: 1.9959e-03 | validation loss: 1.6077e-03\n",
      "Epoch: 208360 | training loss: 2.0312e-03 | validation loss: 1.6038e-03\n",
      "Epoch: 208370 | training loss: 3.5843e-03 | validation loss: 2.2173e-03\n",
      "Epoch: 208380 | training loss: 2.2822e-03 | validation loss: 1.8067e-03\n",
      "Epoch: 208390 | training loss: 2.1309e-03 | validation loss: 1.7299e-03\n",
      "Epoch: 208400 | training loss: 2.0043e-03 | validation loss: 1.6206e-03\n",
      "Epoch: 208410 | training loss: 2.0144e-03 | validation loss: 1.5993e-03\n",
      "Epoch: 208420 | training loss: 1.9978e-03 | validation loss: 1.6046e-03\n",
      "Epoch: 208430 | training loss: 1.9976e-03 | validation loss: 1.6203e-03\n",
      "Epoch: 208440 | training loss: 1.9951e-03 | validation loss: 1.6115e-03\n",
      "Epoch: 208450 | training loss: 1.9954e-03 | validation loss: 1.6076e-03\n",
      "Epoch: 208460 | training loss: 1.9952e-03 | validation loss: 1.6124e-03\n",
      "Epoch: 208470 | training loss: 1.9950e-03 | validation loss: 1.6099e-03\n",
      "Epoch: 208480 | training loss: 1.9950e-03 | validation loss: 1.6106e-03\n",
      "Epoch: 208490 | training loss: 1.9950e-03 | validation loss: 1.6104e-03\n",
      "Epoch: 208500 | training loss: 1.9950e-03 | validation loss: 1.6104e-03\n",
      "Epoch: 208510 | training loss: 1.9949e-03 | validation loss: 1.6102e-03\n",
      "Epoch: 208520 | training loss: 1.9949e-03 | validation loss: 1.6104e-03\n",
      "Epoch: 208530 | training loss: 1.9949e-03 | validation loss: 1.6104e-03\n",
      "Epoch: 208540 | training loss: 1.9949e-03 | validation loss: 1.6105e-03\n",
      "Epoch: 208550 | training loss: 1.9953e-03 | validation loss: 1.6117e-03\n",
      "Epoch: 208560 | training loss: 2.0426e-03 | validation loss: 1.6536e-03\n",
      "Epoch: 208570 | training loss: 2.2289e-03 | validation loss: 1.7216e-03\n",
      "Epoch: 208580 | training loss: 2.3668e-03 | validation loss: 1.7790e-03\n",
      "Epoch: 208590 | training loss: 2.0942e-03 | validation loss: 1.7184e-03\n",
      "Epoch: 208600 | training loss: 2.0042e-03 | validation loss: 1.6167e-03\n",
      "Epoch: 208610 | training loss: 2.0028e-03 | validation loss: 1.6031e-03\n",
      "Epoch: 208620 | training loss: 1.9975e-03 | validation loss: 1.6170e-03\n",
      "Epoch: 208630 | training loss: 2.0003e-03 | validation loss: 1.6202e-03\n",
      "Epoch: 208640 | training loss: 2.0040e-03 | validation loss: 1.6262e-03\n",
      "Epoch: 208650 | training loss: 2.0639e-03 | validation loss: 1.6797e-03\n",
      "Epoch: 208660 | training loss: 2.4873e-03 | validation loss: 1.9442e-03\n",
      "Epoch: 208670 | training loss: 2.0859e-03 | validation loss: 1.6137e-03\n",
      "Epoch: 208680 | training loss: 2.0117e-03 | validation loss: 1.6353e-03\n",
      "Epoch: 208690 | training loss: 1.9950e-03 | validation loss: 1.6069e-03\n",
      "Epoch: 208700 | training loss: 1.9979e-03 | validation loss: 1.6035e-03\n",
      "Epoch: 208710 | training loss: 1.9992e-03 | validation loss: 1.6212e-03\n",
      "Epoch: 208720 | training loss: 1.9946e-03 | validation loss: 1.6110e-03\n",
      "Epoch: 208730 | training loss: 1.9953e-03 | validation loss: 1.6063e-03\n",
      "Epoch: 208740 | training loss: 1.9981e-03 | validation loss: 1.6035e-03\n",
      "Epoch: 208750 | training loss: 2.0360e-03 | validation loss: 1.6027e-03\n",
      "Epoch: 208760 | training loss: 2.6225e-03 | validation loss: 1.8075e-03\n",
      "Epoch: 208770 | training loss: 2.1212e-03 | validation loss: 1.7169e-03\n",
      "Epoch: 208780 | training loss: 2.0670e-03 | validation loss: 1.6128e-03\n",
      "Epoch: 208790 | training loss: 2.0289e-03 | validation loss: 1.6469e-03\n",
      "Epoch: 208800 | training loss: 2.0077e-03 | validation loss: 1.6021e-03\n",
      "Epoch: 208810 | training loss: 1.9983e-03 | validation loss: 1.6196e-03\n",
      "Epoch: 208820 | training loss: 1.9946e-03 | validation loss: 1.6075e-03\n",
      "Epoch: 208830 | training loss: 1.9948e-03 | validation loss: 1.6075e-03\n",
      "Epoch: 208840 | training loss: 1.9946e-03 | validation loss: 1.6118e-03\n",
      "Epoch: 208850 | training loss: 1.9947e-03 | validation loss: 1.6123e-03\n",
      "Epoch: 208860 | training loss: 1.9950e-03 | validation loss: 1.6131e-03\n",
      "Epoch: 208870 | training loss: 2.0017e-03 | validation loss: 1.6236e-03\n",
      "Epoch: 208880 | training loss: 2.2014e-03 | validation loss: 1.7641e-03\n",
      "Epoch: 208890 | training loss: 2.2843e-03 | validation loss: 1.8110e-03\n",
      "Epoch: 208900 | training loss: 1.9965e-03 | validation loss: 1.6112e-03\n",
      "Epoch: 208910 | training loss: 2.0354e-03 | validation loss: 1.6015e-03\n",
      "Epoch: 208920 | training loss: 2.0237e-03 | validation loss: 1.6453e-03\n",
      "Epoch: 208930 | training loss: 2.0020e-03 | validation loss: 1.6025e-03\n",
      "Epoch: 208940 | training loss: 1.9958e-03 | validation loss: 1.6153e-03\n",
      "Epoch: 208950 | training loss: 1.9947e-03 | validation loss: 1.6065e-03\n",
      "Epoch: 208960 | training loss: 1.9945e-03 | validation loss: 1.6128e-03\n",
      "Epoch: 208970 | training loss: 1.9943e-03 | validation loss: 1.6072e-03\n",
      "Epoch: 208980 | training loss: 1.9940e-03 | validation loss: 1.6098e-03\n",
      "Epoch: 208990 | training loss: 1.9941e-03 | validation loss: 1.6102e-03\n",
      "Epoch: 209000 | training loss: 1.9940e-03 | validation loss: 1.6092e-03\n",
      "Epoch: 209010 | training loss: 1.9946e-03 | validation loss: 1.6076e-03\n",
      "Epoch: 209020 | training loss: 2.0295e-03 | validation loss: 1.6166e-03\n",
      "Epoch: 209030 | training loss: 2.2751e-03 | validation loss: 1.7507e-03\n",
      "Epoch: 209040 | training loss: 2.0533e-03 | validation loss: 1.6025e-03\n",
      "Epoch: 209050 | training loss: 2.4119e-03 | validation loss: 1.7420e-03\n",
      "Epoch: 209060 | training loss: 2.0248e-03 | validation loss: 1.6550e-03\n",
      "Epoch: 209070 | training loss: 1.9965e-03 | validation loss: 1.6172e-03\n",
      "Epoch: 209080 | training loss: 2.0115e-03 | validation loss: 1.5971e-03\n",
      "Epoch: 209090 | training loss: 2.0027e-03 | validation loss: 1.6235e-03\n",
      "Epoch: 209100 | training loss: 1.9947e-03 | validation loss: 1.6143e-03\n",
      "Epoch: 209110 | training loss: 1.9949e-03 | validation loss: 1.6058e-03\n",
      "Epoch: 209120 | training loss: 1.9986e-03 | validation loss: 1.6014e-03\n",
      "Epoch: 209130 | training loss: 2.0385e-03 | validation loss: 1.6019e-03\n",
      "Epoch: 209140 | training loss: 2.5399e-03 | validation loss: 1.7732e-03\n",
      "Epoch: 209150 | training loss: 2.0647e-03 | validation loss: 1.6783e-03\n",
      "Epoch: 209160 | training loss: 2.0215e-03 | validation loss: 1.6018e-03\n",
      "Epoch: 209170 | training loss: 2.0028e-03 | validation loss: 1.6238e-03\n",
      "Epoch: 209180 | training loss: 1.9945e-03 | validation loss: 1.6057e-03\n",
      "Epoch: 209190 | training loss: 1.9943e-03 | validation loss: 1.6062e-03\n",
      "Epoch: 209200 | training loss: 1.9959e-03 | validation loss: 1.6156e-03\n",
      "Epoch: 209210 | training loss: 1.9937e-03 | validation loss: 1.6078e-03\n",
      "Epoch: 209220 | training loss: 1.9944e-03 | validation loss: 1.6056e-03\n",
      "Epoch: 209230 | training loss: 1.9954e-03 | validation loss: 1.6044e-03\n",
      "Epoch: 209240 | training loss: 2.0107e-03 | validation loss: 1.6008e-03\n",
      "Epoch: 209250 | training loss: 2.3564e-03 | validation loss: 1.7093e-03\n",
      "Epoch: 209260 | training loss: 2.0053e-03 | validation loss: 1.6068e-03\n",
      "Epoch: 209270 | training loss: 2.0034e-03 | validation loss: 1.5977e-03\n",
      "Epoch: 209280 | training loss: 2.0173e-03 | validation loss: 1.6408e-03\n",
      "Epoch: 209290 | training loss: 2.0084e-03 | validation loss: 1.5998e-03\n",
      "Epoch: 209300 | training loss: 1.9995e-03 | validation loss: 1.6219e-03\n",
      "Epoch: 209310 | training loss: 1.9951e-03 | validation loss: 1.6035e-03\n",
      "Epoch: 209320 | training loss: 1.9936e-03 | validation loss: 1.6112e-03\n",
      "Epoch: 209330 | training loss: 1.9936e-03 | validation loss: 1.6097e-03\n",
      "Epoch: 209340 | training loss: 1.9935e-03 | validation loss: 1.6068e-03\n",
      "Epoch: 209350 | training loss: 1.9934e-03 | validation loss: 1.6076e-03\n",
      "Epoch: 209360 | training loss: 1.9936e-03 | validation loss: 1.6082e-03\n",
      "Epoch: 209370 | training loss: 1.9979e-03 | validation loss: 1.6101e-03\n",
      "Epoch: 209380 | training loss: 2.1346e-03 | validation loss: 1.6834e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 209390 | training loss: 2.5858e-03 | validation loss: 1.7883e-03\n",
      "Epoch: 209400 | training loss: 2.2742e-03 | validation loss: 1.8646e-03\n",
      "Epoch: 209410 | training loss: 2.0528e-03 | validation loss: 1.6178e-03\n",
      "Epoch: 209420 | training loss: 2.0185e-03 | validation loss: 1.6302e-03\n",
      "Epoch: 209430 | training loss: 2.0075e-03 | validation loss: 1.5978e-03\n",
      "Epoch: 209440 | training loss: 1.9960e-03 | validation loss: 1.6150e-03\n",
      "Epoch: 209450 | training loss: 1.9949e-03 | validation loss: 1.6083e-03\n",
      "Epoch: 209460 | training loss: 1.9931e-03 | validation loss: 1.6094e-03\n",
      "Epoch: 209470 | training loss: 1.9934e-03 | validation loss: 1.6091e-03\n",
      "Epoch: 209480 | training loss: 1.9931e-03 | validation loss: 1.6087e-03\n",
      "Epoch: 209490 | training loss: 1.9930e-03 | validation loss: 1.6079e-03\n",
      "Epoch: 209500 | training loss: 1.9931e-03 | validation loss: 1.6064e-03\n",
      "Epoch: 209510 | training loss: 1.9965e-03 | validation loss: 1.6022e-03\n",
      "Epoch: 209520 | training loss: 2.2071e-03 | validation loss: 1.6507e-03\n",
      "Epoch: 209530 | training loss: 2.1804e-03 | validation loss: 1.6555e-03\n",
      "Epoch: 209540 | training loss: 2.1921e-03 | validation loss: 1.6434e-03\n",
      "Epoch: 209550 | training loss: 2.0300e-03 | validation loss: 1.6367e-03\n",
      "Epoch: 209560 | training loss: 2.0210e-03 | validation loss: 1.6424e-03\n",
      "Epoch: 209570 | training loss: 1.9991e-03 | validation loss: 1.6109e-03\n",
      "Epoch: 209580 | training loss: 1.9948e-03 | validation loss: 1.6028e-03\n",
      "Epoch: 209590 | training loss: 1.9946e-03 | validation loss: 1.6110e-03\n",
      "Epoch: 209600 | training loss: 1.9932e-03 | validation loss: 1.6086e-03\n",
      "Epoch: 209610 | training loss: 1.9928e-03 | validation loss: 1.6075e-03\n",
      "Epoch: 209620 | training loss: 1.9928e-03 | validation loss: 1.6082e-03\n",
      "Epoch: 209630 | training loss: 1.9928e-03 | validation loss: 1.6079e-03\n",
      "Epoch: 209640 | training loss: 1.9927e-03 | validation loss: 1.6078e-03\n",
      "Epoch: 209650 | training loss: 1.9927e-03 | validation loss: 1.6080e-03\n",
      "Epoch: 209660 | training loss: 1.9927e-03 | validation loss: 1.6078e-03\n",
      "Epoch: 209670 | training loss: 1.9927e-03 | validation loss: 1.6077e-03\n",
      "Epoch: 209680 | training loss: 1.9927e-03 | validation loss: 1.6078e-03\n",
      "Epoch: 209690 | training loss: 1.9926e-03 | validation loss: 1.6078e-03\n",
      "Epoch: 209700 | training loss: 1.9926e-03 | validation loss: 1.6079e-03\n",
      "Epoch: 209710 | training loss: 1.9927e-03 | validation loss: 1.6088e-03\n",
      "Epoch: 209720 | training loss: 2.0062e-03 | validation loss: 1.6242e-03\n",
      "Epoch: 209730 | training loss: 3.5212e-03 | validation loss: 2.4756e-03\n",
      "Epoch: 209740 | training loss: 2.4500e-03 | validation loss: 1.7386e-03\n",
      "Epoch: 209750 | training loss: 2.0279e-03 | validation loss: 1.6250e-03\n",
      "Epoch: 209760 | training loss: 2.0119e-03 | validation loss: 1.6365e-03\n",
      "Epoch: 209770 | training loss: 2.0068e-03 | validation loss: 1.6221e-03\n",
      "Epoch: 209780 | training loss: 1.9999e-03 | validation loss: 1.6037e-03\n",
      "Epoch: 209790 | training loss: 1.9965e-03 | validation loss: 1.6023e-03\n",
      "Epoch: 209800 | training loss: 1.9938e-03 | validation loss: 1.6088e-03\n",
      "Epoch: 209810 | training loss: 1.9928e-03 | validation loss: 1.6074e-03\n",
      "Epoch: 209820 | training loss: 1.9926e-03 | validation loss: 1.6059e-03\n",
      "Epoch: 209830 | training loss: 1.9925e-03 | validation loss: 1.6079e-03\n",
      "Epoch: 209840 | training loss: 1.9924e-03 | validation loss: 1.6068e-03\n",
      "Epoch: 209850 | training loss: 1.9923e-03 | validation loss: 1.6076e-03\n",
      "Epoch: 209860 | training loss: 1.9923e-03 | validation loss: 1.6073e-03\n",
      "Epoch: 209870 | training loss: 1.9923e-03 | validation loss: 1.6075e-03\n",
      "Epoch: 209880 | training loss: 1.9923e-03 | validation loss: 1.6074e-03\n",
      "Epoch: 209890 | training loss: 1.9923e-03 | validation loss: 1.6073e-03\n",
      "Epoch: 209900 | training loss: 1.9922e-03 | validation loss: 1.6073e-03\n",
      "Epoch: 209910 | training loss: 1.9922e-03 | validation loss: 1.6072e-03\n",
      "Epoch: 209920 | training loss: 1.9922e-03 | validation loss: 1.6071e-03\n",
      "Epoch: 209930 | training loss: 1.9923e-03 | validation loss: 1.6060e-03\n",
      "Epoch: 209940 | training loss: 1.9992e-03 | validation loss: 1.5982e-03\n",
      "Epoch: 209950 | training loss: 2.9204e-03 | validation loss: 1.9252e-03\n",
      "Epoch: 209960 | training loss: 2.7224e-03 | validation loss: 2.0918e-03\n",
      "Epoch: 209970 | training loss: 2.1207e-03 | validation loss: 1.7442e-03\n",
      "Epoch: 209980 | training loss: 2.0721e-03 | validation loss: 1.6757e-03\n",
      "Epoch: 209990 | training loss: 2.0216e-03 | validation loss: 1.6333e-03\n",
      "Epoch: 210000 | training loss: 1.9972e-03 | validation loss: 1.6204e-03\n",
      "Epoch: 210010 | training loss: 1.9947e-03 | validation loss: 1.6164e-03\n",
      "Epoch: 210020 | training loss: 1.9920e-03 | validation loss: 1.6074e-03\n",
      "Epoch: 210030 | training loss: 1.9923e-03 | validation loss: 1.6043e-03\n",
      "Epoch: 210040 | training loss: 1.9921e-03 | validation loss: 1.6062e-03\n",
      "Epoch: 210050 | training loss: 1.9920e-03 | validation loss: 1.6065e-03\n",
      "Epoch: 210060 | training loss: 1.9919e-03 | validation loss: 1.6070e-03\n",
      "Epoch: 210070 | training loss: 1.9919e-03 | validation loss: 1.6074e-03\n",
      "Epoch: 210080 | training loss: 1.9919e-03 | validation loss: 1.6066e-03\n",
      "Epoch: 210090 | training loss: 1.9919e-03 | validation loss: 1.6068e-03\n",
      "Epoch: 210100 | training loss: 1.9919e-03 | validation loss: 1.6069e-03\n",
      "Epoch: 210110 | training loss: 1.9918e-03 | validation loss: 1.6067e-03\n",
      "Epoch: 210120 | training loss: 1.9918e-03 | validation loss: 1.6068e-03\n",
      "Epoch: 210130 | training loss: 1.9918e-03 | validation loss: 1.6068e-03\n",
      "Epoch: 210140 | training loss: 1.9918e-03 | validation loss: 1.6068e-03\n",
      "Epoch: 210150 | training loss: 1.9918e-03 | validation loss: 1.6069e-03\n",
      "Epoch: 210160 | training loss: 1.9918e-03 | validation loss: 1.6077e-03\n",
      "Epoch: 210170 | training loss: 2.0001e-03 | validation loss: 1.6198e-03\n",
      "Epoch: 210180 | training loss: 3.0760e-03 | validation loss: 2.2428e-03\n",
      "Epoch: 210190 | training loss: 2.8620e-03 | validation loss: 1.9239e-03\n",
      "Epoch: 210200 | training loss: 2.1476e-03 | validation loss: 1.6768e-03\n",
      "Epoch: 210210 | training loss: 2.0415e-03 | validation loss: 1.6625e-03\n",
      "Epoch: 210220 | training loss: 2.0193e-03 | validation loss: 1.6498e-03\n",
      "Epoch: 210230 | training loss: 1.9972e-03 | validation loss: 1.6220e-03\n",
      "Epoch: 210240 | training loss: 1.9918e-03 | validation loss: 1.6047e-03\n",
      "Epoch: 210250 | training loss: 1.9928e-03 | validation loss: 1.6021e-03\n",
      "Epoch: 210260 | training loss: 1.9919e-03 | validation loss: 1.6062e-03\n",
      "Epoch: 210270 | training loss: 1.9917e-03 | validation loss: 1.6081e-03\n",
      "Epoch: 210280 | training loss: 1.9915e-03 | validation loss: 1.6061e-03\n",
      "Epoch: 210290 | training loss: 1.9915e-03 | validation loss: 1.6065e-03\n",
      "Epoch: 210300 | training loss: 1.9915e-03 | validation loss: 1.6068e-03\n",
      "Epoch: 210310 | training loss: 1.9915e-03 | validation loss: 1.6062e-03\n",
      "Epoch: 210320 | training loss: 1.9914e-03 | validation loss: 1.6066e-03\n",
      "Epoch: 210330 | training loss: 1.9914e-03 | validation loss: 1.6063e-03\n",
      "Epoch: 210340 | training loss: 1.9914e-03 | validation loss: 1.6064e-03\n",
      "Epoch: 210350 | training loss: 1.9914e-03 | validation loss: 1.6064e-03\n",
      "Epoch: 210360 | training loss: 1.9914e-03 | validation loss: 1.6063e-03\n",
      "Epoch: 210370 | training loss: 1.9913e-03 | validation loss: 1.6063e-03\n",
      "Epoch: 210380 | training loss: 1.9913e-03 | validation loss: 1.6063e-03\n",
      "Epoch: 210390 | training loss: 1.9913e-03 | validation loss: 1.6063e-03\n",
      "Epoch: 210400 | training loss: 1.9913e-03 | validation loss: 1.6060e-03\n",
      "Epoch: 210410 | training loss: 1.9914e-03 | validation loss: 1.6042e-03\n",
      "Epoch: 210420 | training loss: 2.0171e-03 | validation loss: 1.5942e-03\n",
      "Epoch: 210430 | training loss: 3.9884e-03 | validation loss: 2.4023e-03\n",
      "Epoch: 210440 | training loss: 2.2830e-03 | validation loss: 1.7496e-03\n",
      "Epoch: 210450 | training loss: 2.1072e-03 | validation loss: 1.6345e-03\n",
      "Epoch: 210460 | training loss: 2.0617e-03 | validation loss: 1.6075e-03\n",
      "Epoch: 210470 | training loss: 2.0148e-03 | validation loss: 1.5994e-03\n",
      "Epoch: 210480 | training loss: 1.9973e-03 | validation loss: 1.6011e-03\n",
      "Epoch: 210490 | training loss: 1.9924e-03 | validation loss: 1.6009e-03\n",
      "Epoch: 210500 | training loss: 1.9914e-03 | validation loss: 1.6034e-03\n",
      "Epoch: 210510 | training loss: 1.9912e-03 | validation loss: 1.6045e-03\n",
      "Epoch: 210520 | training loss: 1.9911e-03 | validation loss: 1.6044e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 210530 | training loss: 1.9911e-03 | validation loss: 1.6049e-03\n",
      "Epoch: 210540 | training loss: 1.9910e-03 | validation loss: 1.6050e-03\n",
      "Epoch: 210550 | training loss: 1.9910e-03 | validation loss: 1.6053e-03\n",
      "Epoch: 210560 | training loss: 1.9910e-03 | validation loss: 1.6056e-03\n",
      "Epoch: 210570 | training loss: 1.9910e-03 | validation loss: 1.6057e-03\n",
      "Epoch: 210580 | training loss: 1.9909e-03 | validation loss: 1.6056e-03\n",
      "Epoch: 210590 | training loss: 1.9909e-03 | validation loss: 1.6054e-03\n",
      "Epoch: 210600 | training loss: 1.9909e-03 | validation loss: 1.6053e-03\n",
      "Epoch: 210610 | training loss: 1.9911e-03 | validation loss: 1.6044e-03\n",
      "Epoch: 210620 | training loss: 2.0122e-03 | validation loss: 1.6043e-03\n",
      "Epoch: 210630 | training loss: 3.7233e-03 | validation loss: 2.3459e-03\n",
      "Epoch: 210640 | training loss: 2.2948e-03 | validation loss: 1.8359e-03\n",
      "Epoch: 210650 | training loss: 2.2075e-03 | validation loss: 1.7744e-03\n",
      "Epoch: 210660 | training loss: 2.0162e-03 | validation loss: 1.6421e-03\n",
      "Epoch: 210670 | training loss: 1.9974e-03 | validation loss: 1.6078e-03\n",
      "Epoch: 210680 | training loss: 2.0009e-03 | validation loss: 1.6038e-03\n",
      "Epoch: 210690 | training loss: 1.9914e-03 | validation loss: 1.6060e-03\n",
      "Epoch: 210700 | training loss: 1.9919e-03 | validation loss: 1.6108e-03\n",
      "Epoch: 210710 | training loss: 1.9908e-03 | validation loss: 1.6068e-03\n",
      "Epoch: 210720 | training loss: 1.9908e-03 | validation loss: 1.6042e-03\n",
      "Epoch: 210730 | training loss: 1.9907e-03 | validation loss: 1.6058e-03\n",
      "Epoch: 210740 | training loss: 1.9906e-03 | validation loss: 1.6053e-03\n",
      "Epoch: 210750 | training loss: 1.9906e-03 | validation loss: 1.6052e-03\n",
      "Epoch: 210760 | training loss: 1.9906e-03 | validation loss: 1.6056e-03\n",
      "Epoch: 210770 | training loss: 1.9906e-03 | validation loss: 1.6053e-03\n",
      "Epoch: 210780 | training loss: 1.9906e-03 | validation loss: 1.6053e-03\n",
      "Epoch: 210790 | training loss: 1.9905e-03 | validation loss: 1.6053e-03\n",
      "Epoch: 210800 | training loss: 1.9905e-03 | validation loss: 1.6053e-03\n",
      "Epoch: 210810 | training loss: 1.9905e-03 | validation loss: 1.6053e-03\n",
      "Epoch: 210820 | training loss: 1.9905e-03 | validation loss: 1.6053e-03\n",
      "Epoch: 210830 | training loss: 1.9905e-03 | validation loss: 1.6052e-03\n",
      "Epoch: 210840 | training loss: 1.9904e-03 | validation loss: 1.6052e-03\n",
      "Epoch: 210850 | training loss: 1.9904e-03 | validation loss: 1.6050e-03\n",
      "Epoch: 210860 | training loss: 1.9910e-03 | validation loss: 1.6028e-03\n",
      "Epoch: 210870 | training loss: 2.1308e-03 | validation loss: 1.6322e-03\n",
      "Epoch: 210880 | training loss: 2.2095e-03 | validation loss: 1.6475e-03\n",
      "Epoch: 210890 | training loss: 2.2907e-03 | validation loss: 1.6787e-03\n",
      "Epoch: 210900 | training loss: 2.0625e-03 | validation loss: 1.6022e-03\n",
      "Epoch: 210910 | training loss: 2.0014e-03 | validation loss: 1.5936e-03\n",
      "Epoch: 210920 | training loss: 1.9917e-03 | validation loss: 1.6001e-03\n",
      "Epoch: 210930 | training loss: 1.9904e-03 | validation loss: 1.6042e-03\n",
      "Epoch: 210940 | training loss: 1.9903e-03 | validation loss: 1.6062e-03\n",
      "Epoch: 210950 | training loss: 1.9904e-03 | validation loss: 1.6064e-03\n",
      "Epoch: 210960 | training loss: 1.9904e-03 | validation loss: 1.6055e-03\n",
      "Epoch: 210970 | training loss: 1.9903e-03 | validation loss: 1.6045e-03\n",
      "Epoch: 210980 | training loss: 1.9903e-03 | validation loss: 1.6046e-03\n",
      "Epoch: 210990 | training loss: 1.9902e-03 | validation loss: 1.6052e-03\n",
      "Epoch: 211000 | training loss: 1.9902e-03 | validation loss: 1.6052e-03\n",
      "Epoch: 211010 | training loss: 1.9901e-03 | validation loss: 1.6051e-03\n",
      "Epoch: 211020 | training loss: 1.9901e-03 | validation loss: 1.6051e-03\n",
      "Epoch: 211030 | training loss: 1.9901e-03 | validation loss: 1.6050e-03\n",
      "Epoch: 211040 | training loss: 1.9901e-03 | validation loss: 1.6050e-03\n",
      "Epoch: 211050 | training loss: 1.9901e-03 | validation loss: 1.6050e-03\n",
      "Epoch: 211060 | training loss: 1.9900e-03 | validation loss: 1.6050e-03\n",
      "Epoch: 211070 | training loss: 1.9900e-03 | validation loss: 1.6049e-03\n",
      "Epoch: 211080 | training loss: 1.9900e-03 | validation loss: 1.6049e-03\n",
      "Epoch: 211090 | training loss: 1.9900e-03 | validation loss: 1.6049e-03\n",
      "Epoch: 211100 | training loss: 1.9900e-03 | validation loss: 1.6049e-03\n",
      "Epoch: 211110 | training loss: 1.9899e-03 | validation loss: 1.6049e-03\n",
      "Epoch: 211120 | training loss: 1.9899e-03 | validation loss: 1.6049e-03\n",
      "Epoch: 211130 | training loss: 1.9899e-03 | validation loss: 1.6051e-03\n",
      "Epoch: 211140 | training loss: 1.9905e-03 | validation loss: 1.6090e-03\n",
      "Epoch: 211150 | training loss: 2.1356e-03 | validation loss: 1.7595e-03\n",
      "Epoch: 211160 | training loss: 2.4106e-03 | validation loss: 1.8645e-03\n",
      "Epoch: 211170 | training loss: 2.0532e-03 | validation loss: 1.6176e-03\n",
      "Epoch: 211180 | training loss: 2.0724e-03 | validation loss: 1.6192e-03\n",
      "Epoch: 211190 | training loss: 2.0268e-03 | validation loss: 1.6238e-03\n",
      "Epoch: 211200 | training loss: 1.9959e-03 | validation loss: 1.5953e-03\n",
      "Epoch: 211210 | training loss: 1.9905e-03 | validation loss: 1.6083e-03\n",
      "Epoch: 211220 | training loss: 1.9903e-03 | validation loss: 1.6037e-03\n",
      "Epoch: 211230 | training loss: 1.9901e-03 | validation loss: 1.6074e-03\n",
      "Epoch: 211240 | training loss: 1.9899e-03 | validation loss: 1.6040e-03\n",
      "Epoch: 211250 | training loss: 1.9897e-03 | validation loss: 1.6049e-03\n",
      "Epoch: 211260 | training loss: 1.9897e-03 | validation loss: 1.6050e-03\n",
      "Epoch: 211270 | training loss: 1.9896e-03 | validation loss: 1.6046e-03\n",
      "Epoch: 211280 | training loss: 1.9896e-03 | validation loss: 1.6045e-03\n",
      "Epoch: 211290 | training loss: 1.9898e-03 | validation loss: 1.6058e-03\n",
      "Epoch: 211300 | training loss: 1.9970e-03 | validation loss: 1.6186e-03\n",
      "Epoch: 211310 | training loss: 2.5299e-03 | validation loss: 1.9562e-03\n",
      "Epoch: 211320 | training loss: 2.2253e-03 | validation loss: 1.6543e-03\n",
      "Epoch: 211330 | training loss: 2.1588e-03 | validation loss: 1.7329e-03\n",
      "Epoch: 211340 | training loss: 2.0541e-03 | validation loss: 1.6646e-03\n",
      "Epoch: 211350 | training loss: 1.9936e-03 | validation loss: 1.5975e-03\n",
      "Epoch: 211360 | training loss: 2.0012e-03 | validation loss: 1.5964e-03\n",
      "Epoch: 211370 | training loss: 1.9899e-03 | validation loss: 1.6068e-03\n",
      "Epoch: 211380 | training loss: 1.9905e-03 | validation loss: 1.6084e-03\n",
      "Epoch: 211390 | training loss: 1.9899e-03 | validation loss: 1.6014e-03\n",
      "Epoch: 211400 | training loss: 1.9894e-03 | validation loss: 1.6046e-03\n",
      "Epoch: 211410 | training loss: 1.9894e-03 | validation loss: 1.6042e-03\n",
      "Epoch: 211420 | training loss: 1.9894e-03 | validation loss: 1.6036e-03\n",
      "Epoch: 211430 | training loss: 1.9893e-03 | validation loss: 1.6041e-03\n",
      "Epoch: 211440 | training loss: 1.9893e-03 | validation loss: 1.6039e-03\n",
      "Epoch: 211450 | training loss: 1.9893e-03 | validation loss: 1.6038e-03\n",
      "Epoch: 211460 | training loss: 1.9893e-03 | validation loss: 1.6040e-03\n",
      "Epoch: 211470 | training loss: 1.9893e-03 | validation loss: 1.6039e-03\n",
      "Epoch: 211480 | training loss: 1.9892e-03 | validation loss: 1.6038e-03\n",
      "Epoch: 211490 | training loss: 1.9892e-03 | validation loss: 1.6038e-03\n",
      "Epoch: 211500 | training loss: 1.9892e-03 | validation loss: 1.6036e-03\n",
      "Epoch: 211510 | training loss: 1.9893e-03 | validation loss: 1.6025e-03\n",
      "Epoch: 211520 | training loss: 2.0025e-03 | validation loss: 1.5963e-03\n",
      "Epoch: 211530 | training loss: 3.6313e-03 | validation loss: 2.2400e-03\n",
      "Epoch: 211540 | training loss: 2.5598e-03 | validation loss: 1.9659e-03\n",
      "Epoch: 211550 | training loss: 2.2802e-03 | validation loss: 1.8072e-03\n",
      "Epoch: 211560 | training loss: 2.0802e-03 | validation loss: 1.6855e-03\n",
      "Epoch: 211570 | training loss: 2.0089e-03 | validation loss: 1.6335e-03\n",
      "Epoch: 211580 | training loss: 1.9906e-03 | validation loss: 1.6112e-03\n",
      "Epoch: 211590 | training loss: 1.9899e-03 | validation loss: 1.6015e-03\n",
      "Epoch: 211600 | training loss: 1.9906e-03 | validation loss: 1.5989e-03\n",
      "Epoch: 211610 | training loss: 1.9893e-03 | validation loss: 1.6011e-03\n",
      "Epoch: 211620 | training loss: 1.9890e-03 | validation loss: 1.6048e-03\n",
      "Epoch: 211630 | training loss: 1.9890e-03 | validation loss: 1.6051e-03\n",
      "Epoch: 211640 | training loss: 1.9889e-03 | validation loss: 1.6033e-03\n",
      "Epoch: 211650 | training loss: 1.9889e-03 | validation loss: 1.6033e-03\n",
      "Epoch: 211660 | training loss: 1.9889e-03 | validation loss: 1.6039e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 211670 | training loss: 1.9889e-03 | validation loss: 1.6035e-03\n",
      "Epoch: 211680 | training loss: 1.9889e-03 | validation loss: 1.6036e-03\n",
      "Epoch: 211690 | training loss: 1.9888e-03 | validation loss: 1.6036e-03\n",
      "Epoch: 211700 | training loss: 1.9888e-03 | validation loss: 1.6035e-03\n",
      "Epoch: 211710 | training loss: 1.9888e-03 | validation loss: 1.6035e-03\n",
      "Epoch: 211720 | training loss: 1.9888e-03 | validation loss: 1.6035e-03\n",
      "Epoch: 211730 | training loss: 1.9888e-03 | validation loss: 1.6035e-03\n",
      "Epoch: 211740 | training loss: 1.9887e-03 | validation loss: 1.6034e-03\n",
      "Epoch: 211750 | training loss: 1.9887e-03 | validation loss: 1.6034e-03\n",
      "Epoch: 211760 | training loss: 1.9887e-03 | validation loss: 1.6036e-03\n",
      "Epoch: 211770 | training loss: 1.9893e-03 | validation loss: 1.6062e-03\n",
      "Epoch: 211780 | training loss: 2.1283e-03 | validation loss: 1.7350e-03\n",
      "Epoch: 211790 | training loss: 2.1796e-03 | validation loss: 1.7137e-03\n",
      "Epoch: 211800 | training loss: 2.1199e-03 | validation loss: 1.7107e-03\n",
      "Epoch: 211810 | training loss: 2.1582e-03 | validation loss: 1.7358e-03\n",
      "Epoch: 211820 | training loss: 2.0349e-03 | validation loss: 1.5929e-03\n",
      "Epoch: 211830 | training loss: 1.9919e-03 | validation loss: 1.5957e-03\n",
      "Epoch: 211840 | training loss: 2.0029e-03 | validation loss: 1.6245e-03\n",
      "Epoch: 211850 | training loss: 1.9935e-03 | validation loss: 1.6142e-03\n",
      "Epoch: 211860 | training loss: 1.9905e-03 | validation loss: 1.6099e-03\n",
      "Epoch: 211870 | training loss: 1.9980e-03 | validation loss: 1.6212e-03\n",
      "Epoch: 211880 | training loss: 2.1714e-03 | validation loss: 1.7498e-03\n",
      "Epoch: 211890 | training loss: 2.2850e-03 | validation loss: 1.8169e-03\n",
      "Epoch: 211900 | training loss: 2.0411e-03 | validation loss: 1.5977e-03\n",
      "Epoch: 211910 | training loss: 1.9900e-03 | validation loss: 1.6087e-03\n",
      "Epoch: 211920 | training loss: 1.9888e-03 | validation loss: 1.6055e-03\n",
      "Epoch: 211930 | training loss: 1.9887e-03 | validation loss: 1.6009e-03\n",
      "Epoch: 211940 | training loss: 1.9884e-03 | validation loss: 1.6023e-03\n",
      "Epoch: 211950 | training loss: 1.9890e-03 | validation loss: 1.6063e-03\n",
      "Epoch: 211960 | training loss: 1.9890e-03 | validation loss: 1.5997e-03\n",
      "Epoch: 211970 | training loss: 1.9883e-03 | validation loss: 1.6024e-03\n",
      "Epoch: 211980 | training loss: 1.9884e-03 | validation loss: 1.6043e-03\n",
      "Epoch: 211990 | training loss: 1.9887e-03 | validation loss: 1.6058e-03\n",
      "Epoch: 212000 | training loss: 1.9933e-03 | validation loss: 1.6144e-03\n",
      "Epoch: 212010 | training loss: 2.1346e-03 | validation loss: 1.7227e-03\n",
      "Epoch: 212020 | training loss: 2.5464e-03 | validation loss: 1.9628e-03\n",
      "Epoch: 212030 | training loss: 2.0220e-03 | validation loss: 1.6014e-03\n",
      "Epoch: 212040 | training loss: 2.0125e-03 | validation loss: 1.5965e-03\n",
      "Epoch: 212050 | training loss: 2.0205e-03 | validation loss: 1.6383e-03\n",
      "Epoch: 212060 | training loss: 1.9982e-03 | validation loss: 1.5947e-03\n",
      "Epoch: 212070 | training loss: 1.9901e-03 | validation loss: 1.6093e-03\n",
      "Epoch: 212080 | training loss: 1.9886e-03 | validation loss: 1.6004e-03\n",
      "Epoch: 212090 | training loss: 1.9884e-03 | validation loss: 1.6048e-03\n",
      "Epoch: 212100 | training loss: 1.9883e-03 | validation loss: 1.6009e-03\n",
      "Epoch: 212110 | training loss: 1.9881e-03 | validation loss: 1.6036e-03\n",
      "Epoch: 212120 | training loss: 1.9880e-03 | validation loss: 1.6030e-03\n",
      "Epoch: 212130 | training loss: 1.9880e-03 | validation loss: 1.6022e-03\n",
      "Epoch: 212140 | training loss: 1.9880e-03 | validation loss: 1.6017e-03\n",
      "Epoch: 212150 | training loss: 1.9884e-03 | validation loss: 1.6003e-03\n",
      "Epoch: 212160 | training loss: 1.9990e-03 | validation loss: 1.5953e-03\n",
      "Epoch: 212170 | training loss: 2.5543e-03 | validation loss: 1.7858e-03\n",
      "Epoch: 212180 | training loss: 2.1572e-03 | validation loss: 1.7362e-03\n",
      "Epoch: 212190 | training loss: 2.1988e-03 | validation loss: 1.6494e-03\n",
      "Epoch: 212200 | training loss: 1.9931e-03 | validation loss: 1.5928e-03\n",
      "Epoch: 212210 | training loss: 2.0170e-03 | validation loss: 1.6385e-03\n",
      "Epoch: 212220 | training loss: 1.9883e-03 | validation loss: 1.6009e-03\n",
      "Epoch: 212230 | training loss: 1.9901e-03 | validation loss: 1.5967e-03\n",
      "Epoch: 212240 | training loss: 1.9893e-03 | validation loss: 1.6086e-03\n",
      "Epoch: 212250 | training loss: 1.9881e-03 | validation loss: 1.5999e-03\n",
      "Epoch: 212260 | training loss: 1.9878e-03 | validation loss: 1.6036e-03\n",
      "Epoch: 212270 | training loss: 1.9878e-03 | validation loss: 1.6017e-03\n",
      "Epoch: 212280 | training loss: 1.9877e-03 | validation loss: 1.6029e-03\n",
      "Epoch: 212290 | training loss: 1.9877e-03 | validation loss: 1.6017e-03\n",
      "Epoch: 212300 | training loss: 1.9877e-03 | validation loss: 1.6023e-03\n",
      "Epoch: 212310 | training loss: 1.9877e-03 | validation loss: 1.6023e-03\n",
      "Epoch: 212320 | training loss: 1.9877e-03 | validation loss: 1.6014e-03\n",
      "Epoch: 212330 | training loss: 1.9950e-03 | validation loss: 1.5988e-03\n",
      "Epoch: 212340 | training loss: 2.5018e-03 | validation loss: 1.8860e-03\n",
      "Epoch: 212350 | training loss: 2.1717e-03 | validation loss: 1.7098e-03\n",
      "Epoch: 212360 | training loss: 2.1361e-03 | validation loss: 1.6603e-03\n",
      "Epoch: 212370 | training loss: 2.0158e-03 | validation loss: 1.5975e-03\n",
      "Epoch: 212380 | training loss: 1.9900e-03 | validation loss: 1.5986e-03\n",
      "Epoch: 212390 | training loss: 1.9956e-03 | validation loss: 1.6106e-03\n",
      "Epoch: 212400 | training loss: 2.0391e-03 | validation loss: 1.6565e-03\n",
      "Epoch: 212410 | training loss: 2.5596e-03 | validation loss: 1.9822e-03\n",
      "Epoch: 212420 | training loss: 2.0986e-03 | validation loss: 1.6125e-03\n",
      "Epoch: 212430 | training loss: 2.0308e-03 | validation loss: 1.6496e-03\n",
      "Epoch: 212440 | training loss: 2.0022e-03 | validation loss: 1.5920e-03\n",
      "Epoch: 212450 | training loss: 1.9898e-03 | validation loss: 1.6087e-03\n",
      "Epoch: 212460 | training loss: 1.9875e-03 | validation loss: 1.6036e-03\n",
      "Epoch: 212470 | training loss: 1.9893e-03 | validation loss: 1.5970e-03\n",
      "Epoch: 212480 | training loss: 1.9875e-03 | validation loss: 1.6033e-03\n",
      "Epoch: 212490 | training loss: 1.9881e-03 | validation loss: 1.6057e-03\n",
      "Epoch: 212500 | training loss: 1.9892e-03 | validation loss: 1.6083e-03\n",
      "Epoch: 212510 | training loss: 2.0059e-03 | validation loss: 1.6282e-03\n",
      "Epoch: 212520 | training loss: 2.3836e-03 | validation loss: 1.8733e-03\n",
      "Epoch: 212530 | training loss: 1.9897e-03 | validation loss: 1.6007e-03\n",
      "Epoch: 212540 | training loss: 2.0052e-03 | validation loss: 1.6282e-03\n",
      "Epoch: 212550 | training loss: 2.0148e-03 | validation loss: 1.5962e-03\n",
      "Epoch: 212560 | training loss: 2.0032e-03 | validation loss: 1.6224e-03\n",
      "Epoch: 212570 | training loss: 1.9935e-03 | validation loss: 1.5959e-03\n",
      "Epoch: 212580 | training loss: 1.9888e-03 | validation loss: 1.6073e-03\n",
      "Epoch: 212590 | training loss: 1.9872e-03 | validation loss: 1.6006e-03\n",
      "Epoch: 212600 | training loss: 1.9874e-03 | validation loss: 1.5997e-03\n",
      "Epoch: 212610 | training loss: 1.9872e-03 | validation loss: 1.6028e-03\n",
      "Epoch: 212620 | training loss: 1.9873e-03 | validation loss: 1.6034e-03\n",
      "Epoch: 212630 | training loss: 1.9876e-03 | validation loss: 1.6045e-03\n",
      "Epoch: 212640 | training loss: 1.9931e-03 | validation loss: 1.6135e-03\n",
      "Epoch: 212650 | training loss: 2.1714e-03 | validation loss: 1.7406e-03\n",
      "Epoch: 212660 | training loss: 2.3774e-03 | validation loss: 1.8617e-03\n",
      "Epoch: 212670 | training loss: 1.9889e-03 | validation loss: 1.5997e-03\n",
      "Epoch: 212680 | training loss: 2.0403e-03 | validation loss: 1.5973e-03\n",
      "Epoch: 212690 | training loss: 2.0183e-03 | validation loss: 1.6380e-03\n",
      "Epoch: 212700 | training loss: 1.9915e-03 | validation loss: 1.5952e-03\n",
      "Epoch: 212710 | training loss: 1.9871e-03 | validation loss: 1.6033e-03\n",
      "Epoch: 212720 | training loss: 1.9869e-03 | validation loss: 1.6011e-03\n",
      "Epoch: 212730 | training loss: 1.9869e-03 | validation loss: 1.6019e-03\n",
      "Epoch: 212740 | training loss: 1.9869e-03 | validation loss: 1.6004e-03\n",
      "Epoch: 212750 | training loss: 1.9869e-03 | validation loss: 1.6027e-03\n",
      "Epoch: 212760 | training loss: 1.9868e-03 | validation loss: 1.6010e-03\n",
      "Epoch: 212770 | training loss: 1.9868e-03 | validation loss: 1.6008e-03\n",
      "Epoch: 212780 | training loss: 1.9872e-03 | validation loss: 1.6028e-03\n",
      "Epoch: 212790 | training loss: 2.0241e-03 | validation loss: 1.6468e-03\n",
      "Epoch: 212800 | training loss: 2.2713e-03 | validation loss: 1.8592e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 212810 | training loss: 2.1916e-03 | validation loss: 1.7967e-03\n",
      "Epoch: 212820 | training loss: 1.9929e-03 | validation loss: 1.6169e-03\n",
      "Epoch: 212830 | training loss: 2.0047e-03 | validation loss: 1.5893e-03\n",
      "Epoch: 212840 | training loss: 2.0168e-03 | validation loss: 1.5898e-03\n",
      "Epoch: 212850 | training loss: 2.0484e-03 | validation loss: 1.5946e-03\n",
      "Epoch: 212860 | training loss: 2.1909e-03 | validation loss: 1.6374e-03\n",
      "Epoch: 212870 | training loss: 2.0120e-03 | validation loss: 1.5931e-03\n",
      "Epoch: 212880 | training loss: 2.0302e-03 | validation loss: 1.6508e-03\n",
      "Epoch: 212890 | training loss: 1.9927e-03 | validation loss: 1.6139e-03\n",
      "Epoch: 212900 | training loss: 1.9888e-03 | validation loss: 1.5950e-03\n",
      "Epoch: 212910 | training loss: 2.0026e-03 | validation loss: 1.5914e-03\n",
      "Epoch: 212920 | training loss: 2.1329e-03 | validation loss: 1.6200e-03\n",
      "Epoch: 212930 | training loss: 2.3075e-03 | validation loss: 1.6817e-03\n",
      "Epoch: 212940 | training loss: 2.1131e-03 | validation loss: 1.7055e-03\n",
      "Epoch: 212950 | training loss: 2.0210e-03 | validation loss: 1.5935e-03\n",
      "Epoch: 212960 | training loss: 1.9883e-03 | validation loss: 1.6072e-03\n",
      "Epoch: 212970 | training loss: 1.9902e-03 | validation loss: 1.6100e-03\n",
      "Epoch: 212980 | training loss: 1.9890e-03 | validation loss: 1.5954e-03\n",
      "Epoch: 212990 | training loss: 1.9891e-03 | validation loss: 1.5953e-03\n",
      "Epoch: 213000 | training loss: 1.9901e-03 | validation loss: 1.5946e-03\n",
      "Epoch: 213010 | training loss: 2.0157e-03 | validation loss: 1.5931e-03\n",
      "Epoch: 213020 | training loss: 2.4494e-03 | validation loss: 1.7381e-03\n",
      "Epoch: 213030 | training loss: 1.9969e-03 | validation loss: 1.6212e-03\n",
      "Epoch: 213040 | training loss: 1.9982e-03 | validation loss: 1.5909e-03\n",
      "Epoch: 213050 | training loss: 1.9949e-03 | validation loss: 1.6154e-03\n",
      "Epoch: 213060 | training loss: 1.9888e-03 | validation loss: 1.5964e-03\n",
      "Epoch: 213070 | training loss: 1.9862e-03 | validation loss: 1.6004e-03\n",
      "Epoch: 213080 | training loss: 1.9872e-03 | validation loss: 1.6051e-03\n",
      "Epoch: 213090 | training loss: 1.9873e-03 | validation loss: 1.5969e-03\n",
      "Epoch: 213100 | training loss: 1.9862e-03 | validation loss: 1.5995e-03\n",
      "Epoch: 213110 | training loss: 1.9862e-03 | validation loss: 1.6018e-03\n",
      "Epoch: 213120 | training loss: 1.9868e-03 | validation loss: 1.6037e-03\n",
      "Epoch: 213130 | training loss: 1.9963e-03 | validation loss: 1.6168e-03\n",
      "Epoch: 213140 | training loss: 2.3135e-03 | validation loss: 1.8242e-03\n",
      "Epoch: 213150 | training loss: 2.0116e-03 | validation loss: 1.6314e-03\n",
      "Epoch: 213160 | training loss: 2.0514e-03 | validation loss: 1.6610e-03\n",
      "Epoch: 213170 | training loss: 2.0617e-03 | validation loss: 1.6043e-03\n",
      "Epoch: 213180 | training loss: 1.9982e-03 | validation loss: 1.6187e-03\n",
      "Epoch: 213190 | training loss: 1.9860e-03 | validation loss: 1.5997e-03\n",
      "Epoch: 213200 | training loss: 1.9864e-03 | validation loss: 1.5981e-03\n",
      "Epoch: 213210 | training loss: 1.9863e-03 | validation loss: 1.6026e-03\n",
      "Epoch: 213220 | training loss: 1.9860e-03 | validation loss: 1.5995e-03\n",
      "Epoch: 213230 | training loss: 1.9859e-03 | validation loss: 1.5999e-03\n",
      "Epoch: 213240 | training loss: 1.9860e-03 | validation loss: 1.6013e-03\n",
      "Epoch: 213250 | training loss: 1.9859e-03 | validation loss: 1.5998e-03\n",
      "Epoch: 213260 | training loss: 1.9859e-03 | validation loss: 1.5996e-03\n",
      "Epoch: 213270 | training loss: 1.9859e-03 | validation loss: 1.6004e-03\n",
      "Epoch: 213280 | training loss: 1.9988e-03 | validation loss: 1.6205e-03\n",
      "Epoch: 213290 | training loss: 2.8077e-03 | validation loss: 2.2678e-03\n",
      "Epoch: 213300 | training loss: 2.2139e-03 | validation loss: 1.8155e-03\n",
      "Epoch: 213310 | training loss: 2.0634e-03 | validation loss: 1.6346e-03\n",
      "Epoch: 213320 | training loss: 2.0057e-03 | validation loss: 1.6230e-03\n",
      "Epoch: 213330 | training loss: 2.0013e-03 | validation loss: 1.6272e-03\n",
      "Epoch: 213340 | training loss: 1.9891e-03 | validation loss: 1.6095e-03\n",
      "Epoch: 213350 | training loss: 1.9864e-03 | validation loss: 1.6016e-03\n",
      "Epoch: 213360 | training loss: 1.9864e-03 | validation loss: 1.5972e-03\n",
      "Epoch: 213370 | training loss: 2.0019e-03 | validation loss: 1.5899e-03\n",
      "Epoch: 213380 | training loss: 2.6414e-03 | validation loss: 1.8037e-03\n",
      "Epoch: 213390 | training loss: 2.2240e-03 | validation loss: 1.7788e-03\n",
      "Epoch: 213400 | training loss: 2.1486e-03 | validation loss: 1.6251e-03\n",
      "Epoch: 213410 | training loss: 1.9861e-03 | validation loss: 1.6025e-03\n",
      "Epoch: 213420 | training loss: 2.0020e-03 | validation loss: 1.6240e-03\n",
      "Epoch: 213430 | training loss: 1.9933e-03 | validation loss: 1.5921e-03\n",
      "Epoch: 213440 | training loss: 1.9865e-03 | validation loss: 1.6043e-03\n",
      "Epoch: 213450 | training loss: 1.9855e-03 | validation loss: 1.5988e-03\n",
      "Epoch: 213460 | training loss: 1.9855e-03 | validation loss: 1.5998e-03\n",
      "Epoch: 213470 | training loss: 1.9855e-03 | validation loss: 1.5990e-03\n",
      "Epoch: 213480 | training loss: 1.9855e-03 | validation loss: 1.6003e-03\n",
      "Epoch: 213490 | training loss: 1.9854e-03 | validation loss: 1.5988e-03\n",
      "Epoch: 213500 | training loss: 1.9854e-03 | validation loss: 1.5993e-03\n",
      "Epoch: 213510 | training loss: 1.9854e-03 | validation loss: 1.5997e-03\n",
      "Epoch: 213520 | training loss: 1.9854e-03 | validation loss: 1.6000e-03\n",
      "Epoch: 213530 | training loss: 1.9855e-03 | validation loss: 1.6012e-03\n",
      "Epoch: 213540 | training loss: 1.9911e-03 | validation loss: 1.6119e-03\n",
      "Epoch: 213550 | training loss: 2.3428e-03 | validation loss: 1.8477e-03\n",
      "Epoch: 213560 | training loss: 1.9950e-03 | validation loss: 1.5891e-03\n",
      "Epoch: 213570 | training loss: 2.2589e-03 | validation loss: 1.7909e-03\n",
      "Epoch: 213580 | training loss: 1.9882e-03 | validation loss: 1.6082e-03\n",
      "Epoch: 213590 | training loss: 2.0204e-03 | validation loss: 1.5988e-03\n",
      "Epoch: 213600 | training loss: 1.9853e-03 | validation loss: 1.5984e-03\n",
      "Epoch: 213610 | training loss: 1.9896e-03 | validation loss: 1.6072e-03\n",
      "Epoch: 213620 | training loss: 1.9861e-03 | validation loss: 1.5968e-03\n",
      "Epoch: 213630 | training loss: 1.9852e-03 | validation loss: 1.5998e-03\n",
      "Epoch: 213640 | training loss: 1.9852e-03 | validation loss: 1.6000e-03\n",
      "Epoch: 213650 | training loss: 1.9851e-03 | validation loss: 1.5986e-03\n",
      "Epoch: 213660 | training loss: 1.9851e-03 | validation loss: 1.5998e-03\n",
      "Epoch: 213670 | training loss: 1.9851e-03 | validation loss: 1.5990e-03\n",
      "Epoch: 213680 | training loss: 1.9850e-03 | validation loss: 1.5991e-03\n",
      "Epoch: 213690 | training loss: 1.9850e-03 | validation loss: 1.5993e-03\n",
      "Epoch: 213700 | training loss: 1.9850e-03 | validation loss: 1.5991e-03\n",
      "Epoch: 213710 | training loss: 1.9850e-03 | validation loss: 1.5990e-03\n",
      "Epoch: 213720 | training loss: 1.9850e-03 | validation loss: 1.5989e-03\n",
      "Epoch: 213730 | training loss: 1.9850e-03 | validation loss: 1.5983e-03\n",
      "Epoch: 213740 | training loss: 1.9870e-03 | validation loss: 1.5950e-03\n",
      "Epoch: 213750 | training loss: 2.1804e-03 | validation loss: 1.6445e-03\n",
      "Epoch: 213760 | training loss: 2.1740e-03 | validation loss: 1.6347e-03\n",
      "Epoch: 213770 | training loss: 2.3857e-03 | validation loss: 1.7293e-03\n",
      "Epoch: 213780 | training loss: 2.0249e-03 | validation loss: 1.6043e-03\n",
      "Epoch: 213790 | training loss: 1.9901e-03 | validation loss: 1.6073e-03\n",
      "Epoch: 213800 | training loss: 2.0021e-03 | validation loss: 1.6185e-03\n",
      "Epoch: 213810 | training loss: 1.9875e-03 | validation loss: 1.6056e-03\n",
      "Epoch: 213820 | training loss: 1.9857e-03 | validation loss: 1.5965e-03\n",
      "Epoch: 213830 | training loss: 1.9854e-03 | validation loss: 1.5975e-03\n",
      "Epoch: 213840 | training loss: 1.9849e-03 | validation loss: 1.6001e-03\n",
      "Epoch: 213850 | training loss: 1.9847e-03 | validation loss: 1.5995e-03\n",
      "Epoch: 213860 | training loss: 1.9847e-03 | validation loss: 1.5984e-03\n",
      "Epoch: 213870 | training loss: 1.9847e-03 | validation loss: 1.5992e-03\n",
      "Epoch: 213880 | training loss: 1.9847e-03 | validation loss: 1.5987e-03\n",
      "Epoch: 213890 | training loss: 1.9846e-03 | validation loss: 1.5989e-03\n",
      "Epoch: 213900 | training loss: 1.9846e-03 | validation loss: 1.5989e-03\n",
      "Epoch: 213910 | training loss: 1.9846e-03 | validation loss: 1.5993e-03\n",
      "Epoch: 213920 | training loss: 1.9852e-03 | validation loss: 1.6027e-03\n",
      "Epoch: 213930 | training loss: 2.0591e-03 | validation loss: 1.6893e-03\n",
      "Epoch: 213940 | training loss: 2.2003e-03 | validation loss: 1.7776e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 213950 | training loss: 2.1667e-03 | validation loss: 1.7012e-03\n",
      "Epoch: 213960 | training loss: 2.0517e-03 | validation loss: 1.6790e-03\n",
      "Epoch: 213970 | training loss: 1.9998e-03 | validation loss: 1.6016e-03\n",
      "Epoch: 213980 | training loss: 1.9856e-03 | validation loss: 1.6036e-03\n",
      "Epoch: 213990 | training loss: 1.9859e-03 | validation loss: 1.5988e-03\n",
      "Epoch: 214000 | training loss: 1.9858e-03 | validation loss: 1.5935e-03\n",
      "Epoch: 214010 | training loss: 1.9849e-03 | validation loss: 1.5953e-03\n",
      "Epoch: 214020 | training loss: 1.9853e-03 | validation loss: 1.5956e-03\n",
      "Epoch: 214030 | training loss: 1.9925e-03 | validation loss: 1.5900e-03\n",
      "Epoch: 214040 | training loss: 2.2072e-03 | validation loss: 1.6408e-03\n",
      "Epoch: 214050 | training loss: 2.1713e-03 | validation loss: 1.6356e-03\n",
      "Epoch: 214060 | training loss: 1.9851e-03 | validation loss: 1.5963e-03\n",
      "Epoch: 214070 | training loss: 2.0169e-03 | validation loss: 1.6340e-03\n",
      "Epoch: 214080 | training loss: 2.0101e-03 | validation loss: 1.5903e-03\n",
      "Epoch: 214090 | training loss: 1.9940e-03 | validation loss: 1.6159e-03\n",
      "Epoch: 214100 | training loss: 1.9876e-03 | validation loss: 1.5927e-03\n",
      "Epoch: 214110 | training loss: 1.9857e-03 | validation loss: 1.6032e-03\n",
      "Epoch: 214120 | training loss: 1.9848e-03 | validation loss: 1.5956e-03\n",
      "Epoch: 214130 | training loss: 1.9842e-03 | validation loss: 1.5989e-03\n",
      "Epoch: 214140 | training loss: 1.9842e-03 | validation loss: 1.5992e-03\n",
      "Epoch: 214150 | training loss: 1.9841e-03 | validation loss: 1.5979e-03\n",
      "Epoch: 214160 | training loss: 1.9842e-03 | validation loss: 1.5972e-03\n",
      "Epoch: 214170 | training loss: 1.9845e-03 | validation loss: 1.5959e-03\n",
      "Epoch: 214180 | training loss: 1.9926e-03 | validation loss: 1.5907e-03\n",
      "Epoch: 214190 | training loss: 2.4010e-03 | validation loss: 1.7189e-03\n",
      "Epoch: 214200 | training loss: 2.0073e-03 | validation loss: 1.6344e-03\n",
      "Epoch: 214210 | training loss: 2.2141e-03 | validation loss: 1.6479e-03\n",
      "Epoch: 214220 | training loss: 1.9953e-03 | validation loss: 1.6066e-03\n",
      "Epoch: 214230 | training loss: 2.0081e-03 | validation loss: 1.6334e-03\n",
      "Epoch: 214240 | training loss: 1.9892e-03 | validation loss: 1.5917e-03\n",
      "Epoch: 214250 | training loss: 1.9846e-03 | validation loss: 1.5945e-03\n",
      "Epoch: 214260 | training loss: 1.9852e-03 | validation loss: 1.6044e-03\n",
      "Epoch: 214270 | training loss: 1.9845e-03 | validation loss: 1.5943e-03\n",
      "Epoch: 214280 | training loss: 1.9841e-03 | validation loss: 1.6004e-03\n",
      "Epoch: 214290 | training loss: 1.9840e-03 | validation loss: 1.5965e-03\n",
      "Epoch: 214300 | training loss: 1.9839e-03 | validation loss: 1.5985e-03\n",
      "Epoch: 214310 | training loss: 1.9838e-03 | validation loss: 1.5975e-03\n",
      "Epoch: 214320 | training loss: 1.9838e-03 | validation loss: 1.5976e-03\n",
      "Epoch: 214330 | training loss: 1.9838e-03 | validation loss: 1.5981e-03\n",
      "Epoch: 214340 | training loss: 1.9840e-03 | validation loss: 1.5991e-03\n",
      "Epoch: 214350 | training loss: 1.9925e-03 | validation loss: 1.6109e-03\n",
      "Epoch: 214360 | training loss: 2.4033e-03 | validation loss: 1.9257e-03\n",
      "Epoch: 214370 | training loss: 2.1932e-03 | validation loss: 1.6764e-03\n",
      "Epoch: 214380 | training loss: 2.2523e-03 | validation loss: 1.6541e-03\n",
      "Epoch: 214390 | training loss: 1.9968e-03 | validation loss: 1.6074e-03\n",
      "Epoch: 214400 | training loss: 2.0353e-03 | validation loss: 1.6617e-03\n",
      "Epoch: 214410 | training loss: 1.9933e-03 | validation loss: 1.5902e-03\n",
      "Epoch: 214420 | training loss: 1.9926e-03 | validation loss: 1.5872e-03\n",
      "Epoch: 214430 | training loss: 1.9841e-03 | validation loss: 1.5942e-03\n",
      "Epoch: 214440 | training loss: 1.9838e-03 | validation loss: 1.5997e-03\n",
      "Epoch: 214450 | training loss: 1.9852e-03 | validation loss: 1.6038e-03\n",
      "Epoch: 214460 | training loss: 2.0424e-03 | validation loss: 1.6581e-03\n",
      "Epoch: 214470 | training loss: 3.1562e-03 | validation loss: 2.3053e-03\n",
      "Epoch: 214480 | training loss: 2.2322e-03 | validation loss: 1.6648e-03\n",
      "Epoch: 214490 | training loss: 1.9932e-03 | validation loss: 1.5866e-03\n",
      "Epoch: 214500 | training loss: 2.0307e-03 | validation loss: 1.6406e-03\n",
      "Epoch: 214510 | training loss: 1.9888e-03 | validation loss: 1.5909e-03\n",
      "Epoch: 214520 | training loss: 1.9841e-03 | validation loss: 1.5970e-03\n",
      "Epoch: 214530 | training loss: 1.9848e-03 | validation loss: 1.6024e-03\n",
      "Epoch: 214540 | training loss: 1.9842e-03 | validation loss: 1.5936e-03\n",
      "Epoch: 214550 | training loss: 1.9837e-03 | validation loss: 1.5998e-03\n",
      "Epoch: 214560 | training loss: 1.9835e-03 | validation loss: 1.5961e-03\n",
      "Epoch: 214570 | training loss: 1.9834e-03 | validation loss: 1.5974e-03\n",
      "Epoch: 214580 | training loss: 1.9833e-03 | validation loss: 1.5976e-03\n",
      "Epoch: 214590 | training loss: 1.9833e-03 | validation loss: 1.5968e-03\n",
      "Epoch: 214600 | training loss: 1.9833e-03 | validation loss: 1.5967e-03\n",
      "Epoch: 214610 | training loss: 1.9833e-03 | validation loss: 1.5965e-03\n",
      "Epoch: 214620 | training loss: 1.9835e-03 | validation loss: 1.5952e-03\n",
      "Epoch: 214630 | training loss: 1.9922e-03 | validation loss: 1.5901e-03\n",
      "Epoch: 214640 | training loss: 2.5617e-03 | validation loss: 1.7859e-03\n",
      "Epoch: 214650 | training loss: 2.2252e-03 | validation loss: 1.7759e-03\n",
      "Epoch: 214660 | training loss: 2.1602e-03 | validation loss: 1.6325e-03\n",
      "Epoch: 214670 | training loss: 2.0304e-03 | validation loss: 1.5892e-03\n",
      "Epoch: 214680 | training loss: 1.9945e-03 | validation loss: 1.6161e-03\n",
      "Epoch: 214690 | training loss: 1.9909e-03 | validation loss: 1.6125e-03\n",
      "Epoch: 214700 | training loss: 1.9856e-03 | validation loss: 1.5916e-03\n",
      "Epoch: 214710 | training loss: 1.9832e-03 | validation loss: 1.5957e-03\n",
      "Epoch: 214720 | training loss: 1.9836e-03 | validation loss: 1.6002e-03\n",
      "Epoch: 214730 | training loss: 1.9832e-03 | validation loss: 1.5952e-03\n",
      "Epoch: 214740 | training loss: 1.9831e-03 | validation loss: 1.5979e-03\n",
      "Epoch: 214750 | training loss: 1.9830e-03 | validation loss: 1.5964e-03\n",
      "Epoch: 214760 | training loss: 1.9830e-03 | validation loss: 1.5972e-03\n",
      "Epoch: 214770 | training loss: 1.9830e-03 | validation loss: 1.5964e-03\n",
      "Epoch: 214780 | training loss: 1.9830e-03 | validation loss: 1.5966e-03\n",
      "Epoch: 214790 | training loss: 1.9834e-03 | validation loss: 1.5950e-03\n",
      "Epoch: 214800 | training loss: 2.0246e-03 | validation loss: 1.6045e-03\n",
      "Epoch: 214810 | training loss: 2.1961e-03 | validation loss: 1.6851e-03\n",
      "Epoch: 214820 | training loss: 2.0749e-03 | validation loss: 1.6303e-03\n",
      "Epoch: 214830 | training loss: 1.9859e-03 | validation loss: 1.6064e-03\n",
      "Epoch: 214840 | training loss: 2.0003e-03 | validation loss: 1.6252e-03\n",
      "Epoch: 214850 | training loss: 1.9971e-03 | validation loss: 1.6232e-03\n",
      "Epoch: 214860 | training loss: 2.0745e-03 | validation loss: 1.6813e-03\n",
      "Epoch: 214870 | training loss: 2.4594e-03 | validation loss: 1.9161e-03\n",
      "Epoch: 214880 | training loss: 2.1146e-03 | validation loss: 1.6110e-03\n",
      "Epoch: 214890 | training loss: 2.0181e-03 | validation loss: 1.6387e-03\n",
      "Epoch: 214900 | training loss: 1.9870e-03 | validation loss: 1.5899e-03\n",
      "Epoch: 214910 | training loss: 1.9835e-03 | validation loss: 1.5927e-03\n",
      "Epoch: 214920 | training loss: 1.9870e-03 | validation loss: 1.6068e-03\n",
      "Epoch: 214930 | training loss: 1.9827e-03 | validation loss: 1.5961e-03\n",
      "Epoch: 214940 | training loss: 1.9838e-03 | validation loss: 1.5924e-03\n",
      "Epoch: 214950 | training loss: 1.9874e-03 | validation loss: 1.5894e-03\n",
      "Epoch: 214960 | training loss: 2.0411e-03 | validation loss: 1.5926e-03\n",
      "Epoch: 214970 | training loss: 2.6700e-03 | validation loss: 1.8199e-03\n",
      "Epoch: 214980 | training loss: 2.1743e-03 | validation loss: 1.7429e-03\n",
      "Epoch: 214990 | training loss: 2.0672e-03 | validation loss: 1.6039e-03\n",
      "Epoch: 215000 | training loss: 2.0169e-03 | validation loss: 1.6323e-03\n",
      "Epoch: 215010 | training loss: 1.9953e-03 | validation loss: 1.5896e-03\n",
      "Epoch: 215020 | training loss: 1.9862e-03 | validation loss: 1.6052e-03\n",
      "Epoch: 215030 | training loss: 1.9826e-03 | validation loss: 1.5946e-03\n",
      "Epoch: 215040 | training loss: 1.9831e-03 | validation loss: 1.5936e-03\n",
      "Epoch: 215050 | training loss: 1.9825e-03 | validation loss: 1.5974e-03\n",
      "Epoch: 215060 | training loss: 1.9829e-03 | validation loss: 1.5989e-03\n",
      "Epoch: 215070 | training loss: 1.9843e-03 | validation loss: 1.6022e-03\n",
      "Epoch: 215080 | training loss: 2.0112e-03 | validation loss: 1.6299e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 215090 | training loss: 2.6594e-03 | validation loss: 2.0190e-03\n",
      "Epoch: 215100 | training loss: 2.1495e-03 | validation loss: 1.6265e-03\n",
      "Epoch: 215110 | training loss: 2.1167e-03 | validation loss: 1.7045e-03\n",
      "Epoch: 215120 | training loss: 2.0226e-03 | validation loss: 1.5911e-03\n",
      "Epoch: 215130 | training loss: 1.9875e-03 | validation loss: 1.6065e-03\n",
      "Epoch: 215140 | training loss: 1.9827e-03 | validation loss: 1.5941e-03\n",
      "Epoch: 215150 | training loss: 1.9824e-03 | validation loss: 1.5976e-03\n",
      "Epoch: 215160 | training loss: 1.9825e-03 | validation loss: 1.5936e-03\n",
      "Epoch: 215170 | training loss: 1.9826e-03 | validation loss: 1.5987e-03\n",
      "Epoch: 215180 | training loss: 1.9823e-03 | validation loss: 1.5948e-03\n",
      "Epoch: 215190 | training loss: 1.9822e-03 | validation loss: 1.5954e-03\n",
      "Epoch: 215200 | training loss: 1.9824e-03 | validation loss: 1.5976e-03\n",
      "Epoch: 215210 | training loss: 1.9922e-03 | validation loss: 1.6138e-03\n",
      "Epoch: 215220 | training loss: 2.4009e-03 | validation loss: 1.9551e-03\n",
      "Epoch: 215230 | training loss: 2.1966e-03 | validation loss: 1.7267e-03\n",
      "Epoch: 215240 | training loss: 2.1041e-03 | validation loss: 1.6936e-03\n",
      "Epoch: 215250 | training loss: 2.0098e-03 | validation loss: 1.6378e-03\n",
      "Epoch: 215260 | training loss: 1.9951e-03 | validation loss: 1.5951e-03\n",
      "Epoch: 215270 | training loss: 2.0154e-03 | validation loss: 1.5849e-03\n",
      "Epoch: 215280 | training loss: 2.1089e-03 | validation loss: 1.6070e-03\n",
      "Epoch: 215290 | training loss: 2.1593e-03 | validation loss: 1.6264e-03\n",
      "Epoch: 215300 | training loss: 1.9975e-03 | validation loss: 1.6198e-03\n",
      "Epoch: 215310 | training loss: 2.0085e-03 | validation loss: 1.6287e-03\n",
      "Epoch: 215320 | training loss: 1.9823e-03 | validation loss: 1.5931e-03\n",
      "Epoch: 215330 | training loss: 1.9920e-03 | validation loss: 1.5873e-03\n",
      "Epoch: 215340 | training loss: 2.0396e-03 | validation loss: 1.5917e-03\n",
      "Epoch: 215350 | training loss: 2.3572e-03 | validation loss: 1.6972e-03\n",
      "Epoch: 215360 | training loss: 1.9852e-03 | validation loss: 1.6054e-03\n",
      "Epoch: 215370 | training loss: 1.9981e-03 | validation loss: 1.6181e-03\n",
      "Epoch: 215380 | training loss: 2.0078e-03 | validation loss: 1.5878e-03\n",
      "Epoch: 215390 | training loss: 1.9862e-03 | validation loss: 1.6057e-03\n",
      "Epoch: 215400 | training loss: 1.9864e-03 | validation loss: 1.6056e-03\n",
      "Epoch: 215410 | training loss: 1.9819e-03 | validation loss: 1.5971e-03\n",
      "Epoch: 215420 | training loss: 1.9818e-03 | validation loss: 1.5944e-03\n",
      "Epoch: 215430 | training loss: 1.9827e-03 | validation loss: 1.5920e-03\n",
      "Epoch: 215440 | training loss: 2.0160e-03 | validation loss: 1.5894e-03\n",
      "Epoch: 215450 | training loss: 3.2164e-03 | validation loss: 2.0567e-03\n",
      "Epoch: 215460 | training loss: 2.4474e-03 | validation loss: 1.9001e-03\n",
      "Epoch: 215470 | training loss: 1.9861e-03 | validation loss: 1.6071e-03\n",
      "Epoch: 215480 | training loss: 2.0401e-03 | validation loss: 1.5899e-03\n",
      "Epoch: 215490 | training loss: 1.9829e-03 | validation loss: 1.5975e-03\n",
      "Epoch: 215500 | training loss: 1.9875e-03 | validation loss: 1.6103e-03\n",
      "Epoch: 215510 | training loss: 1.9841e-03 | validation loss: 1.5887e-03\n",
      "Epoch: 215520 | training loss: 1.9818e-03 | validation loss: 1.5977e-03\n",
      "Epoch: 215530 | training loss: 1.9815e-03 | validation loss: 1.5951e-03\n",
      "Epoch: 215540 | training loss: 1.9815e-03 | validation loss: 1.5948e-03\n",
      "Epoch: 215550 | training loss: 1.9815e-03 | validation loss: 1.5954e-03\n",
      "Epoch: 215560 | training loss: 1.9815e-03 | validation loss: 1.5952e-03\n",
      "Epoch: 215570 | training loss: 1.9815e-03 | validation loss: 1.5945e-03\n",
      "Epoch: 215580 | training loss: 1.9814e-03 | validation loss: 1.5949e-03\n",
      "Epoch: 215590 | training loss: 1.9816e-03 | validation loss: 1.5942e-03\n",
      "Epoch: 215600 | training loss: 1.9898e-03 | validation loss: 1.5929e-03\n",
      "Epoch: 215610 | training loss: 2.4050e-03 | validation loss: 1.8305e-03\n",
      "Epoch: 215620 | training loss: 2.1416e-03 | validation loss: 1.7308e-03\n",
      "Epoch: 215630 | training loss: 1.9918e-03 | validation loss: 1.6022e-03\n",
      "Epoch: 215640 | training loss: 2.0132e-03 | validation loss: 1.5866e-03\n",
      "Epoch: 215650 | training loss: 2.2778e-03 | validation loss: 1.6610e-03\n",
      "Epoch: 215660 | training loss: 2.0281e-03 | validation loss: 1.5952e-03\n",
      "Epoch: 215670 | training loss: 1.9885e-03 | validation loss: 1.6120e-03\n",
      "Epoch: 215680 | training loss: 1.9813e-03 | validation loss: 1.5946e-03\n",
      "Epoch: 215690 | training loss: 1.9815e-03 | validation loss: 1.5932e-03\n",
      "Epoch: 215700 | training loss: 1.9818e-03 | validation loss: 1.5912e-03\n",
      "Epoch: 215710 | training loss: 1.9830e-03 | validation loss: 1.6015e-03\n",
      "Epoch: 215720 | training loss: 1.9823e-03 | validation loss: 1.5908e-03\n",
      "Epoch: 215730 | training loss: 1.9812e-03 | validation loss: 1.5934e-03\n",
      "Epoch: 215740 | training loss: 1.9813e-03 | validation loss: 1.5965e-03\n",
      "Epoch: 215750 | training loss: 1.9819e-03 | validation loss: 1.5985e-03\n",
      "Epoch: 215760 | training loss: 1.9891e-03 | validation loss: 1.6100e-03\n",
      "Epoch: 215770 | training loss: 2.1936e-03 | validation loss: 1.7565e-03\n",
      "Epoch: 215780 | training loss: 2.2354e-03 | validation loss: 1.7741e-03\n",
      "Epoch: 215790 | training loss: 1.9905e-03 | validation loss: 1.6018e-03\n",
      "Epoch: 215800 | training loss: 2.0051e-03 | validation loss: 1.5845e-03\n",
      "Epoch: 215810 | training loss: 2.0050e-03 | validation loss: 1.6214e-03\n",
      "Epoch: 215820 | training loss: 1.9916e-03 | validation loss: 1.5913e-03\n",
      "Epoch: 215830 | training loss: 1.9849e-03 | validation loss: 1.6014e-03\n",
      "Epoch: 215840 | training loss: 1.9825e-03 | validation loss: 1.5918e-03\n",
      "Epoch: 215850 | training loss: 1.9814e-03 | validation loss: 1.5966e-03\n",
      "Epoch: 215860 | training loss: 1.9809e-03 | validation loss: 1.5944e-03\n",
      "Epoch: 215870 | training loss: 1.9810e-03 | validation loss: 1.5930e-03\n",
      "Epoch: 215880 | training loss: 1.9809e-03 | validation loss: 1.5943e-03\n",
      "Epoch: 215890 | training loss: 1.9809e-03 | validation loss: 1.5949e-03\n",
      "Epoch: 215900 | training loss: 1.9810e-03 | validation loss: 1.5958e-03\n",
      "Epoch: 215910 | training loss: 1.9854e-03 | validation loss: 1.6038e-03\n",
      "Epoch: 215920 | training loss: 2.2344e-03 | validation loss: 1.7719e-03\n",
      "Epoch: 215930 | training loss: 2.0780e-03 | validation loss: 1.6845e-03\n",
      "Epoch: 215940 | training loss: 2.1934e-03 | validation loss: 1.7397e-03\n",
      "Epoch: 215950 | training loss: 2.0059e-03 | validation loss: 1.5879e-03\n",
      "Epoch: 215960 | training loss: 2.0114e-03 | validation loss: 1.5987e-03\n",
      "Epoch: 215970 | training loss: 1.9850e-03 | validation loss: 1.6002e-03\n",
      "Epoch: 215980 | training loss: 1.9828e-03 | validation loss: 1.5996e-03\n",
      "Epoch: 215990 | training loss: 1.9825e-03 | validation loss: 1.5912e-03\n",
      "Epoch: 216000 | training loss: 1.9809e-03 | validation loss: 1.5958e-03\n",
      "Epoch: 216010 | training loss: 1.9806e-03 | validation loss: 1.5941e-03\n",
      "Epoch: 216020 | training loss: 1.9806e-03 | validation loss: 1.5938e-03\n",
      "Epoch: 216030 | training loss: 1.9806e-03 | validation loss: 1.5942e-03\n",
      "Epoch: 216040 | training loss: 1.9806e-03 | validation loss: 1.5941e-03\n",
      "Epoch: 216050 | training loss: 1.9805e-03 | validation loss: 1.5940e-03\n",
      "Epoch: 216060 | training loss: 1.9805e-03 | validation loss: 1.5947e-03\n",
      "Epoch: 216070 | training loss: 1.9814e-03 | validation loss: 1.5991e-03\n",
      "Epoch: 216080 | training loss: 2.1048e-03 | validation loss: 1.7301e-03\n",
      "Epoch: 216090 | training loss: 2.2379e-03 | validation loss: 1.7681e-03\n",
      "Epoch: 216100 | training loss: 2.0780e-03 | validation loss: 1.6291e-03\n",
      "Epoch: 216110 | training loss: 2.0317e-03 | validation loss: 1.6586e-03\n",
      "Epoch: 216120 | training loss: 2.0044e-03 | validation loss: 1.6045e-03\n",
      "Epoch: 216130 | training loss: 1.9894e-03 | validation loss: 1.6122e-03\n",
      "Epoch: 216140 | training loss: 1.9824e-03 | validation loss: 1.5990e-03\n",
      "Epoch: 216150 | training loss: 1.9813e-03 | validation loss: 1.5915e-03\n",
      "Epoch: 216160 | training loss: 1.9805e-03 | validation loss: 1.5917e-03\n",
      "Epoch: 216170 | training loss: 1.9804e-03 | validation loss: 1.5925e-03\n",
      "Epoch: 216180 | training loss: 1.9803e-03 | validation loss: 1.5933e-03\n",
      "Epoch: 216190 | training loss: 1.9803e-03 | validation loss: 1.5940e-03\n",
      "Epoch: 216200 | training loss: 1.9806e-03 | validation loss: 1.5963e-03\n",
      "Epoch: 216210 | training loss: 2.0315e-03 | validation loss: 1.6479e-03\n",
      "Epoch: 216220 | training loss: 3.9622e-03 | validation loss: 2.7272e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 216230 | training loss: 2.2142e-03 | validation loss: 1.7779e-03\n",
      "Epoch: 216240 | training loss: 1.9970e-03 | validation loss: 1.6238e-03\n",
      "Epoch: 216250 | training loss: 1.9884e-03 | validation loss: 1.5985e-03\n",
      "Epoch: 216260 | training loss: 1.9919e-03 | validation loss: 1.5924e-03\n",
      "Epoch: 216270 | training loss: 1.9891e-03 | validation loss: 1.5904e-03\n",
      "Epoch: 216280 | training loss: 1.9830e-03 | validation loss: 1.5910e-03\n",
      "Epoch: 216290 | training loss: 1.9803e-03 | validation loss: 1.5941e-03\n",
      "Epoch: 216300 | training loss: 1.9804e-03 | validation loss: 1.5962e-03\n",
      "Epoch: 216310 | training loss: 1.9802e-03 | validation loss: 1.5947e-03\n",
      "Epoch: 216320 | training loss: 1.9801e-03 | validation loss: 1.5927e-03\n",
      "Epoch: 216330 | training loss: 1.9800e-03 | validation loss: 1.5928e-03\n",
      "Epoch: 216340 | training loss: 1.9800e-03 | validation loss: 1.5935e-03\n",
      "Epoch: 216350 | training loss: 1.9800e-03 | validation loss: 1.5933e-03\n",
      "Epoch: 216360 | training loss: 1.9800e-03 | validation loss: 1.5932e-03\n",
      "Epoch: 216370 | training loss: 1.9799e-03 | validation loss: 1.5932e-03\n",
      "Epoch: 216380 | training loss: 1.9799e-03 | validation loss: 1.5932e-03\n",
      "Epoch: 216390 | training loss: 1.9799e-03 | validation loss: 1.5932e-03\n",
      "Epoch: 216400 | training loss: 1.9799e-03 | validation loss: 1.5931e-03\n",
      "Epoch: 216410 | training loss: 1.9799e-03 | validation loss: 1.5931e-03\n",
      "Epoch: 216420 | training loss: 1.9799e-03 | validation loss: 1.5931e-03\n",
      "Epoch: 216430 | training loss: 1.9798e-03 | validation loss: 1.5931e-03\n",
      "Epoch: 216440 | training loss: 1.9798e-03 | validation loss: 1.5931e-03\n",
      "Epoch: 216450 | training loss: 1.9798e-03 | validation loss: 1.5930e-03\n",
      "Epoch: 216460 | training loss: 1.9798e-03 | validation loss: 1.5930e-03\n",
      "Epoch: 216470 | training loss: 1.9798e-03 | validation loss: 1.5932e-03\n",
      "Epoch: 216480 | training loss: 1.9802e-03 | validation loss: 1.5955e-03\n",
      "Epoch: 216490 | training loss: 2.0860e-03 | validation loss: 1.6816e-03\n",
      "Epoch: 216500 | training loss: 2.5665e-03 | validation loss: 1.9646e-03\n",
      "Epoch: 216510 | training loss: 2.4061e-03 | validation loss: 1.8730e-03\n",
      "Epoch: 216520 | training loss: 2.0638e-03 | validation loss: 1.6663e-03\n",
      "Epoch: 216530 | training loss: 1.9858e-03 | validation loss: 1.6039e-03\n",
      "Epoch: 216540 | training loss: 1.9797e-03 | validation loss: 1.5926e-03\n",
      "Epoch: 216550 | training loss: 1.9801e-03 | validation loss: 1.5909e-03\n",
      "Epoch: 216560 | training loss: 1.9798e-03 | validation loss: 1.5914e-03\n",
      "Epoch: 216570 | training loss: 1.9796e-03 | validation loss: 1.5925e-03\n",
      "Epoch: 216580 | training loss: 1.9796e-03 | validation loss: 1.5935e-03\n",
      "Epoch: 216590 | training loss: 1.9796e-03 | validation loss: 1.5939e-03\n",
      "Epoch: 216600 | training loss: 1.9796e-03 | validation loss: 1.5938e-03\n",
      "Epoch: 216610 | training loss: 1.9795e-03 | validation loss: 1.5934e-03\n",
      "Epoch: 216620 | training loss: 1.9795e-03 | validation loss: 1.5929e-03\n",
      "Epoch: 216630 | training loss: 1.9795e-03 | validation loss: 1.5927e-03\n",
      "Epoch: 216640 | training loss: 1.9795e-03 | validation loss: 1.5927e-03\n",
      "Epoch: 216650 | training loss: 1.9794e-03 | validation loss: 1.5928e-03\n",
      "Epoch: 216660 | training loss: 1.9794e-03 | validation loss: 1.5928e-03\n",
      "Epoch: 216670 | training loss: 1.9794e-03 | validation loss: 1.5927e-03\n",
      "Epoch: 216680 | training loss: 1.9794e-03 | validation loss: 1.5927e-03\n",
      "Epoch: 216690 | training loss: 1.9794e-03 | validation loss: 1.5927e-03\n",
      "Epoch: 216700 | training loss: 1.9793e-03 | validation loss: 1.5927e-03\n",
      "Epoch: 216710 | training loss: 1.9793e-03 | validation loss: 1.5927e-03\n",
      "Epoch: 216720 | training loss: 1.9793e-03 | validation loss: 1.5931e-03\n",
      "Epoch: 216730 | training loss: 1.9832e-03 | validation loss: 1.6031e-03\n",
      "Epoch: 216740 | training loss: 2.9271e-03 | validation loss: 2.3726e-03\n",
      "Epoch: 216750 | training loss: 2.0307e-03 | validation loss: 1.6096e-03\n",
      "Epoch: 216760 | training loss: 2.0213e-03 | validation loss: 1.6479e-03\n",
      "Epoch: 216770 | training loss: 2.0119e-03 | validation loss: 1.6123e-03\n",
      "Epoch: 216780 | training loss: 1.9947e-03 | validation loss: 1.6161e-03\n",
      "Epoch: 216790 | training loss: 1.9845e-03 | validation loss: 1.5971e-03\n",
      "Epoch: 216800 | training loss: 1.9807e-03 | validation loss: 1.5959e-03\n",
      "Epoch: 216810 | training loss: 1.9798e-03 | validation loss: 1.5960e-03\n",
      "Epoch: 216820 | training loss: 1.9793e-03 | validation loss: 1.5931e-03\n",
      "Epoch: 216830 | training loss: 1.9792e-03 | validation loss: 1.5918e-03\n",
      "Epoch: 216840 | training loss: 1.9794e-03 | validation loss: 1.5901e-03\n",
      "Epoch: 216850 | training loss: 1.9844e-03 | validation loss: 1.5845e-03\n",
      "Epoch: 216860 | training loss: 2.2067e-03 | validation loss: 1.6356e-03\n",
      "Epoch: 216870 | training loss: 2.1033e-03 | validation loss: 1.6081e-03\n",
      "Epoch: 216880 | training loss: 2.0517e-03 | validation loss: 1.5921e-03\n",
      "Epoch: 216890 | training loss: 2.0609e-03 | validation loss: 1.6685e-03\n",
      "Epoch: 216900 | training loss: 1.9798e-03 | validation loss: 1.5882e-03\n",
      "Epoch: 216910 | training loss: 1.9852e-03 | validation loss: 1.5845e-03\n",
      "Epoch: 216920 | training loss: 1.9833e-03 | validation loss: 1.6022e-03\n",
      "Epoch: 216930 | training loss: 1.9801e-03 | validation loss: 1.5878e-03\n",
      "Epoch: 216940 | training loss: 1.9792e-03 | validation loss: 1.5942e-03\n",
      "Epoch: 216950 | training loss: 1.9790e-03 | validation loss: 1.5904e-03\n",
      "Epoch: 216960 | training loss: 1.9789e-03 | validation loss: 1.5931e-03\n",
      "Epoch: 216970 | training loss: 1.9789e-03 | validation loss: 1.5911e-03\n",
      "Epoch: 216980 | training loss: 1.9788e-03 | validation loss: 1.5916e-03\n",
      "Epoch: 216990 | training loss: 1.9788e-03 | validation loss: 1.5921e-03\n",
      "Epoch: 217000 | training loss: 1.9788e-03 | validation loss: 1.5922e-03\n",
      "Epoch: 217010 | training loss: 1.9788e-03 | validation loss: 1.5928e-03\n",
      "Epoch: 217020 | training loss: 1.9800e-03 | validation loss: 1.5967e-03\n",
      "Epoch: 217030 | training loss: 2.0380e-03 | validation loss: 1.6514e-03\n",
      "Epoch: 217040 | training loss: 3.3957e-03 | validation loss: 2.4222e-03\n",
      "Epoch: 217050 | training loss: 2.0996e-03 | validation loss: 1.6232e-03\n",
      "Epoch: 217060 | training loss: 2.0917e-03 | validation loss: 1.6027e-03\n",
      "Epoch: 217070 | training loss: 1.9929e-03 | validation loss: 1.6009e-03\n",
      "Epoch: 217080 | training loss: 1.9922e-03 | validation loss: 1.6115e-03\n",
      "Epoch: 217090 | training loss: 1.9826e-03 | validation loss: 1.5922e-03\n",
      "Epoch: 217100 | training loss: 1.9790e-03 | validation loss: 1.5889e-03\n",
      "Epoch: 217110 | training loss: 1.9794e-03 | validation loss: 1.5938e-03\n",
      "Epoch: 217120 | training loss: 1.9789e-03 | validation loss: 1.5912e-03\n",
      "Epoch: 217130 | training loss: 1.9786e-03 | validation loss: 1.5920e-03\n",
      "Epoch: 217140 | training loss: 1.9786e-03 | validation loss: 1.5913e-03\n",
      "Epoch: 217150 | training loss: 1.9785e-03 | validation loss: 1.5918e-03\n",
      "Epoch: 217160 | training loss: 1.9785e-03 | validation loss: 1.5913e-03\n",
      "Epoch: 217170 | training loss: 1.9785e-03 | validation loss: 1.5914e-03\n",
      "Epoch: 217180 | training loss: 1.9784e-03 | validation loss: 1.5916e-03\n",
      "Epoch: 217190 | training loss: 1.9784e-03 | validation loss: 1.5915e-03\n",
      "Epoch: 217200 | training loss: 1.9784e-03 | validation loss: 1.5914e-03\n",
      "Epoch: 217210 | training loss: 1.9784e-03 | validation loss: 1.5915e-03\n",
      "Epoch: 217220 | training loss: 1.9784e-03 | validation loss: 1.5921e-03\n",
      "Epoch: 217230 | training loss: 1.9847e-03 | validation loss: 1.6009e-03\n",
      "Epoch: 217240 | training loss: 2.8053e-03 | validation loss: 2.0823e-03\n",
      "Epoch: 217250 | training loss: 2.6990e-03 | validation loss: 1.9160e-03\n",
      "Epoch: 217260 | training loss: 2.2228e-03 | validation loss: 1.8109e-03\n",
      "Epoch: 217270 | training loss: 2.0919e-03 | validation loss: 1.7119e-03\n",
      "Epoch: 217280 | training loss: 2.0006e-03 | validation loss: 1.6180e-03\n",
      "Epoch: 217290 | training loss: 1.9862e-03 | validation loss: 1.5899e-03\n",
      "Epoch: 217300 | training loss: 1.9792e-03 | validation loss: 1.5908e-03\n",
      "Epoch: 217310 | training loss: 1.9789e-03 | validation loss: 1.5958e-03\n",
      "Epoch: 217320 | training loss: 1.9783e-03 | validation loss: 1.5914e-03\n",
      "Epoch: 217330 | training loss: 1.9783e-03 | validation loss: 1.5908e-03\n",
      "Epoch: 217340 | training loss: 1.9782e-03 | validation loss: 1.5924e-03\n",
      "Epoch: 217350 | training loss: 1.9782e-03 | validation loss: 1.5909e-03\n",
      "Epoch: 217360 | training loss: 1.9781e-03 | validation loss: 1.5915e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 217370 | training loss: 1.9781e-03 | validation loss: 1.5909e-03\n",
      "Epoch: 217380 | training loss: 1.9781e-03 | validation loss: 1.5911e-03\n",
      "Epoch: 217390 | training loss: 1.9781e-03 | validation loss: 1.5910e-03\n",
      "Epoch: 217400 | training loss: 1.9780e-03 | validation loss: 1.5910e-03\n",
      "Epoch: 217410 | training loss: 1.9780e-03 | validation loss: 1.5910e-03\n",
      "Epoch: 217420 | training loss: 1.9780e-03 | validation loss: 1.5909e-03\n",
      "Epoch: 217430 | training loss: 1.9780e-03 | validation loss: 1.5909e-03\n",
      "Epoch: 217440 | training loss: 1.9780e-03 | validation loss: 1.5909e-03\n",
      "Epoch: 217450 | training loss: 1.9780e-03 | validation loss: 1.5903e-03\n",
      "Epoch: 217460 | training loss: 1.9803e-03 | validation loss: 1.5848e-03\n",
      "Epoch: 217470 | training loss: 2.5606e-03 | validation loss: 1.7699e-03\n",
      "Epoch: 217480 | training loss: 2.8954e-03 | validation loss: 2.2260e-03\n",
      "Epoch: 217490 | training loss: 2.2582e-03 | validation loss: 1.8515e-03\n",
      "Epoch: 217500 | training loss: 2.0696e-03 | validation loss: 1.6837e-03\n",
      "Epoch: 217510 | training loss: 1.9912e-03 | validation loss: 1.6010e-03\n",
      "Epoch: 217520 | training loss: 1.9804e-03 | validation loss: 1.5841e-03\n",
      "Epoch: 217530 | training loss: 1.9801e-03 | validation loss: 1.5856e-03\n",
      "Epoch: 217540 | training loss: 1.9796e-03 | validation loss: 1.5882e-03\n",
      "Epoch: 217550 | training loss: 1.9784e-03 | validation loss: 1.5883e-03\n",
      "Epoch: 217560 | training loss: 1.9780e-03 | validation loss: 1.5882e-03\n",
      "Epoch: 217570 | training loss: 1.9778e-03 | validation loss: 1.5891e-03\n",
      "Epoch: 217580 | training loss: 1.9777e-03 | validation loss: 1.5900e-03\n",
      "Epoch: 217590 | training loss: 1.9777e-03 | validation loss: 1.5901e-03\n",
      "Epoch: 217600 | training loss: 1.9777e-03 | validation loss: 1.5903e-03\n",
      "Epoch: 217610 | training loss: 1.9777e-03 | validation loss: 1.5905e-03\n",
      "Epoch: 217620 | training loss: 1.9776e-03 | validation loss: 1.5904e-03\n",
      "Epoch: 217630 | training loss: 1.9776e-03 | validation loss: 1.5904e-03\n",
      "Epoch: 217640 | training loss: 1.9776e-03 | validation loss: 1.5903e-03\n",
      "Epoch: 217650 | training loss: 1.9776e-03 | validation loss: 1.5903e-03\n",
      "Epoch: 217660 | training loss: 1.9776e-03 | validation loss: 1.5903e-03\n",
      "Epoch: 217670 | training loss: 1.9775e-03 | validation loss: 1.5903e-03\n",
      "Epoch: 217680 | training loss: 1.9775e-03 | validation loss: 1.5902e-03\n",
      "Epoch: 217690 | training loss: 1.9775e-03 | validation loss: 1.5902e-03\n",
      "Epoch: 217700 | training loss: 1.9775e-03 | validation loss: 1.5902e-03\n",
      "Epoch: 217710 | training loss: 1.9775e-03 | validation loss: 1.5905e-03\n",
      "Epoch: 217720 | training loss: 1.9797e-03 | validation loss: 1.5954e-03\n",
      "Epoch: 217730 | training loss: 2.4946e-03 | validation loss: 1.9131e-03\n",
      "Epoch: 217740 | training loss: 2.7231e-03 | validation loss: 1.8904e-03\n",
      "Epoch: 217750 | training loss: 2.0664e-03 | validation loss: 1.6463e-03\n",
      "Epoch: 217760 | training loss: 1.9936e-03 | validation loss: 1.6085e-03\n",
      "Epoch: 217770 | training loss: 1.9793e-03 | validation loss: 1.5972e-03\n",
      "Epoch: 217780 | training loss: 1.9793e-03 | validation loss: 1.5971e-03\n",
      "Epoch: 217790 | training loss: 1.9800e-03 | validation loss: 1.5973e-03\n",
      "Epoch: 217800 | training loss: 1.9792e-03 | validation loss: 1.5959e-03\n",
      "Epoch: 217810 | training loss: 1.9778e-03 | validation loss: 1.5931e-03\n",
      "Epoch: 217820 | training loss: 1.9773e-03 | validation loss: 1.5906e-03\n",
      "Epoch: 217830 | training loss: 1.9773e-03 | validation loss: 1.5895e-03\n",
      "Epoch: 217840 | training loss: 1.9773e-03 | validation loss: 1.5898e-03\n",
      "Epoch: 217850 | training loss: 1.9772e-03 | validation loss: 1.5905e-03\n",
      "Epoch: 217860 | training loss: 1.9772e-03 | validation loss: 1.5902e-03\n",
      "Epoch: 217870 | training loss: 1.9772e-03 | validation loss: 1.5898e-03\n",
      "Epoch: 217880 | training loss: 1.9772e-03 | validation loss: 1.5899e-03\n",
      "Epoch: 217890 | training loss: 1.9772e-03 | validation loss: 1.5900e-03\n",
      "Epoch: 217900 | training loss: 1.9771e-03 | validation loss: 1.5899e-03\n",
      "Epoch: 217910 | training loss: 1.9771e-03 | validation loss: 1.5899e-03\n",
      "Epoch: 217920 | training loss: 1.9771e-03 | validation loss: 1.5899e-03\n",
      "Epoch: 217930 | training loss: 1.9771e-03 | validation loss: 1.5899e-03\n",
      "Epoch: 217940 | training loss: 1.9771e-03 | validation loss: 1.5898e-03\n",
      "Epoch: 217950 | training loss: 1.9770e-03 | validation loss: 1.5898e-03\n",
      "Epoch: 217960 | training loss: 1.9770e-03 | validation loss: 1.5898e-03\n",
      "Epoch: 217970 | training loss: 1.9770e-03 | validation loss: 1.5898e-03\n",
      "Epoch: 217980 | training loss: 1.9770e-03 | validation loss: 1.5898e-03\n",
      "Epoch: 217990 | training loss: 1.9770e-03 | validation loss: 1.5897e-03\n",
      "Epoch: 218000 | training loss: 1.9769e-03 | validation loss: 1.5897e-03\n",
      "Epoch: 218010 | training loss: 1.9770e-03 | validation loss: 1.5889e-03\n",
      "Epoch: 218020 | training loss: 1.9862e-03 | validation loss: 1.5820e-03\n",
      "Epoch: 218030 | training loss: 4.4708e-03 | validation loss: 2.5944e-03\n",
      "Epoch: 218040 | training loss: 2.0311e-03 | validation loss: 1.6144e-03\n",
      "Epoch: 218050 | training loss: 2.0426e-03 | validation loss: 1.5815e-03\n",
      "Epoch: 218060 | training loss: 2.0866e-03 | validation loss: 1.5923e-03\n",
      "Epoch: 218070 | training loss: 2.0220e-03 | validation loss: 1.5810e-03\n",
      "Epoch: 218080 | training loss: 1.9832e-03 | validation loss: 1.5820e-03\n",
      "Epoch: 218090 | training loss: 1.9769e-03 | validation loss: 1.5880e-03\n",
      "Epoch: 218100 | training loss: 1.9770e-03 | validation loss: 1.5912e-03\n",
      "Epoch: 218110 | training loss: 1.9770e-03 | validation loss: 1.5921e-03\n",
      "Epoch: 218120 | training loss: 1.9768e-03 | validation loss: 1.5912e-03\n",
      "Epoch: 218130 | training loss: 1.9768e-03 | validation loss: 1.5903e-03\n",
      "Epoch: 218140 | training loss: 1.9767e-03 | validation loss: 1.5899e-03\n",
      "Epoch: 218150 | training loss: 1.9767e-03 | validation loss: 1.5894e-03\n",
      "Epoch: 218160 | training loss: 1.9767e-03 | validation loss: 1.5893e-03\n",
      "Epoch: 218170 | training loss: 1.9766e-03 | validation loss: 1.5892e-03\n",
      "Epoch: 218180 | training loss: 1.9766e-03 | validation loss: 1.5893e-03\n",
      "Epoch: 218190 | training loss: 1.9766e-03 | validation loss: 1.5894e-03\n",
      "Epoch: 218200 | training loss: 1.9766e-03 | validation loss: 1.5894e-03\n",
      "Epoch: 218210 | training loss: 1.9766e-03 | validation loss: 1.5893e-03\n",
      "Epoch: 218220 | training loss: 1.9765e-03 | validation loss: 1.5893e-03\n",
      "Epoch: 218230 | training loss: 1.9765e-03 | validation loss: 1.5893e-03\n",
      "Epoch: 218240 | training loss: 1.9765e-03 | validation loss: 1.5892e-03\n",
      "Epoch: 218250 | training loss: 1.9766e-03 | validation loss: 1.5887e-03\n",
      "Epoch: 218260 | training loss: 1.9900e-03 | validation loss: 1.5906e-03\n",
      "Epoch: 218270 | training loss: 2.7056e-03 | validation loss: 2.0263e-03\n",
      "Epoch: 218280 | training loss: 2.1068e-03 | validation loss: 1.6311e-03\n",
      "Epoch: 218290 | training loss: 1.9996e-03 | validation loss: 1.5981e-03\n",
      "Epoch: 218300 | training loss: 1.9805e-03 | validation loss: 1.5849e-03\n",
      "Epoch: 218310 | training loss: 1.9767e-03 | validation loss: 1.5870e-03\n",
      "Epoch: 218320 | training loss: 1.9765e-03 | validation loss: 1.5898e-03\n",
      "Epoch: 218330 | training loss: 1.9769e-03 | validation loss: 1.5915e-03\n",
      "Epoch: 218340 | training loss: 1.9768e-03 | validation loss: 1.5898e-03\n",
      "Epoch: 218350 | training loss: 1.9764e-03 | validation loss: 1.5902e-03\n",
      "Epoch: 218360 | training loss: 1.9763e-03 | validation loss: 1.5892e-03\n",
      "Epoch: 218370 | training loss: 1.9763e-03 | validation loss: 1.5887e-03\n",
      "Epoch: 218380 | training loss: 1.9763e-03 | validation loss: 1.5891e-03\n",
      "Epoch: 218390 | training loss: 1.9764e-03 | validation loss: 1.5907e-03\n",
      "Epoch: 218400 | training loss: 1.9847e-03 | validation loss: 1.6051e-03\n",
      "Epoch: 218410 | training loss: 2.6753e-03 | validation loss: 2.0356e-03\n",
      "Epoch: 218420 | training loss: 2.4532e-03 | validation loss: 1.7255e-03\n",
      "Epoch: 218430 | training loss: 2.0418e-03 | validation loss: 1.6370e-03\n",
      "Epoch: 218440 | training loss: 2.0657e-03 | validation loss: 1.6601e-03\n",
      "Epoch: 218450 | training loss: 1.9777e-03 | validation loss: 1.5925e-03\n",
      "Epoch: 218460 | training loss: 1.9860e-03 | validation loss: 1.5836e-03\n",
      "Epoch: 218470 | training loss: 1.9769e-03 | validation loss: 1.5883e-03\n",
      "Epoch: 218480 | training loss: 1.9776e-03 | validation loss: 1.5945e-03\n",
      "Epoch: 218490 | training loss: 1.9761e-03 | validation loss: 1.5875e-03\n",
      "Epoch: 218500 | training loss: 1.9761e-03 | validation loss: 1.5874e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 218510 | training loss: 1.9761e-03 | validation loss: 1.5897e-03\n",
      "Epoch: 218520 | training loss: 1.9760e-03 | validation loss: 1.5880e-03\n",
      "Epoch: 218530 | training loss: 1.9760e-03 | validation loss: 1.5888e-03\n",
      "Epoch: 218540 | training loss: 1.9760e-03 | validation loss: 1.5883e-03\n",
      "Epoch: 218550 | training loss: 1.9759e-03 | validation loss: 1.5886e-03\n",
      "Epoch: 218560 | training loss: 1.9759e-03 | validation loss: 1.5885e-03\n",
      "Epoch: 218570 | training loss: 1.9759e-03 | validation loss: 1.5884e-03\n",
      "Epoch: 218580 | training loss: 1.9759e-03 | validation loss: 1.5884e-03\n",
      "Epoch: 218590 | training loss: 1.9759e-03 | validation loss: 1.5883e-03\n",
      "Epoch: 218600 | training loss: 1.9758e-03 | validation loss: 1.5881e-03\n",
      "Epoch: 218610 | training loss: 1.9760e-03 | validation loss: 1.5871e-03\n",
      "Epoch: 218620 | training loss: 1.9861e-03 | validation loss: 1.5815e-03\n",
      "Epoch: 218630 | training loss: 3.1608e-03 | validation loss: 2.0329e-03\n",
      "Epoch: 218640 | training loss: 2.8490e-03 | validation loss: 2.1189e-03\n",
      "Epoch: 218650 | training loss: 2.1464e-03 | validation loss: 1.7220e-03\n",
      "Epoch: 218660 | training loss: 1.9832e-03 | validation loss: 1.6029e-03\n",
      "Epoch: 218670 | training loss: 1.9806e-03 | validation loss: 1.5812e-03\n",
      "Epoch: 218680 | training loss: 1.9867e-03 | validation loss: 1.5807e-03\n",
      "Epoch: 218690 | training loss: 1.9795e-03 | validation loss: 1.5829e-03\n",
      "Epoch: 218700 | training loss: 1.9757e-03 | validation loss: 1.5882e-03\n",
      "Epoch: 218710 | training loss: 1.9763e-03 | validation loss: 1.5917e-03\n",
      "Epoch: 218720 | training loss: 1.9757e-03 | validation loss: 1.5890e-03\n",
      "Epoch: 218730 | training loss: 1.9757e-03 | validation loss: 1.5872e-03\n",
      "Epoch: 218740 | training loss: 1.9756e-03 | validation loss: 1.5883e-03\n",
      "Epoch: 218750 | training loss: 1.9756e-03 | validation loss: 1.5885e-03\n",
      "Epoch: 218760 | training loss: 1.9756e-03 | validation loss: 1.5879e-03\n",
      "Epoch: 218770 | training loss: 1.9755e-03 | validation loss: 1.5883e-03\n",
      "Epoch: 218780 | training loss: 1.9755e-03 | validation loss: 1.5880e-03\n",
      "Epoch: 218790 | training loss: 1.9755e-03 | validation loss: 1.5881e-03\n",
      "Epoch: 218800 | training loss: 1.9755e-03 | validation loss: 1.5879e-03\n",
      "Epoch: 218810 | training loss: 1.9756e-03 | validation loss: 1.5870e-03\n",
      "Epoch: 218820 | training loss: 1.9908e-03 | validation loss: 1.5851e-03\n",
      "Epoch: 218830 | training loss: 2.6528e-03 | validation loss: 1.9581e-03\n",
      "Epoch: 218840 | training loss: 2.0577e-03 | validation loss: 1.6285e-03\n",
      "Epoch: 218850 | training loss: 1.9806e-03 | validation loss: 1.5907e-03\n",
      "Epoch: 218860 | training loss: 1.9784e-03 | validation loss: 1.5826e-03\n",
      "Epoch: 218870 | training loss: 1.9775e-03 | validation loss: 1.5933e-03\n",
      "Epoch: 218880 | training loss: 1.9786e-03 | validation loss: 1.5973e-03\n",
      "Epoch: 218890 | training loss: 1.9782e-03 | validation loss: 1.5974e-03\n",
      "Epoch: 218900 | training loss: 1.9896e-03 | validation loss: 1.6118e-03\n",
      "Epoch: 218910 | training loss: 2.2592e-03 | validation loss: 1.7974e-03\n",
      "Epoch: 218920 | training loss: 2.0293e-03 | validation loss: 1.6407e-03\n",
      "Epoch: 218930 | training loss: 1.9764e-03 | validation loss: 1.5826e-03\n",
      "Epoch: 218940 | training loss: 1.9789e-03 | validation loss: 1.5810e-03\n",
      "Epoch: 218950 | training loss: 1.9792e-03 | validation loss: 1.5978e-03\n",
      "Epoch: 218960 | training loss: 1.9763e-03 | validation loss: 1.5840e-03\n",
      "Epoch: 218970 | training loss: 1.9752e-03 | validation loss: 1.5875e-03\n",
      "Epoch: 218980 | training loss: 1.9758e-03 | validation loss: 1.5908e-03\n",
      "Epoch: 218990 | training loss: 1.9756e-03 | validation loss: 1.5847e-03\n",
      "Epoch: 219000 | training loss: 1.9752e-03 | validation loss: 1.5862e-03\n",
      "Epoch: 219010 | training loss: 1.9751e-03 | validation loss: 1.5876e-03\n",
      "Epoch: 219020 | training loss: 1.9751e-03 | validation loss: 1.5885e-03\n",
      "Epoch: 219030 | training loss: 1.9764e-03 | validation loss: 1.5928e-03\n",
      "Epoch: 219040 | training loss: 2.0353e-03 | validation loss: 1.6481e-03\n",
      "Epoch: 219050 | training loss: 3.3089e-03 | validation loss: 2.3746e-03\n",
      "Epoch: 219060 | training loss: 2.1381e-03 | validation loss: 1.6315e-03\n",
      "Epoch: 219070 | training loss: 2.0490e-03 | validation loss: 1.5860e-03\n",
      "Epoch: 219080 | training loss: 2.0060e-03 | validation loss: 1.6136e-03\n",
      "Epoch: 219090 | training loss: 1.9793e-03 | validation loss: 1.5984e-03\n",
      "Epoch: 219100 | training loss: 1.9817e-03 | validation loss: 1.5855e-03\n",
      "Epoch: 219110 | training loss: 1.9756e-03 | validation loss: 1.5888e-03\n",
      "Epoch: 219120 | training loss: 1.9749e-03 | validation loss: 1.5875e-03\n",
      "Epoch: 219130 | training loss: 1.9750e-03 | validation loss: 1.5870e-03\n",
      "Epoch: 219140 | training loss: 1.9749e-03 | validation loss: 1.5877e-03\n",
      "Epoch: 219150 | training loss: 1.9749e-03 | validation loss: 1.5871e-03\n",
      "Epoch: 219160 | training loss: 1.9748e-03 | validation loss: 1.5871e-03\n",
      "Epoch: 219170 | training loss: 1.9748e-03 | validation loss: 1.5875e-03\n",
      "Epoch: 219180 | training loss: 1.9748e-03 | validation loss: 1.5869e-03\n",
      "Epoch: 219190 | training loss: 1.9748e-03 | validation loss: 1.5870e-03\n",
      "Epoch: 219200 | training loss: 1.9747e-03 | validation loss: 1.5870e-03\n",
      "Epoch: 219210 | training loss: 1.9747e-03 | validation loss: 1.5867e-03\n",
      "Epoch: 219220 | training loss: 1.9751e-03 | validation loss: 1.5852e-03\n",
      "Epoch: 219230 | training loss: 2.0001e-03 | validation loss: 1.5834e-03\n",
      "Epoch: 219240 | training loss: 3.5698e-03 | validation loss: 2.2287e-03\n",
      "Epoch: 219250 | training loss: 2.3807e-03 | validation loss: 1.8574e-03\n",
      "Epoch: 219260 | training loss: 2.1548e-03 | validation loss: 1.6989e-03\n",
      "Epoch: 219270 | training loss: 1.9992e-03 | validation loss: 1.5919e-03\n",
      "Epoch: 219280 | training loss: 1.9763e-03 | validation loss: 1.5899e-03\n",
      "Epoch: 219290 | training loss: 1.9823e-03 | validation loss: 1.5937e-03\n",
      "Epoch: 219300 | training loss: 1.9771e-03 | validation loss: 1.5856e-03\n",
      "Epoch: 219310 | training loss: 1.9747e-03 | validation loss: 1.5885e-03\n",
      "Epoch: 219320 | training loss: 1.9750e-03 | validation loss: 1.5869e-03\n",
      "Epoch: 219330 | training loss: 1.9745e-03 | validation loss: 1.5874e-03\n",
      "Epoch: 219340 | training loss: 1.9745e-03 | validation loss: 1.5866e-03\n",
      "Epoch: 219350 | training loss: 1.9745e-03 | validation loss: 1.5872e-03\n",
      "Epoch: 219360 | training loss: 1.9744e-03 | validation loss: 1.5869e-03\n",
      "Epoch: 219370 | training loss: 1.9744e-03 | validation loss: 1.5868e-03\n",
      "Epoch: 219380 | training loss: 1.9744e-03 | validation loss: 1.5868e-03\n",
      "Epoch: 219390 | training loss: 1.9744e-03 | validation loss: 1.5867e-03\n",
      "Epoch: 219400 | training loss: 1.9744e-03 | validation loss: 1.5864e-03\n",
      "Epoch: 219410 | training loss: 1.9746e-03 | validation loss: 1.5842e-03\n",
      "Epoch: 219420 | training loss: 1.9993e-03 | validation loss: 1.5759e-03\n",
      "Epoch: 219430 | training loss: 3.2484e-03 | validation loss: 2.1172e-03\n",
      "Epoch: 219440 | training loss: 2.1387e-03 | validation loss: 1.6974e-03\n",
      "Epoch: 219450 | training loss: 1.9856e-03 | validation loss: 1.5940e-03\n",
      "Epoch: 219460 | training loss: 1.9857e-03 | validation loss: 1.5964e-03\n",
      "Epoch: 219470 | training loss: 1.9850e-03 | validation loss: 1.6080e-03\n",
      "Epoch: 219480 | training loss: 1.9799e-03 | validation loss: 1.5917e-03\n",
      "Epoch: 219490 | training loss: 1.9764e-03 | validation loss: 1.5947e-03\n",
      "Epoch: 219500 | training loss: 1.9747e-03 | validation loss: 1.5867e-03\n",
      "Epoch: 219510 | training loss: 1.9743e-03 | validation loss: 1.5873e-03\n",
      "Epoch: 219520 | training loss: 1.9742e-03 | validation loss: 1.5848e-03\n",
      "Epoch: 219530 | training loss: 1.9741e-03 | validation loss: 1.5857e-03\n",
      "Epoch: 219540 | training loss: 1.9741e-03 | validation loss: 1.5868e-03\n",
      "Epoch: 219550 | training loss: 1.9741e-03 | validation loss: 1.5867e-03\n",
      "Epoch: 219560 | training loss: 1.9741e-03 | validation loss: 1.5868e-03\n",
      "Epoch: 219570 | training loss: 1.9748e-03 | validation loss: 1.5898e-03\n",
      "Epoch: 219580 | training loss: 2.0096e-03 | validation loss: 1.6252e-03\n",
      "Epoch: 219590 | training loss: 3.4045e-03 | validation loss: 2.4145e-03\n",
      "Epoch: 219600 | training loss: 2.3672e-03 | validation loss: 1.7032e-03\n",
      "Epoch: 219610 | training loss: 2.0305e-03 | validation loss: 1.5849e-03\n",
      "Epoch: 219620 | training loss: 2.0080e-03 | validation loss: 1.6228e-03\n",
      "Epoch: 219630 | training loss: 1.9861e-03 | validation loss: 1.6044e-03\n",
      "Epoch: 219640 | training loss: 1.9790e-03 | validation loss: 1.5805e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 219650 | training loss: 1.9743e-03 | validation loss: 1.5841e-03\n",
      "Epoch: 219660 | training loss: 1.9750e-03 | validation loss: 1.5905e-03\n",
      "Epoch: 219670 | training loss: 1.9742e-03 | validation loss: 1.5843e-03\n",
      "Epoch: 219680 | training loss: 1.9739e-03 | validation loss: 1.5868e-03\n",
      "Epoch: 219690 | training loss: 1.9738e-03 | validation loss: 1.5859e-03\n",
      "Epoch: 219700 | training loss: 1.9738e-03 | validation loss: 1.5862e-03\n",
      "Epoch: 219710 | training loss: 1.9738e-03 | validation loss: 1.5858e-03\n",
      "Epoch: 219720 | training loss: 1.9738e-03 | validation loss: 1.5862e-03\n",
      "Epoch: 219730 | training loss: 1.9738e-03 | validation loss: 1.5859e-03\n",
      "Epoch: 219740 | training loss: 1.9737e-03 | validation loss: 1.5858e-03\n",
      "Epoch: 219750 | training loss: 1.9737e-03 | validation loss: 1.5858e-03\n",
      "Epoch: 219760 | training loss: 1.9737e-03 | validation loss: 1.5857e-03\n",
      "Epoch: 219770 | training loss: 1.9738e-03 | validation loss: 1.5849e-03\n",
      "Epoch: 219780 | training loss: 1.9775e-03 | validation loss: 1.5807e-03\n",
      "Epoch: 219790 | training loss: 2.3499e-03 | validation loss: 1.6979e-03\n",
      "Epoch: 219800 | training loss: 2.0474e-03 | validation loss: 1.6525e-03\n",
      "Epoch: 219810 | training loss: 2.2014e-03 | validation loss: 1.6406e-03\n",
      "Epoch: 219820 | training loss: 2.0930e-03 | validation loss: 1.6043e-03\n",
      "Epoch: 219830 | training loss: 1.9827e-03 | validation loss: 1.5794e-03\n",
      "Epoch: 219840 | training loss: 1.9779e-03 | validation loss: 1.5951e-03\n",
      "Epoch: 219850 | training loss: 1.9793e-03 | validation loss: 1.5971e-03\n",
      "Epoch: 219860 | training loss: 1.9735e-03 | validation loss: 1.5862e-03\n",
      "Epoch: 219870 | training loss: 1.9743e-03 | validation loss: 1.5832e-03\n",
      "Epoch: 219880 | training loss: 1.9735e-03 | validation loss: 1.5861e-03\n",
      "Epoch: 219890 | training loss: 1.9736e-03 | validation loss: 1.5868e-03\n",
      "Epoch: 219900 | training loss: 1.9735e-03 | validation loss: 1.5851e-03\n",
      "Epoch: 219910 | training loss: 1.9734e-03 | validation loss: 1.5860e-03\n",
      "Epoch: 219920 | training loss: 1.9734e-03 | validation loss: 1.5857e-03\n",
      "Epoch: 219930 | training loss: 1.9734e-03 | validation loss: 1.5857e-03\n",
      "Epoch: 219940 | training loss: 1.9734e-03 | validation loss: 1.5856e-03\n",
      "Epoch: 219950 | training loss: 1.9734e-03 | validation loss: 1.5857e-03\n",
      "Epoch: 219960 | training loss: 1.9733e-03 | validation loss: 1.5857e-03\n",
      "Epoch: 219970 | training loss: 1.9735e-03 | validation loss: 1.5875e-03\n",
      "Epoch: 219980 | training loss: 2.0180e-03 | validation loss: 1.6431e-03\n",
      "Epoch: 219990 | training loss: 2.2000e-03 | validation loss: 1.7873e-03\n",
      "Epoch: 220000 | training loss: 2.1270e-03 | validation loss: 1.6882e-03\n",
      "Epoch: 220010 | training loss: 2.0228e-03 | validation loss: 1.6396e-03\n",
      "Epoch: 220020 | training loss: 1.9917e-03 | validation loss: 1.6127e-03\n",
      "Epoch: 220030 | training loss: 1.9822e-03 | validation loss: 1.5891e-03\n",
      "Epoch: 220040 | training loss: 1.9761e-03 | validation loss: 1.5889e-03\n",
      "Epoch: 220050 | training loss: 1.9745e-03 | validation loss: 1.5901e-03\n",
      "Epoch: 220060 | training loss: 1.9749e-03 | validation loss: 1.5924e-03\n",
      "Epoch: 220070 | training loss: 1.9948e-03 | validation loss: 1.6159e-03\n",
      "Epoch: 220080 | training loss: 2.5275e-03 | validation loss: 1.9528e-03\n",
      "Epoch: 220090 | training loss: 2.0646e-03 | validation loss: 1.5879e-03\n",
      "Epoch: 220100 | training loss: 2.0882e-03 | validation loss: 1.6826e-03\n",
      "Epoch: 220110 | training loss: 2.0219e-03 | validation loss: 1.5788e-03\n",
      "Epoch: 220120 | training loss: 1.9832e-03 | validation loss: 1.6020e-03\n",
      "Epoch: 220130 | training loss: 1.9748e-03 | validation loss: 1.5802e-03\n",
      "Epoch: 220140 | training loss: 1.9737e-03 | validation loss: 1.5886e-03\n",
      "Epoch: 220150 | training loss: 1.9736e-03 | validation loss: 1.5822e-03\n",
      "Epoch: 220160 | training loss: 1.9734e-03 | validation loss: 1.5879e-03\n",
      "Epoch: 220170 | training loss: 1.9730e-03 | validation loss: 1.5841e-03\n",
      "Epoch: 220180 | training loss: 1.9730e-03 | validation loss: 1.5837e-03\n",
      "Epoch: 220190 | training loss: 1.9729e-03 | validation loss: 1.5844e-03\n",
      "Epoch: 220200 | training loss: 1.9729e-03 | validation loss: 1.5844e-03\n",
      "Epoch: 220210 | training loss: 1.9730e-03 | validation loss: 1.5835e-03\n",
      "Epoch: 220220 | training loss: 1.9776e-03 | validation loss: 1.5783e-03\n",
      "Epoch: 220230 | training loss: 2.3385e-03 | validation loss: 1.6834e-03\n",
      "Epoch: 220240 | training loss: 2.0067e-03 | validation loss: 1.6326e-03\n",
      "Epoch: 220250 | training loss: 2.2565e-03 | validation loss: 1.6643e-03\n",
      "Epoch: 220260 | training loss: 2.0090e-03 | validation loss: 1.5744e-03\n",
      "Epoch: 220270 | training loss: 1.9897e-03 | validation loss: 1.5993e-03\n",
      "Epoch: 220280 | training loss: 1.9829e-03 | validation loss: 1.5998e-03\n",
      "Epoch: 220290 | training loss: 1.9743e-03 | validation loss: 1.5849e-03\n",
      "Epoch: 220300 | training loss: 1.9738e-03 | validation loss: 1.5829e-03\n",
      "Epoch: 220310 | training loss: 1.9733e-03 | validation loss: 1.5865e-03\n",
      "Epoch: 220320 | training loss: 1.9727e-03 | validation loss: 1.5844e-03\n",
      "Epoch: 220330 | training loss: 1.9727e-03 | validation loss: 1.5847e-03\n",
      "Epoch: 220340 | training loss: 1.9727e-03 | validation loss: 1.5849e-03\n",
      "Epoch: 220350 | training loss: 1.9726e-03 | validation loss: 1.5845e-03\n",
      "Epoch: 220360 | training loss: 1.9726e-03 | validation loss: 1.5848e-03\n",
      "Epoch: 220370 | training loss: 1.9726e-03 | validation loss: 1.5846e-03\n",
      "Epoch: 220380 | training loss: 1.9726e-03 | validation loss: 1.5845e-03\n",
      "Epoch: 220390 | training loss: 1.9726e-03 | validation loss: 1.5846e-03\n",
      "Epoch: 220400 | training loss: 1.9725e-03 | validation loss: 1.5846e-03\n",
      "Epoch: 220410 | training loss: 1.9725e-03 | validation loss: 1.5846e-03\n",
      "Epoch: 220420 | training loss: 1.9725e-03 | validation loss: 1.5848e-03\n",
      "Epoch: 220430 | training loss: 1.9727e-03 | validation loss: 1.5862e-03\n",
      "Epoch: 220440 | training loss: 1.9921e-03 | validation loss: 1.6079e-03\n",
      "Epoch: 220450 | training loss: 3.5902e-03 | validation loss: 2.5031e-03\n",
      "Epoch: 220460 | training loss: 2.4328e-03 | validation loss: 1.7271e-03\n",
      "Epoch: 220470 | training loss: 2.1830e-03 | validation loss: 1.6890e-03\n",
      "Epoch: 220480 | training loss: 2.0438e-03 | validation loss: 1.6455e-03\n",
      "Epoch: 220490 | training loss: 1.9853e-03 | validation loss: 1.5955e-03\n",
      "Epoch: 220500 | training loss: 1.9739e-03 | validation loss: 1.5806e-03\n",
      "Epoch: 220510 | training loss: 1.9728e-03 | validation loss: 1.5858e-03\n",
      "Epoch: 220520 | training loss: 1.9734e-03 | validation loss: 1.5856e-03\n",
      "Epoch: 220530 | training loss: 1.9727e-03 | validation loss: 1.5837e-03\n",
      "Epoch: 220540 | training loss: 1.9723e-03 | validation loss: 1.5852e-03\n",
      "Epoch: 220550 | training loss: 1.9723e-03 | validation loss: 1.5841e-03\n",
      "Epoch: 220560 | training loss: 1.9723e-03 | validation loss: 1.5846e-03\n",
      "Epoch: 220570 | training loss: 1.9722e-03 | validation loss: 1.5842e-03\n",
      "Epoch: 220580 | training loss: 1.9722e-03 | validation loss: 1.5842e-03\n",
      "Epoch: 220590 | training loss: 1.9722e-03 | validation loss: 1.5843e-03\n",
      "Epoch: 220600 | training loss: 1.9722e-03 | validation loss: 1.5843e-03\n",
      "Epoch: 220610 | training loss: 1.9722e-03 | validation loss: 1.5843e-03\n",
      "Epoch: 220620 | training loss: 1.9721e-03 | validation loss: 1.5845e-03\n",
      "Epoch: 220630 | training loss: 1.9723e-03 | validation loss: 1.5861e-03\n",
      "Epoch: 220640 | training loss: 1.9829e-03 | validation loss: 1.6072e-03\n",
      "Epoch: 220650 | training loss: 3.0061e-03 | validation loss: 2.3262e-03\n",
      "Epoch: 220660 | training loss: 2.3368e-03 | validation loss: 1.6803e-03\n",
      "Epoch: 220670 | training loss: 2.1225e-03 | validation loss: 1.6578e-03\n",
      "Epoch: 220680 | training loss: 2.0242e-03 | validation loss: 1.6004e-03\n",
      "Epoch: 220690 | training loss: 1.9932e-03 | validation loss: 1.5733e-03\n",
      "Epoch: 220700 | training loss: 1.9763e-03 | validation loss: 1.5829e-03\n",
      "Epoch: 220710 | training loss: 1.9722e-03 | validation loss: 1.5821e-03\n",
      "Epoch: 220720 | training loss: 1.9721e-03 | validation loss: 1.5842e-03\n",
      "Epoch: 220730 | training loss: 1.9722e-03 | validation loss: 1.5861e-03\n",
      "Epoch: 220740 | training loss: 1.9720e-03 | validation loss: 1.5844e-03\n",
      "Epoch: 220750 | training loss: 1.9719e-03 | validation loss: 1.5832e-03\n",
      "Epoch: 220760 | training loss: 1.9719e-03 | validation loss: 1.5835e-03\n",
      "Epoch: 220770 | training loss: 1.9719e-03 | validation loss: 1.5836e-03\n",
      "Epoch: 220780 | training loss: 1.9718e-03 | validation loss: 1.5835e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 220790 | training loss: 1.9718e-03 | validation loss: 1.5832e-03\n",
      "Epoch: 220800 | training loss: 1.9719e-03 | validation loss: 1.5826e-03\n",
      "Epoch: 220810 | training loss: 1.9744e-03 | validation loss: 1.5791e-03\n",
      "Epoch: 220820 | training loss: 2.1559e-03 | validation loss: 1.6253e-03\n",
      "Epoch: 220830 | training loss: 2.2686e-03 | validation loss: 1.6607e-03\n",
      "Epoch: 220840 | training loss: 2.2381e-03 | validation loss: 1.6532e-03\n",
      "Epoch: 220850 | training loss: 1.9772e-03 | validation loss: 1.5963e-03\n",
      "Epoch: 220860 | training loss: 2.0171e-03 | validation loss: 1.6309e-03\n",
      "Epoch: 220870 | training loss: 1.9740e-03 | validation loss: 1.5903e-03\n",
      "Epoch: 220880 | training loss: 1.9767e-03 | validation loss: 1.5773e-03\n",
      "Epoch: 220890 | training loss: 1.9718e-03 | validation loss: 1.5818e-03\n",
      "Epoch: 220900 | training loss: 1.9725e-03 | validation loss: 1.5874e-03\n",
      "Epoch: 220910 | training loss: 1.9717e-03 | validation loss: 1.5824e-03\n",
      "Epoch: 220920 | training loss: 1.9716e-03 | validation loss: 1.5834e-03\n",
      "Epoch: 220930 | training loss: 1.9716e-03 | validation loss: 1.5838e-03\n",
      "Epoch: 220940 | training loss: 1.9716e-03 | validation loss: 1.5832e-03\n",
      "Epoch: 220950 | training loss: 1.9715e-03 | validation loss: 1.5835e-03\n",
      "Epoch: 220960 | training loss: 1.9715e-03 | validation loss: 1.5834e-03\n",
      "Epoch: 220970 | training loss: 1.9715e-03 | validation loss: 1.5832e-03\n",
      "Epoch: 220980 | training loss: 1.9715e-03 | validation loss: 1.5834e-03\n",
      "Epoch: 220990 | training loss: 1.9715e-03 | validation loss: 1.5834e-03\n",
      "Epoch: 221000 | training loss: 1.9714e-03 | validation loss: 1.5833e-03\n",
      "Epoch: 221010 | training loss: 1.9714e-03 | validation loss: 1.5834e-03\n",
      "Epoch: 221020 | training loss: 1.9714e-03 | validation loss: 1.5840e-03\n",
      "Epoch: 221030 | training loss: 1.9743e-03 | validation loss: 1.5906e-03\n",
      "Epoch: 221040 | training loss: 2.3165e-03 | validation loss: 1.8158e-03\n",
      "Epoch: 221050 | training loss: 2.0417e-03 | validation loss: 1.5905e-03\n",
      "Epoch: 221060 | training loss: 2.1600e-03 | validation loss: 1.7224e-03\n",
      "Epoch: 221070 | training loss: 2.1069e-03 | validation loss: 1.6868e-03\n",
      "Epoch: 221080 | training loss: 2.0107e-03 | validation loss: 1.6225e-03\n",
      "Epoch: 221090 | training loss: 1.9733e-03 | validation loss: 1.5893e-03\n",
      "Epoch: 221100 | training loss: 1.9731e-03 | validation loss: 1.5796e-03\n",
      "Epoch: 221110 | training loss: 1.9736e-03 | validation loss: 1.5794e-03\n",
      "Epoch: 221120 | training loss: 1.9713e-03 | validation loss: 1.5826e-03\n",
      "Epoch: 221130 | training loss: 1.9715e-03 | validation loss: 1.5852e-03\n",
      "Epoch: 221140 | training loss: 1.9712e-03 | validation loss: 1.5834e-03\n",
      "Epoch: 221150 | training loss: 1.9712e-03 | validation loss: 1.5825e-03\n",
      "Epoch: 221160 | training loss: 1.9712e-03 | validation loss: 1.5834e-03\n",
      "Epoch: 221170 | training loss: 1.9711e-03 | validation loss: 1.5831e-03\n",
      "Epoch: 221180 | training loss: 1.9711e-03 | validation loss: 1.5829e-03\n",
      "Epoch: 221190 | training loss: 1.9711e-03 | validation loss: 1.5830e-03\n",
      "Epoch: 221200 | training loss: 1.9711e-03 | validation loss: 1.5828e-03\n",
      "Epoch: 221210 | training loss: 1.9712e-03 | validation loss: 1.5818e-03\n",
      "Epoch: 221220 | training loss: 1.9834e-03 | validation loss: 1.5774e-03\n",
      "Epoch: 221230 | training loss: 2.7475e-03 | validation loss: 1.9934e-03\n",
      "Epoch: 221240 | training loss: 2.0247e-03 | validation loss: 1.6247e-03\n",
      "Epoch: 221250 | training loss: 1.9826e-03 | validation loss: 1.5785e-03\n",
      "Epoch: 221260 | training loss: 1.9800e-03 | validation loss: 1.6006e-03\n",
      "Epoch: 221270 | training loss: 1.9777e-03 | validation loss: 1.5936e-03\n",
      "Epoch: 221280 | training loss: 1.9752e-03 | validation loss: 1.5849e-03\n",
      "Epoch: 221290 | training loss: 1.9716e-03 | validation loss: 1.5865e-03\n",
      "Epoch: 221300 | training loss: 1.9714e-03 | validation loss: 1.5852e-03\n",
      "Epoch: 221310 | training loss: 1.9717e-03 | validation loss: 1.5857e-03\n",
      "Epoch: 221320 | training loss: 1.9767e-03 | validation loss: 1.5955e-03\n",
      "Epoch: 221330 | training loss: 2.1140e-03 | validation loss: 1.7042e-03\n",
      "Epoch: 221340 | training loss: 2.4555e-03 | validation loss: 1.9068e-03\n",
      "Epoch: 221350 | training loss: 2.0334e-03 | validation loss: 1.5839e-03\n",
      "Epoch: 221360 | training loss: 1.9722e-03 | validation loss: 1.5794e-03\n",
      "Epoch: 221370 | training loss: 1.9834e-03 | validation loss: 1.6018e-03\n",
      "Epoch: 221380 | training loss: 1.9795e-03 | validation loss: 1.5741e-03\n",
      "Epoch: 221390 | training loss: 1.9744e-03 | validation loss: 1.5917e-03\n",
      "Epoch: 221400 | training loss: 1.9719e-03 | validation loss: 1.5788e-03\n",
      "Epoch: 221410 | training loss: 1.9708e-03 | validation loss: 1.5838e-03\n",
      "Epoch: 221420 | training loss: 1.9707e-03 | validation loss: 1.5833e-03\n",
      "Epoch: 221430 | training loss: 1.9708e-03 | validation loss: 1.5810e-03\n",
      "Epoch: 221440 | training loss: 1.9707e-03 | validation loss: 1.5812e-03\n",
      "Epoch: 221450 | training loss: 1.9707e-03 | validation loss: 1.5814e-03\n",
      "Epoch: 221460 | training loss: 1.9710e-03 | validation loss: 1.5800e-03\n",
      "Epoch: 221470 | training loss: 1.9804e-03 | validation loss: 1.5745e-03\n",
      "Epoch: 221480 | training loss: 2.4893e-03 | validation loss: 1.7418e-03\n",
      "Epoch: 221490 | training loss: 2.0958e-03 | validation loss: 1.6951e-03\n",
      "Epoch: 221500 | training loss: 2.1872e-03 | validation loss: 1.6316e-03\n",
      "Epoch: 221510 | training loss: 1.9824e-03 | validation loss: 1.5738e-03\n",
      "Epoch: 221520 | training loss: 1.9949e-03 | validation loss: 1.6152e-03\n",
      "Epoch: 221530 | training loss: 1.9732e-03 | validation loss: 1.5861e-03\n",
      "Epoch: 221540 | training loss: 1.9735e-03 | validation loss: 1.5752e-03\n",
      "Epoch: 221550 | training loss: 1.9714e-03 | validation loss: 1.5877e-03\n",
      "Epoch: 221560 | training loss: 1.9705e-03 | validation loss: 1.5807e-03\n",
      "Epoch: 221570 | training loss: 1.9704e-03 | validation loss: 1.5823e-03\n",
      "Epoch: 221580 | training loss: 1.9704e-03 | validation loss: 1.5820e-03\n",
      "Epoch: 221590 | training loss: 1.9704e-03 | validation loss: 1.5823e-03\n",
      "Epoch: 221600 | training loss: 1.9704e-03 | validation loss: 1.5816e-03\n",
      "Epoch: 221610 | training loss: 1.9703e-03 | validation loss: 1.5822e-03\n",
      "Epoch: 221620 | training loss: 1.9703e-03 | validation loss: 1.5820e-03\n",
      "Epoch: 221630 | training loss: 1.9703e-03 | validation loss: 1.5817e-03\n",
      "Epoch: 221640 | training loss: 1.9705e-03 | validation loss: 1.5813e-03\n",
      "Epoch: 221650 | training loss: 1.9807e-03 | validation loss: 1.5842e-03\n",
      "Epoch: 221660 | training loss: 2.4892e-03 | validation loss: 1.8917e-03\n",
      "Epoch: 221670 | training loss: 2.5509e-03 | validation loss: 2.0265e-03\n",
      "Epoch: 221680 | training loss: 2.1645e-03 | validation loss: 1.6679e-03\n",
      "Epoch: 221690 | training loss: 2.0222e-03 | validation loss: 1.6479e-03\n",
      "Epoch: 221700 | training loss: 1.9783e-03 | validation loss: 1.5787e-03\n",
      "Epoch: 221710 | training loss: 1.9703e-03 | validation loss: 1.5808e-03\n",
      "Epoch: 221720 | training loss: 1.9734e-03 | validation loss: 1.5869e-03\n",
      "Epoch: 221730 | training loss: 1.9713e-03 | validation loss: 1.5769e-03\n",
      "Epoch: 221740 | training loss: 1.9711e-03 | validation loss: 1.5777e-03\n",
      "Epoch: 221750 | training loss: 1.9713e-03 | validation loss: 1.5782e-03\n",
      "Epoch: 221760 | training loss: 1.9785e-03 | validation loss: 1.5734e-03\n",
      "Epoch: 221770 | training loss: 2.1545e-03 | validation loss: 1.6126e-03\n",
      "Epoch: 221780 | training loss: 2.2837e-03 | validation loss: 1.6623e-03\n",
      "Epoch: 221790 | training loss: 2.0101e-03 | validation loss: 1.6226e-03\n",
      "Epoch: 221800 | training loss: 1.9703e-03 | validation loss: 1.5820e-03\n",
      "Epoch: 221810 | training loss: 1.9751e-03 | validation loss: 1.5752e-03\n",
      "Epoch: 221820 | training loss: 1.9735e-03 | validation loss: 1.5909e-03\n",
      "Epoch: 221830 | training loss: 1.9709e-03 | validation loss: 1.5779e-03\n",
      "Epoch: 221840 | training loss: 1.9699e-03 | validation loss: 1.5816e-03\n",
      "Epoch: 221850 | training loss: 1.9702e-03 | validation loss: 1.5838e-03\n",
      "Epoch: 221860 | training loss: 1.9701e-03 | validation loss: 1.5796e-03\n",
      "Epoch: 221870 | training loss: 1.9700e-03 | validation loss: 1.5802e-03\n",
      "Epoch: 221880 | training loss: 1.9699e-03 | validation loss: 1.5808e-03\n",
      "Epoch: 221890 | training loss: 1.9699e-03 | validation loss: 1.5805e-03\n",
      "Epoch: 221900 | training loss: 1.9708e-03 | validation loss: 1.5779e-03\n",
      "Epoch: 221910 | training loss: 2.0214e-03 | validation loss: 1.5782e-03\n",
      "Epoch: 221920 | training loss: 3.5039e-03 | validation loss: 2.1728e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 221930 | training loss: 2.1103e-03 | validation loss: 1.6852e-03\n",
      "Epoch: 221940 | training loss: 2.1085e-03 | validation loss: 1.6990e-03\n",
      "Epoch: 221950 | training loss: 1.9759e-03 | validation loss: 1.5883e-03\n",
      "Epoch: 221960 | training loss: 1.9890e-03 | validation loss: 1.5701e-03\n",
      "Epoch: 221970 | training loss: 1.9703e-03 | validation loss: 1.5790e-03\n",
      "Epoch: 221980 | training loss: 1.9725e-03 | validation loss: 1.5915e-03\n",
      "Epoch: 221990 | training loss: 1.9700e-03 | validation loss: 1.5785e-03\n",
      "Epoch: 222000 | training loss: 1.9697e-03 | validation loss: 1.5806e-03\n",
      "Epoch: 222010 | training loss: 1.9697e-03 | validation loss: 1.5826e-03\n",
      "Epoch: 222020 | training loss: 1.9696e-03 | validation loss: 1.5803e-03\n",
      "Epoch: 222030 | training loss: 1.9696e-03 | validation loss: 1.5817e-03\n",
      "Epoch: 222040 | training loss: 1.9696e-03 | validation loss: 1.5807e-03\n",
      "Epoch: 222050 | training loss: 1.9695e-03 | validation loss: 1.5813e-03\n",
      "Epoch: 222060 | training loss: 1.9695e-03 | validation loss: 1.5812e-03\n",
      "Epoch: 222070 | training loss: 1.9695e-03 | validation loss: 1.5810e-03\n",
      "Epoch: 222080 | training loss: 1.9695e-03 | validation loss: 1.5813e-03\n",
      "Epoch: 222090 | training loss: 1.9712e-03 | validation loss: 1.5848e-03\n",
      "Epoch: 222100 | training loss: 2.1248e-03 | validation loss: 1.7107e-03\n",
      "Epoch: 222110 | training loss: 2.0924e-03 | validation loss: 1.6083e-03\n",
      "Epoch: 222120 | training loss: 2.1838e-03 | validation loss: 1.6485e-03\n",
      "Epoch: 222130 | training loss: 2.0122e-03 | validation loss: 1.6160e-03\n",
      "Epoch: 222140 | training loss: 2.0053e-03 | validation loss: 1.6291e-03\n",
      "Epoch: 222150 | training loss: 1.9769e-03 | validation loss: 1.5949e-03\n",
      "Epoch: 222160 | training loss: 1.9714e-03 | validation loss: 1.5814e-03\n",
      "Epoch: 222170 | training loss: 1.9696e-03 | validation loss: 1.5805e-03\n",
      "Epoch: 222180 | training loss: 1.9699e-03 | validation loss: 1.5849e-03\n",
      "Epoch: 222190 | training loss: 1.9934e-03 | validation loss: 1.6140e-03\n",
      "Epoch: 222200 | training loss: 3.2424e-03 | validation loss: 2.3482e-03\n",
      "Epoch: 222210 | training loss: 2.4758e-03 | validation loss: 1.7390e-03\n",
      "Epoch: 222220 | training loss: 1.9894e-03 | validation loss: 1.5687e-03\n",
      "Epoch: 222230 | training loss: 2.0229e-03 | validation loss: 1.6281e-03\n",
      "Epoch: 222240 | training loss: 1.9764e-03 | validation loss: 1.5903e-03\n",
      "Epoch: 222250 | training loss: 1.9762e-03 | validation loss: 1.5732e-03\n",
      "Epoch: 222260 | training loss: 1.9692e-03 | validation loss: 1.5804e-03\n",
      "Epoch: 222270 | training loss: 1.9702e-03 | validation loss: 1.5855e-03\n",
      "Epoch: 222280 | training loss: 1.9695e-03 | validation loss: 1.5784e-03\n",
      "Epoch: 222290 | training loss: 1.9692e-03 | validation loss: 1.5812e-03\n",
      "Epoch: 222300 | training loss: 1.9691e-03 | validation loss: 1.5802e-03\n",
      "Epoch: 222310 | training loss: 1.9691e-03 | validation loss: 1.5808e-03\n",
      "Epoch: 222320 | training loss: 1.9690e-03 | validation loss: 1.5801e-03\n",
      "Epoch: 222330 | training loss: 1.9690e-03 | validation loss: 1.5807e-03\n",
      "Epoch: 222340 | training loss: 1.9690e-03 | validation loss: 1.5803e-03\n",
      "Epoch: 222350 | training loss: 1.9690e-03 | validation loss: 1.5802e-03\n",
      "Epoch: 222360 | training loss: 1.9690e-03 | validation loss: 1.5802e-03\n",
      "Epoch: 222370 | training loss: 1.9689e-03 | validation loss: 1.5801e-03\n",
      "Epoch: 222380 | training loss: 1.9690e-03 | validation loss: 1.5795e-03\n",
      "Epoch: 222390 | training loss: 1.9711e-03 | validation loss: 1.5758e-03\n",
      "Epoch: 222400 | training loss: 2.1905e-03 | validation loss: 1.6315e-03\n",
      "Epoch: 222410 | training loss: 2.0495e-03 | validation loss: 1.5936e-03\n",
      "Epoch: 222420 | training loss: 2.3698e-03 | validation loss: 1.6960e-03\n",
      "Epoch: 222430 | training loss: 2.0461e-03 | validation loss: 1.5774e-03\n",
      "Epoch: 222440 | training loss: 1.9690e-03 | validation loss: 1.5783e-03\n",
      "Epoch: 222450 | training loss: 1.9807e-03 | validation loss: 1.6019e-03\n",
      "Epoch: 222460 | training loss: 1.9741e-03 | validation loss: 1.5922e-03\n",
      "Epoch: 222470 | training loss: 1.9688e-03 | validation loss: 1.5792e-03\n",
      "Epoch: 222480 | training loss: 1.9696e-03 | validation loss: 1.5770e-03\n",
      "Epoch: 222490 | training loss: 1.9687e-03 | validation loss: 1.5806e-03\n",
      "Epoch: 222500 | training loss: 1.9688e-03 | validation loss: 1.5816e-03\n",
      "Epoch: 222510 | training loss: 1.9687e-03 | validation loss: 1.5794e-03\n",
      "Epoch: 222520 | training loss: 1.9687e-03 | validation loss: 1.5803e-03\n",
      "Epoch: 222530 | training loss: 1.9687e-03 | validation loss: 1.5802e-03\n",
      "Epoch: 222540 | training loss: 1.9686e-03 | validation loss: 1.5800e-03\n",
      "Epoch: 222550 | training loss: 1.9686e-03 | validation loss: 1.5801e-03\n",
      "Epoch: 222560 | training loss: 1.9686e-03 | validation loss: 1.5801e-03\n",
      "Epoch: 222570 | training loss: 1.9686e-03 | validation loss: 1.5804e-03\n",
      "Epoch: 222580 | training loss: 1.9699e-03 | validation loss: 1.5845e-03\n",
      "Epoch: 222590 | training loss: 2.1299e-03 | validation loss: 1.7317e-03\n",
      "Epoch: 222600 | training loss: 2.0934e-03 | validation loss: 1.6479e-03\n",
      "Epoch: 222610 | training loss: 1.9870e-03 | validation loss: 1.5986e-03\n",
      "Epoch: 222620 | training loss: 2.0028e-03 | validation loss: 1.6115e-03\n",
      "Epoch: 222630 | training loss: 1.9789e-03 | validation loss: 1.5932e-03\n",
      "Epoch: 222640 | training loss: 1.9686e-03 | validation loss: 1.5816e-03\n",
      "Epoch: 222650 | training loss: 1.9709e-03 | validation loss: 1.5831e-03\n",
      "Epoch: 222660 | training loss: 2.0014e-03 | validation loss: 1.6189e-03\n",
      "Epoch: 222670 | training loss: 2.9508e-03 | validation loss: 2.1912e-03\n",
      "Epoch: 222680 | training loss: 2.3675e-03 | validation loss: 1.6924e-03\n",
      "Epoch: 222690 | training loss: 2.0149e-03 | validation loss: 1.6273e-03\n",
      "Epoch: 222700 | training loss: 1.9805e-03 | validation loss: 1.5969e-03\n",
      "Epoch: 222710 | training loss: 1.9871e-03 | validation loss: 1.5702e-03\n",
      "Epoch: 222720 | training loss: 1.9727e-03 | validation loss: 1.5898e-03\n",
      "Epoch: 222730 | training loss: 1.9687e-03 | validation loss: 1.5776e-03\n",
      "Epoch: 222740 | training loss: 1.9683e-03 | validation loss: 1.5803e-03\n",
      "Epoch: 222750 | training loss: 1.9683e-03 | validation loss: 1.5788e-03\n",
      "Epoch: 222760 | training loss: 1.9683e-03 | validation loss: 1.5804e-03\n",
      "Epoch: 222770 | training loss: 1.9683e-03 | validation loss: 1.5785e-03\n",
      "Epoch: 222780 | training loss: 1.9682e-03 | validation loss: 1.5796e-03\n",
      "Epoch: 222790 | training loss: 1.9682e-03 | validation loss: 1.5799e-03\n",
      "Epoch: 222800 | training loss: 1.9682e-03 | validation loss: 1.5798e-03\n",
      "Epoch: 222810 | training loss: 1.9682e-03 | validation loss: 1.5802e-03\n",
      "Epoch: 222820 | training loss: 1.9690e-03 | validation loss: 1.5834e-03\n",
      "Epoch: 222830 | training loss: 2.0100e-03 | validation loss: 1.6249e-03\n",
      "Epoch: 222840 | training loss: 3.4790e-03 | validation loss: 2.4573e-03\n",
      "Epoch: 222850 | training loss: 2.2384e-03 | validation loss: 1.6579e-03\n",
      "Epoch: 222860 | training loss: 2.0714e-03 | validation loss: 1.5826e-03\n",
      "Epoch: 222870 | training loss: 1.9836e-03 | validation loss: 1.5928e-03\n",
      "Epoch: 222880 | training loss: 1.9868e-03 | validation loss: 1.6111e-03\n",
      "Epoch: 222890 | training loss: 1.9689e-03 | validation loss: 1.5784e-03\n",
      "Epoch: 222900 | training loss: 1.9705e-03 | validation loss: 1.5726e-03\n",
      "Epoch: 222910 | training loss: 1.9687e-03 | validation loss: 1.5839e-03\n",
      "Epoch: 222920 | training loss: 1.9680e-03 | validation loss: 1.5790e-03\n",
      "Epoch: 222930 | training loss: 1.9680e-03 | validation loss: 1.5784e-03\n",
      "Epoch: 222940 | training loss: 1.9680e-03 | validation loss: 1.5800e-03\n",
      "Epoch: 222950 | training loss: 1.9679e-03 | validation loss: 1.5787e-03\n",
      "Epoch: 222960 | training loss: 1.9679e-03 | validation loss: 1.5794e-03\n",
      "Epoch: 222970 | training loss: 1.9679e-03 | validation loss: 1.5790e-03\n",
      "Epoch: 222980 | training loss: 1.9678e-03 | validation loss: 1.5789e-03\n",
      "Epoch: 222990 | training loss: 1.9678e-03 | validation loss: 1.5791e-03\n",
      "Epoch: 223000 | training loss: 1.9678e-03 | validation loss: 1.5789e-03\n",
      "Epoch: 223010 | training loss: 1.9688e-03 | validation loss: 1.5782e-03\n",
      "Epoch: 223020 | training loss: 2.0563e-03 | validation loss: 1.6229e-03\n",
      "Epoch: 223030 | training loss: 2.0998e-03 | validation loss: 1.6912e-03\n",
      "Epoch: 223040 | training loss: 2.2285e-03 | validation loss: 1.7432e-03\n",
      "Epoch: 223050 | training loss: 2.0696e-03 | validation loss: 1.5900e-03\n",
      "Epoch: 223060 | training loss: 1.9765e-03 | validation loss: 1.5842e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 223070 | training loss: 1.9756e-03 | validation loss: 1.5948e-03\n",
      "Epoch: 223080 | training loss: 1.9698e-03 | validation loss: 1.5789e-03\n",
      "Epoch: 223090 | training loss: 1.9734e-03 | validation loss: 1.5741e-03\n",
      "Epoch: 223100 | training loss: 1.9894e-03 | validation loss: 1.5693e-03\n",
      "Epoch: 223110 | training loss: 2.1919e-03 | validation loss: 1.6224e-03\n",
      "Epoch: 223120 | training loss: 2.0939e-03 | validation loss: 1.5931e-03\n",
      "Epoch: 223130 | training loss: 2.0510e-03 | validation loss: 1.6561e-03\n",
      "Epoch: 223140 | training loss: 2.0087e-03 | validation loss: 1.5717e-03\n",
      "Epoch: 223150 | training loss: 1.9803e-03 | validation loss: 1.5991e-03\n",
      "Epoch: 223160 | training loss: 1.9675e-03 | validation loss: 1.5779e-03\n",
      "Epoch: 223170 | training loss: 1.9709e-03 | validation loss: 1.5726e-03\n",
      "Epoch: 223180 | training loss: 1.9678e-03 | validation loss: 1.5764e-03\n",
      "Epoch: 223190 | training loss: 1.9675e-03 | validation loss: 1.5789e-03\n",
      "Epoch: 223200 | training loss: 1.9677e-03 | validation loss: 1.5807e-03\n",
      "Epoch: 223210 | training loss: 1.9747e-03 | validation loss: 1.5930e-03\n",
      "Epoch: 223220 | training loss: 2.3669e-03 | validation loss: 1.8523e-03\n",
      "Epoch: 223230 | training loss: 1.9942e-03 | validation loss: 1.5669e-03\n",
      "Epoch: 223240 | training loss: 2.2048e-03 | validation loss: 1.7495e-03\n",
      "Epoch: 223250 | training loss: 1.9795e-03 | validation loss: 1.5902e-03\n",
      "Epoch: 223260 | training loss: 1.9924e-03 | validation loss: 1.5735e-03\n",
      "Epoch: 223270 | training loss: 1.9732e-03 | validation loss: 1.5835e-03\n",
      "Epoch: 223280 | training loss: 1.9678e-03 | validation loss: 1.5818e-03\n",
      "Epoch: 223290 | training loss: 1.9686e-03 | validation loss: 1.5771e-03\n",
      "Epoch: 223300 | training loss: 1.9679e-03 | validation loss: 1.5798e-03\n",
      "Epoch: 223310 | training loss: 1.9674e-03 | validation loss: 1.5777e-03\n",
      "Epoch: 223320 | training loss: 1.9673e-03 | validation loss: 1.5791e-03\n",
      "Epoch: 223330 | training loss: 1.9672e-03 | validation loss: 1.5779e-03\n",
      "Epoch: 223340 | training loss: 1.9672e-03 | validation loss: 1.5785e-03\n",
      "Epoch: 223350 | training loss: 1.9672e-03 | validation loss: 1.5784e-03\n",
      "Epoch: 223360 | training loss: 1.9671e-03 | validation loss: 1.5780e-03\n",
      "Epoch: 223370 | training loss: 1.9671e-03 | validation loss: 1.5781e-03\n",
      "Epoch: 223380 | training loss: 1.9671e-03 | validation loss: 1.5779e-03\n",
      "Epoch: 223390 | training loss: 1.9673e-03 | validation loss: 1.5770e-03\n",
      "Epoch: 223400 | training loss: 1.9756e-03 | validation loss: 1.5739e-03\n",
      "Epoch: 223410 | training loss: 2.6242e-03 | validation loss: 1.8242e-03\n",
      "Epoch: 223420 | training loss: 2.3822e-03 | validation loss: 1.8343e-03\n",
      "Epoch: 223430 | training loss: 2.0421e-03 | validation loss: 1.5737e-03\n",
      "Epoch: 223440 | training loss: 2.0165e-03 | validation loss: 1.6126e-03\n",
      "Epoch: 223450 | training loss: 1.9999e-03 | validation loss: 1.6157e-03\n",
      "Epoch: 223460 | training loss: 1.9729e-03 | validation loss: 1.5775e-03\n",
      "Epoch: 223470 | training loss: 1.9670e-03 | validation loss: 1.5783e-03\n",
      "Epoch: 223480 | training loss: 1.9682e-03 | validation loss: 1.5792e-03\n",
      "Epoch: 223490 | training loss: 1.9672e-03 | validation loss: 1.5768e-03\n",
      "Epoch: 223500 | training loss: 1.9670e-03 | validation loss: 1.5792e-03\n",
      "Epoch: 223510 | training loss: 1.9669e-03 | validation loss: 1.5779e-03\n",
      "Epoch: 223520 | training loss: 1.9669e-03 | validation loss: 1.5778e-03\n",
      "Epoch: 223530 | training loss: 1.9668e-03 | validation loss: 1.5783e-03\n",
      "Epoch: 223540 | training loss: 1.9668e-03 | validation loss: 1.5780e-03\n",
      "Epoch: 223550 | training loss: 1.9668e-03 | validation loss: 1.5778e-03\n",
      "Epoch: 223560 | training loss: 1.9668e-03 | validation loss: 1.5776e-03\n",
      "Epoch: 223570 | training loss: 1.9668e-03 | validation loss: 1.5767e-03\n",
      "Epoch: 223580 | training loss: 1.9690e-03 | validation loss: 1.5715e-03\n",
      "Epoch: 223590 | training loss: 2.1604e-03 | validation loss: 1.6134e-03\n",
      "Epoch: 223600 | training loss: 2.0830e-03 | validation loss: 1.6174e-03\n",
      "Epoch: 223610 | training loss: 2.1365e-03 | validation loss: 1.6196e-03\n",
      "Epoch: 223620 | training loss: 2.0561e-03 | validation loss: 1.6178e-03\n",
      "Epoch: 223630 | training loss: 1.9789e-03 | validation loss: 1.5670e-03\n",
      "Epoch: 223640 | training loss: 1.9667e-03 | validation loss: 1.5781e-03\n",
      "Epoch: 223650 | training loss: 1.9693e-03 | validation loss: 1.5869e-03\n",
      "Epoch: 223660 | training loss: 1.9679e-03 | validation loss: 1.5809e-03\n",
      "Epoch: 223670 | training loss: 1.9667e-03 | validation loss: 1.5782e-03\n",
      "Epoch: 223680 | training loss: 1.9668e-03 | validation loss: 1.5755e-03\n",
      "Epoch: 223690 | training loss: 1.9666e-03 | validation loss: 1.5773e-03\n",
      "Epoch: 223700 | training loss: 1.9666e-03 | validation loss: 1.5783e-03\n",
      "Epoch: 223710 | training loss: 1.9665e-03 | validation loss: 1.5771e-03\n",
      "Epoch: 223720 | training loss: 1.9665e-03 | validation loss: 1.5771e-03\n",
      "Epoch: 223730 | training loss: 1.9665e-03 | validation loss: 1.5770e-03\n",
      "Epoch: 223740 | training loss: 1.9667e-03 | validation loss: 1.5759e-03\n",
      "Epoch: 223750 | training loss: 1.9757e-03 | validation loss: 1.5726e-03\n",
      "Epoch: 223760 | training loss: 2.6279e-03 | validation loss: 1.8224e-03\n",
      "Epoch: 223770 | training loss: 2.3338e-03 | validation loss: 1.8171e-03\n",
      "Epoch: 223780 | training loss: 2.0908e-03 | validation loss: 1.5863e-03\n",
      "Epoch: 223790 | training loss: 2.0291e-03 | validation loss: 1.5760e-03\n",
      "Epoch: 223800 | training loss: 1.9720e-03 | validation loss: 1.5921e-03\n",
      "Epoch: 223810 | training loss: 1.9766e-03 | validation loss: 1.5968e-03\n",
      "Epoch: 223820 | training loss: 1.9674e-03 | validation loss: 1.5732e-03\n",
      "Epoch: 223830 | training loss: 1.9670e-03 | validation loss: 1.5737e-03\n",
      "Epoch: 223840 | training loss: 1.9668e-03 | validation loss: 1.5808e-03\n",
      "Epoch: 223850 | training loss: 1.9664e-03 | validation loss: 1.5763e-03\n",
      "Epoch: 223860 | training loss: 1.9663e-03 | validation loss: 1.5774e-03\n",
      "Epoch: 223870 | training loss: 1.9662e-03 | validation loss: 1.5772e-03\n",
      "Epoch: 223880 | training loss: 1.9662e-03 | validation loss: 1.5773e-03\n",
      "Epoch: 223890 | training loss: 1.9662e-03 | validation loss: 1.5770e-03\n",
      "Epoch: 223900 | training loss: 1.9662e-03 | validation loss: 1.5772e-03\n",
      "Epoch: 223910 | training loss: 1.9662e-03 | validation loss: 1.5771e-03\n",
      "Epoch: 223920 | training loss: 1.9661e-03 | validation loss: 1.5770e-03\n",
      "Epoch: 223930 | training loss: 1.9661e-03 | validation loss: 1.5769e-03\n",
      "Epoch: 223940 | training loss: 1.9661e-03 | validation loss: 1.5767e-03\n",
      "Epoch: 223950 | training loss: 1.9663e-03 | validation loss: 1.5753e-03\n",
      "Epoch: 223960 | training loss: 1.9822e-03 | validation loss: 1.5698e-03\n",
      "Epoch: 223970 | training loss: 3.3228e-03 | validation loss: 2.0948e-03\n",
      "Epoch: 223980 | training loss: 2.6540e-03 | validation loss: 2.0092e-03\n",
      "Epoch: 223990 | training loss: 2.1031e-03 | validation loss: 1.6961e-03\n",
      "Epoch: 224000 | training loss: 1.9661e-03 | validation loss: 1.5743e-03\n",
      "Epoch: 224010 | training loss: 1.9862e-03 | validation loss: 1.5677e-03\n",
      "Epoch: 224020 | training loss: 1.9759e-03 | validation loss: 1.5689e-03\n",
      "Epoch: 224030 | training loss: 1.9659e-03 | validation loss: 1.5771e-03\n",
      "Epoch: 224040 | training loss: 1.9674e-03 | validation loss: 1.5828e-03\n",
      "Epoch: 224050 | training loss: 1.9659e-03 | validation loss: 1.5774e-03\n",
      "Epoch: 224060 | training loss: 1.9661e-03 | validation loss: 1.5751e-03\n",
      "Epoch: 224070 | training loss: 1.9659e-03 | validation loss: 1.5776e-03\n",
      "Epoch: 224080 | training loss: 1.9658e-03 | validation loss: 1.5771e-03\n",
      "Epoch: 224090 | training loss: 1.9658e-03 | validation loss: 1.5766e-03\n",
      "Epoch: 224100 | training loss: 1.9658e-03 | validation loss: 1.5774e-03\n",
      "Epoch: 224110 | training loss: 1.9663e-03 | validation loss: 1.5792e-03\n",
      "Epoch: 224120 | training loss: 2.0038e-03 | validation loss: 1.6228e-03\n",
      "Epoch: 224130 | training loss: 2.1896e-03 | validation loss: 1.7785e-03\n",
      "Epoch: 224140 | training loss: 2.0142e-03 | validation loss: 1.6173e-03\n",
      "Epoch: 224150 | training loss: 1.9794e-03 | validation loss: 1.5807e-03\n",
      "Epoch: 224160 | training loss: 1.9805e-03 | validation loss: 1.5759e-03\n",
      "Epoch: 224170 | training loss: 1.9657e-03 | validation loss: 1.5755e-03\n",
      "Epoch: 224180 | training loss: 1.9678e-03 | validation loss: 1.5822e-03\n",
      "Epoch: 224190 | training loss: 1.9657e-03 | validation loss: 1.5773e-03\n",
      "Epoch: 224200 | training loss: 1.9660e-03 | validation loss: 1.5771e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 224210 | training loss: 1.9672e-03 | validation loss: 1.5833e-03\n",
      "Epoch: 224220 | training loss: 2.0166e-03 | validation loss: 1.6322e-03\n",
      "Epoch: 224230 | training loss: 3.0996e-03 | validation loss: 2.2699e-03\n",
      "Epoch: 224240 | training loss: 2.2620e-03 | validation loss: 1.6587e-03\n",
      "Epoch: 224250 | training loss: 1.9780e-03 | validation loss: 1.5733e-03\n",
      "Epoch: 224260 | training loss: 1.9983e-03 | validation loss: 1.6094e-03\n",
      "Epoch: 224270 | training loss: 1.9798e-03 | validation loss: 1.5764e-03\n",
      "Epoch: 224280 | training loss: 1.9662e-03 | validation loss: 1.5800e-03\n",
      "Epoch: 224290 | training loss: 1.9658e-03 | validation loss: 1.5752e-03\n",
      "Epoch: 224300 | training loss: 1.9656e-03 | validation loss: 1.5760e-03\n",
      "Epoch: 224310 | training loss: 1.9655e-03 | validation loss: 1.5767e-03\n",
      "Epoch: 224320 | training loss: 1.9654e-03 | validation loss: 1.5762e-03\n",
      "Epoch: 224330 | training loss: 1.9654e-03 | validation loss: 1.5757e-03\n",
      "Epoch: 224340 | training loss: 1.9654e-03 | validation loss: 1.5765e-03\n",
      "Epoch: 224350 | training loss: 1.9654e-03 | validation loss: 1.5765e-03\n",
      "Epoch: 224360 | training loss: 1.9653e-03 | validation loss: 1.5763e-03\n",
      "Epoch: 224370 | training loss: 1.9653e-03 | validation loss: 1.5764e-03\n",
      "Epoch: 224380 | training loss: 1.9655e-03 | validation loss: 1.5776e-03\n",
      "Epoch: 224390 | training loss: 1.9745e-03 | validation loss: 1.5911e-03\n",
      "Epoch: 224400 | training loss: 2.7598e-03 | validation loss: 2.0591e-03\n",
      "Epoch: 224410 | training loss: 2.5841e-03 | validation loss: 1.7895e-03\n",
      "Epoch: 224420 | training loss: 1.9730e-03 | validation loss: 1.5913e-03\n",
      "Epoch: 224430 | training loss: 2.0481e-03 | validation loss: 1.6447e-03\n",
      "Epoch: 224440 | training loss: 1.9882e-03 | validation loss: 1.6012e-03\n",
      "Epoch: 224450 | training loss: 1.9660e-03 | validation loss: 1.5743e-03\n",
      "Epoch: 224460 | training loss: 1.9698e-03 | validation loss: 1.5714e-03\n",
      "Epoch: 224470 | training loss: 1.9652e-03 | validation loss: 1.5762e-03\n",
      "Epoch: 224480 | training loss: 1.9657e-03 | validation loss: 1.5787e-03\n",
      "Epoch: 224490 | training loss: 1.9652e-03 | validation loss: 1.5752e-03\n",
      "Epoch: 224500 | training loss: 1.9651e-03 | validation loss: 1.5757e-03\n",
      "Epoch: 224510 | training loss: 1.9651e-03 | validation loss: 1.5764e-03\n",
      "Epoch: 224520 | training loss: 1.9651e-03 | validation loss: 1.5756e-03\n",
      "Epoch: 224530 | training loss: 1.9650e-03 | validation loss: 1.5761e-03\n",
      "Epoch: 224540 | training loss: 1.9650e-03 | validation loss: 1.5757e-03\n",
      "Epoch: 224550 | training loss: 1.9650e-03 | validation loss: 1.5758e-03\n",
      "Epoch: 224560 | training loss: 1.9650e-03 | validation loss: 1.5754e-03\n",
      "Epoch: 224570 | training loss: 1.9658e-03 | validation loss: 1.5725e-03\n",
      "Epoch: 224580 | training loss: 2.0946e-03 | validation loss: 1.6190e-03\n",
      "Epoch: 224590 | training loss: 2.1760e-03 | validation loss: 1.6624e-03\n",
      "Epoch: 224600 | training loss: 2.0202e-03 | validation loss: 1.6233e-03\n",
      "Epoch: 224610 | training loss: 1.9941e-03 | validation loss: 1.5694e-03\n",
      "Epoch: 224620 | training loss: 1.9777e-03 | validation loss: 1.5767e-03\n",
      "Epoch: 224630 | training loss: 1.9730e-03 | validation loss: 1.5802e-03\n",
      "Epoch: 224640 | training loss: 1.9668e-03 | validation loss: 1.5713e-03\n",
      "Epoch: 224650 | training loss: 1.9661e-03 | validation loss: 1.5706e-03\n",
      "Epoch: 224660 | training loss: 1.9694e-03 | validation loss: 1.5693e-03\n",
      "Epoch: 224670 | training loss: 2.0214e-03 | validation loss: 1.5712e-03\n",
      "Epoch: 224680 | training loss: 2.6425e-03 | validation loss: 1.7924e-03\n",
      "Epoch: 224690 | training loss: 2.1627e-03 | validation loss: 1.7262e-03\n",
      "Epoch: 224700 | training loss: 2.0514e-03 | validation loss: 1.5791e-03\n",
      "Epoch: 224710 | training loss: 1.9988e-03 | validation loss: 1.6151e-03\n",
      "Epoch: 224720 | training loss: 1.9777e-03 | validation loss: 1.5672e-03\n",
      "Epoch: 224730 | training loss: 1.9691e-03 | validation loss: 1.5857e-03\n",
      "Epoch: 224740 | training loss: 1.9651e-03 | validation loss: 1.5725e-03\n",
      "Epoch: 224750 | training loss: 1.9650e-03 | validation loss: 1.5730e-03\n",
      "Epoch: 224760 | training loss: 1.9649e-03 | validation loss: 1.5775e-03\n",
      "Epoch: 224770 | training loss: 1.9650e-03 | validation loss: 1.5780e-03\n",
      "Epoch: 224780 | training loss: 1.9658e-03 | validation loss: 1.5802e-03\n",
      "Epoch: 224790 | training loss: 1.9807e-03 | validation loss: 1.5989e-03\n",
      "Epoch: 224800 | training loss: 2.4020e-03 | validation loss: 1.8686e-03\n",
      "Epoch: 224810 | training loss: 1.9731e-03 | validation loss: 1.5650e-03\n",
      "Epoch: 224820 | training loss: 2.0405e-03 | validation loss: 1.6452e-03\n",
      "Epoch: 224830 | training loss: 2.0247e-03 | validation loss: 1.5784e-03\n",
      "Epoch: 224840 | training loss: 1.9824e-03 | validation loss: 1.5969e-03\n",
      "Epoch: 224850 | training loss: 1.9682e-03 | validation loss: 1.5700e-03\n",
      "Epoch: 224860 | training loss: 1.9656e-03 | validation loss: 1.5800e-03\n",
      "Epoch: 224870 | training loss: 1.9652e-03 | validation loss: 1.5718e-03\n",
      "Epoch: 224880 | training loss: 1.9649e-03 | validation loss: 1.5782e-03\n",
      "Epoch: 224890 | training loss: 1.9645e-03 | validation loss: 1.5738e-03\n",
      "Epoch: 224900 | training loss: 1.9644e-03 | validation loss: 1.5743e-03\n",
      "Epoch: 224910 | training loss: 1.9643e-03 | validation loss: 1.5751e-03\n",
      "Epoch: 224920 | training loss: 1.9644e-03 | validation loss: 1.5756e-03\n",
      "Epoch: 224930 | training loss: 1.9648e-03 | validation loss: 1.5775e-03\n",
      "Epoch: 224940 | training loss: 1.9778e-03 | validation loss: 1.5946e-03\n",
      "Epoch: 224950 | training loss: 2.6662e-03 | validation loss: 2.0092e-03\n",
      "Epoch: 224960 | training loss: 2.2886e-03 | validation loss: 1.6660e-03\n",
      "Epoch: 224970 | training loss: 2.1121e-03 | validation loss: 1.6892e-03\n",
      "Epoch: 224980 | training loss: 1.9893e-03 | validation loss: 1.6048e-03\n",
      "Epoch: 224990 | training loss: 1.9864e-03 | validation loss: 1.5680e-03\n",
      "Epoch: 225000 | training loss: 1.9647e-03 | validation loss: 1.5727e-03\n",
      "Epoch: 225010 | training loss: 1.9678e-03 | validation loss: 1.5835e-03\n",
      "Epoch: 225020 | training loss: 1.9651e-03 | validation loss: 1.5718e-03\n",
      "Epoch: 225030 | training loss: 1.9642e-03 | validation loss: 1.5759e-03\n",
      "Epoch: 225040 | training loss: 1.9641e-03 | validation loss: 1.5748e-03\n",
      "Epoch: 225050 | training loss: 1.9641e-03 | validation loss: 1.5748e-03\n",
      "Epoch: 225060 | training loss: 1.9641e-03 | validation loss: 1.5746e-03\n",
      "Epoch: 225070 | training loss: 1.9641e-03 | validation loss: 1.5751e-03\n",
      "Epoch: 225080 | training loss: 1.9640e-03 | validation loss: 1.5747e-03\n",
      "Epoch: 225090 | training loss: 1.9642e-03 | validation loss: 1.5763e-03\n",
      "Epoch: 225100 | training loss: 1.9867e-03 | validation loss: 1.6081e-03\n",
      "Epoch: 225110 | training loss: 2.5199e-03 | validation loss: 2.0426e-03\n",
      "Epoch: 225120 | training loss: 2.0490e-03 | validation loss: 1.6623e-03\n",
      "Epoch: 225130 | training loss: 1.9792e-03 | validation loss: 1.5675e-03\n",
      "Epoch: 225140 | training loss: 1.9680e-03 | validation loss: 1.5738e-03\n",
      "Epoch: 225150 | training loss: 1.9740e-03 | validation loss: 1.5823e-03\n",
      "Epoch: 225160 | training loss: 1.9733e-03 | validation loss: 1.5862e-03\n",
      "Epoch: 225170 | training loss: 2.0028e-03 | validation loss: 1.6186e-03\n",
      "Epoch: 225180 | training loss: 2.3615e-03 | validation loss: 1.8522e-03\n",
      "Epoch: 225190 | training loss: 1.9667e-03 | validation loss: 1.5690e-03\n",
      "Epoch: 225200 | training loss: 1.9660e-03 | validation loss: 1.5690e-03\n",
      "Epoch: 225210 | training loss: 1.9719e-03 | validation loss: 1.5892e-03\n",
      "Epoch: 225220 | training loss: 1.9748e-03 | validation loss: 1.5656e-03\n",
      "Epoch: 225230 | training loss: 1.9678e-03 | validation loss: 1.5843e-03\n",
      "Epoch: 225240 | training loss: 1.9644e-03 | validation loss: 1.5777e-03\n",
      "Epoch: 225250 | training loss: 1.9643e-03 | validation loss: 1.5713e-03\n",
      "Epoch: 225260 | training loss: 1.9661e-03 | validation loss: 1.5689e-03\n",
      "Epoch: 225270 | training loss: 1.9849e-03 | validation loss: 1.5655e-03\n",
      "Epoch: 225280 | training loss: 2.3485e-03 | validation loss: 1.6797e-03\n",
      "Epoch: 225290 | training loss: 1.9667e-03 | validation loss: 1.5744e-03\n",
      "Epoch: 225300 | training loss: 1.9670e-03 | validation loss: 1.5673e-03\n",
      "Epoch: 225310 | training loss: 1.9717e-03 | validation loss: 1.5869e-03\n",
      "Epoch: 225320 | training loss: 1.9678e-03 | validation loss: 1.5690e-03\n",
      "Epoch: 225330 | training loss: 1.9642e-03 | validation loss: 1.5774e-03\n",
      "Epoch: 225340 | training loss: 1.9638e-03 | validation loss: 1.5753e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 225350 | training loss: 1.9646e-03 | validation loss: 1.5708e-03\n",
      "Epoch: 225360 | training loss: 1.9637e-03 | validation loss: 1.5755e-03\n",
      "Epoch: 225370 | training loss: 1.9639e-03 | validation loss: 1.5766e-03\n",
      "Epoch: 225380 | training loss: 1.9640e-03 | validation loss: 1.5769e-03\n",
      "Epoch: 225390 | training loss: 1.9673e-03 | validation loss: 1.5831e-03\n",
      "Epoch: 225400 | training loss: 2.0541e-03 | validation loss: 1.6531e-03\n",
      "Epoch: 225410 | training loss: 2.8686e-03 | validation loss: 2.1212e-03\n",
      "Epoch: 225420 | training loss: 2.1732e-03 | validation loss: 1.6249e-03\n",
      "Epoch: 225430 | training loss: 1.9676e-03 | validation loss: 1.5777e-03\n",
      "Epoch: 225440 | training loss: 1.9772e-03 | validation loss: 1.5968e-03\n",
      "Epoch: 225450 | training loss: 1.9759e-03 | validation loss: 1.5650e-03\n",
      "Epoch: 225460 | training loss: 1.9685e-03 | validation loss: 1.5859e-03\n",
      "Epoch: 225470 | training loss: 1.9652e-03 | validation loss: 1.5687e-03\n",
      "Epoch: 225480 | training loss: 1.9641e-03 | validation loss: 1.5785e-03\n",
      "Epoch: 225490 | training loss: 1.9635e-03 | validation loss: 1.5717e-03\n",
      "Epoch: 225500 | training loss: 1.9633e-03 | validation loss: 1.5736e-03\n",
      "Epoch: 225510 | training loss: 1.9633e-03 | validation loss: 1.5750e-03\n",
      "Epoch: 225520 | training loss: 1.9633e-03 | validation loss: 1.5744e-03\n",
      "Epoch: 225530 | training loss: 1.9633e-03 | validation loss: 1.5748e-03\n",
      "Epoch: 225540 | training loss: 1.9663e-03 | validation loss: 1.5813e-03\n",
      "Epoch: 225550 | training loss: 2.1747e-03 | validation loss: 1.7635e-03\n",
      "Epoch: 225560 | training loss: 2.0918e-03 | validation loss: 1.6474e-03\n",
      "Epoch: 225570 | training loss: 2.2128e-03 | validation loss: 1.7852e-03\n",
      "Epoch: 225580 | training loss: 2.1341e-03 | validation loss: 1.7302e-03\n",
      "Epoch: 225590 | training loss: 2.0489e-03 | validation loss: 1.5813e-03\n",
      "Epoch: 225600 | training loss: 2.0025e-03 | validation loss: 1.6154e-03\n",
      "Epoch: 225610 | training loss: 1.9756e-03 | validation loss: 1.5626e-03\n",
      "Epoch: 225620 | training loss: 1.9632e-03 | validation loss: 1.5721e-03\n",
      "Epoch: 225630 | training loss: 1.9664e-03 | validation loss: 1.5835e-03\n",
      "Epoch: 225640 | training loss: 1.9635e-03 | validation loss: 1.5769e-03\n",
      "Epoch: 225650 | training loss: 1.9631e-03 | validation loss: 1.5736e-03\n",
      "Epoch: 225660 | training loss: 1.9631e-03 | validation loss: 1.5745e-03\n",
      "Epoch: 225670 | training loss: 1.9650e-03 | validation loss: 1.5802e-03\n",
      "Epoch: 225680 | training loss: 2.0902e-03 | validation loss: 1.6809e-03\n",
      "Epoch: 225690 | training loss: 2.6792e-03 | validation loss: 2.0193e-03\n",
      "Epoch: 225700 | training loss: 2.0490e-03 | validation loss: 1.6606e-03\n",
      "Epoch: 225710 | training loss: 2.0489e-03 | validation loss: 1.5947e-03\n",
      "Epoch: 225720 | training loss: 1.9858e-03 | validation loss: 1.5663e-03\n",
      "Epoch: 225730 | training loss: 1.9711e-03 | validation loss: 1.5819e-03\n",
      "Epoch: 225740 | training loss: 1.9656e-03 | validation loss: 1.5782e-03\n",
      "Epoch: 225750 | training loss: 1.9648e-03 | validation loss: 1.5708e-03\n",
      "Epoch: 225760 | training loss: 1.9629e-03 | validation loss: 1.5746e-03\n",
      "Epoch: 225770 | training loss: 1.9629e-03 | validation loss: 1.5737e-03\n",
      "Epoch: 225780 | training loss: 1.9629e-03 | validation loss: 1.5722e-03\n",
      "Epoch: 225790 | training loss: 1.9628e-03 | validation loss: 1.5740e-03\n",
      "Epoch: 225800 | training loss: 1.9628e-03 | validation loss: 1.5726e-03\n",
      "Epoch: 225810 | training loss: 1.9627e-03 | validation loss: 1.5733e-03\n",
      "Epoch: 225820 | training loss: 1.9627e-03 | validation loss: 1.5730e-03\n",
      "Epoch: 225830 | training loss: 1.9627e-03 | validation loss: 1.5729e-03\n",
      "Epoch: 225840 | training loss: 1.9627e-03 | validation loss: 1.5730e-03\n",
      "Epoch: 225850 | training loss: 1.9627e-03 | validation loss: 1.5730e-03\n",
      "Epoch: 225860 | training loss: 1.9626e-03 | validation loss: 1.5731e-03\n",
      "Epoch: 225870 | training loss: 1.9626e-03 | validation loss: 1.5734e-03\n",
      "Epoch: 225880 | training loss: 1.9634e-03 | validation loss: 1.5764e-03\n",
      "Epoch: 225890 | training loss: 2.0598e-03 | validation loss: 1.6535e-03\n",
      "Epoch: 225900 | training loss: 3.0765e-03 | validation loss: 2.2332e-03\n",
      "Epoch: 225910 | training loss: 2.3798e-03 | validation loss: 1.8358e-03\n",
      "Epoch: 225920 | training loss: 2.0138e-03 | validation loss: 1.6092e-03\n",
      "Epoch: 225930 | training loss: 1.9631e-03 | validation loss: 1.5709e-03\n",
      "Epoch: 225940 | training loss: 1.9678e-03 | validation loss: 1.5729e-03\n",
      "Epoch: 225950 | training loss: 1.9697e-03 | validation loss: 1.5713e-03\n",
      "Epoch: 225960 | training loss: 1.9650e-03 | validation loss: 1.5702e-03\n",
      "Epoch: 225970 | training loss: 1.9625e-03 | validation loss: 1.5732e-03\n",
      "Epoch: 225980 | training loss: 1.9628e-03 | validation loss: 1.5742e-03\n",
      "Epoch: 225990 | training loss: 1.9625e-03 | validation loss: 1.5735e-03\n",
      "Epoch: 226000 | training loss: 1.9624e-03 | validation loss: 1.5725e-03\n",
      "Epoch: 226010 | training loss: 1.9624e-03 | validation loss: 1.5728e-03\n",
      "Epoch: 226020 | training loss: 1.9624e-03 | validation loss: 1.5729e-03\n",
      "Epoch: 226030 | training loss: 1.9623e-03 | validation loss: 1.5727e-03\n",
      "Epoch: 226040 | training loss: 1.9623e-03 | validation loss: 1.5728e-03\n",
      "Epoch: 226050 | training loss: 1.9623e-03 | validation loss: 1.5727e-03\n",
      "Epoch: 226060 | training loss: 1.9623e-03 | validation loss: 1.5727e-03\n",
      "Epoch: 226070 | training loss: 1.9623e-03 | validation loss: 1.5728e-03\n",
      "Epoch: 226080 | training loss: 1.9623e-03 | validation loss: 1.5736e-03\n",
      "Epoch: 226090 | training loss: 1.9658e-03 | validation loss: 1.5839e-03\n",
      "Epoch: 226100 | training loss: 2.4838e-03 | validation loss: 2.0277e-03\n",
      "Epoch: 226110 | training loss: 2.2283e-03 | validation loss: 1.6991e-03\n",
      "Epoch: 226120 | training loss: 2.0879e-03 | validation loss: 1.6424e-03\n",
      "Epoch: 226130 | training loss: 2.0126e-03 | validation loss: 1.5732e-03\n",
      "Epoch: 226140 | training loss: 1.9801e-03 | validation loss: 1.5839e-03\n",
      "Epoch: 226150 | training loss: 1.9672e-03 | validation loss: 1.5645e-03\n",
      "Epoch: 226160 | training loss: 1.9628e-03 | validation loss: 1.5739e-03\n",
      "Epoch: 226170 | training loss: 1.9621e-03 | validation loss: 1.5728e-03\n",
      "Epoch: 226180 | training loss: 1.9624e-03 | validation loss: 1.5714e-03\n",
      "Epoch: 226190 | training loss: 1.9621e-03 | validation loss: 1.5728e-03\n",
      "Epoch: 226200 | training loss: 1.9621e-03 | validation loss: 1.5730e-03\n",
      "Epoch: 226210 | training loss: 1.9623e-03 | validation loss: 1.5742e-03\n",
      "Epoch: 226220 | training loss: 1.9673e-03 | validation loss: 1.5841e-03\n",
      "Epoch: 226230 | training loss: 2.1880e-03 | validation loss: 1.7434e-03\n",
      "Epoch: 226240 | training loss: 2.1298e-03 | validation loss: 1.6989e-03\n",
      "Epoch: 226250 | training loss: 2.0272e-03 | validation loss: 1.6356e-03\n",
      "Epoch: 226260 | training loss: 2.0506e-03 | validation loss: 1.5819e-03\n",
      "Epoch: 226270 | training loss: 1.9632e-03 | validation loss: 1.5778e-03\n",
      "Epoch: 226280 | training loss: 1.9687e-03 | validation loss: 1.5851e-03\n",
      "Epoch: 226290 | training loss: 1.9665e-03 | validation loss: 1.5656e-03\n",
      "Epoch: 226300 | training loss: 1.9630e-03 | validation loss: 1.5763e-03\n",
      "Epoch: 226310 | training loss: 1.9621e-03 | validation loss: 1.5704e-03\n",
      "Epoch: 226320 | training loss: 1.9619e-03 | validation loss: 1.5734e-03\n",
      "Epoch: 226330 | training loss: 1.9619e-03 | validation loss: 1.5708e-03\n",
      "Epoch: 226340 | training loss: 1.9618e-03 | validation loss: 1.5727e-03\n",
      "Epoch: 226350 | training loss: 1.9618e-03 | validation loss: 1.5718e-03\n",
      "Epoch: 226360 | training loss: 1.9618e-03 | validation loss: 1.5715e-03\n",
      "Epoch: 226370 | training loss: 1.9617e-03 | validation loss: 1.5715e-03\n",
      "Epoch: 226380 | training loss: 1.9617e-03 | validation loss: 1.5712e-03\n",
      "Epoch: 226390 | training loss: 1.9622e-03 | validation loss: 1.5694e-03\n",
      "Epoch: 226400 | training loss: 1.9844e-03 | validation loss: 1.5649e-03\n",
      "Epoch: 226410 | training loss: 3.2113e-03 | validation loss: 2.0433e-03\n",
      "Epoch: 226420 | training loss: 2.5507e-03 | validation loss: 1.9457e-03\n",
      "Epoch: 226430 | training loss: 1.9797e-03 | validation loss: 1.6019e-03\n",
      "Epoch: 226440 | training loss: 2.0144e-03 | validation loss: 1.5689e-03\n",
      "Epoch: 226450 | training loss: 1.9740e-03 | validation loss: 1.5608e-03\n",
      "Epoch: 226460 | training loss: 1.9666e-03 | validation loss: 1.5842e-03\n",
      "Epoch: 226470 | training loss: 1.9626e-03 | validation loss: 1.5771e-03\n",
      "Epoch: 226480 | training loss: 1.9628e-03 | validation loss: 1.5672e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 226490 | training loss: 1.9616e-03 | validation loss: 1.5734e-03\n",
      "Epoch: 226500 | training loss: 1.9615e-03 | validation loss: 1.5721e-03\n",
      "Epoch: 226510 | training loss: 1.9615e-03 | validation loss: 1.5710e-03\n",
      "Epoch: 226520 | training loss: 1.9615e-03 | validation loss: 1.5722e-03\n",
      "Epoch: 226530 | training loss: 1.9614e-03 | validation loss: 1.5713e-03\n",
      "Epoch: 226540 | training loss: 1.9614e-03 | validation loss: 1.5716e-03\n",
      "Epoch: 226550 | training loss: 1.9614e-03 | validation loss: 1.5717e-03\n",
      "Epoch: 226560 | training loss: 1.9614e-03 | validation loss: 1.5713e-03\n",
      "Epoch: 226570 | training loss: 1.9614e-03 | validation loss: 1.5707e-03\n",
      "Epoch: 226580 | training loss: 1.9664e-03 | validation loss: 1.5686e-03\n",
      "Epoch: 226590 | training loss: 2.3867e-03 | validation loss: 1.8079e-03\n",
      "Epoch: 226600 | training loss: 2.1761e-03 | validation loss: 1.7422e-03\n",
      "Epoch: 226610 | training loss: 2.0016e-03 | validation loss: 1.5964e-03\n",
      "Epoch: 226620 | training loss: 2.0080e-03 | validation loss: 1.5623e-03\n",
      "Epoch: 226630 | training loss: 2.2919e-03 | validation loss: 1.6518e-03\n",
      "Epoch: 226640 | training loss: 1.9656e-03 | validation loss: 1.5647e-03\n",
      "Epoch: 226650 | training loss: 1.9925e-03 | validation loss: 1.6086e-03\n",
      "Epoch: 226660 | training loss: 1.9889e-03 | validation loss: 1.5638e-03\n",
      "Epoch: 226670 | training loss: 1.9633e-03 | validation loss: 1.5791e-03\n",
      "Epoch: 226680 | training loss: 1.9669e-03 | validation loss: 1.5839e-03\n",
      "Epoch: 226690 | training loss: 1.9620e-03 | validation loss: 1.5748e-03\n",
      "Epoch: 226700 | training loss: 1.9615e-03 | validation loss: 1.5738e-03\n",
      "Epoch: 226710 | training loss: 1.9646e-03 | validation loss: 1.5807e-03\n",
      "Epoch: 226720 | training loss: 2.0748e-03 | validation loss: 1.6702e-03\n",
      "Epoch: 226730 | training loss: 2.7297e-03 | validation loss: 2.0508e-03\n",
      "Epoch: 226740 | training loss: 2.0227e-03 | validation loss: 1.5804e-03\n",
      "Epoch: 226750 | training loss: 1.9914e-03 | validation loss: 1.5650e-03\n",
      "Epoch: 226760 | training loss: 1.9987e-03 | validation loss: 1.6086e-03\n",
      "Epoch: 226770 | training loss: 1.9673e-03 | validation loss: 1.5638e-03\n",
      "Epoch: 226780 | training loss: 1.9612e-03 | validation loss: 1.5733e-03\n",
      "Epoch: 226790 | training loss: 1.9611e-03 | validation loss: 1.5721e-03\n",
      "Epoch: 226800 | training loss: 1.9610e-03 | validation loss: 1.5699e-03\n",
      "Epoch: 226810 | training loss: 1.9609e-03 | validation loss: 1.5711e-03\n",
      "Epoch: 226820 | training loss: 1.9610e-03 | validation loss: 1.5718e-03\n",
      "Epoch: 226830 | training loss: 1.9610e-03 | validation loss: 1.5701e-03\n",
      "Epoch: 226840 | training loss: 1.9609e-03 | validation loss: 1.5707e-03\n",
      "Epoch: 226850 | training loss: 1.9609e-03 | validation loss: 1.5712e-03\n",
      "Epoch: 226860 | training loss: 1.9609e-03 | validation loss: 1.5715e-03\n",
      "Epoch: 226870 | training loss: 1.9613e-03 | validation loss: 1.5735e-03\n",
      "Epoch: 226880 | training loss: 1.9768e-03 | validation loss: 1.5933e-03\n",
      "Epoch: 226890 | training loss: 2.8599e-03 | validation loss: 2.1153e-03\n",
      "Epoch: 226900 | training loss: 2.4855e-03 | validation loss: 1.7376e-03\n",
      "Epoch: 226910 | training loss: 2.0018e-03 | validation loss: 1.6094e-03\n",
      "Epoch: 226920 | training loss: 2.0313e-03 | validation loss: 1.6398e-03\n",
      "Epoch: 226930 | training loss: 1.9625e-03 | validation loss: 1.5667e-03\n",
      "Epoch: 226940 | training loss: 1.9697e-03 | validation loss: 1.5622e-03\n",
      "Epoch: 226950 | training loss: 1.9622e-03 | validation loss: 1.5771e-03\n",
      "Epoch: 226960 | training loss: 1.9609e-03 | validation loss: 1.5722e-03\n",
      "Epoch: 226970 | training loss: 1.9611e-03 | validation loss: 1.5685e-03\n",
      "Epoch: 226980 | training loss: 1.9608e-03 | validation loss: 1.5727e-03\n",
      "Epoch: 226990 | training loss: 1.9607e-03 | validation loss: 1.5696e-03\n",
      "Epoch: 227000 | training loss: 1.9606e-03 | validation loss: 1.5714e-03\n",
      "Epoch: 227010 | training loss: 1.9606e-03 | validation loss: 1.5703e-03\n",
      "Epoch: 227020 | training loss: 1.9606e-03 | validation loss: 1.5709e-03\n",
      "Epoch: 227030 | training loss: 1.9606e-03 | validation loss: 1.5713e-03\n",
      "Epoch: 227040 | training loss: 1.9621e-03 | validation loss: 1.5754e-03\n",
      "Epoch: 227050 | training loss: 2.0929e-03 | validation loss: 1.6987e-03\n",
      "Epoch: 227060 | training loss: 2.0068e-03 | validation loss: 1.6021e-03\n",
      "Epoch: 227070 | training loss: 2.0406e-03 | validation loss: 1.6539e-03\n",
      "Epoch: 227080 | training loss: 1.9815e-03 | validation loss: 1.5946e-03\n",
      "Epoch: 227090 | training loss: 1.9626e-03 | validation loss: 1.5643e-03\n",
      "Epoch: 227100 | training loss: 1.9768e-03 | validation loss: 1.5596e-03\n",
      "Epoch: 227110 | training loss: 2.2399e-03 | validation loss: 1.6337e-03\n",
      "Epoch: 227120 | training loss: 2.0258e-03 | validation loss: 1.5760e-03\n",
      "Epoch: 227130 | training loss: 1.9609e-03 | validation loss: 1.5686e-03\n",
      "Epoch: 227140 | training loss: 1.9779e-03 | validation loss: 1.5938e-03\n",
      "Epoch: 227150 | training loss: 1.9753e-03 | validation loss: 1.5605e-03\n",
      "Epoch: 227160 | training loss: 1.9670e-03 | validation loss: 1.5836e-03\n",
      "Epoch: 227170 | training loss: 1.9625e-03 | validation loss: 1.5659e-03\n",
      "Epoch: 227180 | training loss: 1.9605e-03 | validation loss: 1.5721e-03\n",
      "Epoch: 227190 | training loss: 1.9604e-03 | validation loss: 1.5713e-03\n",
      "Epoch: 227200 | training loss: 1.9605e-03 | validation loss: 1.5683e-03\n",
      "Epoch: 227210 | training loss: 1.9603e-03 | validation loss: 1.5690e-03\n",
      "Epoch: 227220 | training loss: 1.9602e-03 | validation loss: 1.5696e-03\n",
      "Epoch: 227230 | training loss: 1.9602e-03 | validation loss: 1.5692e-03\n",
      "Epoch: 227240 | training loss: 1.9613e-03 | validation loss: 1.5663e-03\n",
      "Epoch: 227250 | training loss: 2.0262e-03 | validation loss: 1.5696e-03\n",
      "Epoch: 227260 | training loss: 3.4135e-03 | validation loss: 2.1265e-03\n",
      "Epoch: 227270 | training loss: 1.9991e-03 | validation loss: 1.5947e-03\n",
      "Epoch: 227280 | training loss: 2.1015e-03 | validation loss: 1.6854e-03\n",
      "Epoch: 227290 | training loss: 1.9776e-03 | validation loss: 1.6008e-03\n",
      "Epoch: 227300 | training loss: 1.9701e-03 | validation loss: 1.5628e-03\n",
      "Epoch: 227310 | training loss: 1.9650e-03 | validation loss: 1.5628e-03\n",
      "Epoch: 227320 | training loss: 1.9612e-03 | validation loss: 1.5759e-03\n",
      "Epoch: 227330 | training loss: 1.9602e-03 | validation loss: 1.5722e-03\n",
      "Epoch: 227340 | training loss: 1.9604e-03 | validation loss: 1.5673e-03\n",
      "Epoch: 227350 | training loss: 1.9601e-03 | validation loss: 1.5715e-03\n",
      "Epoch: 227360 | training loss: 1.9600e-03 | validation loss: 1.5693e-03\n",
      "Epoch: 227370 | training loss: 1.9599e-03 | validation loss: 1.5700e-03\n",
      "Epoch: 227380 | training loss: 1.9599e-03 | validation loss: 1.5697e-03\n",
      "Epoch: 227390 | training loss: 1.9599e-03 | validation loss: 1.5699e-03\n",
      "Epoch: 227400 | training loss: 1.9599e-03 | validation loss: 1.5696e-03\n",
      "Epoch: 227410 | training loss: 1.9599e-03 | validation loss: 1.5698e-03\n",
      "Epoch: 227420 | training loss: 1.9599e-03 | validation loss: 1.5698e-03\n",
      "Epoch: 227430 | training loss: 1.9599e-03 | validation loss: 1.5700e-03\n",
      "Epoch: 227440 | training loss: 1.9614e-03 | validation loss: 1.5721e-03\n",
      "Epoch: 227450 | training loss: 2.1252e-03 | validation loss: 1.6866e-03\n",
      "Epoch: 227460 | training loss: 2.5293e-03 | validation loss: 1.7726e-03\n",
      "Epoch: 227470 | training loss: 2.0261e-03 | validation loss: 1.6419e-03\n",
      "Epoch: 227480 | training loss: 1.9939e-03 | validation loss: 1.5695e-03\n",
      "Epoch: 227490 | training loss: 1.9745e-03 | validation loss: 1.5957e-03\n",
      "Epoch: 227500 | training loss: 1.9648e-03 | validation loss: 1.5695e-03\n",
      "Epoch: 227510 | training loss: 1.9615e-03 | validation loss: 1.5740e-03\n",
      "Epoch: 227520 | training loss: 1.9615e-03 | validation loss: 1.5769e-03\n",
      "Epoch: 227530 | training loss: 1.9603e-03 | validation loss: 1.5684e-03\n",
      "Epoch: 227540 | training loss: 1.9601e-03 | validation loss: 1.5670e-03\n",
      "Epoch: 227550 | training loss: 1.9601e-03 | validation loss: 1.5665e-03\n",
      "Epoch: 227560 | training loss: 1.9623e-03 | validation loss: 1.5634e-03\n",
      "Epoch: 227570 | training loss: 2.0240e-03 | validation loss: 1.5666e-03\n",
      "Epoch: 227580 | training loss: 2.9099e-03 | validation loss: 1.9004e-03\n",
      "Epoch: 227590 | training loss: 2.2444e-03 | validation loss: 1.7697e-03\n",
      "Epoch: 227600 | training loss: 1.9793e-03 | validation loss: 1.5609e-03\n",
      "Epoch: 227610 | training loss: 1.9649e-03 | validation loss: 1.5632e-03\n",
      "Epoch: 227620 | training loss: 1.9703e-03 | validation loss: 1.5876e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 227630 | training loss: 1.9650e-03 | validation loss: 1.5629e-03\n",
      "Epoch: 227640 | training loss: 1.9616e-03 | validation loss: 1.5756e-03\n",
      "Epoch: 227650 | training loss: 1.9602e-03 | validation loss: 1.5660e-03\n",
      "Epoch: 227660 | training loss: 1.9596e-03 | validation loss: 1.5709e-03\n",
      "Epoch: 227670 | training loss: 1.9594e-03 | validation loss: 1.5694e-03\n",
      "Epoch: 227680 | training loss: 1.9595e-03 | validation loss: 1.5681e-03\n",
      "Epoch: 227690 | training loss: 1.9594e-03 | validation loss: 1.5686e-03\n",
      "Epoch: 227700 | training loss: 1.9594e-03 | validation loss: 1.5688e-03\n",
      "Epoch: 227710 | training loss: 1.9594e-03 | validation loss: 1.5684e-03\n",
      "Epoch: 227720 | training loss: 1.9601e-03 | validation loss: 1.5660e-03\n",
      "Epoch: 227730 | training loss: 2.0090e-03 | validation loss: 1.5658e-03\n",
      "Epoch: 227740 | training loss: 3.6701e-03 | validation loss: 2.2399e-03\n",
      "Epoch: 227750 | training loss: 2.0127e-03 | validation loss: 1.6103e-03\n",
      "Epoch: 227760 | training loss: 2.1284e-03 | validation loss: 1.6999e-03\n",
      "Epoch: 227770 | training loss: 1.9881e-03 | validation loss: 1.6111e-03\n",
      "Epoch: 227780 | training loss: 1.9642e-03 | validation loss: 1.5669e-03\n",
      "Epoch: 227790 | training loss: 1.9671e-03 | validation loss: 1.5592e-03\n",
      "Epoch: 227800 | training loss: 1.9593e-03 | validation loss: 1.5683e-03\n",
      "Epoch: 227810 | training loss: 1.9603e-03 | validation loss: 1.5751e-03\n",
      "Epoch: 227820 | training loss: 1.9592e-03 | validation loss: 1.5676e-03\n",
      "Epoch: 227830 | training loss: 1.9592e-03 | validation loss: 1.5682e-03\n",
      "Epoch: 227840 | training loss: 1.9592e-03 | validation loss: 1.5699e-03\n",
      "Epoch: 227850 | training loss: 1.9591e-03 | validation loss: 1.5683e-03\n",
      "Epoch: 227860 | training loss: 1.9591e-03 | validation loss: 1.5691e-03\n",
      "Epoch: 227870 | training loss: 1.9591e-03 | validation loss: 1.5686e-03\n",
      "Epoch: 227880 | training loss: 1.9590e-03 | validation loss: 1.5690e-03\n",
      "Epoch: 227890 | training loss: 1.9590e-03 | validation loss: 1.5687e-03\n",
      "Epoch: 227900 | training loss: 1.9590e-03 | validation loss: 1.5687e-03\n",
      "Epoch: 227910 | training loss: 1.9590e-03 | validation loss: 1.5687e-03\n",
      "Epoch: 227920 | training loss: 1.9590e-03 | validation loss: 1.5684e-03\n",
      "Epoch: 227930 | training loss: 1.9620e-03 | validation loss: 1.5675e-03\n",
      "Epoch: 227940 | training loss: 2.2957e-03 | validation loss: 1.7605e-03\n",
      "Epoch: 227950 | training loss: 2.3672e-03 | validation loss: 1.9248e-03\n",
      "Epoch: 227960 | training loss: 2.0998e-03 | validation loss: 1.7076e-03\n",
      "Epoch: 227970 | training loss: 1.9962e-03 | validation loss: 1.5636e-03\n",
      "Epoch: 227980 | training loss: 1.9622e-03 | validation loss: 1.5609e-03\n",
      "Epoch: 227990 | training loss: 1.9699e-03 | validation loss: 1.5825e-03\n",
      "Epoch: 228000 | training loss: 1.9714e-03 | validation loss: 1.5855e-03\n",
      "Epoch: 228010 | training loss: 1.9858e-03 | validation loss: 1.6020e-03\n",
      "Epoch: 228020 | training loss: 2.1442e-03 | validation loss: 1.7164e-03\n",
      "Epoch: 228030 | training loss: 2.1167e-03 | validation loss: 1.6973e-03\n",
      "Epoch: 228040 | training loss: 2.0497e-03 | validation loss: 1.5729e-03\n",
      "Epoch: 228050 | training loss: 1.9740e-03 | validation loss: 1.5918e-03\n",
      "Epoch: 228060 | training loss: 1.9613e-03 | validation loss: 1.5758e-03\n",
      "Epoch: 228070 | training loss: 1.9654e-03 | validation loss: 1.5607e-03\n",
      "Epoch: 228080 | training loss: 1.9630e-03 | validation loss: 1.5617e-03\n",
      "Epoch: 228090 | training loss: 1.9636e-03 | validation loss: 1.5614e-03\n",
      "Epoch: 228100 | training loss: 1.9960e-03 | validation loss: 1.5611e-03\n",
      "Epoch: 228110 | training loss: 2.4563e-03 | validation loss: 1.7174e-03\n",
      "Epoch: 228120 | training loss: 1.9915e-03 | validation loss: 1.6084e-03\n",
      "Epoch: 228130 | training loss: 1.9724e-03 | validation loss: 1.5607e-03\n",
      "Epoch: 228140 | training loss: 1.9634e-03 | validation loss: 1.5774e-03\n",
      "Epoch: 228150 | training loss: 1.9588e-03 | validation loss: 1.5665e-03\n",
      "Epoch: 228160 | training loss: 1.9597e-03 | validation loss: 1.5650e-03\n",
      "Epoch: 228170 | training loss: 1.9610e-03 | validation loss: 1.5751e-03\n",
      "Epoch: 228180 | training loss: 1.9586e-03 | validation loss: 1.5671e-03\n",
      "Epoch: 228190 | training loss: 1.9593e-03 | validation loss: 1.5650e-03\n",
      "Epoch: 228200 | training loss: 1.9599e-03 | validation loss: 1.5642e-03\n",
      "Epoch: 228210 | training loss: 1.9698e-03 | validation loss: 1.5608e-03\n",
      "Epoch: 228220 | training loss: 2.1981e-03 | validation loss: 1.6253e-03\n",
      "Epoch: 228230 | training loss: 2.1430e-03 | validation loss: 1.6109e-03\n",
      "Epoch: 228240 | training loss: 1.9712e-03 | validation loss: 1.5816e-03\n",
      "Epoch: 228250 | training loss: 1.9638e-03 | validation loss: 1.5817e-03\n",
      "Epoch: 228260 | training loss: 1.9672e-03 | validation loss: 1.5592e-03\n",
      "Epoch: 228270 | training loss: 1.9629e-03 | validation loss: 1.5793e-03\n",
      "Epoch: 228280 | training loss: 1.9596e-03 | validation loss: 1.5635e-03\n",
      "Epoch: 228290 | training loss: 1.9585e-03 | validation loss: 1.5699e-03\n",
      "Epoch: 228300 | training loss: 1.9586e-03 | validation loss: 1.5690e-03\n",
      "Epoch: 228310 | training loss: 1.9584e-03 | validation loss: 1.5664e-03\n",
      "Epoch: 228320 | training loss: 1.9584e-03 | validation loss: 1.5675e-03\n",
      "Epoch: 228330 | training loss: 1.9584e-03 | validation loss: 1.5681e-03\n",
      "Epoch: 228340 | training loss: 1.9595e-03 | validation loss: 1.5686e-03\n",
      "Epoch: 228350 | training loss: 1.9886e-03 | validation loss: 1.5841e-03\n",
      "Epoch: 228360 | training loss: 2.5373e-03 | validation loss: 1.8301e-03\n",
      "Epoch: 228370 | training loss: 2.0346e-03 | validation loss: 1.5795e-03\n",
      "Epoch: 228380 | training loss: 2.0647e-03 | validation loss: 1.5936e-03\n",
      "Epoch: 228390 | training loss: 1.9838e-03 | validation loss: 1.5893e-03\n",
      "Epoch: 228400 | training loss: 1.9722e-03 | validation loss: 1.5689e-03\n",
      "Epoch: 228410 | training loss: 1.9671e-03 | validation loss: 1.5874e-03\n",
      "Epoch: 228420 | training loss: 1.9597e-03 | validation loss: 1.5646e-03\n",
      "Epoch: 228430 | training loss: 1.9591e-03 | validation loss: 1.5684e-03\n",
      "Epoch: 228440 | training loss: 1.9581e-03 | validation loss: 1.5667e-03\n",
      "Epoch: 228450 | training loss: 1.9582e-03 | validation loss: 1.5673e-03\n",
      "Epoch: 228460 | training loss: 1.9580e-03 | validation loss: 1.5674e-03\n",
      "Epoch: 228470 | training loss: 1.9580e-03 | validation loss: 1.5681e-03\n",
      "Epoch: 228480 | training loss: 1.9583e-03 | validation loss: 1.5697e-03\n",
      "Epoch: 228490 | training loss: 1.9636e-03 | validation loss: 1.5791e-03\n",
      "Epoch: 228500 | training loss: 2.2172e-03 | validation loss: 1.7540e-03\n",
      "Epoch: 228510 | training loss: 2.0557e-03 | validation loss: 1.6464e-03\n",
      "Epoch: 228520 | training loss: 2.0912e-03 | validation loss: 1.6771e-03\n",
      "Epoch: 228530 | training loss: 2.0247e-03 | validation loss: 1.5748e-03\n",
      "Epoch: 228540 | training loss: 1.9623e-03 | validation loss: 1.5593e-03\n",
      "Epoch: 228550 | training loss: 1.9702e-03 | validation loss: 1.5845e-03\n",
      "Epoch: 228560 | training loss: 1.9600e-03 | validation loss: 1.5656e-03\n",
      "Epoch: 228570 | training loss: 1.9579e-03 | validation loss: 1.5666e-03\n",
      "Epoch: 228580 | training loss: 1.9579e-03 | validation loss: 1.5686e-03\n",
      "Epoch: 228590 | training loss: 1.9579e-03 | validation loss: 1.5666e-03\n",
      "Epoch: 228600 | training loss: 1.9578e-03 | validation loss: 1.5677e-03\n",
      "Epoch: 228610 | training loss: 1.9577e-03 | validation loss: 1.5674e-03\n",
      "Epoch: 228620 | training loss: 1.9577e-03 | validation loss: 1.5670e-03\n",
      "Epoch: 228630 | training loss: 1.9577e-03 | validation loss: 1.5673e-03\n",
      "Epoch: 228640 | training loss: 1.9577e-03 | validation loss: 1.5675e-03\n",
      "Epoch: 228650 | training loss: 1.9577e-03 | validation loss: 1.5675e-03\n",
      "Epoch: 228660 | training loss: 1.9577e-03 | validation loss: 1.5679e-03\n",
      "Epoch: 228670 | training loss: 1.9589e-03 | validation loss: 1.5712e-03\n",
      "Epoch: 228680 | training loss: 2.0350e-03 | validation loss: 1.6323e-03\n",
      "Epoch: 228690 | training loss: 3.3276e-03 | validation loss: 2.3599e-03\n",
      "Epoch: 228700 | training loss: 2.0019e-03 | validation loss: 1.5730e-03\n",
      "Epoch: 228710 | training loss: 2.0483e-03 | validation loss: 1.5961e-03\n",
      "Epoch: 228720 | training loss: 2.0104e-03 | validation loss: 1.6090e-03\n",
      "Epoch: 228730 | training loss: 1.9607e-03 | validation loss: 1.5664e-03\n",
      "Epoch: 228740 | training loss: 1.9600e-03 | validation loss: 1.5680e-03\n",
      "Epoch: 228750 | training loss: 1.9598e-03 | validation loss: 1.5697e-03\n",
      "Epoch: 228760 | training loss: 1.9575e-03 | validation loss: 1.5662e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 228770 | training loss: 1.9578e-03 | validation loss: 1.5679e-03\n",
      "Epoch: 228780 | training loss: 1.9575e-03 | validation loss: 1.5670e-03\n",
      "Epoch: 228790 | training loss: 1.9574e-03 | validation loss: 1.5668e-03\n",
      "Epoch: 228800 | training loss: 1.9574e-03 | validation loss: 1.5672e-03\n",
      "Epoch: 228810 | training loss: 1.9574e-03 | validation loss: 1.5671e-03\n",
      "Epoch: 228820 | training loss: 1.9574e-03 | validation loss: 1.5671e-03\n",
      "Epoch: 228830 | training loss: 1.9574e-03 | validation loss: 1.5672e-03\n",
      "Epoch: 228840 | training loss: 1.9574e-03 | validation loss: 1.5683e-03\n",
      "Epoch: 228850 | training loss: 1.9616e-03 | validation loss: 1.5799e-03\n",
      "Epoch: 228860 | training loss: 2.4069e-03 | validation loss: 1.9344e-03\n",
      "Epoch: 228870 | training loss: 2.2859e-03 | validation loss: 1.6997e-03\n",
      "Epoch: 228880 | training loss: 2.0364e-03 | validation loss: 1.6200e-03\n",
      "Epoch: 228890 | training loss: 1.9706e-03 | validation loss: 1.5631e-03\n",
      "Epoch: 228900 | training loss: 1.9613e-03 | validation loss: 1.5765e-03\n",
      "Epoch: 228910 | training loss: 1.9629e-03 | validation loss: 1.5805e-03\n",
      "Epoch: 228920 | training loss: 1.9605e-03 | validation loss: 1.5709e-03\n",
      "Epoch: 228930 | training loss: 1.9580e-03 | validation loss: 1.5711e-03\n",
      "Epoch: 228940 | training loss: 1.9573e-03 | validation loss: 1.5648e-03\n",
      "Epoch: 228950 | training loss: 1.9573e-03 | validation loss: 1.5657e-03\n",
      "Epoch: 228960 | training loss: 1.9571e-03 | validation loss: 1.5663e-03\n",
      "Epoch: 228970 | training loss: 1.9571e-03 | validation loss: 1.5664e-03\n",
      "Epoch: 228980 | training loss: 1.9571e-03 | validation loss: 1.5661e-03\n",
      "Epoch: 228990 | training loss: 1.9571e-03 | validation loss: 1.5659e-03\n",
      "Epoch: 229000 | training loss: 1.9571e-03 | validation loss: 1.5659e-03\n",
      "Epoch: 229010 | training loss: 1.9575e-03 | validation loss: 1.5641e-03\n",
      "Epoch: 229020 | training loss: 1.9855e-03 | validation loss: 1.5611e-03\n",
      "Epoch: 229030 | training loss: 3.6329e-03 | validation loss: 2.2310e-03\n",
      "Epoch: 229040 | training loss: 2.2897e-03 | validation loss: 1.7942e-03\n",
      "Epoch: 229050 | training loss: 2.1453e-03 | validation loss: 1.7056e-03\n",
      "Epoch: 229060 | training loss: 1.9612e-03 | validation loss: 1.5737e-03\n",
      "Epoch: 229070 | training loss: 1.9719e-03 | validation loss: 1.5588e-03\n",
      "Epoch: 229080 | training loss: 1.9647e-03 | validation loss: 1.5600e-03\n",
      "Epoch: 229090 | training loss: 1.9573e-03 | validation loss: 1.5688e-03\n",
      "Epoch: 229100 | training loss: 1.9580e-03 | validation loss: 1.5708e-03\n",
      "Epoch: 229110 | training loss: 1.9570e-03 | validation loss: 1.5649e-03\n",
      "Epoch: 229120 | training loss: 1.9569e-03 | validation loss: 1.5655e-03\n",
      "Epoch: 229130 | training loss: 1.9569e-03 | validation loss: 1.5672e-03\n",
      "Epoch: 229140 | training loss: 1.9568e-03 | validation loss: 1.5657e-03\n",
      "Epoch: 229150 | training loss: 1.9568e-03 | validation loss: 1.5664e-03\n",
      "Epoch: 229160 | training loss: 1.9568e-03 | validation loss: 1.5660e-03\n",
      "Epoch: 229170 | training loss: 1.9568e-03 | validation loss: 1.5662e-03\n",
      "Epoch: 229180 | training loss: 1.9567e-03 | validation loss: 1.5660e-03\n",
      "Epoch: 229190 | training loss: 1.9567e-03 | validation loss: 1.5661e-03\n",
      "Epoch: 229200 | training loss: 1.9567e-03 | validation loss: 1.5661e-03\n",
      "Epoch: 229210 | training loss: 1.9567e-03 | validation loss: 1.5660e-03\n",
      "Epoch: 229220 | training loss: 1.9567e-03 | validation loss: 1.5660e-03\n",
      "Epoch: 229230 | training loss: 1.9566e-03 | validation loss: 1.5661e-03\n",
      "Epoch: 229240 | training loss: 1.9567e-03 | validation loss: 1.5666e-03\n",
      "Epoch: 229250 | training loss: 1.9598e-03 | validation loss: 1.5737e-03\n",
      "Epoch: 229260 | training loss: 2.4576e-03 | validation loss: 1.8868e-03\n",
      "Epoch: 229270 | training loss: 2.4993e-03 | validation loss: 1.7478e-03\n",
      "Epoch: 229280 | training loss: 1.9581e-03 | validation loss: 1.5630e-03\n",
      "Epoch: 229290 | training loss: 1.9666e-03 | validation loss: 1.5803e-03\n",
      "Epoch: 229300 | training loss: 1.9723e-03 | validation loss: 1.5861e-03\n",
      "Epoch: 229310 | training loss: 1.9698e-03 | validation loss: 1.5848e-03\n",
      "Epoch: 229320 | training loss: 1.9631e-03 | validation loss: 1.5778e-03\n",
      "Epoch: 229330 | training loss: 1.9578e-03 | validation loss: 1.5703e-03\n",
      "Epoch: 229340 | training loss: 1.9565e-03 | validation loss: 1.5658e-03\n",
      "Epoch: 229350 | training loss: 1.9567e-03 | validation loss: 1.5645e-03\n",
      "Epoch: 229360 | training loss: 1.9565e-03 | validation loss: 1.5653e-03\n",
      "Epoch: 229370 | training loss: 1.9564e-03 | validation loss: 1.5664e-03\n",
      "Epoch: 229380 | training loss: 1.9564e-03 | validation loss: 1.5661e-03\n",
      "Epoch: 229390 | training loss: 1.9564e-03 | validation loss: 1.5656e-03\n",
      "Epoch: 229400 | training loss: 1.9564e-03 | validation loss: 1.5658e-03\n",
      "Epoch: 229410 | training loss: 1.9563e-03 | validation loss: 1.5658e-03\n",
      "Epoch: 229420 | training loss: 1.9563e-03 | validation loss: 1.5657e-03\n",
      "Epoch: 229430 | training loss: 1.9563e-03 | validation loss: 1.5658e-03\n",
      "Epoch: 229440 | training loss: 1.9563e-03 | validation loss: 1.5667e-03\n",
      "Epoch: 229450 | training loss: 1.9676e-03 | validation loss: 1.5871e-03\n",
      "Epoch: 229460 | training loss: 2.8360e-03 | validation loss: 2.2860e-03\n",
      "Epoch: 229470 | training loss: 2.1190e-03 | validation loss: 1.6514e-03\n",
      "Epoch: 229480 | training loss: 2.0035e-03 | validation loss: 1.6266e-03\n",
      "Epoch: 229490 | training loss: 1.9726e-03 | validation loss: 1.5691e-03\n",
      "Epoch: 229500 | training loss: 1.9617e-03 | validation loss: 1.5789e-03\n",
      "Epoch: 229510 | training loss: 1.9575e-03 | validation loss: 1.5632e-03\n",
      "Epoch: 229520 | training loss: 1.9562e-03 | validation loss: 1.5661e-03\n",
      "Epoch: 229530 | training loss: 1.9564e-03 | validation loss: 1.5664e-03\n",
      "Epoch: 229540 | training loss: 1.9563e-03 | validation loss: 1.5641e-03\n",
      "Epoch: 229550 | training loss: 1.9562e-03 | validation loss: 1.5635e-03\n",
      "Epoch: 229560 | training loss: 1.9566e-03 | validation loss: 1.5622e-03\n",
      "Epoch: 229570 | training loss: 1.9679e-03 | validation loss: 1.5561e-03\n",
      "Epoch: 229580 | training loss: 2.4396e-03 | validation loss: 1.7050e-03\n",
      "Epoch: 229590 | training loss: 2.0293e-03 | validation loss: 1.6407e-03\n",
      "Epoch: 229600 | training loss: 2.1375e-03 | validation loss: 1.6017e-03\n",
      "Epoch: 229610 | training loss: 1.9800e-03 | validation loss: 1.5958e-03\n",
      "Epoch: 229620 | training loss: 1.9596e-03 | validation loss: 1.5731e-03\n",
      "Epoch: 229630 | training loss: 1.9641e-03 | validation loss: 1.5569e-03\n",
      "Epoch: 229640 | training loss: 1.9592e-03 | validation loss: 1.5734e-03\n",
      "Epoch: 229650 | training loss: 1.9569e-03 | validation loss: 1.5619e-03\n",
      "Epoch: 229660 | training loss: 1.9562e-03 | validation loss: 1.5675e-03\n",
      "Epoch: 229670 | training loss: 1.9561e-03 | validation loss: 1.5633e-03\n",
      "Epoch: 229680 | training loss: 1.9559e-03 | validation loss: 1.5660e-03\n",
      "Epoch: 229690 | training loss: 1.9558e-03 | validation loss: 1.5649e-03\n",
      "Epoch: 229700 | training loss: 1.9558e-03 | validation loss: 1.5643e-03\n",
      "Epoch: 229710 | training loss: 1.9558e-03 | validation loss: 1.5645e-03\n",
      "Epoch: 229720 | training loss: 1.9558e-03 | validation loss: 1.5644e-03\n",
      "Epoch: 229730 | training loss: 1.9559e-03 | validation loss: 1.5633e-03\n",
      "Epoch: 229740 | training loss: 1.9619e-03 | validation loss: 1.5581e-03\n",
      "Epoch: 229750 | training loss: 2.4058e-03 | validation loss: 1.6994e-03\n",
      "Epoch: 229760 | training loss: 2.0688e-03 | validation loss: 1.6710e-03\n",
      "Epoch: 229770 | training loss: 2.1846e-03 | validation loss: 1.6214e-03\n",
      "Epoch: 229780 | training loss: 2.0107e-03 | validation loss: 1.5560e-03\n",
      "Epoch: 229790 | training loss: 1.9636e-03 | validation loss: 1.5738e-03\n",
      "Epoch: 229800 | training loss: 1.9680e-03 | validation loss: 1.5897e-03\n",
      "Epoch: 229810 | training loss: 1.9558e-03 | validation loss: 1.5640e-03\n",
      "Epoch: 229820 | training loss: 1.9572e-03 | validation loss: 1.5592e-03\n",
      "Epoch: 229830 | training loss: 1.9559e-03 | validation loss: 1.5679e-03\n",
      "Epoch: 229840 | training loss: 1.9556e-03 | validation loss: 1.5650e-03\n",
      "Epoch: 229850 | training loss: 1.9556e-03 | validation loss: 1.5639e-03\n",
      "Epoch: 229860 | training loss: 1.9556e-03 | validation loss: 1.5654e-03\n",
      "Epoch: 229870 | training loss: 1.9555e-03 | validation loss: 1.5642e-03\n",
      "Epoch: 229880 | training loss: 1.9555e-03 | validation loss: 1.5650e-03\n",
      "Epoch: 229890 | training loss: 1.9555e-03 | validation loss: 1.5645e-03\n",
      "Epoch: 229900 | training loss: 1.9555e-03 | validation loss: 1.5645e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 229910 | training loss: 1.9555e-03 | validation loss: 1.5646e-03\n",
      "Epoch: 229920 | training loss: 1.9555e-03 | validation loss: 1.5642e-03\n",
      "Epoch: 229930 | training loss: 1.9589e-03 | validation loss: 1.5638e-03\n",
      "Epoch: 229940 | training loss: 2.2466e-03 | validation loss: 1.7305e-03\n",
      "Epoch: 229950 | training loss: 2.3444e-03 | validation loss: 1.9048e-03\n",
      "Epoch: 229960 | training loss: 1.9842e-03 | validation loss: 1.6048e-03\n",
      "Epoch: 229970 | training loss: 2.0030e-03 | validation loss: 1.5568e-03\n",
      "Epoch: 229980 | training loss: 1.9679e-03 | validation loss: 1.5618e-03\n",
      "Epoch: 229990 | training loss: 1.9654e-03 | validation loss: 1.5773e-03\n",
      "Epoch: 230000 | training loss: 1.9706e-03 | validation loss: 1.5890e-03\n",
      "Epoch: 230010 | training loss: 2.0256e-03 | validation loss: 1.6361e-03\n",
      "Epoch: 230020 | training loss: 2.2904e-03 | validation loss: 1.8041e-03\n",
      "Epoch: 230030 | training loss: 1.9631e-03 | validation loss: 1.5556e-03\n",
      "Epoch: 230040 | training loss: 1.9728e-03 | validation loss: 1.5545e-03\n",
      "Epoch: 230050 | training loss: 1.9781e-03 | validation loss: 1.5946e-03\n",
      "Epoch: 230060 | training loss: 1.9552e-03 | validation loss: 1.5645e-03\n",
      "Epoch: 230070 | training loss: 1.9609e-03 | validation loss: 1.5570e-03\n",
      "Epoch: 230080 | training loss: 1.9672e-03 | validation loss: 1.5556e-03\n",
      "Epoch: 230090 | training loss: 2.0251e-03 | validation loss: 1.5637e-03\n",
      "Epoch: 230100 | training loss: 2.3779e-03 | validation loss: 1.6855e-03\n",
      "Epoch: 230110 | training loss: 1.9883e-03 | validation loss: 1.6030e-03\n",
      "Epoch: 230120 | training loss: 1.9556e-03 | validation loss: 1.5672e-03\n",
      "Epoch: 230130 | training loss: 1.9691e-03 | validation loss: 1.5557e-03\n",
      "Epoch: 230140 | training loss: 1.9651e-03 | validation loss: 1.5811e-03\n",
      "Epoch: 230150 | training loss: 1.9556e-03 | validation loss: 1.5670e-03\n",
      "Epoch: 230160 | training loss: 1.9565e-03 | validation loss: 1.5600e-03\n",
      "Epoch: 230170 | training loss: 1.9612e-03 | validation loss: 1.5575e-03\n",
      "Epoch: 230180 | training loss: 2.0122e-03 | validation loss: 1.5629e-03\n",
      "Epoch: 230190 | training loss: 2.5444e-03 | validation loss: 1.7577e-03\n",
      "Epoch: 230200 | training loss: 2.0626e-03 | validation loss: 1.6559e-03\n",
      "Epoch: 230210 | training loss: 1.9934e-03 | validation loss: 1.5580e-03\n",
      "Epoch: 230220 | training loss: 1.9663e-03 | validation loss: 1.5822e-03\n",
      "Epoch: 230230 | training loss: 1.9559e-03 | validation loss: 1.5598e-03\n",
      "Epoch: 230240 | training loss: 1.9558e-03 | validation loss: 1.5623e-03\n",
      "Epoch: 230250 | training loss: 1.9572e-03 | validation loss: 1.5695e-03\n",
      "Epoch: 230260 | training loss: 1.9548e-03 | validation loss: 1.5639e-03\n",
      "Epoch: 230270 | training loss: 1.9554e-03 | validation loss: 1.5620e-03\n",
      "Epoch: 230280 | training loss: 1.9571e-03 | validation loss: 1.5602e-03\n",
      "Epoch: 230290 | training loss: 1.9828e-03 | validation loss: 1.5603e-03\n",
      "Epoch: 230300 | training loss: 2.5339e-03 | validation loss: 1.7648e-03\n",
      "Epoch: 230310 | training loss: 2.0380e-03 | validation loss: 1.6266e-03\n",
      "Epoch: 230320 | training loss: 2.0474e-03 | validation loss: 1.5896e-03\n",
      "Epoch: 230330 | training loss: 2.0039e-03 | validation loss: 1.6026e-03\n",
      "Epoch: 230340 | training loss: 1.9711e-03 | validation loss: 1.5656e-03\n",
      "Epoch: 230350 | training loss: 1.9601e-03 | validation loss: 1.5706e-03\n",
      "Epoch: 230360 | training loss: 1.9571e-03 | validation loss: 1.5610e-03\n",
      "Epoch: 230370 | training loss: 1.9557e-03 | validation loss: 1.5667e-03\n",
      "Epoch: 230380 | training loss: 1.9548e-03 | validation loss: 1.5637e-03\n",
      "Epoch: 230390 | training loss: 1.9549e-03 | validation loss: 1.5654e-03\n",
      "Epoch: 230400 | training loss: 1.9617e-03 | validation loss: 1.5810e-03\n",
      "Epoch: 230410 | training loss: 2.3042e-03 | validation loss: 1.8816e-03\n",
      "Epoch: 230420 | training loss: 2.1888e-03 | validation loss: 1.6996e-03\n",
      "Epoch: 230430 | training loss: 2.0034e-03 | validation loss: 1.5752e-03\n",
      "Epoch: 230440 | training loss: 1.9938e-03 | validation loss: 1.6169e-03\n",
      "Epoch: 230450 | training loss: 1.9545e-03 | validation loss: 1.5631e-03\n",
      "Epoch: 230460 | training loss: 1.9613e-03 | validation loss: 1.5547e-03\n",
      "Epoch: 230470 | training loss: 1.9551e-03 | validation loss: 1.5606e-03\n",
      "Epoch: 230480 | training loss: 1.9551e-03 | validation loss: 1.5637e-03\n",
      "Epoch: 230490 | training loss: 1.9558e-03 | validation loss: 1.5584e-03\n",
      "Epoch: 230500 | training loss: 1.9897e-03 | validation loss: 1.5565e-03\n",
      "Epoch: 230510 | training loss: 3.0293e-03 | validation loss: 1.9513e-03\n",
      "Epoch: 230520 | training loss: 2.3856e-03 | validation loss: 1.8475e-03\n",
      "Epoch: 230530 | training loss: 1.9848e-03 | validation loss: 1.5652e-03\n",
      "Epoch: 230540 | training loss: 1.9799e-03 | validation loss: 1.5580e-03\n",
      "Epoch: 230550 | training loss: 1.9732e-03 | validation loss: 1.5856e-03\n",
      "Epoch: 230560 | training loss: 1.9557e-03 | validation loss: 1.5591e-03\n",
      "Epoch: 230570 | training loss: 1.9544e-03 | validation loss: 1.5631e-03\n",
      "Epoch: 230580 | training loss: 1.9545e-03 | validation loss: 1.5650e-03\n",
      "Epoch: 230590 | training loss: 1.9544e-03 | validation loss: 1.5618e-03\n",
      "Epoch: 230600 | training loss: 1.9543e-03 | validation loss: 1.5638e-03\n",
      "Epoch: 230610 | training loss: 1.9542e-03 | validation loss: 1.5632e-03\n",
      "Epoch: 230620 | training loss: 1.9542e-03 | validation loss: 1.5626e-03\n",
      "Epoch: 230630 | training loss: 1.9542e-03 | validation loss: 1.5631e-03\n",
      "Epoch: 230640 | training loss: 1.9542e-03 | validation loss: 1.5634e-03\n",
      "Epoch: 230650 | training loss: 1.9542e-03 | validation loss: 1.5636e-03\n",
      "Epoch: 230660 | training loss: 1.9545e-03 | validation loss: 1.5652e-03\n",
      "Epoch: 230670 | training loss: 1.9655e-03 | validation loss: 1.5805e-03\n",
      "Epoch: 230680 | training loss: 2.6337e-03 | validation loss: 1.9847e-03\n",
      "Epoch: 230690 | training loss: 2.2956e-03 | validation loss: 1.6601e-03\n",
      "Epoch: 230700 | training loss: 2.0887e-03 | validation loss: 1.6685e-03\n",
      "Epoch: 230710 | training loss: 2.0029e-03 | validation loss: 1.6127e-03\n",
      "Epoch: 230720 | training loss: 1.9655e-03 | validation loss: 1.5553e-03\n",
      "Epoch: 230730 | training loss: 1.9600e-03 | validation loss: 1.5569e-03\n",
      "Epoch: 230740 | training loss: 1.9571e-03 | validation loss: 1.5711e-03\n",
      "Epoch: 230750 | training loss: 1.9540e-03 | validation loss: 1.5629e-03\n",
      "Epoch: 230760 | training loss: 1.9542e-03 | validation loss: 1.5611e-03\n",
      "Epoch: 230770 | training loss: 1.9541e-03 | validation loss: 1.5648e-03\n",
      "Epoch: 230780 | training loss: 1.9540e-03 | validation loss: 1.5619e-03\n",
      "Epoch: 230790 | training loss: 1.9539e-03 | validation loss: 1.5635e-03\n",
      "Epoch: 230800 | training loss: 1.9539e-03 | validation loss: 1.5624e-03\n",
      "Epoch: 230810 | training loss: 1.9539e-03 | validation loss: 1.5628e-03\n",
      "Epoch: 230820 | training loss: 1.9539e-03 | validation loss: 1.5623e-03\n",
      "Epoch: 230830 | training loss: 1.9563e-03 | validation loss: 1.5589e-03\n",
      "Epoch: 230840 | training loss: 2.2612e-03 | validation loss: 1.7158e-03\n",
      "Epoch: 230850 | training loss: 2.2705e-03 | validation loss: 1.7796e-03\n",
      "Epoch: 230860 | training loss: 1.9856e-03 | validation loss: 1.6071e-03\n",
      "Epoch: 230870 | training loss: 1.9568e-03 | validation loss: 1.5684e-03\n",
      "Epoch: 230880 | training loss: 1.9615e-03 | validation loss: 1.5566e-03\n",
      "Epoch: 230890 | training loss: 1.9623e-03 | validation loss: 1.5540e-03\n",
      "Epoch: 230900 | training loss: 1.9762e-03 | validation loss: 1.5519e-03\n",
      "Epoch: 230910 | training loss: 2.2272e-03 | validation loss: 1.6250e-03\n",
      "Epoch: 230920 | training loss: 2.0136e-03 | validation loss: 1.5622e-03\n",
      "Epoch: 230930 | training loss: 1.9927e-03 | validation loss: 1.6073e-03\n",
      "Epoch: 230940 | training loss: 1.9765e-03 | validation loss: 1.5533e-03\n",
      "Epoch: 230950 | training loss: 1.9686e-03 | validation loss: 1.5844e-03\n",
      "Epoch: 230960 | training loss: 1.9593e-03 | validation loss: 1.5551e-03\n",
      "Epoch: 230970 | training loss: 1.9536e-03 | validation loss: 1.5622e-03\n",
      "Epoch: 230980 | training loss: 1.9552e-03 | validation loss: 1.5682e-03\n",
      "Epoch: 230990 | training loss: 1.9547e-03 | validation loss: 1.5669e-03\n",
      "Epoch: 231000 | training loss: 1.9564e-03 | validation loss: 1.5704e-03\n",
      "Epoch: 231010 | training loss: 1.9919e-03 | validation loss: 1.6056e-03\n",
      "Epoch: 231020 | training loss: 2.6509e-03 | validation loss: 2.0038e-03\n",
      "Epoch: 231030 | training loss: 2.1349e-03 | validation loss: 1.5962e-03\n",
      "Epoch: 231040 | training loss: 2.0616e-03 | validation loss: 1.6503e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 231050 | training loss: 1.9956e-03 | validation loss: 1.5606e-03\n",
      "Epoch: 231060 | training loss: 1.9668e-03 | validation loss: 1.5819e-03\n",
      "Epoch: 231070 | training loss: 1.9586e-03 | validation loss: 1.5556e-03\n",
      "Epoch: 231080 | training loss: 1.9559e-03 | validation loss: 1.5695e-03\n",
      "Epoch: 231090 | training loss: 1.9541e-03 | validation loss: 1.5591e-03\n",
      "Epoch: 231100 | training loss: 1.9534e-03 | validation loss: 1.5619e-03\n",
      "Epoch: 231110 | training loss: 1.9536e-03 | validation loss: 1.5638e-03\n",
      "Epoch: 231120 | training loss: 1.9535e-03 | validation loss: 1.5636e-03\n",
      "Epoch: 231130 | training loss: 1.9537e-03 | validation loss: 1.5643e-03\n",
      "Epoch: 231140 | training loss: 1.9581e-03 | validation loss: 1.5723e-03\n",
      "Epoch: 231150 | training loss: 2.1237e-03 | validation loss: 1.6918e-03\n",
      "Epoch: 231160 | training loss: 2.4200e-03 | validation loss: 1.8640e-03\n",
      "Epoch: 231170 | training loss: 1.9547e-03 | validation loss: 1.5684e-03\n",
      "Epoch: 231180 | training loss: 2.0372e-03 | validation loss: 1.5657e-03\n",
      "Epoch: 231190 | training loss: 1.9737e-03 | validation loss: 1.5882e-03\n",
      "Epoch: 231200 | training loss: 1.9533e-03 | validation loss: 1.5639e-03\n",
      "Epoch: 231210 | training loss: 1.9556e-03 | validation loss: 1.5566e-03\n",
      "Epoch: 231220 | training loss: 1.9548e-03 | validation loss: 1.5679e-03\n",
      "Epoch: 231230 | training loss: 1.9538e-03 | validation loss: 1.5591e-03\n",
      "Epoch: 231240 | training loss: 1.9534e-03 | validation loss: 1.5639e-03\n",
      "Epoch: 231250 | training loss: 1.9531e-03 | validation loss: 1.5609e-03\n",
      "Epoch: 231260 | training loss: 1.9531e-03 | validation loss: 1.5617e-03\n",
      "Epoch: 231270 | training loss: 1.9531e-03 | validation loss: 1.5624e-03\n",
      "Epoch: 231280 | training loss: 1.9530e-03 | validation loss: 1.5621e-03\n",
      "Epoch: 231290 | training loss: 1.9530e-03 | validation loss: 1.5623e-03\n",
      "Epoch: 231300 | training loss: 1.9560e-03 | validation loss: 1.5693e-03\n",
      "Epoch: 231310 | training loss: 2.4093e-03 | validation loss: 1.9470e-03\n",
      "Epoch: 231320 | training loss: 2.2908e-03 | validation loss: 1.7657e-03\n",
      "Epoch: 231330 | training loss: 2.1231e-03 | validation loss: 1.6688e-03\n",
      "Epoch: 231340 | training loss: 1.9886e-03 | validation loss: 1.5676e-03\n",
      "Epoch: 231350 | training loss: 1.9798e-03 | validation loss: 1.5518e-03\n",
      "Epoch: 231360 | training loss: 1.9881e-03 | validation loss: 1.5515e-03\n",
      "Epoch: 231370 | training loss: 2.0723e-03 | validation loss: 1.5724e-03\n",
      "Epoch: 231380 | training loss: 2.1276e-03 | validation loss: 1.5907e-03\n",
      "Epoch: 231390 | training loss: 1.9642e-03 | validation loss: 1.5813e-03\n",
      "Epoch: 231400 | training loss: 1.9820e-03 | validation loss: 1.5984e-03\n",
      "Epoch: 231410 | training loss: 1.9528e-03 | validation loss: 1.5618e-03\n",
      "Epoch: 231420 | training loss: 1.9593e-03 | validation loss: 1.5539e-03\n",
      "Epoch: 231430 | training loss: 1.9973e-03 | validation loss: 1.5550e-03\n",
      "Epoch: 231440 | training loss: 2.3408e-03 | validation loss: 1.6674e-03\n",
      "Epoch: 231450 | training loss: 1.9545e-03 | validation loss: 1.5677e-03\n",
      "Epoch: 231460 | training loss: 1.9607e-03 | validation loss: 1.5764e-03\n",
      "Epoch: 231470 | training loss: 1.9704e-03 | validation loss: 1.5529e-03\n",
      "Epoch: 231480 | training loss: 1.9654e-03 | validation loss: 1.5812e-03\n",
      "Epoch: 231490 | training loss: 1.9530e-03 | validation loss: 1.5591e-03\n",
      "Epoch: 231500 | training loss: 1.9559e-03 | validation loss: 1.5556e-03\n",
      "Epoch: 231510 | training loss: 1.9547e-03 | validation loss: 1.5565e-03\n",
      "Epoch: 231520 | training loss: 1.9584e-03 | validation loss: 1.5544e-03\n",
      "Epoch: 231530 | training loss: 2.0328e-03 | validation loss: 1.5645e-03\n",
      "Epoch: 231540 | training loss: 2.6744e-03 | validation loss: 1.8058e-03\n",
      "Epoch: 231550 | training loss: 2.1893e-03 | validation loss: 1.7315e-03\n",
      "Epoch: 231560 | training loss: 2.0365e-03 | validation loss: 1.5695e-03\n",
      "Epoch: 231570 | training loss: 1.9799e-03 | validation loss: 1.5929e-03\n",
      "Epoch: 231580 | training loss: 1.9631e-03 | validation loss: 1.5537e-03\n",
      "Epoch: 231590 | training loss: 1.9574e-03 | validation loss: 1.5719e-03\n",
      "Epoch: 231600 | training loss: 1.9539e-03 | validation loss: 1.5572e-03\n",
      "Epoch: 231610 | training loss: 1.9525e-03 | validation loss: 1.5607e-03\n",
      "Epoch: 231620 | training loss: 1.9529e-03 | validation loss: 1.5637e-03\n",
      "Epoch: 231630 | training loss: 1.9527e-03 | validation loss: 1.5628e-03\n",
      "Epoch: 231640 | training loss: 1.9528e-03 | validation loss: 1.5633e-03\n",
      "Epoch: 231650 | training loss: 1.9567e-03 | validation loss: 1.5706e-03\n",
      "Epoch: 231660 | training loss: 2.0940e-03 | validation loss: 1.6727e-03\n",
      "Epoch: 231670 | training loss: 2.5964e-03 | validation loss: 1.9631e-03\n",
      "Epoch: 231680 | training loss: 1.9666e-03 | validation loss: 1.5580e-03\n",
      "Epoch: 231690 | training loss: 2.0142e-03 | validation loss: 1.5590e-03\n",
      "Epoch: 231700 | training loss: 1.9840e-03 | validation loss: 1.5964e-03\n",
      "Epoch: 231710 | training loss: 1.9535e-03 | validation loss: 1.5588e-03\n",
      "Epoch: 231720 | training loss: 1.9531e-03 | validation loss: 1.5571e-03\n",
      "Epoch: 231730 | training loss: 1.9533e-03 | validation loss: 1.5658e-03\n",
      "Epoch: 231740 | training loss: 1.9527e-03 | validation loss: 1.5582e-03\n",
      "Epoch: 231750 | training loss: 1.9524e-03 | validation loss: 1.5623e-03\n",
      "Epoch: 231760 | training loss: 1.9522e-03 | validation loss: 1.5608e-03\n",
      "Epoch: 231770 | training loss: 1.9522e-03 | validation loss: 1.5603e-03\n",
      "Epoch: 231780 | training loss: 1.9522e-03 | validation loss: 1.5614e-03\n",
      "Epoch: 231790 | training loss: 1.9522e-03 | validation loss: 1.5615e-03\n",
      "Epoch: 231800 | training loss: 1.9523e-03 | validation loss: 1.5627e-03\n",
      "Epoch: 231810 | training loss: 1.9634e-03 | validation loss: 1.5806e-03\n",
      "Epoch: 231820 | training loss: 2.5160e-03 | validation loss: 2.0308e-03\n",
      "Epoch: 231830 | training loss: 2.1178e-03 | validation loss: 1.6722e-03\n",
      "Epoch: 231840 | training loss: 2.1244e-03 | validation loss: 1.6735e-03\n",
      "Epoch: 231850 | training loss: 1.9667e-03 | validation loss: 1.5530e-03\n",
      "Epoch: 231860 | training loss: 1.9773e-03 | validation loss: 1.5524e-03\n",
      "Epoch: 231870 | training loss: 1.9699e-03 | validation loss: 1.5572e-03\n",
      "Epoch: 231880 | training loss: 1.9763e-03 | validation loss: 1.5541e-03\n",
      "Epoch: 231890 | training loss: 2.0944e-03 | validation loss: 1.5792e-03\n",
      "Epoch: 231900 | training loss: 2.2111e-03 | validation loss: 1.6190e-03\n",
      "Epoch: 231910 | training loss: 2.0350e-03 | validation loss: 1.6358e-03\n",
      "Epoch: 231920 | training loss: 1.9551e-03 | validation loss: 1.5547e-03\n",
      "Epoch: 231930 | training loss: 1.9621e-03 | validation loss: 1.5526e-03\n",
      "Epoch: 231940 | training loss: 1.9558e-03 | validation loss: 1.5700e-03\n",
      "Epoch: 231950 | training loss: 1.9586e-03 | validation loss: 1.5737e-03\n",
      "Epoch: 231960 | training loss: 1.9631e-03 | validation loss: 1.5790e-03\n",
      "Epoch: 231970 | training loss: 2.0306e-03 | validation loss: 1.6333e-03\n",
      "Epoch: 231980 | training loss: 2.4395e-03 | validation loss: 1.8813e-03\n",
      "Epoch: 231990 | training loss: 2.0364e-03 | validation loss: 1.5659e-03\n",
      "Epoch: 232000 | training loss: 1.9621e-03 | validation loss: 1.5761e-03\n",
      "Epoch: 232010 | training loss: 1.9522e-03 | validation loss: 1.5628e-03\n",
      "Epoch: 232020 | training loss: 1.9591e-03 | validation loss: 1.5535e-03\n",
      "Epoch: 232030 | training loss: 1.9549e-03 | validation loss: 1.5682e-03\n",
      "Epoch: 232040 | training loss: 1.9535e-03 | validation loss: 1.5660e-03\n",
      "Epoch: 232050 | training loss: 1.9519e-03 | validation loss: 1.5619e-03\n",
      "Epoch: 232060 | training loss: 1.9519e-03 | validation loss: 1.5621e-03\n",
      "Epoch: 232070 | training loss: 1.9556e-03 | validation loss: 1.5694e-03\n",
      "Epoch: 232080 | training loss: 2.1209e-03 | validation loss: 1.6900e-03\n",
      "Epoch: 232090 | training loss: 2.4173e-03 | validation loss: 1.8613e-03\n",
      "Epoch: 232100 | training loss: 1.9762e-03 | validation loss: 1.5951e-03\n",
      "Epoch: 232110 | training loss: 2.0564e-03 | validation loss: 1.5715e-03\n",
      "Epoch: 232120 | training loss: 1.9540e-03 | validation loss: 1.5605e-03\n",
      "Epoch: 232130 | training loss: 1.9621e-03 | validation loss: 1.5803e-03\n",
      "Epoch: 232140 | training loss: 1.9555e-03 | validation loss: 1.5535e-03\n",
      "Epoch: 232150 | training loss: 1.9518e-03 | validation loss: 1.5619e-03\n",
      "Epoch: 232160 | training loss: 1.9515e-03 | validation loss: 1.5607e-03\n",
      "Epoch: 232170 | training loss: 1.9515e-03 | validation loss: 1.5594e-03\n",
      "Epoch: 232180 | training loss: 1.9515e-03 | validation loss: 1.5601e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 232190 | training loss: 1.9515e-03 | validation loss: 1.5603e-03\n",
      "Epoch: 232200 | training loss: 1.9514e-03 | validation loss: 1.5595e-03\n",
      "Epoch: 232210 | training loss: 1.9514e-03 | validation loss: 1.5602e-03\n",
      "Epoch: 232220 | training loss: 1.9514e-03 | validation loss: 1.5602e-03\n",
      "Epoch: 232230 | training loss: 1.9514e-03 | validation loss: 1.5602e-03\n",
      "Epoch: 232240 | training loss: 1.9514e-03 | validation loss: 1.5610e-03\n",
      "Epoch: 232250 | training loss: 1.9581e-03 | validation loss: 1.5737e-03\n",
      "Epoch: 232260 | training loss: 2.5899e-03 | validation loss: 2.0803e-03\n",
      "Epoch: 232270 | training loss: 2.0825e-03 | validation loss: 1.6464e-03\n",
      "Epoch: 232280 | training loss: 2.2702e-03 | validation loss: 1.7646e-03\n",
      "Epoch: 232290 | training loss: 1.9854e-03 | validation loss: 1.5668e-03\n",
      "Epoch: 232300 | training loss: 1.9973e-03 | validation loss: 1.5515e-03\n",
      "Epoch: 232310 | training loss: 1.9591e-03 | validation loss: 1.5672e-03\n",
      "Epoch: 232320 | training loss: 1.9635e-03 | validation loss: 1.5776e-03\n",
      "Epoch: 232330 | training loss: 1.9622e-03 | validation loss: 1.5784e-03\n",
      "Epoch: 232340 | training loss: 1.9939e-03 | validation loss: 1.6078e-03\n",
      "Epoch: 232350 | training loss: 2.2792e-03 | validation loss: 1.7929e-03\n",
      "Epoch: 232360 | training loss: 1.9558e-03 | validation loss: 1.5700e-03\n",
      "Epoch: 232370 | training loss: 1.9849e-03 | validation loss: 1.5517e-03\n",
      "Epoch: 232380 | training loss: 1.9812e-03 | validation loss: 1.5952e-03\n",
      "Epoch: 232390 | training loss: 1.9565e-03 | validation loss: 1.5525e-03\n",
      "Epoch: 232400 | training loss: 1.9543e-03 | validation loss: 1.5538e-03\n",
      "Epoch: 232410 | training loss: 1.9514e-03 | validation loss: 1.5618e-03\n",
      "Epoch: 232420 | training loss: 1.9541e-03 | validation loss: 1.5677e-03\n",
      "Epoch: 232430 | training loss: 1.9772e-03 | validation loss: 1.5922e-03\n",
      "Epoch: 232440 | training loss: 2.3645e-03 | validation loss: 1.8391e-03\n",
      "Epoch: 232450 | training loss: 1.9517e-03 | validation loss: 1.5556e-03\n",
      "Epoch: 232460 | training loss: 1.9533e-03 | validation loss: 1.5654e-03\n",
      "Epoch: 232470 | training loss: 1.9539e-03 | validation loss: 1.5546e-03\n",
      "Epoch: 232480 | training loss: 1.9515e-03 | validation loss: 1.5621e-03\n",
      "Epoch: 232490 | training loss: 1.9512e-03 | validation loss: 1.5610e-03\n",
      "Epoch: 232500 | training loss: 1.9528e-03 | validation loss: 1.5551e-03\n",
      "Epoch: 232510 | training loss: 1.9518e-03 | validation loss: 1.5632e-03\n",
      "Epoch: 232520 | training loss: 1.9512e-03 | validation loss: 1.5613e-03\n",
      "Epoch: 232530 | training loss: 1.9509e-03 | validation loss: 1.5591e-03\n",
      "Epoch: 232540 | training loss: 1.9509e-03 | validation loss: 1.5580e-03\n",
      "Epoch: 232550 | training loss: 1.9524e-03 | validation loss: 1.5552e-03\n",
      "Epoch: 232560 | training loss: 2.0175e-03 | validation loss: 1.5610e-03\n",
      "Epoch: 232570 | training loss: 3.2506e-03 | validation loss: 2.0563e-03\n",
      "Epoch: 232580 | training loss: 2.0949e-03 | validation loss: 1.6678e-03\n",
      "Epoch: 232590 | training loss: 2.0277e-03 | validation loss: 1.6316e-03\n",
      "Epoch: 232600 | training loss: 1.9788e-03 | validation loss: 1.5556e-03\n",
      "Epoch: 232610 | training loss: 1.9561e-03 | validation loss: 1.5506e-03\n",
      "Epoch: 232620 | training loss: 1.9570e-03 | validation loss: 1.5723e-03\n",
      "Epoch: 232630 | training loss: 1.9513e-03 | validation loss: 1.5575e-03\n",
      "Epoch: 232640 | training loss: 1.9508e-03 | validation loss: 1.5574e-03\n",
      "Epoch: 232650 | training loss: 1.9508e-03 | validation loss: 1.5613e-03\n",
      "Epoch: 232660 | training loss: 1.9507e-03 | validation loss: 1.5576e-03\n",
      "Epoch: 232670 | training loss: 1.9506e-03 | validation loss: 1.5597e-03\n",
      "Epoch: 232680 | training loss: 1.9506e-03 | validation loss: 1.5589e-03\n",
      "Epoch: 232690 | training loss: 1.9506e-03 | validation loss: 1.5586e-03\n",
      "Epoch: 232700 | training loss: 1.9506e-03 | validation loss: 1.5591e-03\n",
      "Epoch: 232710 | training loss: 1.9505e-03 | validation loss: 1.5590e-03\n",
      "Epoch: 232720 | training loss: 1.9505e-03 | validation loss: 1.5588e-03\n",
      "Epoch: 232730 | training loss: 1.9509e-03 | validation loss: 1.5581e-03\n",
      "Epoch: 232740 | training loss: 1.9786e-03 | validation loss: 1.5676e-03\n",
      "Epoch: 232750 | training loss: 2.3544e-03 | validation loss: 1.7957e-03\n",
      "Epoch: 232760 | training loss: 2.2276e-03 | validation loss: 1.7510e-03\n",
      "Epoch: 232770 | training loss: 2.0091e-03 | validation loss: 1.6259e-03\n",
      "Epoch: 232780 | training loss: 1.9973e-03 | validation loss: 1.5684e-03\n",
      "Epoch: 232790 | training loss: 1.9790e-03 | validation loss: 1.6009e-03\n",
      "Epoch: 232800 | training loss: 1.9659e-03 | validation loss: 1.5553e-03\n",
      "Epoch: 232810 | training loss: 1.9548e-03 | validation loss: 1.5702e-03\n",
      "Epoch: 232820 | training loss: 1.9506e-03 | validation loss: 1.5587e-03\n",
      "Epoch: 232830 | training loss: 1.9518e-03 | validation loss: 1.5535e-03\n",
      "Epoch: 232840 | training loss: 1.9517e-03 | validation loss: 1.5542e-03\n",
      "Epoch: 232850 | training loss: 1.9552e-03 | validation loss: 1.5519e-03\n",
      "Epoch: 232860 | training loss: 2.0217e-03 | validation loss: 1.5578e-03\n",
      "Epoch: 232870 | training loss: 2.6776e-03 | validation loss: 1.7998e-03\n",
      "Epoch: 232880 | training loss: 2.1888e-03 | validation loss: 1.7320e-03\n",
      "Epoch: 232890 | training loss: 2.0359e-03 | validation loss: 1.5652e-03\n",
      "Epoch: 232900 | training loss: 1.9772e-03 | validation loss: 1.5917e-03\n",
      "Epoch: 232910 | training loss: 1.9601e-03 | validation loss: 1.5505e-03\n",
      "Epoch: 232920 | training loss: 1.9549e-03 | validation loss: 1.5687e-03\n",
      "Epoch: 232930 | training loss: 1.9519e-03 | validation loss: 1.5543e-03\n",
      "Epoch: 232940 | training loss: 1.9502e-03 | validation loss: 1.5588e-03\n",
      "Epoch: 232950 | training loss: 1.9506e-03 | validation loss: 1.5610e-03\n",
      "Epoch: 232960 | training loss: 1.9502e-03 | validation loss: 1.5592e-03\n",
      "Epoch: 232970 | training loss: 1.9501e-03 | validation loss: 1.5585e-03\n",
      "Epoch: 232980 | training loss: 1.9501e-03 | validation loss: 1.5589e-03\n",
      "Epoch: 232990 | training loss: 1.9511e-03 | validation loss: 1.5625e-03\n",
      "Epoch: 233000 | training loss: 2.0221e-03 | validation loss: 1.6253e-03\n",
      "Epoch: 233010 | training loss: 3.4443e-03 | validation loss: 2.4232e-03\n",
      "Epoch: 233020 | training loss: 1.9748e-03 | validation loss: 1.5986e-03\n",
      "Epoch: 233030 | training loss: 2.0518e-03 | validation loss: 1.5720e-03\n",
      "Epoch: 233040 | training loss: 2.0023e-03 | validation loss: 1.5494e-03\n",
      "Epoch: 233050 | training loss: 1.9520e-03 | validation loss: 1.5530e-03\n",
      "Epoch: 233060 | training loss: 1.9559e-03 | validation loss: 1.5737e-03\n",
      "Epoch: 233070 | training loss: 1.9509e-03 | validation loss: 1.5632e-03\n",
      "Epoch: 233080 | training loss: 1.9507e-03 | validation loss: 1.5543e-03\n",
      "Epoch: 233090 | training loss: 1.9499e-03 | validation loss: 1.5578e-03\n",
      "Epoch: 233100 | training loss: 1.9500e-03 | validation loss: 1.5596e-03\n",
      "Epoch: 233110 | training loss: 1.9499e-03 | validation loss: 1.5574e-03\n",
      "Epoch: 233120 | training loss: 1.9498e-03 | validation loss: 1.5583e-03\n",
      "Epoch: 233130 | training loss: 1.9498e-03 | validation loss: 1.5580e-03\n",
      "Epoch: 233140 | training loss: 1.9498e-03 | validation loss: 1.5581e-03\n",
      "Epoch: 233150 | training loss: 1.9498e-03 | validation loss: 1.5579e-03\n",
      "Epoch: 233160 | training loss: 1.9498e-03 | validation loss: 1.5581e-03\n",
      "Epoch: 233170 | training loss: 1.9497e-03 | validation loss: 1.5579e-03\n",
      "Epoch: 233180 | training loss: 1.9497e-03 | validation loss: 1.5578e-03\n",
      "Epoch: 233190 | training loss: 1.9498e-03 | validation loss: 1.5573e-03\n",
      "Epoch: 233200 | training loss: 1.9607e-03 | validation loss: 1.5582e-03\n",
      "Epoch: 233210 | training loss: 2.6894e-03 | validation loss: 2.0015e-03\n",
      "Epoch: 233220 | training loss: 2.0346e-03 | validation loss: 1.6295e-03\n",
      "Epoch: 233230 | training loss: 1.9859e-03 | validation loss: 1.6050e-03\n",
      "Epoch: 233240 | training loss: 1.9562e-03 | validation loss: 1.5653e-03\n",
      "Epoch: 233250 | training loss: 1.9652e-03 | validation loss: 1.5571e-03\n",
      "Epoch: 233260 | training loss: 2.0081e-03 | validation loss: 1.5581e-03\n",
      "Epoch: 233270 | training loss: 2.2791e-03 | validation loss: 1.6421e-03\n",
      "Epoch: 233280 | training loss: 1.9505e-03 | validation loss: 1.5626e-03\n",
      "Epoch: 233290 | training loss: 1.9743e-03 | validation loss: 1.5906e-03\n",
      "Epoch: 233300 | training loss: 1.9744e-03 | validation loss: 1.5486e-03\n",
      "Epoch: 233310 | training loss: 1.9499e-03 | validation loss: 1.5599e-03\n",
      "Epoch: 233320 | training loss: 1.9561e-03 | validation loss: 1.5710e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 233330 | training loss: 1.9550e-03 | validation loss: 1.5698e-03\n",
      "Epoch: 233340 | training loss: 1.9680e-03 | validation loss: 1.5843e-03\n",
      "Epoch: 233350 | training loss: 2.1558e-03 | validation loss: 1.7165e-03\n",
      "Epoch: 233360 | training loss: 2.1392e-03 | validation loss: 1.7027e-03\n",
      "Epoch: 233370 | training loss: 2.0472e-03 | validation loss: 1.5656e-03\n",
      "Epoch: 233380 | training loss: 1.9942e-03 | validation loss: 1.6055e-03\n",
      "Epoch: 233390 | training loss: 1.9672e-03 | validation loss: 1.5492e-03\n",
      "Epoch: 233400 | training loss: 1.9515e-03 | validation loss: 1.5639e-03\n",
      "Epoch: 233410 | training loss: 1.9507e-03 | validation loss: 1.5626e-03\n",
      "Epoch: 233420 | training loss: 1.9501e-03 | validation loss: 1.5542e-03\n",
      "Epoch: 233430 | training loss: 1.9513e-03 | validation loss: 1.5527e-03\n",
      "Epoch: 233440 | training loss: 1.9575e-03 | validation loss: 1.5501e-03\n",
      "Epoch: 233450 | training loss: 2.0642e-03 | validation loss: 1.5709e-03\n",
      "Epoch: 233460 | training loss: 2.5350e-03 | validation loss: 1.7492e-03\n",
      "Epoch: 233470 | training loss: 2.1459e-03 | validation loss: 1.7018e-03\n",
      "Epoch: 233480 | training loss: 2.0147e-03 | validation loss: 1.5617e-03\n",
      "Epoch: 233490 | training loss: 1.9727e-03 | validation loss: 1.5848e-03\n",
      "Epoch: 233500 | training loss: 1.9595e-03 | validation loss: 1.5506e-03\n",
      "Epoch: 233510 | training loss: 1.9532e-03 | validation loss: 1.5666e-03\n",
      "Epoch: 233520 | training loss: 1.9494e-03 | validation loss: 1.5553e-03\n",
      "Epoch: 233530 | training loss: 1.9498e-03 | validation loss: 1.5548e-03\n",
      "Epoch: 233540 | training loss: 1.9491e-03 | validation loss: 1.5575e-03\n",
      "Epoch: 233550 | training loss: 1.9494e-03 | validation loss: 1.5591e-03\n",
      "Epoch: 233560 | training loss: 1.9514e-03 | validation loss: 1.5637e-03\n",
      "Epoch: 233570 | training loss: 2.0026e-03 | validation loss: 1.6092e-03\n",
      "Epoch: 233580 | training loss: 2.9790e-03 | validation loss: 2.1704e-03\n",
      "Epoch: 233590 | training loss: 2.3056e-03 | validation loss: 1.6623e-03\n",
      "Epoch: 233600 | training loss: 1.9860e-03 | validation loss: 1.5949e-03\n",
      "Epoch: 233610 | training loss: 1.9546e-03 | validation loss: 1.5691e-03\n",
      "Epoch: 233620 | training loss: 1.9627e-03 | validation loss: 1.5491e-03\n",
      "Epoch: 233630 | training loss: 1.9553e-03 | validation loss: 1.5701e-03\n",
      "Epoch: 233640 | training loss: 1.9511e-03 | validation loss: 1.5523e-03\n",
      "Epoch: 233650 | training loss: 1.9498e-03 | validation loss: 1.5609e-03\n",
      "Epoch: 233660 | training loss: 1.9493e-03 | validation loss: 1.5551e-03\n",
      "Epoch: 233670 | training loss: 1.9490e-03 | validation loss: 1.5581e-03\n",
      "Epoch: 233680 | training loss: 1.9489e-03 | validation loss: 1.5575e-03\n",
      "Epoch: 233690 | training loss: 1.9489e-03 | validation loss: 1.5566e-03\n",
      "Epoch: 233700 | training loss: 1.9491e-03 | validation loss: 1.5579e-03\n",
      "Epoch: 233710 | training loss: 1.9669e-03 | validation loss: 1.5825e-03\n",
      "Epoch: 233720 | training loss: 2.4994e-03 | validation loss: 2.0168e-03\n",
      "Epoch: 233730 | training loss: 2.0521e-03 | validation loss: 1.6525e-03\n",
      "Epoch: 233740 | training loss: 2.0020e-03 | validation loss: 1.5878e-03\n",
      "Epoch: 233750 | training loss: 1.9802e-03 | validation loss: 1.5518e-03\n",
      "Epoch: 233760 | training loss: 1.9726e-03 | validation loss: 1.5454e-03\n",
      "Epoch: 233770 | training loss: 2.0043e-03 | validation loss: 1.5542e-03\n",
      "Epoch: 233780 | training loss: 2.1428e-03 | validation loss: 1.5944e-03\n",
      "Epoch: 233790 | training loss: 1.9918e-03 | validation loss: 1.5509e-03\n",
      "Epoch: 233800 | training loss: 1.9895e-03 | validation loss: 1.6014e-03\n",
      "Epoch: 233810 | training loss: 1.9583e-03 | validation loss: 1.5731e-03\n",
      "Epoch: 233820 | training loss: 1.9498e-03 | validation loss: 1.5527e-03\n",
      "Epoch: 233830 | training loss: 1.9606e-03 | validation loss: 1.5482e-03\n",
      "Epoch: 233840 | training loss: 2.0616e-03 | validation loss: 1.5675e-03\n",
      "Epoch: 233850 | training loss: 2.3860e-03 | validation loss: 1.6844e-03\n",
      "Epoch: 233860 | training loss: 2.0853e-03 | validation loss: 1.6665e-03\n",
      "Epoch: 233870 | training loss: 1.9847e-03 | validation loss: 1.5510e-03\n",
      "Epoch: 233880 | training loss: 1.9518e-03 | validation loss: 1.5648e-03\n",
      "Epoch: 233890 | training loss: 1.9504e-03 | validation loss: 1.5622e-03\n",
      "Epoch: 233900 | training loss: 1.9527e-03 | validation loss: 1.5506e-03\n",
      "Epoch: 233910 | training loss: 1.9489e-03 | validation loss: 1.5539e-03\n",
      "Epoch: 233920 | training loss: 1.9485e-03 | validation loss: 1.5575e-03\n",
      "Epoch: 233930 | training loss: 1.9496e-03 | validation loss: 1.5610e-03\n",
      "Epoch: 233940 | training loss: 1.9706e-03 | validation loss: 1.5849e-03\n",
      "Epoch: 233950 | training loss: 2.6184e-03 | validation loss: 1.9778e-03\n",
      "Epoch: 233960 | training loss: 2.1325e-03 | validation loss: 1.5911e-03\n",
      "Epoch: 233970 | training loss: 2.1076e-03 | validation loss: 1.6779e-03\n",
      "Epoch: 233980 | training loss: 1.9728e-03 | validation loss: 1.5563e-03\n",
      "Epoch: 233990 | training loss: 1.9502e-03 | validation loss: 1.5510e-03\n",
      "Epoch: 234000 | training loss: 1.9516e-03 | validation loss: 1.5662e-03\n",
      "Epoch: 234010 | training loss: 1.9502e-03 | validation loss: 1.5515e-03\n",
      "Epoch: 234020 | training loss: 1.9490e-03 | validation loss: 1.5599e-03\n",
      "Epoch: 234030 | training loss: 1.9484e-03 | validation loss: 1.5544e-03\n",
      "Epoch: 234040 | training loss: 1.9483e-03 | validation loss: 1.5571e-03\n",
      "Epoch: 234050 | training loss: 1.9483e-03 | validation loss: 1.5563e-03\n",
      "Epoch: 234060 | training loss: 1.9482e-03 | validation loss: 1.5557e-03\n",
      "Epoch: 234070 | training loss: 1.9482e-03 | validation loss: 1.5560e-03\n",
      "Epoch: 234080 | training loss: 1.9482e-03 | validation loss: 1.5560e-03\n",
      "Epoch: 234090 | training loss: 1.9493e-03 | validation loss: 1.5560e-03\n",
      "Epoch: 234100 | training loss: 1.9948e-03 | validation loss: 1.5766e-03\n",
      "Epoch: 234110 | training loss: 2.8515e-03 | validation loss: 1.9201e-03\n",
      "Epoch: 234120 | training loss: 2.1610e-03 | validation loss: 1.7467e-03\n",
      "Epoch: 234130 | training loss: 2.0278e-03 | validation loss: 1.5535e-03\n",
      "Epoch: 234140 | training loss: 1.9692e-03 | validation loss: 1.5650e-03\n",
      "Epoch: 234150 | training loss: 1.9608e-03 | validation loss: 1.5591e-03\n",
      "Epoch: 234160 | training loss: 1.9538e-03 | validation loss: 1.5479e-03\n",
      "Epoch: 234170 | training loss: 1.9493e-03 | validation loss: 1.5595e-03\n",
      "Epoch: 234180 | training loss: 1.9487e-03 | validation loss: 1.5549e-03\n",
      "Epoch: 234190 | training loss: 1.9483e-03 | validation loss: 1.5585e-03\n",
      "Epoch: 234200 | training loss: 1.9480e-03 | validation loss: 1.5555e-03\n",
      "Epoch: 234210 | training loss: 1.9480e-03 | validation loss: 1.5545e-03\n",
      "Epoch: 234220 | training loss: 1.9479e-03 | validation loss: 1.5559e-03\n",
      "Epoch: 234230 | training loss: 1.9479e-03 | validation loss: 1.5562e-03\n",
      "Epoch: 234240 | training loss: 1.9480e-03 | validation loss: 1.5566e-03\n",
      "Epoch: 234250 | training loss: 1.9490e-03 | validation loss: 1.5604e-03\n",
      "Epoch: 234260 | training loss: 1.9963e-03 | validation loss: 1.6066e-03\n",
      "Epoch: 234270 | training loss: 3.3271e-03 | validation loss: 2.3658e-03\n",
      "Epoch: 234280 | training loss: 2.2165e-03 | validation loss: 1.6350e-03\n",
      "Epoch: 234290 | training loss: 1.9989e-03 | validation loss: 1.5482e-03\n",
      "Epoch: 234300 | training loss: 1.9887e-03 | validation loss: 1.5906e-03\n",
      "Epoch: 234310 | training loss: 1.9508e-03 | validation loss: 1.5653e-03\n",
      "Epoch: 234320 | training loss: 1.9549e-03 | validation loss: 1.5533e-03\n",
      "Epoch: 234330 | training loss: 1.9488e-03 | validation loss: 1.5572e-03\n",
      "Epoch: 234340 | training loss: 1.9477e-03 | validation loss: 1.5559e-03\n",
      "Epoch: 234350 | training loss: 1.9478e-03 | validation loss: 1.5552e-03\n",
      "Epoch: 234360 | training loss: 1.9478e-03 | validation loss: 1.5560e-03\n",
      "Epoch: 234370 | training loss: 1.9477e-03 | validation loss: 1.5553e-03\n",
      "Epoch: 234380 | training loss: 1.9477e-03 | validation loss: 1.5554e-03\n",
      "Epoch: 234390 | training loss: 1.9476e-03 | validation loss: 1.5557e-03\n",
      "Epoch: 234400 | training loss: 1.9476e-03 | validation loss: 1.5552e-03\n",
      "Epoch: 234410 | training loss: 1.9476e-03 | validation loss: 1.5553e-03\n",
      "Epoch: 234420 | training loss: 1.9476e-03 | validation loss: 1.5554e-03\n",
      "Epoch: 234430 | training loss: 1.9476e-03 | validation loss: 1.5553e-03\n",
      "Epoch: 234440 | training loss: 1.9475e-03 | validation loss: 1.5551e-03\n",
      "Epoch: 234450 | training loss: 1.9480e-03 | validation loss: 1.5535e-03\n",
      "Epoch: 234460 | training loss: 2.0027e-03 | validation loss: 1.5601e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 234470 | training loss: 3.9060e-03 | validation loss: 2.3479e-03\n",
      "Epoch: 234480 | training loss: 2.1926e-03 | validation loss: 1.7006e-03\n",
      "Epoch: 234490 | training loss: 2.0163e-03 | validation loss: 1.6288e-03\n",
      "Epoch: 234500 | training loss: 1.9662e-03 | validation loss: 1.5782e-03\n",
      "Epoch: 234510 | training loss: 1.9541e-03 | validation loss: 1.5555e-03\n",
      "Epoch: 234520 | training loss: 1.9497e-03 | validation loss: 1.5534e-03\n",
      "Epoch: 234530 | training loss: 1.9478e-03 | validation loss: 1.5576e-03\n",
      "Epoch: 234540 | training loss: 1.9474e-03 | validation loss: 1.5560e-03\n",
      "Epoch: 234550 | training loss: 1.9475e-03 | validation loss: 1.5543e-03\n",
      "Epoch: 234560 | training loss: 1.9475e-03 | validation loss: 1.5557e-03\n",
      "Epoch: 234570 | training loss: 1.9474e-03 | validation loss: 1.5550e-03\n",
      "Epoch: 234580 | training loss: 1.9473e-03 | validation loss: 1.5553e-03\n",
      "Epoch: 234590 | training loss: 1.9473e-03 | validation loss: 1.5551e-03\n",
      "Epoch: 234600 | training loss: 1.9473e-03 | validation loss: 1.5552e-03\n",
      "Epoch: 234610 | training loss: 1.9473e-03 | validation loss: 1.5550e-03\n",
      "Epoch: 234620 | training loss: 1.9472e-03 | validation loss: 1.5551e-03\n",
      "Epoch: 234630 | training loss: 1.9472e-03 | validation loss: 1.5551e-03\n",
      "Epoch: 234640 | training loss: 1.9472e-03 | validation loss: 1.5551e-03\n",
      "Epoch: 234650 | training loss: 1.9472e-03 | validation loss: 1.5551e-03\n",
      "Epoch: 234660 | training loss: 1.9472e-03 | validation loss: 1.5556e-03\n",
      "Epoch: 234670 | training loss: 1.9480e-03 | validation loss: 1.5601e-03\n",
      "Epoch: 234680 | training loss: 2.0583e-03 | validation loss: 1.6699e-03\n",
      "Epoch: 234690 | training loss: 2.2903e-03 | validation loss: 1.7972e-03\n",
      "Epoch: 234700 | training loss: 2.1463e-03 | validation loss: 1.7440e-03\n",
      "Epoch: 234710 | training loss: 2.0584e-03 | validation loss: 1.6438e-03\n",
      "Epoch: 234720 | training loss: 1.9793e-03 | validation loss: 1.5920e-03\n",
      "Epoch: 234730 | training loss: 1.9627e-03 | validation loss: 1.5824e-03\n",
      "Epoch: 234740 | training loss: 1.9510e-03 | validation loss: 1.5623e-03\n",
      "Epoch: 234750 | training loss: 1.9480e-03 | validation loss: 1.5571e-03\n",
      "Epoch: 234760 | training loss: 1.9472e-03 | validation loss: 1.5567e-03\n",
      "Epoch: 234770 | training loss: 1.9471e-03 | validation loss: 1.5528e-03\n",
      "Epoch: 234780 | training loss: 1.9471e-03 | validation loss: 1.5542e-03\n",
      "Epoch: 234790 | training loss: 1.9470e-03 | validation loss: 1.5537e-03\n",
      "Epoch: 234800 | training loss: 1.9469e-03 | validation loss: 1.5548e-03\n",
      "Epoch: 234810 | training loss: 1.9469e-03 | validation loss: 1.5546e-03\n",
      "Epoch: 234820 | training loss: 1.9469e-03 | validation loss: 1.5542e-03\n",
      "Epoch: 234830 | training loss: 1.9469e-03 | validation loss: 1.5542e-03\n",
      "Epoch: 234840 | training loss: 1.9469e-03 | validation loss: 1.5542e-03\n",
      "Epoch: 234850 | training loss: 1.9469e-03 | validation loss: 1.5538e-03\n",
      "Epoch: 234860 | training loss: 1.9475e-03 | validation loss: 1.5518e-03\n",
      "Epoch: 234870 | training loss: 2.0015e-03 | validation loss: 1.5559e-03\n",
      "Epoch: 234880 | training loss: 3.7247e-03 | validation loss: 2.2687e-03\n",
      "Epoch: 234890 | training loss: 1.9508e-03 | validation loss: 1.5607e-03\n",
      "Epoch: 234900 | training loss: 2.0677e-03 | validation loss: 1.6553e-03\n",
      "Epoch: 234910 | training loss: 2.0097e-03 | validation loss: 1.6124e-03\n",
      "Epoch: 234920 | training loss: 1.9500e-03 | validation loss: 1.5603e-03\n",
      "Epoch: 234930 | training loss: 1.9511e-03 | validation loss: 1.5476e-03\n",
      "Epoch: 234940 | training loss: 1.9491e-03 | validation loss: 1.5498e-03\n",
      "Epoch: 234950 | training loss: 1.9469e-03 | validation loss: 1.5568e-03\n",
      "Epoch: 234960 | training loss: 1.9470e-03 | validation loss: 1.5566e-03\n",
      "Epoch: 234970 | training loss: 1.9468e-03 | validation loss: 1.5530e-03\n",
      "Epoch: 234980 | training loss: 1.9466e-03 | validation loss: 1.5543e-03\n",
      "Epoch: 234990 | training loss: 1.9466e-03 | validation loss: 1.5546e-03\n",
      "Epoch: 235000 | training loss: 1.9466e-03 | validation loss: 1.5539e-03\n",
      "Epoch: 235010 | training loss: 1.9466e-03 | validation loss: 1.5544e-03\n",
      "Epoch: 235020 | training loss: 1.9466e-03 | validation loss: 1.5540e-03\n",
      "Epoch: 235030 | training loss: 1.9465e-03 | validation loss: 1.5542e-03\n",
      "Epoch: 235040 | training loss: 1.9465e-03 | validation loss: 1.5541e-03\n",
      "Epoch: 235050 | training loss: 1.9465e-03 | validation loss: 1.5541e-03\n",
      "Epoch: 235060 | training loss: 1.9465e-03 | validation loss: 1.5541e-03\n",
      "Epoch: 235070 | training loss: 1.9465e-03 | validation loss: 1.5541e-03\n",
      "Epoch: 235080 | training loss: 1.9465e-03 | validation loss: 1.5540e-03\n",
      "Epoch: 235090 | training loss: 1.9464e-03 | validation loss: 1.5540e-03\n",
      "Epoch: 235100 | training loss: 1.9464e-03 | validation loss: 1.5542e-03\n",
      "Epoch: 235110 | training loss: 1.9467e-03 | validation loss: 1.5564e-03\n",
      "Epoch: 235120 | training loss: 2.0289e-03 | validation loss: 1.6297e-03\n",
      "Epoch: 235130 | training loss: 3.1038e-03 | validation loss: 2.2284e-03\n",
      "Epoch: 235140 | training loss: 2.5005e-03 | validation loss: 1.9490e-03\n",
      "Epoch: 235150 | training loss: 2.0466e-03 | validation loss: 1.6572e-03\n",
      "Epoch: 235160 | training loss: 1.9483e-03 | validation loss: 1.5586e-03\n",
      "Epoch: 235170 | training loss: 1.9504e-03 | validation loss: 1.5460e-03\n",
      "Epoch: 235180 | training loss: 1.9515e-03 | validation loss: 1.5465e-03\n",
      "Epoch: 235190 | training loss: 1.9491e-03 | validation loss: 1.5471e-03\n",
      "Epoch: 235200 | training loss: 1.9474e-03 | validation loss: 1.5493e-03\n",
      "Epoch: 235210 | training loss: 1.9466e-03 | validation loss: 1.5513e-03\n",
      "Epoch: 235220 | training loss: 1.9463e-03 | validation loss: 1.5523e-03\n",
      "Epoch: 235230 | training loss: 1.9462e-03 | validation loss: 1.5532e-03\n",
      "Epoch: 235240 | training loss: 1.9462e-03 | validation loss: 1.5537e-03\n",
      "Epoch: 235250 | training loss: 1.9462e-03 | validation loss: 1.5540e-03\n",
      "Epoch: 235260 | training loss: 1.9462e-03 | validation loss: 1.5540e-03\n",
      "Epoch: 235270 | training loss: 1.9461e-03 | validation loss: 1.5537e-03\n",
      "Epoch: 235280 | training loss: 1.9461e-03 | validation loss: 1.5536e-03\n",
      "Epoch: 235290 | training loss: 1.9461e-03 | validation loss: 1.5536e-03\n",
      "Epoch: 235300 | training loss: 1.9461e-03 | validation loss: 1.5541e-03\n",
      "Epoch: 235310 | training loss: 1.9495e-03 | validation loss: 1.5597e-03\n",
      "Epoch: 235320 | training loss: 2.3045e-03 | validation loss: 1.8323e-03\n",
      "Epoch: 235330 | training loss: 2.1821e-03 | validation loss: 1.6860e-03\n",
      "Epoch: 235340 | training loss: 1.9896e-03 | validation loss: 1.5489e-03\n",
      "Epoch: 235350 | training loss: 1.9468e-03 | validation loss: 1.5576e-03\n",
      "Epoch: 235360 | training loss: 1.9537e-03 | validation loss: 1.5696e-03\n",
      "Epoch: 235370 | training loss: 1.9490e-03 | validation loss: 1.5567e-03\n",
      "Epoch: 235380 | training loss: 1.9463e-03 | validation loss: 1.5548e-03\n",
      "Epoch: 235390 | training loss: 1.9465e-03 | validation loss: 1.5502e-03\n",
      "Epoch: 235400 | training loss: 1.9460e-03 | validation loss: 1.5542e-03\n",
      "Epoch: 235410 | training loss: 1.9459e-03 | validation loss: 1.5538e-03\n",
      "Epoch: 235420 | training loss: 1.9459e-03 | validation loss: 1.5525e-03\n",
      "Epoch: 235430 | training loss: 1.9459e-03 | validation loss: 1.5533e-03\n",
      "Epoch: 235440 | training loss: 1.9458e-03 | validation loss: 1.5534e-03\n",
      "Epoch: 235450 | training loss: 1.9458e-03 | validation loss: 1.5536e-03\n",
      "Epoch: 235460 | training loss: 1.9462e-03 | validation loss: 1.5556e-03\n",
      "Epoch: 235470 | training loss: 1.9628e-03 | validation loss: 1.5770e-03\n",
      "Epoch: 235480 | training loss: 3.0443e-03 | validation loss: 2.2073e-03\n",
      "Epoch: 235490 | training loss: 2.5820e-03 | validation loss: 1.7646e-03\n",
      "Epoch: 235500 | training loss: 1.9630e-03 | validation loss: 1.5456e-03\n",
      "Epoch: 235510 | training loss: 2.0036e-03 | validation loss: 1.6066e-03\n",
      "Epoch: 235520 | training loss: 1.9611e-03 | validation loss: 1.5815e-03\n",
      "Epoch: 235530 | training loss: 1.9490e-03 | validation loss: 1.5495e-03\n",
      "Epoch: 235540 | training loss: 1.9486e-03 | validation loss: 1.5466e-03\n",
      "Epoch: 235550 | training loss: 1.9462e-03 | validation loss: 1.5572e-03\n",
      "Epoch: 235560 | training loss: 1.9457e-03 | validation loss: 1.5542e-03\n",
      "Epoch: 235570 | training loss: 1.9458e-03 | validation loss: 1.5512e-03\n",
      "Epoch: 235580 | training loss: 1.9457e-03 | validation loss: 1.5543e-03\n",
      "Epoch: 235590 | training loss: 1.9456e-03 | validation loss: 1.5523e-03\n",
      "Epoch: 235600 | training loss: 1.9456e-03 | validation loss: 1.5533e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 235610 | training loss: 1.9456e-03 | validation loss: 1.5527e-03\n",
      "Epoch: 235620 | training loss: 1.9455e-03 | validation loss: 1.5530e-03\n",
      "Epoch: 235630 | training loss: 1.9455e-03 | validation loss: 1.5528e-03\n",
      "Epoch: 235640 | training loss: 1.9455e-03 | validation loss: 1.5527e-03\n",
      "Epoch: 235650 | training loss: 1.9455e-03 | validation loss: 1.5526e-03\n",
      "Epoch: 235660 | training loss: 1.9460e-03 | validation loss: 1.5522e-03\n",
      "Epoch: 235670 | training loss: 1.9844e-03 | validation loss: 1.5702e-03\n",
      "Epoch: 235680 | training loss: 2.3500e-03 | validation loss: 1.7996e-03\n",
      "Epoch: 235690 | training loss: 2.2341e-03 | validation loss: 1.7376e-03\n",
      "Epoch: 235700 | training loss: 2.0339e-03 | validation loss: 1.5535e-03\n",
      "Epoch: 235710 | training loss: 1.9776e-03 | validation loss: 1.5920e-03\n",
      "Epoch: 235720 | training loss: 1.9559e-03 | validation loss: 1.5493e-03\n",
      "Epoch: 235730 | training loss: 1.9480e-03 | validation loss: 1.5580e-03\n",
      "Epoch: 235740 | training loss: 1.9489e-03 | validation loss: 1.5635e-03\n",
      "Epoch: 235750 | training loss: 1.9455e-03 | validation loss: 1.5543e-03\n",
      "Epoch: 235760 | training loss: 1.9455e-03 | validation loss: 1.5504e-03\n",
      "Epoch: 235770 | training loss: 1.9467e-03 | validation loss: 1.5478e-03\n",
      "Epoch: 235780 | training loss: 1.9767e-03 | validation loss: 1.5444e-03\n",
      "Epoch: 235790 | training loss: 2.8226e-03 | validation loss: 1.8540e-03\n",
      "Epoch: 235800 | training loss: 2.2946e-03 | validation loss: 1.7935e-03\n",
      "Epoch: 235810 | training loss: 2.0449e-03 | validation loss: 1.5652e-03\n",
      "Epoch: 235820 | training loss: 1.9455e-03 | validation loss: 1.5547e-03\n",
      "Epoch: 235830 | training loss: 1.9543e-03 | validation loss: 1.5680e-03\n",
      "Epoch: 235840 | training loss: 1.9519e-03 | validation loss: 1.5451e-03\n",
      "Epoch: 235850 | training loss: 1.9478e-03 | validation loss: 1.5595e-03\n",
      "Epoch: 235860 | training loss: 1.9461e-03 | validation loss: 1.5493e-03\n",
      "Epoch: 235870 | training loss: 1.9455e-03 | validation loss: 1.5549e-03\n",
      "Epoch: 235880 | training loss: 1.9452e-03 | validation loss: 1.5512e-03\n",
      "Epoch: 235890 | training loss: 1.9451e-03 | validation loss: 1.5520e-03\n",
      "Epoch: 235900 | training loss: 1.9451e-03 | validation loss: 1.5529e-03\n",
      "Epoch: 235910 | training loss: 1.9451e-03 | validation loss: 1.5528e-03\n",
      "Epoch: 235920 | training loss: 1.9451e-03 | validation loss: 1.5531e-03\n",
      "Epoch: 235930 | training loss: 1.9456e-03 | validation loss: 1.5555e-03\n",
      "Epoch: 235940 | training loss: 1.9676e-03 | validation loss: 1.5809e-03\n",
      "Epoch: 235950 | training loss: 3.0166e-03 | validation loss: 2.1919e-03\n",
      "Epoch: 235960 | training loss: 2.4876e-03 | validation loss: 1.7276e-03\n",
      "Epoch: 235970 | training loss: 1.9618e-03 | validation loss: 1.5636e-03\n",
      "Epoch: 235980 | training loss: 2.0075e-03 | validation loss: 1.6154e-03\n",
      "Epoch: 235990 | training loss: 1.9482e-03 | validation loss: 1.5537e-03\n",
      "Epoch: 236000 | training loss: 1.9524e-03 | validation loss: 1.5428e-03\n",
      "Epoch: 236010 | training loss: 1.9465e-03 | validation loss: 1.5590e-03\n",
      "Epoch: 236020 | training loss: 1.9449e-03 | validation loss: 1.5528e-03\n",
      "Epoch: 236030 | training loss: 1.9450e-03 | validation loss: 1.5501e-03\n",
      "Epoch: 236040 | training loss: 1.9450e-03 | validation loss: 1.5539e-03\n",
      "Epoch: 236050 | training loss: 1.9449e-03 | validation loss: 1.5511e-03\n",
      "Epoch: 236060 | training loss: 1.9448e-03 | validation loss: 1.5525e-03\n",
      "Epoch: 236070 | training loss: 1.9448e-03 | validation loss: 1.5518e-03\n",
      "Epoch: 236080 | training loss: 1.9448e-03 | validation loss: 1.5519e-03\n",
      "Epoch: 236090 | training loss: 1.9447e-03 | validation loss: 1.5523e-03\n",
      "Epoch: 236100 | training loss: 1.9447e-03 | validation loss: 1.5525e-03\n",
      "Epoch: 236110 | training loss: 1.9458e-03 | validation loss: 1.5552e-03\n",
      "Epoch: 236120 | training loss: 2.0256e-03 | validation loss: 1.6265e-03\n",
      "Epoch: 236130 | training loss: 1.9831e-03 | validation loss: 1.5610e-03\n",
      "Epoch: 236140 | training loss: 2.2048e-03 | validation loss: 1.6598e-03\n",
      "Epoch: 236150 | training loss: 2.0404e-03 | validation loss: 1.5615e-03\n",
      "Epoch: 236160 | training loss: 1.9933e-03 | validation loss: 1.5959e-03\n",
      "Epoch: 236170 | training loss: 1.9541e-03 | validation loss: 1.5586e-03\n",
      "Epoch: 236180 | training loss: 1.9536e-03 | validation loss: 1.5426e-03\n",
      "Epoch: 236190 | training loss: 1.9603e-03 | validation loss: 1.5449e-03\n",
      "Epoch: 236200 | training loss: 1.9906e-03 | validation loss: 1.5462e-03\n",
      "Epoch: 236210 | training loss: 2.1969e-03 | validation loss: 1.6075e-03\n",
      "Epoch: 236220 | training loss: 1.9758e-03 | validation loss: 1.5443e-03\n",
      "Epoch: 236230 | training loss: 2.0084e-03 | validation loss: 1.6146e-03\n",
      "Epoch: 236240 | training loss: 1.9532e-03 | validation loss: 1.5436e-03\n",
      "Epoch: 236250 | training loss: 1.9539e-03 | validation loss: 1.5436e-03\n",
      "Epoch: 236260 | training loss: 1.9445e-03 | validation loss: 1.5521e-03\n",
      "Epoch: 236270 | training loss: 1.9473e-03 | validation loss: 1.5595e-03\n",
      "Epoch: 236280 | training loss: 1.9728e-03 | validation loss: 1.5864e-03\n",
      "Epoch: 236290 | training loss: 2.4022e-03 | validation loss: 1.8568e-03\n",
      "Epoch: 236300 | training loss: 1.9545e-03 | validation loss: 1.5414e-03\n",
      "Epoch: 236310 | training loss: 1.9554e-03 | validation loss: 1.5705e-03\n",
      "Epoch: 236320 | training loss: 1.9524e-03 | validation loss: 1.5452e-03\n",
      "Epoch: 236330 | training loss: 1.9468e-03 | validation loss: 1.5568e-03\n",
      "Epoch: 236340 | training loss: 1.9444e-03 | validation loss: 1.5521e-03\n",
      "Epoch: 236350 | training loss: 1.9454e-03 | validation loss: 1.5476e-03\n",
      "Epoch: 236360 | training loss: 1.9454e-03 | validation loss: 1.5561e-03\n",
      "Epoch: 236370 | training loss: 1.9443e-03 | validation loss: 1.5523e-03\n",
      "Epoch: 236380 | training loss: 1.9443e-03 | validation loss: 1.5505e-03\n",
      "Epoch: 236390 | training loss: 1.9448e-03 | validation loss: 1.5488e-03\n",
      "Epoch: 236400 | training loss: 1.9533e-03 | validation loss: 1.5449e-03\n",
      "Epoch: 236410 | training loss: 2.2456e-03 | validation loss: 1.6352e-03\n",
      "Epoch: 236420 | training loss: 1.9969e-03 | validation loss: 1.5537e-03\n",
      "Epoch: 236430 | training loss: 2.0026e-03 | validation loss: 1.5493e-03\n",
      "Epoch: 236440 | training loss: 2.0233e-03 | validation loss: 1.6223e-03\n",
      "Epoch: 236450 | training loss: 1.9554e-03 | validation loss: 1.5441e-03\n",
      "Epoch: 236460 | training loss: 1.9441e-03 | validation loss: 1.5506e-03\n",
      "Epoch: 236470 | training loss: 1.9452e-03 | validation loss: 1.5557e-03\n",
      "Epoch: 236480 | training loss: 1.9448e-03 | validation loss: 1.5486e-03\n",
      "Epoch: 236490 | training loss: 1.9443e-03 | validation loss: 1.5531e-03\n",
      "Epoch: 236500 | training loss: 1.9440e-03 | validation loss: 1.5508e-03\n",
      "Epoch: 236510 | training loss: 1.9440e-03 | validation loss: 1.5504e-03\n",
      "Epoch: 236520 | training loss: 1.9440e-03 | validation loss: 1.5517e-03\n",
      "Epoch: 236530 | training loss: 1.9441e-03 | validation loss: 1.5504e-03\n",
      "Epoch: 236540 | training loss: 1.9522e-03 | validation loss: 1.5474e-03\n",
      "Epoch: 236550 | training loss: 2.4176e-03 | validation loss: 1.8062e-03\n",
      "Epoch: 236560 | training loss: 2.1535e-03 | validation loss: 1.6762e-03\n",
      "Epoch: 236570 | training loss: 1.9884e-03 | validation loss: 1.5785e-03\n",
      "Epoch: 236580 | training loss: 1.9470e-03 | validation loss: 1.5512e-03\n",
      "Epoch: 236590 | training loss: 1.9599e-03 | validation loss: 1.5636e-03\n",
      "Epoch: 236600 | training loss: 2.0117e-03 | validation loss: 1.6168e-03\n",
      "Epoch: 236610 | training loss: 2.4589e-03 | validation loss: 1.8980e-03\n",
      "Epoch: 236620 | training loss: 2.0435e-03 | validation loss: 1.5605e-03\n",
      "Epoch: 236630 | training loss: 1.9662e-03 | validation loss: 1.5793e-03\n",
      "Epoch: 236640 | training loss: 1.9456e-03 | validation loss: 1.5451e-03\n",
      "Epoch: 236650 | training loss: 1.9454e-03 | validation loss: 1.5464e-03\n",
      "Epoch: 236660 | training loss: 1.9485e-03 | validation loss: 1.5620e-03\n",
      "Epoch: 236670 | training loss: 1.9438e-03 | validation loss: 1.5493e-03\n",
      "Epoch: 236680 | training loss: 1.9452e-03 | validation loss: 1.5464e-03\n",
      "Epoch: 236690 | training loss: 1.9474e-03 | validation loss: 1.5449e-03\n",
      "Epoch: 236700 | training loss: 1.9758e-03 | validation loss: 1.5436e-03\n",
      "Epoch: 236710 | training loss: 2.4411e-03 | validation loss: 1.7034e-03\n",
      "Epoch: 236720 | training loss: 1.9709e-03 | validation loss: 1.5867e-03\n",
      "Epoch: 236730 | training loss: 1.9641e-03 | validation loss: 1.5422e-03\n",
      "Epoch: 236740 | training loss: 1.9559e-03 | validation loss: 1.5678e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 236750 | training loss: 1.9473e-03 | validation loss: 1.5467e-03\n",
      "Epoch: 236760 | training loss: 1.9437e-03 | validation loss: 1.5510e-03\n",
      "Epoch: 236770 | training loss: 1.9445e-03 | validation loss: 1.5547e-03\n",
      "Epoch: 236780 | training loss: 1.9447e-03 | validation loss: 1.5470e-03\n",
      "Epoch: 236790 | training loss: 1.9436e-03 | validation loss: 1.5497e-03\n",
      "Epoch: 236800 | training loss: 1.9436e-03 | validation loss: 1.5515e-03\n",
      "Epoch: 236810 | training loss: 1.9441e-03 | validation loss: 1.5536e-03\n",
      "Epoch: 236820 | training loss: 1.9531e-03 | validation loss: 1.5663e-03\n",
      "Epoch: 236830 | training loss: 2.2740e-03 | validation loss: 1.7754e-03\n",
      "Epoch: 236840 | training loss: 1.9651e-03 | validation loss: 1.5752e-03\n",
      "Epoch: 236850 | training loss: 2.0297e-03 | validation loss: 1.6273e-03\n",
      "Epoch: 236860 | training loss: 2.0197e-03 | validation loss: 1.5541e-03\n",
      "Epoch: 236870 | training loss: 1.9493e-03 | validation loss: 1.5616e-03\n",
      "Epoch: 236880 | training loss: 1.9442e-03 | validation loss: 1.5548e-03\n",
      "Epoch: 236890 | training loss: 1.9454e-03 | validation loss: 1.5459e-03\n",
      "Epoch: 236900 | training loss: 1.9445e-03 | validation loss: 1.5549e-03\n",
      "Epoch: 236910 | training loss: 1.9437e-03 | validation loss: 1.5484e-03\n",
      "Epoch: 236920 | training loss: 1.9434e-03 | validation loss: 1.5515e-03\n",
      "Epoch: 236930 | training loss: 1.9433e-03 | validation loss: 1.5506e-03\n",
      "Epoch: 236940 | training loss: 1.9433e-03 | validation loss: 1.5495e-03\n",
      "Epoch: 236950 | training loss: 1.9433e-03 | validation loss: 1.5500e-03\n",
      "Epoch: 236960 | training loss: 1.9434e-03 | validation loss: 1.5492e-03\n",
      "Epoch: 236970 | training loss: 1.9641e-03 | validation loss: 1.5505e-03\n",
      "Epoch: 236980 | training loss: 2.4881e-03 | validation loss: 1.8416e-03\n",
      "Epoch: 236990 | training loss: 2.0666e-03 | validation loss: 1.5697e-03\n",
      "Epoch: 237000 | training loss: 1.9505e-03 | validation loss: 1.5481e-03\n",
      "Epoch: 237010 | training loss: 1.9630e-03 | validation loss: 1.5821e-03\n",
      "Epoch: 237020 | training loss: 1.9732e-03 | validation loss: 1.5928e-03\n",
      "Epoch: 237030 | training loss: 2.0286e-03 | validation loss: 1.6336e-03\n",
      "Epoch: 237040 | training loss: 2.1985e-03 | validation loss: 1.7396e-03\n",
      "Epoch: 237050 | training loss: 1.9450e-03 | validation loss: 1.5441e-03\n",
      "Epoch: 237060 | training loss: 1.9787e-03 | validation loss: 1.5415e-03\n",
      "Epoch: 237070 | training loss: 1.9471e-03 | validation loss: 1.5594e-03\n",
      "Epoch: 237080 | training loss: 1.9576e-03 | validation loss: 1.5726e-03\n",
      "Epoch: 237090 | training loss: 1.9638e-03 | validation loss: 1.5786e-03\n",
      "Epoch: 237100 | training loss: 2.0326e-03 | validation loss: 1.6314e-03\n",
      "Epoch: 237110 | training loss: 2.2568e-03 | validation loss: 1.7720e-03\n",
      "Epoch: 237120 | training loss: 1.9590e-03 | validation loss: 1.5416e-03\n",
      "Epoch: 237130 | training loss: 1.9593e-03 | validation loss: 1.5413e-03\n",
      "Epoch: 237140 | training loss: 1.9622e-03 | validation loss: 1.5759e-03\n",
      "Epoch: 237150 | training loss: 1.9455e-03 | validation loss: 1.5573e-03\n",
      "Epoch: 237160 | training loss: 1.9437e-03 | validation loss: 1.5465e-03\n",
      "Epoch: 237170 | training loss: 1.9501e-03 | validation loss: 1.5427e-03\n",
      "Epoch: 237180 | training loss: 2.0346e-03 | validation loss: 1.5567e-03\n",
      "Epoch: 237190 | training loss: 2.5760e-03 | validation loss: 1.7603e-03\n",
      "Epoch: 237200 | training loss: 2.1477e-03 | validation loss: 1.7002e-03\n",
      "Epoch: 237210 | training loss: 2.0199e-03 | validation loss: 1.5568e-03\n",
      "Epoch: 237220 | training loss: 1.9721e-03 | validation loss: 1.5818e-03\n",
      "Epoch: 237230 | training loss: 1.9532e-03 | validation loss: 1.5433e-03\n",
      "Epoch: 237240 | training loss: 1.9446e-03 | validation loss: 1.5555e-03\n",
      "Epoch: 237250 | training loss: 1.9429e-03 | validation loss: 1.5510e-03\n",
      "Epoch: 237260 | training loss: 1.9437e-03 | validation loss: 1.5467e-03\n",
      "Epoch: 237270 | training loss: 1.9430e-03 | validation loss: 1.5477e-03\n",
      "Epoch: 237280 | training loss: 1.9429e-03 | validation loss: 1.5480e-03\n",
      "Epoch: 237290 | training loss: 1.9442e-03 | validation loss: 1.5459e-03\n",
      "Epoch: 237300 | training loss: 1.9858e-03 | validation loss: 1.5469e-03\n",
      "Epoch: 237310 | training loss: 3.0692e-03 | validation loss: 1.9758e-03\n",
      "Epoch: 237320 | training loss: 2.3547e-03 | validation loss: 1.8215e-03\n",
      "Epoch: 237330 | training loss: 1.9566e-03 | validation loss: 1.5446e-03\n",
      "Epoch: 237340 | training loss: 1.9736e-03 | validation loss: 1.5415e-03\n",
      "Epoch: 237350 | training loss: 1.9597e-03 | validation loss: 1.5739e-03\n",
      "Epoch: 237360 | training loss: 1.9436e-03 | validation loss: 1.5460e-03\n",
      "Epoch: 237370 | training loss: 1.9427e-03 | validation loss: 1.5485e-03\n",
      "Epoch: 237380 | training loss: 1.9428e-03 | validation loss: 1.5513e-03\n",
      "Epoch: 237390 | training loss: 1.9426e-03 | validation loss: 1.5484e-03\n",
      "Epoch: 237400 | training loss: 1.9425e-03 | validation loss: 1.5500e-03\n",
      "Epoch: 237410 | training loss: 1.9425e-03 | validation loss: 1.5498e-03\n",
      "Epoch: 237420 | training loss: 1.9425e-03 | validation loss: 1.5488e-03\n",
      "Epoch: 237430 | training loss: 1.9424e-03 | validation loss: 1.5494e-03\n",
      "Epoch: 237440 | training loss: 1.9425e-03 | validation loss: 1.5504e-03\n",
      "Epoch: 237450 | training loss: 1.9472e-03 | validation loss: 1.5606e-03\n",
      "Epoch: 237460 | training loss: 2.4586e-03 | validation loss: 1.9861e-03\n",
      "Epoch: 237470 | training loss: 2.2046e-03 | validation loss: 1.7085e-03\n",
      "Epoch: 237480 | training loss: 2.0412e-03 | validation loss: 1.6036e-03\n",
      "Epoch: 237490 | training loss: 1.9650e-03 | validation loss: 1.5446e-03\n",
      "Epoch: 237500 | training loss: 1.9586e-03 | validation loss: 1.5383e-03\n",
      "Epoch: 237510 | training loss: 2.0014e-03 | validation loss: 1.5474e-03\n",
      "Epoch: 237520 | training loss: 2.2166e-03 | validation loss: 1.6153e-03\n",
      "Epoch: 237530 | training loss: 1.9471e-03 | validation loss: 1.5449e-03\n",
      "Epoch: 237540 | training loss: 1.9909e-03 | validation loss: 1.6013e-03\n",
      "Epoch: 237550 | training loss: 1.9512e-03 | validation loss: 1.5407e-03\n",
      "Epoch: 237560 | training loss: 1.9525e-03 | validation loss: 1.5403e-03\n",
      "Epoch: 237570 | training loss: 1.9431e-03 | validation loss: 1.5453e-03\n",
      "Epoch: 237580 | training loss: 1.9423e-03 | validation loss: 1.5477e-03\n",
      "Epoch: 237590 | training loss: 1.9427e-03 | validation loss: 1.5461e-03\n",
      "Epoch: 237600 | training loss: 1.9621e-03 | validation loss: 1.5403e-03\n",
      "Epoch: 237610 | training loss: 2.9230e-03 | validation loss: 1.8966e-03\n",
      "Epoch: 237620 | training loss: 2.4545e-03 | validation loss: 1.8822e-03\n",
      "Epoch: 237630 | training loss: 1.9754e-03 | validation loss: 1.5518e-03\n",
      "Epoch: 237640 | training loss: 1.9982e-03 | validation loss: 1.5524e-03\n",
      "Epoch: 237650 | training loss: 1.9495e-03 | validation loss: 1.5624e-03\n",
      "Epoch: 237660 | training loss: 1.9457e-03 | validation loss: 1.5561e-03\n",
      "Epoch: 237670 | training loss: 1.9453e-03 | validation loss: 1.5430e-03\n",
      "Epoch: 237680 | training loss: 1.9424e-03 | validation loss: 1.5515e-03\n",
      "Epoch: 237690 | training loss: 1.9420e-03 | validation loss: 1.5489e-03\n",
      "Epoch: 237700 | training loss: 1.9420e-03 | validation loss: 1.5483e-03\n",
      "Epoch: 237710 | training loss: 1.9420e-03 | validation loss: 1.5488e-03\n",
      "Epoch: 237720 | training loss: 1.9419e-03 | validation loss: 1.5489e-03\n",
      "Epoch: 237730 | training loss: 1.9419e-03 | validation loss: 1.5483e-03\n",
      "Epoch: 237740 | training loss: 1.9419e-03 | validation loss: 1.5489e-03\n",
      "Epoch: 237750 | training loss: 1.9419e-03 | validation loss: 1.5487e-03\n",
      "Epoch: 237760 | training loss: 1.9419e-03 | validation loss: 1.5486e-03\n",
      "Epoch: 237770 | training loss: 1.9419e-03 | validation loss: 1.5486e-03\n",
      "Epoch: 237780 | training loss: 1.9418e-03 | validation loss: 1.5486e-03\n",
      "Epoch: 237790 | training loss: 1.9418e-03 | validation loss: 1.5487e-03\n",
      "Epoch: 237800 | training loss: 1.9421e-03 | validation loss: 1.5504e-03\n",
      "Epoch: 237810 | training loss: 1.9927e-03 | validation loss: 1.5989e-03\n",
      "Epoch: 237820 | training loss: 4.2030e-03 | validation loss: 2.8133e-03\n",
      "Epoch: 237830 | training loss: 2.6099e-03 | validation loss: 1.9732e-03\n",
      "Epoch: 237840 | training loss: 2.1838e-03 | validation loss: 1.7303e-03\n",
      "Epoch: 237850 | training loss: 2.0339e-03 | validation loss: 1.6324e-03\n",
      "Epoch: 237860 | training loss: 1.9745e-03 | validation loss: 1.5874e-03\n",
      "Epoch: 237870 | training loss: 1.9534e-03 | validation loss: 1.5674e-03\n",
      "Epoch: 237880 | training loss: 1.9459e-03 | validation loss: 1.5588e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 237890 | training loss: 1.9431e-03 | validation loss: 1.5542e-03\n",
      "Epoch: 237900 | training loss: 1.9420e-03 | validation loss: 1.5509e-03\n",
      "Epoch: 237910 | training loss: 1.9416e-03 | validation loss: 1.5490e-03\n",
      "Epoch: 237920 | training loss: 1.9416e-03 | validation loss: 1.5480e-03\n",
      "Epoch: 237930 | training loss: 1.9416e-03 | validation loss: 1.5478e-03\n",
      "Epoch: 237940 | training loss: 1.9416e-03 | validation loss: 1.5482e-03\n",
      "Epoch: 237950 | training loss: 1.9416e-03 | validation loss: 1.5485e-03\n",
      "Epoch: 237960 | training loss: 1.9415e-03 | validation loss: 1.5485e-03\n",
      "Epoch: 237970 | training loss: 1.9415e-03 | validation loss: 1.5483e-03\n",
      "Epoch: 237980 | training loss: 1.9415e-03 | validation loss: 1.5483e-03\n",
      "Epoch: 237990 | training loss: 1.9415e-03 | validation loss: 1.5483e-03\n",
      "Epoch: 238000 | training loss: 1.9415e-03 | validation loss: 1.5482e-03\n",
      "Epoch: 238010 | training loss: 1.9415e-03 | validation loss: 1.5482e-03\n",
      "Epoch: 238020 | training loss: 1.9414e-03 | validation loss: 1.5481e-03\n",
      "Epoch: 238030 | training loss: 1.9416e-03 | validation loss: 1.5471e-03\n",
      "Epoch: 238040 | training loss: 1.9690e-03 | validation loss: 1.5507e-03\n",
      "Epoch: 238050 | training loss: 2.2537e-03 | validation loss: 1.7021e-03\n",
      "Epoch: 238060 | training loss: 2.1410e-03 | validation loss: 1.6604e-03\n",
      "Epoch: 238070 | training loss: 2.0104e-03 | validation loss: 1.5694e-03\n",
      "Epoch: 238080 | training loss: 1.9649e-03 | validation loss: 1.5551e-03\n",
      "Epoch: 238090 | training loss: 1.9487e-03 | validation loss: 1.5457e-03\n",
      "Epoch: 238100 | training loss: 1.9432e-03 | validation loss: 1.5447e-03\n",
      "Epoch: 238110 | training loss: 1.9415e-03 | validation loss: 1.5475e-03\n",
      "Epoch: 238120 | training loss: 1.9414e-03 | validation loss: 1.5493e-03\n",
      "Epoch: 238130 | training loss: 1.9417e-03 | validation loss: 1.5512e-03\n",
      "Epoch: 238140 | training loss: 1.9471e-03 | validation loss: 1.5612e-03\n",
      "Epoch: 238150 | training loss: 2.1622e-03 | validation loss: 1.7181e-03\n",
      "Epoch: 238160 | training loss: 2.0985e-03 | validation loss: 1.6698e-03\n",
      "Epoch: 238170 | training loss: 1.9743e-03 | validation loss: 1.5862e-03\n",
      "Epoch: 238180 | training loss: 2.0238e-03 | validation loss: 1.5523e-03\n",
      "Epoch: 238190 | training loss: 1.9524e-03 | validation loss: 1.5667e-03\n",
      "Epoch: 238200 | training loss: 1.9416e-03 | validation loss: 1.5508e-03\n",
      "Epoch: 238210 | training loss: 1.9432e-03 | validation loss: 1.5430e-03\n",
      "Epoch: 238220 | training loss: 1.9424e-03 | validation loss: 1.5523e-03\n",
      "Epoch: 238230 | training loss: 1.9416e-03 | validation loss: 1.5449e-03\n",
      "Epoch: 238240 | training loss: 1.9412e-03 | validation loss: 1.5491e-03\n",
      "Epoch: 238250 | training loss: 1.9411e-03 | validation loss: 1.5473e-03\n",
      "Epoch: 238260 | training loss: 1.9411e-03 | validation loss: 1.5468e-03\n",
      "Epoch: 238270 | training loss: 1.9410e-03 | validation loss: 1.5478e-03\n",
      "Epoch: 238280 | training loss: 1.9410e-03 | validation loss: 1.5480e-03\n",
      "Epoch: 238290 | training loss: 1.9410e-03 | validation loss: 1.5484e-03\n",
      "Epoch: 238300 | training loss: 1.9417e-03 | validation loss: 1.5511e-03\n",
      "Epoch: 238310 | training loss: 1.9675e-03 | validation loss: 1.5802e-03\n",
      "Epoch: 238320 | training loss: 3.0887e-03 | validation loss: 2.2320e-03\n",
      "Epoch: 238330 | training loss: 2.4577e-03 | validation loss: 1.7132e-03\n",
      "Epoch: 238340 | training loss: 1.9565e-03 | validation loss: 1.5507e-03\n",
      "Epoch: 238350 | training loss: 1.9960e-03 | validation loss: 1.6027e-03\n",
      "Epoch: 238360 | training loss: 1.9475e-03 | validation loss: 1.5542e-03\n",
      "Epoch: 238370 | training loss: 1.9459e-03 | validation loss: 1.5391e-03\n",
      "Epoch: 238380 | training loss: 1.9424e-03 | validation loss: 1.5516e-03\n",
      "Epoch: 238390 | training loss: 1.9414e-03 | validation loss: 1.5493e-03\n",
      "Epoch: 238400 | training loss: 1.9411e-03 | validation loss: 1.5454e-03\n",
      "Epoch: 238410 | training loss: 1.9409e-03 | validation loss: 1.5490e-03\n",
      "Epoch: 238420 | training loss: 1.9408e-03 | validation loss: 1.5464e-03\n",
      "Epoch: 238430 | training loss: 1.9408e-03 | validation loss: 1.5479e-03\n",
      "Epoch: 238440 | training loss: 1.9407e-03 | validation loss: 1.5470e-03\n",
      "Epoch: 238450 | training loss: 1.9407e-03 | validation loss: 1.5471e-03\n",
      "Epoch: 238460 | training loss: 1.9407e-03 | validation loss: 1.5474e-03\n",
      "Epoch: 238470 | training loss: 1.9407e-03 | validation loss: 1.5473e-03\n",
      "Epoch: 238480 | training loss: 1.9407e-03 | validation loss: 1.5473e-03\n",
      "Epoch: 238490 | training loss: 1.9412e-03 | validation loss: 1.5479e-03\n",
      "Epoch: 238500 | training loss: 1.9890e-03 | validation loss: 1.5777e-03\n",
      "Epoch: 238510 | training loss: 2.7969e-03 | validation loss: 2.0520e-03\n",
      "Epoch: 238520 | training loss: 2.3548e-03 | validation loss: 1.7118e-03\n",
      "Epoch: 238530 | training loss: 2.1124e-03 | validation loss: 1.6543e-03\n",
      "Epoch: 238540 | training loss: 1.9796e-03 | validation loss: 1.5489e-03\n",
      "Epoch: 238550 | training loss: 1.9543e-03 | validation loss: 1.5416e-03\n",
      "Epoch: 238560 | training loss: 1.9474e-03 | validation loss: 1.5519e-03\n",
      "Epoch: 238570 | training loss: 1.9434e-03 | validation loss: 1.5407e-03\n",
      "Epoch: 238580 | training loss: 1.9416e-03 | validation loss: 1.5484e-03\n",
      "Epoch: 238590 | training loss: 1.9408e-03 | validation loss: 1.5444e-03\n",
      "Epoch: 238600 | training loss: 1.9405e-03 | validation loss: 1.5467e-03\n",
      "Epoch: 238610 | training loss: 1.9405e-03 | validation loss: 1.5475e-03\n",
      "Epoch: 238620 | training loss: 1.9404e-03 | validation loss: 1.5466e-03\n",
      "Epoch: 238630 | training loss: 1.9404e-03 | validation loss: 1.5462e-03\n",
      "Epoch: 238640 | training loss: 1.9405e-03 | validation loss: 1.5456e-03\n",
      "Epoch: 238650 | training loss: 1.9415e-03 | validation loss: 1.5428e-03\n",
      "Epoch: 238660 | training loss: 1.9848e-03 | validation loss: 1.5410e-03\n",
      "Epoch: 238670 | training loss: 3.2524e-03 | validation loss: 2.0357e-03\n",
      "Epoch: 238680 | training loss: 2.2586e-03 | validation loss: 1.7621e-03\n",
      "Epoch: 238690 | training loss: 1.9645e-03 | validation loss: 1.5763e-03\n",
      "Epoch: 238700 | training loss: 1.9901e-03 | validation loss: 1.5466e-03\n",
      "Epoch: 238710 | training loss: 1.9407e-03 | validation loss: 1.5473e-03\n",
      "Epoch: 238720 | training loss: 1.9468e-03 | validation loss: 1.5597e-03\n",
      "Epoch: 238730 | training loss: 1.9421e-03 | validation loss: 1.5421e-03\n",
      "Epoch: 238740 | training loss: 1.9403e-03 | validation loss: 1.5474e-03\n",
      "Epoch: 238750 | training loss: 1.9402e-03 | validation loss: 1.5470e-03\n",
      "Epoch: 238760 | training loss: 1.9402e-03 | validation loss: 1.5464e-03\n",
      "Epoch: 238770 | training loss: 1.9402e-03 | validation loss: 1.5465e-03\n",
      "Epoch: 238780 | training loss: 1.9402e-03 | validation loss: 1.5468e-03\n",
      "Epoch: 238790 | training loss: 1.9402e-03 | validation loss: 1.5462e-03\n",
      "Epoch: 238800 | training loss: 1.9401e-03 | validation loss: 1.5465e-03\n",
      "Epoch: 238810 | training loss: 1.9401e-03 | validation loss: 1.5467e-03\n",
      "Epoch: 238820 | training loss: 1.9401e-03 | validation loss: 1.5467e-03\n",
      "Epoch: 238830 | training loss: 1.9401e-03 | validation loss: 1.5473e-03\n",
      "Epoch: 238840 | training loss: 1.9417e-03 | validation loss: 1.5519e-03\n",
      "Epoch: 238850 | training loss: 2.0481e-03 | validation loss: 1.6365e-03\n",
      "Epoch: 238860 | training loss: 2.9391e-03 | validation loss: 2.1423e-03\n",
      "Epoch: 238870 | training loss: 2.0080e-03 | validation loss: 1.6156e-03\n",
      "Epoch: 238880 | training loss: 2.0190e-03 | validation loss: 1.5528e-03\n",
      "Epoch: 238890 | training loss: 1.9827e-03 | validation loss: 1.5380e-03\n",
      "Epoch: 238900 | training loss: 1.9407e-03 | validation loss: 1.5497e-03\n",
      "Epoch: 238910 | training loss: 1.9468e-03 | validation loss: 1.5616e-03\n",
      "Epoch: 238920 | training loss: 1.9401e-03 | validation loss: 1.5444e-03\n",
      "Epoch: 238930 | training loss: 1.9406e-03 | validation loss: 1.5438e-03\n",
      "Epoch: 238940 | training loss: 1.9402e-03 | validation loss: 1.5485e-03\n",
      "Epoch: 238950 | training loss: 1.9399e-03 | validation loss: 1.5460e-03\n",
      "Epoch: 238960 | training loss: 1.9399e-03 | validation loss: 1.5461e-03\n",
      "Epoch: 238970 | training loss: 1.9398e-03 | validation loss: 1.5465e-03\n",
      "Epoch: 238980 | training loss: 1.9398e-03 | validation loss: 1.5462e-03\n",
      "Epoch: 238990 | training loss: 1.9398e-03 | validation loss: 1.5462e-03\n",
      "Epoch: 239000 | training loss: 1.9398e-03 | validation loss: 1.5464e-03\n",
      "Epoch: 239010 | training loss: 1.9398e-03 | validation loss: 1.5465e-03\n",
      "Epoch: 239020 | training loss: 1.9405e-03 | validation loss: 1.5492e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 239030 | training loss: 2.0131e-03 | validation loss: 1.6223e-03\n",
      "Epoch: 239040 | training loss: 1.9641e-03 | validation loss: 1.5835e-03\n",
      "Epoch: 239050 | training loss: 2.0616e-03 | validation loss: 1.6631e-03\n",
      "Epoch: 239060 | training loss: 1.9491e-03 | validation loss: 1.5636e-03\n",
      "Epoch: 239070 | training loss: 1.9468e-03 | validation loss: 1.5509e-03\n",
      "Epoch: 239080 | training loss: 1.9689e-03 | validation loss: 1.5759e-03\n",
      "Epoch: 239090 | training loss: 2.4302e-03 | validation loss: 1.8768e-03\n",
      "Epoch: 239100 | training loss: 1.9759e-03 | validation loss: 1.5389e-03\n",
      "Epoch: 239110 | training loss: 1.9915e-03 | validation loss: 1.5998e-03\n",
      "Epoch: 239120 | training loss: 1.9759e-03 | validation loss: 1.5393e-03\n",
      "Epoch: 239130 | training loss: 1.9559e-03 | validation loss: 1.5693e-03\n",
      "Epoch: 239140 | training loss: 1.9456e-03 | validation loss: 1.5388e-03\n",
      "Epoch: 239150 | training loss: 1.9411e-03 | validation loss: 1.5514e-03\n",
      "Epoch: 239160 | training loss: 1.9395e-03 | validation loss: 1.5452e-03\n",
      "Epoch: 239170 | training loss: 1.9399e-03 | validation loss: 1.5434e-03\n",
      "Epoch: 239180 | training loss: 1.9395e-03 | validation loss: 1.5464e-03\n",
      "Epoch: 239190 | training loss: 1.9397e-03 | validation loss: 1.5476e-03\n",
      "Epoch: 239200 | training loss: 1.9404e-03 | validation loss: 1.5499e-03\n",
      "Epoch: 239210 | training loss: 1.9548e-03 | validation loss: 1.5684e-03\n",
      "Epoch: 239220 | training loss: 2.4196e-03 | validation loss: 1.8632e-03\n",
      "Epoch: 239230 | training loss: 1.9734e-03 | validation loss: 1.5355e-03\n",
      "Epoch: 239240 | training loss: 2.0655e-03 | validation loss: 1.6490e-03\n",
      "Epoch: 239250 | training loss: 1.9963e-03 | validation loss: 1.5516e-03\n",
      "Epoch: 239260 | training loss: 1.9459e-03 | validation loss: 1.5524e-03\n",
      "Epoch: 239270 | training loss: 1.9395e-03 | validation loss: 1.5476e-03\n",
      "Epoch: 239280 | training loss: 1.9396e-03 | validation loss: 1.5440e-03\n",
      "Epoch: 239290 | training loss: 1.9394e-03 | validation loss: 1.5462e-03\n",
      "Epoch: 239300 | training loss: 1.9393e-03 | validation loss: 1.5460e-03\n",
      "Epoch: 239310 | training loss: 1.9394e-03 | validation loss: 1.5444e-03\n",
      "Epoch: 239320 | training loss: 1.9393e-03 | validation loss: 1.5465e-03\n",
      "Epoch: 239330 | training loss: 1.9392e-03 | validation loss: 1.5459e-03\n",
      "Epoch: 239340 | training loss: 1.9392e-03 | validation loss: 1.5452e-03\n",
      "Epoch: 239350 | training loss: 1.9392e-03 | validation loss: 1.5448e-03\n",
      "Epoch: 239360 | training loss: 1.9396e-03 | validation loss: 1.5434e-03\n",
      "Epoch: 239370 | training loss: 1.9580e-03 | validation loss: 1.5405e-03\n",
      "Epoch: 239380 | training loss: 3.0218e-03 | validation loss: 1.9647e-03\n",
      "Epoch: 239390 | training loss: 2.5451e-03 | validation loss: 1.9188e-03\n",
      "Epoch: 239400 | training loss: 1.9469e-03 | validation loss: 1.5376e-03\n",
      "Epoch: 239410 | training loss: 2.0067e-03 | validation loss: 1.5657e-03\n",
      "Epoch: 239420 | training loss: 1.9464e-03 | validation loss: 1.5467e-03\n",
      "Epoch: 239430 | training loss: 1.9457e-03 | validation loss: 1.5517e-03\n",
      "Epoch: 239440 | training loss: 1.9400e-03 | validation loss: 1.5486e-03\n",
      "Epoch: 239450 | training loss: 1.9403e-03 | validation loss: 1.5436e-03\n",
      "Epoch: 239460 | training loss: 1.9391e-03 | validation loss: 1.5462e-03\n",
      "Epoch: 239470 | training loss: 1.9390e-03 | validation loss: 1.5457e-03\n",
      "Epoch: 239480 | training loss: 1.9390e-03 | validation loss: 1.5448e-03\n",
      "Epoch: 239490 | training loss: 1.9390e-03 | validation loss: 1.5456e-03\n",
      "Epoch: 239500 | training loss: 1.9390e-03 | validation loss: 1.5452e-03\n",
      "Epoch: 239510 | training loss: 1.9389e-03 | validation loss: 1.5456e-03\n",
      "Epoch: 239520 | training loss: 1.9390e-03 | validation loss: 1.5464e-03\n",
      "Epoch: 239530 | training loss: 1.9418e-03 | validation loss: 1.5552e-03\n",
      "Epoch: 239540 | training loss: 2.1911e-03 | validation loss: 1.7856e-03\n",
      "Epoch: 239550 | training loss: 2.1508e-03 | validation loss: 1.6703e-03\n",
      "Epoch: 239560 | training loss: 1.9599e-03 | validation loss: 1.5550e-03\n",
      "Epoch: 239570 | training loss: 1.9720e-03 | validation loss: 1.5883e-03\n",
      "Epoch: 239580 | training loss: 1.9529e-03 | validation loss: 1.5594e-03\n",
      "Epoch: 239590 | training loss: 1.9410e-03 | validation loss: 1.5528e-03\n",
      "Epoch: 239600 | training loss: 1.9408e-03 | validation loss: 1.5390e-03\n",
      "Epoch: 239610 | training loss: 1.9395e-03 | validation loss: 1.5445e-03\n",
      "Epoch: 239620 | training loss: 1.9391e-03 | validation loss: 1.5473e-03\n",
      "Epoch: 239630 | training loss: 1.9389e-03 | validation loss: 1.5468e-03\n",
      "Epoch: 239640 | training loss: 1.9389e-03 | validation loss: 1.5461e-03\n",
      "Epoch: 239650 | training loss: 1.9411e-03 | validation loss: 1.5518e-03\n",
      "Epoch: 239660 | training loss: 2.0352e-03 | validation loss: 1.6303e-03\n",
      "Epoch: 239670 | training loss: 2.9326e-03 | validation loss: 2.1440e-03\n",
      "Epoch: 239680 | training loss: 1.9808e-03 | validation loss: 1.5517e-03\n",
      "Epoch: 239690 | training loss: 2.0232e-03 | validation loss: 1.5531e-03\n",
      "Epoch: 239700 | training loss: 1.9626e-03 | validation loss: 1.5699e-03\n",
      "Epoch: 239710 | training loss: 1.9407e-03 | validation loss: 1.5490e-03\n",
      "Epoch: 239720 | training loss: 1.9441e-03 | validation loss: 1.5399e-03\n",
      "Epoch: 239730 | training loss: 1.9403e-03 | validation loss: 1.5507e-03\n",
      "Epoch: 239740 | training loss: 1.9388e-03 | validation loss: 1.5425e-03\n",
      "Epoch: 239750 | training loss: 1.9386e-03 | validation loss: 1.5455e-03\n",
      "Epoch: 239760 | training loss: 1.9385e-03 | validation loss: 1.5441e-03\n",
      "Epoch: 239770 | training loss: 1.9385e-03 | validation loss: 1.5451e-03\n",
      "Epoch: 239780 | training loss: 1.9385e-03 | validation loss: 1.5441e-03\n",
      "Epoch: 239790 | training loss: 1.9385e-03 | validation loss: 1.5444e-03\n",
      "Epoch: 239800 | training loss: 1.9384e-03 | validation loss: 1.5448e-03\n",
      "Epoch: 239810 | training loss: 1.9384e-03 | validation loss: 1.5448e-03\n",
      "Epoch: 239820 | training loss: 1.9385e-03 | validation loss: 1.5453e-03\n",
      "Epoch: 239830 | training loss: 1.9397e-03 | validation loss: 1.5492e-03\n",
      "Epoch: 239840 | training loss: 2.0132e-03 | validation loss: 1.6112e-03\n",
      "Epoch: 239850 | training loss: 3.3244e-03 | validation loss: 2.3470e-03\n",
      "Epoch: 239860 | training loss: 1.9471e-03 | validation loss: 1.5407e-03\n",
      "Epoch: 239870 | training loss: 2.0929e-03 | validation loss: 1.5727e-03\n",
      "Epoch: 239880 | training loss: 1.9439e-03 | validation loss: 1.5368e-03\n",
      "Epoch: 239890 | training loss: 1.9547e-03 | validation loss: 1.5678e-03\n",
      "Epoch: 239900 | training loss: 1.9394e-03 | validation loss: 1.5488e-03\n",
      "Epoch: 239910 | training loss: 1.9409e-03 | validation loss: 1.5399e-03\n",
      "Epoch: 239920 | training loss: 1.9383e-03 | validation loss: 1.5456e-03\n",
      "Epoch: 239930 | training loss: 1.9383e-03 | validation loss: 1.5458e-03\n",
      "Epoch: 239940 | training loss: 1.9383e-03 | validation loss: 1.5432e-03\n",
      "Epoch: 239950 | training loss: 1.9382e-03 | validation loss: 1.5452e-03\n",
      "Epoch: 239960 | training loss: 1.9382e-03 | validation loss: 1.5438e-03\n",
      "Epoch: 239970 | training loss: 1.9382e-03 | validation loss: 1.5446e-03\n",
      "Epoch: 239980 | training loss: 1.9381e-03 | validation loss: 1.5442e-03\n",
      "Epoch: 239990 | training loss: 1.9381e-03 | validation loss: 1.5444e-03\n",
      "Epoch: 240000 | training loss: 1.9383e-03 | validation loss: 1.5461e-03\n",
      "Epoch: 240010 | training loss: 1.9714e-03 | validation loss: 1.5878e-03\n",
      "Epoch: 240020 | training loss: 2.2690e-03 | validation loss: 1.8360e-03\n",
      "Epoch: 240030 | training loss: 2.1065e-03 | validation loss: 1.6909e-03\n",
      "Epoch: 240040 | training loss: 1.9850e-03 | validation loss: 1.5733e-03\n",
      "Epoch: 240050 | training loss: 1.9429e-03 | validation loss: 1.5534e-03\n",
      "Epoch: 240060 | training loss: 1.9410e-03 | validation loss: 1.5522e-03\n",
      "Epoch: 240070 | training loss: 1.9467e-03 | validation loss: 1.5574e-03\n",
      "Epoch: 240080 | training loss: 2.0028e-03 | validation loss: 1.6060e-03\n",
      "Epoch: 240090 | training loss: 2.4489e-03 | validation loss: 1.8830e-03\n",
      "Epoch: 240100 | training loss: 2.0306e-03 | validation loss: 1.5493e-03\n",
      "Epoch: 240110 | training loss: 1.9589e-03 | validation loss: 1.5725e-03\n",
      "Epoch: 240120 | training loss: 1.9396e-03 | validation loss: 1.5393e-03\n",
      "Epoch: 240130 | training loss: 1.9393e-03 | validation loss: 1.5397e-03\n",
      "Epoch: 240140 | training loss: 1.9424e-03 | validation loss: 1.5544e-03\n",
      "Epoch: 240150 | training loss: 1.9381e-03 | validation loss: 1.5418e-03\n",
      "Epoch: 240160 | training loss: 1.9394e-03 | validation loss: 1.5394e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 240170 | training loss: 1.9407e-03 | validation loss: 1.5382e-03\n",
      "Epoch: 240180 | training loss: 1.9604e-03 | validation loss: 1.5355e-03\n",
      "Epoch: 240190 | training loss: 2.3229e-03 | validation loss: 1.6514e-03\n",
      "Epoch: 240200 | training loss: 1.9404e-03 | validation loss: 1.5434e-03\n",
      "Epoch: 240210 | training loss: 1.9390e-03 | validation loss: 1.5390e-03\n",
      "Epoch: 240220 | training loss: 1.9416e-03 | validation loss: 1.5511e-03\n",
      "Epoch: 240230 | training loss: 1.9391e-03 | validation loss: 1.5401e-03\n",
      "Epoch: 240240 | training loss: 1.9377e-03 | validation loss: 1.5438e-03\n",
      "Epoch: 240250 | training loss: 1.9388e-03 | validation loss: 1.5476e-03\n",
      "Epoch: 240260 | training loss: 1.9389e-03 | validation loss: 1.5401e-03\n",
      "Epoch: 240270 | training loss: 1.9377e-03 | validation loss: 1.5431e-03\n",
      "Epoch: 240280 | training loss: 1.9379e-03 | validation loss: 1.5455e-03\n",
      "Epoch: 240290 | training loss: 1.9386e-03 | validation loss: 1.5476e-03\n",
      "Epoch: 240300 | training loss: 1.9486e-03 | validation loss: 1.5610e-03\n",
      "Epoch: 240310 | training loss: 2.2315e-03 | validation loss: 1.7485e-03\n",
      "Epoch: 240320 | training loss: 2.0104e-03 | validation loss: 1.6059e-03\n",
      "Epoch: 240330 | training loss: 1.9507e-03 | validation loss: 1.5663e-03\n",
      "Epoch: 240340 | training loss: 1.9868e-03 | validation loss: 1.5394e-03\n",
      "Epoch: 240350 | training loss: 1.9627e-03 | validation loss: 1.5732e-03\n",
      "Epoch: 240360 | training loss: 1.9448e-03 | validation loss: 1.5386e-03\n",
      "Epoch: 240370 | training loss: 1.9398e-03 | validation loss: 1.5489e-03\n",
      "Epoch: 240380 | training loss: 1.9384e-03 | validation loss: 1.5408e-03\n",
      "Epoch: 240390 | training loss: 1.9379e-03 | validation loss: 1.5465e-03\n",
      "Epoch: 240400 | training loss: 1.9376e-03 | validation loss: 1.5416e-03\n",
      "Epoch: 240410 | training loss: 1.9374e-03 | validation loss: 1.5430e-03\n",
      "Epoch: 240420 | training loss: 1.9374e-03 | validation loss: 1.5444e-03\n",
      "Epoch: 240430 | training loss: 1.9375e-03 | validation loss: 1.5452e-03\n",
      "Epoch: 240440 | training loss: 1.9388e-03 | validation loss: 1.5491e-03\n",
      "Epoch: 240450 | training loss: 1.9799e-03 | validation loss: 1.5971e-03\n",
      "Epoch: 240460 | training loss: 2.2027e-03 | validation loss: 1.7859e-03\n",
      "Epoch: 240470 | training loss: 2.2588e-03 | validation loss: 1.7619e-03\n",
      "Epoch: 240480 | training loss: 2.0101e-03 | validation loss: 1.5797e-03\n",
      "Epoch: 240490 | training loss: 1.9573e-03 | validation loss: 1.5337e-03\n",
      "Epoch: 240500 | training loss: 1.9632e-03 | validation loss: 1.5816e-03\n",
      "Epoch: 240510 | training loss: 1.9508e-03 | validation loss: 1.5394e-03\n",
      "Epoch: 240520 | training loss: 1.9401e-03 | validation loss: 1.5474e-03\n",
      "Epoch: 240530 | training loss: 1.9386e-03 | validation loss: 1.5470e-03\n",
      "Epoch: 240540 | training loss: 1.9375e-03 | validation loss: 1.5429e-03\n",
      "Epoch: 240550 | training loss: 1.9384e-03 | validation loss: 1.5389e-03\n",
      "Epoch: 240560 | training loss: 1.9483e-03 | validation loss: 1.5354e-03\n",
      "Epoch: 240570 | training loss: 2.1796e-03 | validation loss: 1.6005e-03\n",
      "Epoch: 240580 | training loss: 2.1080e-03 | validation loss: 1.5802e-03\n",
      "Epoch: 240590 | training loss: 1.9465e-03 | validation loss: 1.5543e-03\n",
      "Epoch: 240600 | training loss: 1.9427e-03 | validation loss: 1.5538e-03\n",
      "Epoch: 240610 | training loss: 1.9469e-03 | validation loss: 1.5375e-03\n",
      "Epoch: 240620 | training loss: 1.9422e-03 | validation loss: 1.5527e-03\n",
      "Epoch: 240630 | training loss: 1.9385e-03 | validation loss: 1.5394e-03\n",
      "Epoch: 240640 | training loss: 1.9371e-03 | validation loss: 1.5438e-03\n",
      "Epoch: 240650 | training loss: 1.9373e-03 | validation loss: 1.5446e-03\n",
      "Epoch: 240660 | training loss: 1.9372e-03 | validation loss: 1.5413e-03\n",
      "Epoch: 240670 | training loss: 1.9371e-03 | validation loss: 1.5417e-03\n",
      "Epoch: 240680 | training loss: 1.9370e-03 | validation loss: 1.5421e-03\n",
      "Epoch: 240690 | training loss: 1.9371e-03 | validation loss: 1.5414e-03\n",
      "Epoch: 240700 | training loss: 1.9410e-03 | validation loss: 1.5376e-03\n",
      "Epoch: 240710 | training loss: 2.1502e-03 | validation loss: 1.5947e-03\n",
      "Epoch: 240720 | training loss: 2.1597e-03 | validation loss: 1.5996e-03\n",
      "Epoch: 240730 | training loss: 2.0885e-03 | validation loss: 1.5704e-03\n",
      "Epoch: 240740 | training loss: 1.9927e-03 | validation loss: 1.5953e-03\n",
      "Epoch: 240750 | training loss: 1.9560e-03 | validation loss: 1.5683e-03\n",
      "Epoch: 240760 | training loss: 1.9468e-03 | validation loss: 1.5358e-03\n",
      "Epoch: 240770 | training loss: 1.9371e-03 | validation loss: 1.5412e-03\n",
      "Epoch: 240780 | training loss: 1.9385e-03 | validation loss: 1.5482e-03\n",
      "Epoch: 240790 | training loss: 1.9374e-03 | validation loss: 1.5402e-03\n",
      "Epoch: 240800 | training loss: 1.9369e-03 | validation loss: 1.5440e-03\n",
      "Epoch: 240810 | training loss: 1.9368e-03 | validation loss: 1.5421e-03\n",
      "Epoch: 240820 | training loss: 1.9367e-03 | validation loss: 1.5431e-03\n",
      "Epoch: 240830 | training loss: 1.9367e-03 | validation loss: 1.5422e-03\n",
      "Epoch: 240840 | training loss: 1.9367e-03 | validation loss: 1.5429e-03\n",
      "Epoch: 240850 | training loss: 1.9367e-03 | validation loss: 1.5431e-03\n",
      "Epoch: 240860 | training loss: 1.9377e-03 | validation loss: 1.5467e-03\n",
      "Epoch: 240870 | training loss: 2.0557e-03 | validation loss: 1.6639e-03\n",
      "Epoch: 240880 | training loss: 2.0315e-03 | validation loss: 1.6136e-03\n",
      "Epoch: 240890 | training loss: 2.0026e-03 | validation loss: 1.6092e-03\n",
      "Epoch: 240900 | training loss: 1.9877e-03 | validation loss: 1.5752e-03\n",
      "Epoch: 240910 | training loss: 1.9459e-03 | validation loss: 1.5447e-03\n",
      "Epoch: 240920 | training loss: 1.9386e-03 | validation loss: 1.5362e-03\n",
      "Epoch: 240930 | training loss: 1.9480e-03 | validation loss: 1.5322e-03\n",
      "Epoch: 240940 | training loss: 2.1272e-03 | validation loss: 1.5763e-03\n",
      "Epoch: 240950 | training loss: 2.2082e-03 | validation loss: 1.6091e-03\n",
      "Epoch: 240960 | training loss: 1.9829e-03 | validation loss: 1.5913e-03\n",
      "Epoch: 240970 | training loss: 1.9376e-03 | validation loss: 1.5379e-03\n",
      "Epoch: 240980 | training loss: 1.9369e-03 | validation loss: 1.5389e-03\n",
      "Epoch: 240990 | training loss: 1.9367e-03 | validation loss: 1.5440e-03\n",
      "Epoch: 241000 | training loss: 1.9365e-03 | validation loss: 1.5430e-03\n",
      "Epoch: 241010 | training loss: 1.9371e-03 | validation loss: 1.5392e-03\n",
      "Epoch: 241020 | training loss: 1.9370e-03 | validation loss: 1.5453e-03\n",
      "Epoch: 241030 | training loss: 1.9364e-03 | validation loss: 1.5425e-03\n",
      "Epoch: 241040 | training loss: 1.9365e-03 | validation loss: 1.5408e-03\n",
      "Epoch: 241050 | training loss: 1.9368e-03 | validation loss: 1.5395e-03\n",
      "Epoch: 241060 | training loss: 1.9415e-03 | validation loss: 1.5356e-03\n",
      "Epoch: 241070 | training loss: 2.0928e-03 | validation loss: 1.5689e-03\n",
      "Epoch: 241080 | training loss: 2.4546e-03 | validation loss: 1.7113e-03\n",
      "Epoch: 241090 | training loss: 1.9574e-03 | validation loss: 1.5591e-03\n",
      "Epoch: 241100 | training loss: 1.9738e-03 | validation loss: 1.5842e-03\n",
      "Epoch: 241110 | training loss: 1.9693e-03 | validation loss: 1.5409e-03\n",
      "Epoch: 241120 | training loss: 1.9440e-03 | validation loss: 1.5521e-03\n",
      "Epoch: 241130 | training loss: 1.9371e-03 | validation loss: 1.5405e-03\n",
      "Epoch: 241140 | training loss: 1.9363e-03 | validation loss: 1.5429e-03\n",
      "Epoch: 241150 | training loss: 1.9363e-03 | validation loss: 1.5407e-03\n",
      "Epoch: 241160 | training loss: 1.9363e-03 | validation loss: 1.5434e-03\n",
      "Epoch: 241170 | training loss: 1.9362e-03 | validation loss: 1.5408e-03\n",
      "Epoch: 241180 | training loss: 1.9361e-03 | validation loss: 1.5418e-03\n",
      "Epoch: 241190 | training loss: 1.9361e-03 | validation loss: 1.5425e-03\n",
      "Epoch: 241200 | training loss: 1.9361e-03 | validation loss: 1.5425e-03\n",
      "Epoch: 241210 | training loss: 1.9364e-03 | validation loss: 1.5437e-03\n",
      "Epoch: 241220 | training loss: 1.9436e-03 | validation loss: 1.5543e-03\n",
      "Epoch: 241230 | training loss: 2.3068e-03 | validation loss: 1.7853e-03\n",
      "Epoch: 241240 | training loss: 1.9438e-03 | validation loss: 1.5555e-03\n",
      "Epoch: 241250 | training loss: 2.1639e-03 | validation loss: 1.6947e-03\n",
      "Epoch: 241260 | training loss: 1.9494e-03 | validation loss: 1.5340e-03\n",
      "Epoch: 241270 | training loss: 1.9608e-03 | validation loss: 1.5472e-03\n",
      "Epoch: 241280 | training loss: 1.9424e-03 | validation loss: 1.5488e-03\n",
      "Epoch: 241290 | training loss: 1.9363e-03 | validation loss: 1.5441e-03\n",
      "Epoch: 241300 | training loss: 1.9372e-03 | validation loss: 1.5394e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 241310 | training loss: 1.9365e-03 | validation loss: 1.5438e-03\n",
      "Epoch: 241320 | training loss: 1.9361e-03 | validation loss: 1.5412e-03\n",
      "Epoch: 241330 | training loss: 1.9359e-03 | validation loss: 1.5422e-03\n",
      "Epoch: 241340 | training loss: 1.9359e-03 | validation loss: 1.5410e-03\n",
      "Epoch: 241350 | training loss: 1.9358e-03 | validation loss: 1.5413e-03\n",
      "Epoch: 241360 | training loss: 1.9360e-03 | validation loss: 1.5395e-03\n",
      "Epoch: 241370 | training loss: 1.9475e-03 | validation loss: 1.5336e-03\n",
      "Epoch: 241380 | training loss: 2.5421e-03 | validation loss: 1.8219e-03\n",
      "Epoch: 241390 | training loss: 2.0735e-03 | validation loss: 1.6719e-03\n",
      "Epoch: 241400 | training loss: 1.9856e-03 | validation loss: 1.5999e-03\n",
      "Epoch: 241410 | training loss: 1.9410e-03 | validation loss: 1.5335e-03\n",
      "Epoch: 241420 | training loss: 1.9488e-03 | validation loss: 1.5501e-03\n",
      "Epoch: 241430 | training loss: 1.9382e-03 | validation loss: 1.5354e-03\n",
      "Epoch: 241440 | training loss: 1.9370e-03 | validation loss: 1.5399e-03\n",
      "Epoch: 241450 | training loss: 1.9359e-03 | validation loss: 1.5420e-03\n",
      "Epoch: 241460 | training loss: 1.9358e-03 | validation loss: 1.5409e-03\n",
      "Epoch: 241470 | training loss: 1.9361e-03 | validation loss: 1.5441e-03\n",
      "Epoch: 241480 | training loss: 1.9536e-03 | validation loss: 1.5671e-03\n",
      "Epoch: 241490 | training loss: 2.8748e-03 | validation loss: 2.1163e-03\n",
      "Epoch: 241500 | training loss: 2.4427e-03 | validation loss: 1.6990e-03\n",
      "Epoch: 241510 | training loss: 1.9835e-03 | validation loss: 1.5779e-03\n",
      "Epoch: 241520 | training loss: 1.9892e-03 | validation loss: 1.5904e-03\n",
      "Epoch: 241530 | training loss: 1.9442e-03 | validation loss: 1.5394e-03\n",
      "Epoch: 241540 | training loss: 1.9396e-03 | validation loss: 1.5385e-03\n",
      "Epoch: 241550 | training loss: 1.9387e-03 | validation loss: 1.5479e-03\n",
      "Epoch: 241560 | training loss: 1.9358e-03 | validation loss: 1.5386e-03\n",
      "Epoch: 241570 | training loss: 1.9355e-03 | validation loss: 1.5414e-03\n",
      "Epoch: 241580 | training loss: 1.9355e-03 | validation loss: 1.5416e-03\n",
      "Epoch: 241590 | training loss: 1.9354e-03 | validation loss: 1.5405e-03\n",
      "Epoch: 241600 | training loss: 1.9354e-03 | validation loss: 1.5411e-03\n",
      "Epoch: 241610 | training loss: 1.9354e-03 | validation loss: 1.5410e-03\n",
      "Epoch: 241620 | training loss: 1.9354e-03 | validation loss: 1.5407e-03\n",
      "Epoch: 241630 | training loss: 1.9354e-03 | validation loss: 1.5408e-03\n",
      "Epoch: 241640 | training loss: 1.9353e-03 | validation loss: 1.5409e-03\n",
      "Epoch: 241650 | training loss: 1.9353e-03 | validation loss: 1.5411e-03\n",
      "Epoch: 241660 | training loss: 1.9354e-03 | validation loss: 1.5416e-03\n",
      "Epoch: 241670 | training loss: 1.9373e-03 | validation loss: 1.5466e-03\n",
      "Epoch: 241680 | training loss: 2.1055e-03 | validation loss: 1.6680e-03\n",
      "Epoch: 241690 | training loss: 2.2924e-03 | validation loss: 1.7820e-03\n",
      "Epoch: 241700 | training loss: 2.2885e-03 | validation loss: 1.7737e-03\n",
      "Epoch: 241710 | training loss: 1.9408e-03 | validation loss: 1.5481e-03\n",
      "Epoch: 241720 | training loss: 1.9611e-03 | validation loss: 1.5372e-03\n",
      "Epoch: 241730 | training loss: 1.9522e-03 | validation loss: 1.5366e-03\n",
      "Epoch: 241740 | training loss: 1.9352e-03 | validation loss: 1.5406e-03\n",
      "Epoch: 241750 | training loss: 1.9376e-03 | validation loss: 1.5470e-03\n",
      "Epoch: 241760 | training loss: 1.9352e-03 | validation loss: 1.5409e-03\n",
      "Epoch: 241770 | training loss: 1.9354e-03 | validation loss: 1.5394e-03\n",
      "Epoch: 241780 | training loss: 1.9352e-03 | validation loss: 1.5416e-03\n",
      "Epoch: 241790 | training loss: 1.9351e-03 | validation loss: 1.5408e-03\n",
      "Epoch: 241800 | training loss: 1.9351e-03 | validation loss: 1.5405e-03\n",
      "Epoch: 241810 | training loss: 1.9351e-03 | validation loss: 1.5409e-03\n",
      "Epoch: 241820 | training loss: 1.9350e-03 | validation loss: 1.5406e-03\n",
      "Epoch: 241830 | training loss: 1.9350e-03 | validation loss: 1.5407e-03\n",
      "Epoch: 241840 | training loss: 1.9350e-03 | validation loss: 1.5408e-03\n",
      "Epoch: 241850 | training loss: 1.9352e-03 | validation loss: 1.5425e-03\n",
      "Epoch: 241860 | training loss: 1.9666e-03 | validation loss: 1.5851e-03\n",
      "Epoch: 241870 | training loss: 2.4153e-03 | validation loss: 1.9416e-03\n",
      "Epoch: 241880 | training loss: 2.1374e-03 | validation loss: 1.6767e-03\n",
      "Epoch: 241890 | training loss: 1.9818e-03 | validation loss: 1.5949e-03\n",
      "Epoch: 241900 | training loss: 1.9445e-03 | validation loss: 1.5564e-03\n",
      "Epoch: 241910 | training loss: 1.9386e-03 | validation loss: 1.5366e-03\n",
      "Epoch: 241920 | training loss: 1.9352e-03 | validation loss: 1.5415e-03\n",
      "Epoch: 241930 | training loss: 1.9366e-03 | validation loss: 1.5435e-03\n",
      "Epoch: 241940 | training loss: 1.9373e-03 | validation loss: 1.5458e-03\n",
      "Epoch: 241950 | training loss: 1.9517e-03 | validation loss: 1.5650e-03\n",
      "Epoch: 241960 | training loss: 2.2626e-03 | validation loss: 1.7743e-03\n",
      "Epoch: 241970 | training loss: 1.9540e-03 | validation loss: 1.5645e-03\n",
      "Epoch: 241980 | training loss: 1.9351e-03 | validation loss: 1.5416e-03\n",
      "Epoch: 241990 | training loss: 1.9413e-03 | validation loss: 1.5329e-03\n",
      "Epoch: 242000 | training loss: 1.9395e-03 | validation loss: 1.5509e-03\n",
      "Epoch: 242010 | training loss: 1.9359e-03 | validation loss: 1.5365e-03\n",
      "Epoch: 242020 | training loss: 1.9347e-03 | validation loss: 1.5396e-03\n",
      "Epoch: 242030 | training loss: 1.9354e-03 | validation loss: 1.5436e-03\n",
      "Epoch: 242040 | training loss: 1.9351e-03 | validation loss: 1.5377e-03\n",
      "Epoch: 242050 | training loss: 1.9349e-03 | validation loss: 1.5383e-03\n",
      "Epoch: 242060 | training loss: 1.9347e-03 | validation loss: 1.5392e-03\n",
      "Epoch: 242070 | training loss: 1.9347e-03 | validation loss: 1.5388e-03\n",
      "Epoch: 242080 | training loss: 1.9362e-03 | validation loss: 1.5357e-03\n",
      "Epoch: 242090 | training loss: 2.0138e-03 | validation loss: 1.5436e-03\n",
      "Epoch: 242100 | training loss: 3.1751e-03 | validation loss: 2.0090e-03\n",
      "Epoch: 242110 | training loss: 1.9728e-03 | validation loss: 1.5679e-03\n",
      "Epoch: 242120 | training loss: 2.0592e-03 | validation loss: 1.6404e-03\n",
      "Epoch: 242130 | training loss: 1.9427e-03 | validation loss: 1.5475e-03\n",
      "Epoch: 242140 | training loss: 1.9484e-03 | validation loss: 1.5341e-03\n",
      "Epoch: 242150 | training loss: 1.9379e-03 | validation loss: 1.5427e-03\n",
      "Epoch: 242160 | training loss: 1.9349e-03 | validation loss: 1.5430e-03\n",
      "Epoch: 242170 | training loss: 1.9351e-03 | validation loss: 1.5385e-03\n",
      "Epoch: 242180 | training loss: 1.9348e-03 | validation loss: 1.5407e-03\n",
      "Epoch: 242190 | training loss: 1.9346e-03 | validation loss: 1.5397e-03\n",
      "Epoch: 242200 | training loss: 1.9345e-03 | validation loss: 1.5400e-03\n",
      "Epoch: 242210 | training loss: 1.9344e-03 | validation loss: 1.5397e-03\n",
      "Epoch: 242220 | training loss: 1.9344e-03 | validation loss: 1.5398e-03\n",
      "Epoch: 242230 | training loss: 1.9344e-03 | validation loss: 1.5399e-03\n",
      "Epoch: 242240 | training loss: 1.9343e-03 | validation loss: 1.5396e-03\n",
      "Epoch: 242250 | training loss: 1.9343e-03 | validation loss: 1.5395e-03\n",
      "Epoch: 242260 | training loss: 1.9343e-03 | validation loss: 1.5394e-03\n",
      "Epoch: 242270 | training loss: 1.9344e-03 | validation loss: 1.5387e-03\n",
      "Epoch: 242280 | training loss: 1.9417e-03 | validation loss: 1.5365e-03\n",
      "Epoch: 242290 | training loss: 2.5399e-03 | validation loss: 1.7798e-03\n",
      "Epoch: 242300 | training loss: 2.3261e-03 | validation loss: 1.7767e-03\n",
      "Epoch: 242310 | training loss: 2.0543e-03 | validation loss: 1.5561e-03\n",
      "Epoch: 242320 | training loss: 1.9360e-03 | validation loss: 1.5457e-03\n",
      "Epoch: 242330 | training loss: 1.9466e-03 | validation loss: 1.5638e-03\n",
      "Epoch: 242340 | training loss: 1.9404e-03 | validation loss: 1.5394e-03\n",
      "Epoch: 242350 | training loss: 1.9370e-03 | validation loss: 1.5436e-03\n",
      "Epoch: 242360 | training loss: 1.9355e-03 | validation loss: 1.5443e-03\n",
      "Epoch: 242370 | training loss: 1.9344e-03 | validation loss: 1.5383e-03\n",
      "Epoch: 242380 | training loss: 1.9342e-03 | validation loss: 1.5405e-03\n",
      "Epoch: 242390 | training loss: 1.9342e-03 | validation loss: 1.5384e-03\n",
      "Epoch: 242400 | training loss: 1.9341e-03 | validation loss: 1.5397e-03\n",
      "Epoch: 242410 | training loss: 1.9341e-03 | validation loss: 1.5397e-03\n",
      "Epoch: 242420 | training loss: 1.9340e-03 | validation loss: 1.5391e-03\n",
      "Epoch: 242430 | training loss: 1.9340e-03 | validation loss: 1.5392e-03\n",
      "Epoch: 242440 | training loss: 1.9340e-03 | validation loss: 1.5392e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 242450 | training loss: 1.9340e-03 | validation loss: 1.5390e-03\n",
      "Epoch: 242460 | training loss: 1.9341e-03 | validation loss: 1.5378e-03\n",
      "Epoch: 242470 | training loss: 1.9417e-03 | validation loss: 1.5304e-03\n",
      "Epoch: 242480 | training loss: 2.7447e-03 | validation loss: 1.8208e-03\n",
      "Epoch: 242490 | training loss: 2.5358e-03 | validation loss: 1.9686e-03\n",
      "Epoch: 242500 | training loss: 2.0724e-03 | validation loss: 1.6668e-03\n",
      "Epoch: 242510 | training loss: 1.9593e-03 | validation loss: 1.5599e-03\n",
      "Epoch: 242520 | training loss: 1.9366e-03 | validation loss: 1.5437e-03\n",
      "Epoch: 242530 | training loss: 1.9375e-03 | validation loss: 1.5375e-03\n",
      "Epoch: 242540 | training loss: 1.9374e-03 | validation loss: 1.5317e-03\n",
      "Epoch: 242550 | training loss: 1.9347e-03 | validation loss: 1.5360e-03\n",
      "Epoch: 242560 | training loss: 1.9338e-03 | validation loss: 1.5392e-03\n",
      "Epoch: 242570 | training loss: 1.9340e-03 | validation loss: 1.5400e-03\n",
      "Epoch: 242580 | training loss: 1.9338e-03 | validation loss: 1.5398e-03\n",
      "Epoch: 242590 | training loss: 1.9338e-03 | validation loss: 1.5382e-03\n",
      "Epoch: 242600 | training loss: 1.9338e-03 | validation loss: 1.5388e-03\n",
      "Epoch: 242610 | training loss: 1.9337e-03 | validation loss: 1.5391e-03\n",
      "Epoch: 242620 | training loss: 1.9337e-03 | validation loss: 1.5387e-03\n",
      "Epoch: 242630 | training loss: 1.9337e-03 | validation loss: 1.5388e-03\n",
      "Epoch: 242640 | training loss: 1.9337e-03 | validation loss: 1.5388e-03\n",
      "Epoch: 242650 | training loss: 1.9337e-03 | validation loss: 1.5388e-03\n",
      "Epoch: 242660 | training loss: 1.9336e-03 | validation loss: 1.5387e-03\n",
      "Epoch: 242670 | training loss: 1.9337e-03 | validation loss: 1.5381e-03\n",
      "Epoch: 242680 | training loss: 1.9429e-03 | validation loss: 1.5356e-03\n",
      "Epoch: 242690 | training loss: 3.4355e-03 | validation loss: 2.1835e-03\n",
      "Epoch: 242700 | training loss: 2.6197e-03 | validation loss: 1.9846e-03\n",
      "Epoch: 242710 | training loss: 2.1234e-03 | validation loss: 1.6854e-03\n",
      "Epoch: 242720 | training loss: 1.9596e-03 | validation loss: 1.5735e-03\n",
      "Epoch: 242730 | training loss: 1.9350e-03 | validation loss: 1.5445e-03\n",
      "Epoch: 242740 | training loss: 1.9373e-03 | validation loss: 1.5365e-03\n",
      "Epoch: 242750 | training loss: 1.9371e-03 | validation loss: 1.5347e-03\n",
      "Epoch: 242760 | training loss: 1.9341e-03 | validation loss: 1.5368e-03\n",
      "Epoch: 242770 | training loss: 1.9336e-03 | validation loss: 1.5402e-03\n",
      "Epoch: 242780 | training loss: 1.9336e-03 | validation loss: 1.5403e-03\n",
      "Epoch: 242790 | training loss: 1.9335e-03 | validation loss: 1.5381e-03\n",
      "Epoch: 242800 | training loss: 1.9335e-03 | validation loss: 1.5378e-03\n",
      "Epoch: 242810 | training loss: 1.9334e-03 | validation loss: 1.5388e-03\n",
      "Epoch: 242820 | training loss: 1.9334e-03 | validation loss: 1.5386e-03\n",
      "Epoch: 242830 | training loss: 1.9334e-03 | validation loss: 1.5385e-03\n",
      "Epoch: 242840 | training loss: 1.9334e-03 | validation loss: 1.5386e-03\n",
      "Epoch: 242850 | training loss: 1.9333e-03 | validation loss: 1.5385e-03\n",
      "Epoch: 242860 | training loss: 1.9333e-03 | validation loss: 1.5385e-03\n",
      "Epoch: 242870 | training loss: 1.9333e-03 | validation loss: 1.5385e-03\n",
      "Epoch: 242880 | training loss: 1.9333e-03 | validation loss: 1.5384e-03\n",
      "Epoch: 242890 | training loss: 1.9333e-03 | validation loss: 1.5385e-03\n",
      "Epoch: 242900 | training loss: 1.9333e-03 | validation loss: 1.5384e-03\n",
      "Epoch: 242910 | training loss: 1.9332e-03 | validation loss: 1.5384e-03\n",
      "Epoch: 242920 | training loss: 1.9332e-03 | validation loss: 1.5384e-03\n",
      "Epoch: 242930 | training loss: 1.9332e-03 | validation loss: 1.5387e-03\n",
      "Epoch: 242940 | training loss: 1.9337e-03 | validation loss: 1.5417e-03\n",
      "Epoch: 242950 | training loss: 2.0240e-03 | validation loss: 1.6233e-03\n",
      "Epoch: 242960 | training loss: 2.9420e-03 | validation loss: 2.1401e-03\n",
      "Epoch: 242970 | training loss: 2.4086e-03 | validation loss: 1.9051e-03\n",
      "Epoch: 242980 | training loss: 2.1130e-03 | validation loss: 1.7154e-03\n",
      "Epoch: 242990 | training loss: 1.9907e-03 | validation loss: 1.6097e-03\n",
      "Epoch: 243000 | training loss: 1.9491e-03 | validation loss: 1.5674e-03\n",
      "Epoch: 243010 | training loss: 1.9390e-03 | validation loss: 1.5530e-03\n",
      "Epoch: 243020 | training loss: 1.9353e-03 | validation loss: 1.5465e-03\n",
      "Epoch: 243030 | training loss: 1.9341e-03 | validation loss: 1.5433e-03\n",
      "Epoch: 243040 | training loss: 1.9334e-03 | validation loss: 1.5414e-03\n",
      "Epoch: 243050 | training loss: 1.9331e-03 | validation loss: 1.5395e-03\n",
      "Epoch: 243060 | training loss: 1.9330e-03 | validation loss: 1.5381e-03\n",
      "Epoch: 243070 | training loss: 1.9330e-03 | validation loss: 1.5375e-03\n",
      "Epoch: 243080 | training loss: 1.9330e-03 | validation loss: 1.5376e-03\n",
      "Epoch: 243090 | training loss: 1.9330e-03 | validation loss: 1.5380e-03\n",
      "Epoch: 243100 | training loss: 1.9329e-03 | validation loss: 1.5381e-03\n",
      "Epoch: 243110 | training loss: 1.9329e-03 | validation loss: 1.5379e-03\n",
      "Epoch: 243120 | training loss: 1.9329e-03 | validation loss: 1.5378e-03\n",
      "Epoch: 243130 | training loss: 1.9329e-03 | validation loss: 1.5379e-03\n",
      "Epoch: 243140 | training loss: 1.9329e-03 | validation loss: 1.5378e-03\n",
      "Epoch: 243150 | training loss: 1.9328e-03 | validation loss: 1.5378e-03\n",
      "Epoch: 243160 | training loss: 1.9328e-03 | validation loss: 1.5378e-03\n",
      "Epoch: 243170 | training loss: 1.9329e-03 | validation loss: 1.5381e-03\n",
      "Epoch: 243180 | training loss: 1.9480e-03 | validation loss: 1.5492e-03\n",
      "Epoch: 243190 | training loss: 3.3369e-03 | validation loss: 2.3532e-03\n",
      "Epoch: 243200 | training loss: 2.4079e-03 | validation loss: 1.6984e-03\n",
      "Epoch: 243210 | training loss: 1.9977e-03 | validation loss: 1.5610e-03\n",
      "Epoch: 243220 | training loss: 1.9940e-03 | validation loss: 1.5779e-03\n",
      "Epoch: 243230 | training loss: 1.9470e-03 | validation loss: 1.5300e-03\n",
      "Epoch: 243240 | training loss: 1.9376e-03 | validation loss: 1.5311e-03\n",
      "Epoch: 243250 | training loss: 1.9357e-03 | validation loss: 1.5411e-03\n",
      "Epoch: 243260 | training loss: 1.9337e-03 | validation loss: 1.5334e-03\n",
      "Epoch: 243270 | training loss: 1.9329e-03 | validation loss: 1.5383e-03\n",
      "Epoch: 243280 | training loss: 1.9327e-03 | validation loss: 1.5364e-03\n",
      "Epoch: 243290 | training loss: 1.9327e-03 | validation loss: 1.5383e-03\n",
      "Epoch: 243300 | training loss: 1.9326e-03 | validation loss: 1.5373e-03\n",
      "Epoch: 243310 | training loss: 1.9326e-03 | validation loss: 1.5377e-03\n",
      "Epoch: 243320 | training loss: 1.9326e-03 | validation loss: 1.5377e-03\n",
      "Epoch: 243330 | training loss: 1.9326e-03 | validation loss: 1.5375e-03\n",
      "Epoch: 243340 | training loss: 1.9325e-03 | validation loss: 1.5376e-03\n",
      "Epoch: 243350 | training loss: 1.9326e-03 | validation loss: 1.5381e-03\n",
      "Epoch: 243360 | training loss: 1.9336e-03 | validation loss: 1.5420e-03\n",
      "Epoch: 243370 | training loss: 2.0292e-03 | validation loss: 1.6220e-03\n",
      "Epoch: 243380 | training loss: 3.0356e-03 | validation loss: 2.1922e-03\n",
      "Epoch: 243390 | training loss: 2.0958e-03 | validation loss: 1.6705e-03\n",
      "Epoch: 243400 | training loss: 1.9602e-03 | validation loss: 1.5460e-03\n",
      "Epoch: 243410 | training loss: 1.9864e-03 | validation loss: 1.5440e-03\n",
      "Epoch: 243420 | training loss: 1.9419e-03 | validation loss: 1.5323e-03\n",
      "Epoch: 243430 | training loss: 1.9340e-03 | validation loss: 1.5408e-03\n",
      "Epoch: 243440 | training loss: 1.9350e-03 | validation loss: 1.5424e-03\n",
      "Epoch: 243450 | training loss: 1.9325e-03 | validation loss: 1.5361e-03\n",
      "Epoch: 243460 | training loss: 1.9326e-03 | validation loss: 1.5364e-03\n",
      "Epoch: 243470 | training loss: 1.9324e-03 | validation loss: 1.5387e-03\n",
      "Epoch: 243480 | training loss: 1.9323e-03 | validation loss: 1.5370e-03\n",
      "Epoch: 243490 | training loss: 1.9323e-03 | validation loss: 1.5372e-03\n",
      "Epoch: 243500 | training loss: 1.9323e-03 | validation loss: 1.5375e-03\n",
      "Epoch: 243510 | training loss: 1.9323e-03 | validation loss: 1.5371e-03\n",
      "Epoch: 243520 | training loss: 1.9322e-03 | validation loss: 1.5373e-03\n",
      "Epoch: 243530 | training loss: 1.9322e-03 | validation loss: 1.5372e-03\n",
      "Epoch: 243540 | training loss: 1.9322e-03 | validation loss: 1.5372e-03\n",
      "Epoch: 243550 | training loss: 1.9322e-03 | validation loss: 1.5372e-03\n",
      "Epoch: 243560 | training loss: 1.9322e-03 | validation loss: 1.5372e-03\n",
      "Epoch: 243570 | training loss: 1.9322e-03 | validation loss: 1.5372e-03\n",
      "Epoch: 243580 | training loss: 1.9321e-03 | validation loss: 1.5373e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 243590 | training loss: 1.9323e-03 | validation loss: 1.5384e-03\n",
      "Epoch: 243600 | training loss: 1.9434e-03 | validation loss: 1.5537e-03\n",
      "Epoch: 243610 | training loss: 3.2900e-03 | validation loss: 2.3198e-03\n",
      "Epoch: 243620 | training loss: 2.7402e-03 | validation loss: 1.8346e-03\n",
      "Epoch: 243630 | training loss: 2.1870e-03 | validation loss: 1.6211e-03\n",
      "Epoch: 243640 | training loss: 1.9775e-03 | validation loss: 1.5459e-03\n",
      "Epoch: 243650 | training loss: 1.9336e-03 | validation loss: 1.5343e-03\n",
      "Epoch: 243660 | training loss: 1.9343e-03 | validation loss: 1.5413e-03\n",
      "Epoch: 243670 | training loss: 1.9363e-03 | validation loss: 1.5448e-03\n",
      "Epoch: 243680 | training loss: 1.9332e-03 | validation loss: 1.5403e-03\n",
      "Epoch: 243690 | training loss: 1.9320e-03 | validation loss: 1.5369e-03\n",
      "Epoch: 243700 | training loss: 1.9322e-03 | validation loss: 1.5362e-03\n",
      "Epoch: 243710 | training loss: 1.9319e-03 | validation loss: 1.5370e-03\n",
      "Epoch: 243720 | training loss: 1.9320e-03 | validation loss: 1.5375e-03\n",
      "Epoch: 243730 | training loss: 1.9319e-03 | validation loss: 1.5369e-03\n",
      "Epoch: 243740 | training loss: 1.9319e-03 | validation loss: 1.5369e-03\n",
      "Epoch: 243750 | training loss: 1.9319e-03 | validation loss: 1.5370e-03\n",
      "Epoch: 243760 | training loss: 1.9319e-03 | validation loss: 1.5369e-03\n",
      "Epoch: 243770 | training loss: 1.9318e-03 | validation loss: 1.5370e-03\n",
      "Epoch: 243780 | training loss: 1.9318e-03 | validation loss: 1.5369e-03\n",
      "Epoch: 243790 | training loss: 1.9318e-03 | validation loss: 1.5370e-03\n",
      "Epoch: 243800 | training loss: 1.9319e-03 | validation loss: 1.5384e-03\n",
      "Epoch: 243810 | training loss: 1.9545e-03 | validation loss: 1.5735e-03\n",
      "Epoch: 243820 | training loss: 2.8085e-03 | validation loss: 2.2121e-03\n",
      "Epoch: 243830 | training loss: 2.2679e-03 | validation loss: 1.7327e-03\n",
      "Epoch: 243840 | training loss: 2.0443e-03 | validation loss: 1.6544e-03\n",
      "Epoch: 243850 | training loss: 1.9667e-03 | validation loss: 1.5596e-03\n",
      "Epoch: 243860 | training loss: 1.9428e-03 | validation loss: 1.5567e-03\n",
      "Epoch: 243870 | training loss: 1.9360e-03 | validation loss: 1.5384e-03\n",
      "Epoch: 243880 | training loss: 1.9336e-03 | validation loss: 1.5435e-03\n",
      "Epoch: 243890 | training loss: 1.9322e-03 | validation loss: 1.5367e-03\n",
      "Epoch: 243900 | training loss: 1.9319e-03 | validation loss: 1.5362e-03\n",
      "Epoch: 243910 | training loss: 1.9316e-03 | validation loss: 1.5371e-03\n",
      "Epoch: 243920 | training loss: 1.9317e-03 | validation loss: 1.5374e-03\n",
      "Epoch: 243930 | training loss: 1.9323e-03 | validation loss: 1.5398e-03\n",
      "Epoch: 243940 | training loss: 1.9511e-03 | validation loss: 1.5636e-03\n",
      "Epoch: 243950 | training loss: 2.7348e-03 | validation loss: 2.0399e-03\n",
      "Epoch: 243960 | training loss: 2.2959e-03 | validation loss: 1.6373e-03\n",
      "Epoch: 243970 | training loss: 2.0471e-03 | validation loss: 1.6299e-03\n",
      "Epoch: 243980 | training loss: 1.9387e-03 | validation loss: 1.5474e-03\n",
      "Epoch: 243990 | training loss: 1.9546e-03 | validation loss: 1.5282e-03\n",
      "Epoch: 244000 | training loss: 1.9348e-03 | validation loss: 1.5443e-03\n",
      "Epoch: 244010 | training loss: 1.9315e-03 | validation loss: 1.5367e-03\n",
      "Epoch: 244020 | training loss: 1.9318e-03 | validation loss: 1.5344e-03\n",
      "Epoch: 244030 | training loss: 1.9316e-03 | validation loss: 1.5381e-03\n",
      "Epoch: 244040 | training loss: 1.9315e-03 | validation loss: 1.5352e-03\n",
      "Epoch: 244050 | training loss: 1.9314e-03 | validation loss: 1.5363e-03\n",
      "Epoch: 244060 | training loss: 1.9314e-03 | validation loss: 1.5365e-03\n",
      "Epoch: 244070 | training loss: 1.9314e-03 | validation loss: 1.5357e-03\n",
      "Epoch: 244080 | training loss: 1.9313e-03 | validation loss: 1.5358e-03\n",
      "Epoch: 244090 | training loss: 1.9313e-03 | validation loss: 1.5359e-03\n",
      "Epoch: 244100 | training loss: 1.9313e-03 | validation loss: 1.5358e-03\n",
      "Epoch: 244110 | training loss: 1.9313e-03 | validation loss: 1.5351e-03\n",
      "Epoch: 244120 | training loss: 1.9351e-03 | validation loss: 1.5306e-03\n",
      "Epoch: 244130 | training loss: 2.3380e-03 | validation loss: 1.6577e-03\n",
      "Epoch: 244140 | training loss: 2.0771e-03 | validation loss: 1.6602e-03\n",
      "Epoch: 244150 | training loss: 2.0976e-03 | validation loss: 1.5786e-03\n",
      "Epoch: 244160 | training loss: 2.0480e-03 | validation loss: 1.5482e-03\n",
      "Epoch: 244170 | training loss: 1.9578e-03 | validation loss: 1.5245e-03\n",
      "Epoch: 244180 | training loss: 1.9324e-03 | validation loss: 1.5345e-03\n",
      "Epoch: 244190 | training loss: 1.9353e-03 | validation loss: 1.5481e-03\n",
      "Epoch: 244200 | training loss: 1.9323e-03 | validation loss: 1.5420e-03\n",
      "Epoch: 244210 | training loss: 1.9314e-03 | validation loss: 1.5335e-03\n",
      "Epoch: 244220 | training loss: 1.9313e-03 | validation loss: 1.5344e-03\n",
      "Epoch: 244230 | training loss: 1.9312e-03 | validation loss: 1.5372e-03\n",
      "Epoch: 244240 | training loss: 1.9311e-03 | validation loss: 1.5359e-03\n",
      "Epoch: 244250 | training loss: 1.9311e-03 | validation loss: 1.5355e-03\n",
      "Epoch: 244260 | training loss: 1.9310e-03 | validation loss: 1.5362e-03\n",
      "Epoch: 244270 | training loss: 1.9310e-03 | validation loss: 1.5356e-03\n",
      "Epoch: 244280 | training loss: 1.9310e-03 | validation loss: 1.5360e-03\n",
      "Epoch: 244290 | training loss: 1.9310e-03 | validation loss: 1.5357e-03\n",
      "Epoch: 244300 | training loss: 1.9310e-03 | validation loss: 1.5358e-03\n",
      "Epoch: 244310 | training loss: 1.9309e-03 | validation loss: 1.5358e-03\n",
      "Epoch: 244320 | training loss: 1.9309e-03 | validation loss: 1.5356e-03\n",
      "Epoch: 244330 | training loss: 1.9310e-03 | validation loss: 1.5353e-03\n",
      "Epoch: 244340 | training loss: 1.9356e-03 | validation loss: 1.5343e-03\n",
      "Epoch: 244350 | training loss: 2.4383e-03 | validation loss: 1.8334e-03\n",
      "Epoch: 244360 | training loss: 2.1985e-03 | validation loss: 1.7801e-03\n",
      "Epoch: 244370 | training loss: 2.1173e-03 | validation loss: 1.7129e-03\n",
      "Epoch: 244380 | training loss: 1.9551e-03 | validation loss: 1.5607e-03\n",
      "Epoch: 244390 | training loss: 1.9545e-03 | validation loss: 1.5317e-03\n",
      "Epoch: 244400 | training loss: 1.9582e-03 | validation loss: 1.5266e-03\n",
      "Epoch: 244410 | training loss: 1.9986e-03 | validation loss: 1.5328e-03\n",
      "Epoch: 244420 | training loss: 2.1269e-03 | validation loss: 1.5715e-03\n",
      "Epoch: 244430 | training loss: 1.9474e-03 | validation loss: 1.5257e-03\n",
      "Epoch: 244440 | training loss: 1.9687e-03 | validation loss: 1.5788e-03\n",
      "Epoch: 244450 | training loss: 1.9426e-03 | validation loss: 1.5554e-03\n",
      "Epoch: 244460 | training loss: 1.9308e-03 | validation loss: 1.5369e-03\n",
      "Epoch: 244470 | training loss: 1.9315e-03 | validation loss: 1.5319e-03\n",
      "Epoch: 244480 | training loss: 1.9451e-03 | validation loss: 1.5267e-03\n",
      "Epoch: 244490 | training loss: 2.3692e-03 | validation loss: 1.6628e-03\n",
      "Epoch: 244500 | training loss: 1.9461e-03 | validation loss: 1.5617e-03\n",
      "Epoch: 244510 | training loss: 2.0331e-03 | validation loss: 1.5476e-03\n",
      "Epoch: 244520 | training loss: 1.9921e-03 | validation loss: 1.5925e-03\n",
      "Epoch: 244530 | training loss: 1.9412e-03 | validation loss: 1.5269e-03\n",
      "Epoch: 244540 | training loss: 1.9312e-03 | validation loss: 1.5382e-03\n",
      "Epoch: 244550 | training loss: 1.9306e-03 | validation loss: 1.5353e-03\n",
      "Epoch: 244560 | training loss: 1.9306e-03 | validation loss: 1.5357e-03\n",
      "Epoch: 244570 | training loss: 1.9307e-03 | validation loss: 1.5337e-03\n",
      "Epoch: 244580 | training loss: 1.9307e-03 | validation loss: 1.5370e-03\n",
      "Epoch: 244590 | training loss: 1.9305e-03 | validation loss: 1.5346e-03\n",
      "Epoch: 244600 | training loss: 1.9305e-03 | validation loss: 1.5342e-03\n",
      "Epoch: 244610 | training loss: 1.9305e-03 | validation loss: 1.5344e-03\n",
      "Epoch: 244620 | training loss: 1.9306e-03 | validation loss: 1.5336e-03\n",
      "Epoch: 244630 | training loss: 1.9339e-03 | validation loss: 1.5299e-03\n",
      "Epoch: 244640 | training loss: 2.0951e-03 | validation loss: 1.5675e-03\n",
      "Epoch: 244650 | training loss: 2.4167e-03 | validation loss: 1.6964e-03\n",
      "Epoch: 244660 | training loss: 1.9798e-03 | validation loss: 1.5276e-03\n",
      "Epoch: 244670 | training loss: 2.0298e-03 | validation loss: 1.6180e-03\n",
      "Epoch: 244680 | training loss: 1.9354e-03 | validation loss: 1.5487e-03\n",
      "Epoch: 244690 | training loss: 1.9441e-03 | validation loss: 1.5257e-03\n",
      "Epoch: 244700 | training loss: 1.9312e-03 | validation loss: 1.5381e-03\n",
      "Epoch: 244710 | training loss: 1.9309e-03 | validation loss: 1.5393e-03\n",
      "Epoch: 244720 | training loss: 1.9310e-03 | validation loss: 1.5314e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 244730 | training loss: 1.9305e-03 | validation loss: 1.5378e-03\n",
      "Epoch: 244740 | training loss: 1.9303e-03 | validation loss: 1.5335e-03\n",
      "Epoch: 244750 | training loss: 1.9303e-03 | validation loss: 1.5358e-03\n",
      "Epoch: 244760 | training loss: 1.9302e-03 | validation loss: 1.5346e-03\n",
      "Epoch: 244770 | training loss: 1.9302e-03 | validation loss: 1.5349e-03\n",
      "Epoch: 244780 | training loss: 1.9302e-03 | validation loss: 1.5352e-03\n",
      "Epoch: 244790 | training loss: 1.9302e-03 | validation loss: 1.5351e-03\n",
      "Epoch: 244800 | training loss: 1.9302e-03 | validation loss: 1.5357e-03\n",
      "Epoch: 244810 | training loss: 1.9364e-03 | validation loss: 1.5450e-03\n",
      "Epoch: 244820 | training loss: 2.4024e-03 | validation loss: 1.9036e-03\n",
      "Epoch: 244830 | training loss: 2.1465e-03 | validation loss: 1.6255e-03\n",
      "Epoch: 244840 | training loss: 2.2617e-03 | validation loss: 1.6277e-03\n",
      "Epoch: 244850 | training loss: 1.9369e-03 | validation loss: 1.5254e-03\n",
      "Epoch: 244860 | training loss: 1.9717e-03 | validation loss: 1.5843e-03\n",
      "Epoch: 244870 | training loss: 1.9544e-03 | validation loss: 1.5318e-03\n",
      "Epoch: 244880 | training loss: 1.9318e-03 | validation loss: 1.5364e-03\n",
      "Epoch: 244890 | training loss: 1.9353e-03 | validation loss: 1.5470e-03\n",
      "Epoch: 244900 | training loss: 1.9378e-03 | validation loss: 1.5487e-03\n",
      "Epoch: 244910 | training loss: 1.9613e-03 | validation loss: 1.5719e-03\n",
      "Epoch: 244920 | training loss: 2.2200e-03 | validation loss: 1.7447e-03\n",
      "Epoch: 244930 | training loss: 1.9651e-03 | validation loss: 1.5737e-03\n",
      "Epoch: 244940 | training loss: 1.9801e-03 | validation loss: 1.5305e-03\n",
      "Epoch: 244950 | training loss: 1.9642e-03 | validation loss: 1.5738e-03\n",
      "Epoch: 244960 | training loss: 1.9401e-03 | validation loss: 1.5268e-03\n",
      "Epoch: 244970 | training loss: 1.9302e-03 | validation loss: 1.5322e-03\n",
      "Epoch: 244980 | training loss: 1.9330e-03 | validation loss: 1.5427e-03\n",
      "Epoch: 244990 | training loss: 1.9329e-03 | validation loss: 1.5426e-03\n",
      "Epoch: 245000 | training loss: 1.9386e-03 | validation loss: 1.5500e-03\n",
      "Epoch: 245010 | training loss: 2.0343e-03 | validation loss: 1.6244e-03\n",
      "Epoch: 245020 | training loss: 2.5132e-03 | validation loss: 1.9071e-03\n",
      "Epoch: 245030 | training loss: 2.1242e-03 | validation loss: 1.5789e-03\n",
      "Epoch: 245040 | training loss: 2.0020e-03 | validation loss: 1.5974e-03\n",
      "Epoch: 245050 | training loss: 1.9570e-03 | validation loss: 1.5299e-03\n",
      "Epoch: 245060 | training loss: 1.9387e-03 | validation loss: 1.5489e-03\n",
      "Epoch: 245070 | training loss: 1.9306e-03 | validation loss: 1.5313e-03\n",
      "Epoch: 245080 | training loss: 1.9304e-03 | validation loss: 1.5316e-03\n",
      "Epoch: 245090 | training loss: 1.9303e-03 | validation loss: 1.5372e-03\n",
      "Epoch: 245100 | training loss: 1.9305e-03 | validation loss: 1.5378e-03\n",
      "Epoch: 245110 | training loss: 1.9318e-03 | validation loss: 1.5404e-03\n",
      "Epoch: 245120 | training loss: 1.9545e-03 | validation loss: 1.5638e-03\n",
      "Epoch: 245130 | training loss: 2.4747e-03 | validation loss: 1.8809e-03\n",
      "Epoch: 245140 | training loss: 1.9833e-03 | validation loss: 1.5326e-03\n",
      "Epoch: 245150 | training loss: 2.0138e-03 | validation loss: 1.6084e-03\n",
      "Epoch: 245160 | training loss: 1.9806e-03 | validation loss: 1.5319e-03\n",
      "Epoch: 245170 | training loss: 1.9479e-03 | validation loss: 1.5589e-03\n",
      "Epoch: 245180 | training loss: 1.9359e-03 | validation loss: 1.5276e-03\n",
      "Epoch: 245190 | training loss: 1.9323e-03 | validation loss: 1.5421e-03\n",
      "Epoch: 245200 | training loss: 1.9306e-03 | validation loss: 1.5304e-03\n",
      "Epoch: 245210 | training loss: 1.9295e-03 | validation loss: 1.5350e-03\n",
      "Epoch: 245220 | training loss: 1.9296e-03 | validation loss: 1.5355e-03\n",
      "Epoch: 245230 | training loss: 1.9294e-03 | validation loss: 1.5334e-03\n",
      "Epoch: 245240 | training loss: 1.9298e-03 | validation loss: 1.5313e-03\n",
      "Epoch: 245250 | training loss: 1.9474e-03 | validation loss: 1.5300e-03\n",
      "Epoch: 245260 | training loss: 2.4479e-03 | validation loss: 1.8051e-03\n",
      "Epoch: 245270 | training loss: 2.0815e-03 | validation loss: 1.5851e-03\n",
      "Epoch: 245280 | training loss: 2.0236e-03 | validation loss: 1.5864e-03\n",
      "Epoch: 245290 | training loss: 1.9390e-03 | validation loss: 1.5533e-03\n",
      "Epoch: 245300 | training loss: 1.9570e-03 | validation loss: 1.5609e-03\n",
      "Epoch: 245310 | training loss: 1.9739e-03 | validation loss: 1.5777e-03\n",
      "Epoch: 245320 | training loss: 2.0732e-03 | validation loss: 1.6525e-03\n",
      "Epoch: 245330 | training loss: 2.0583e-03 | validation loss: 1.6425e-03\n",
      "Epoch: 245340 | training loss: 1.9455e-03 | validation loss: 1.5254e-03\n",
      "Epoch: 245350 | training loss: 1.9580e-03 | validation loss: 1.5257e-03\n",
      "Epoch: 245360 | training loss: 1.9310e-03 | validation loss: 1.5293e-03\n",
      "Epoch: 245370 | training loss: 1.9294e-03 | validation loss: 1.5356e-03\n",
      "Epoch: 245380 | training loss: 1.9336e-03 | validation loss: 1.5436e-03\n",
      "Epoch: 245390 | training loss: 2.0435e-03 | validation loss: 1.6303e-03\n",
      "Epoch: 245400 | training loss: 2.6795e-03 | validation loss: 1.9984e-03\n",
      "Epoch: 245410 | training loss: 2.0506e-03 | validation loss: 1.5584e-03\n",
      "Epoch: 245420 | training loss: 1.9303e-03 | validation loss: 1.5284e-03\n",
      "Epoch: 245430 | training loss: 1.9495e-03 | validation loss: 1.5570e-03\n",
      "Epoch: 245440 | training loss: 1.9420e-03 | validation loss: 1.5281e-03\n",
      "Epoch: 245450 | training loss: 1.9338e-03 | validation loss: 1.5438e-03\n",
      "Epoch: 245460 | training loss: 1.9309e-03 | validation loss: 1.5291e-03\n",
      "Epoch: 245470 | training loss: 1.9298e-03 | validation loss: 1.5371e-03\n",
      "Epoch: 245480 | training loss: 1.9292e-03 | validation loss: 1.5320e-03\n",
      "Epoch: 245490 | training loss: 1.9290e-03 | validation loss: 1.5332e-03\n",
      "Epoch: 245500 | training loss: 1.9290e-03 | validation loss: 1.5343e-03\n",
      "Epoch: 245510 | training loss: 1.9290e-03 | validation loss: 1.5341e-03\n",
      "Epoch: 245520 | training loss: 1.9290e-03 | validation loss: 1.5343e-03\n",
      "Epoch: 245530 | training loss: 1.9297e-03 | validation loss: 1.5368e-03\n",
      "Epoch: 245540 | training loss: 1.9556e-03 | validation loss: 1.5644e-03\n",
      "Epoch: 245550 | training loss: 3.0705e-03 | validation loss: 2.2054e-03\n",
      "Epoch: 245560 | training loss: 2.4609e-03 | validation loss: 1.7102e-03\n",
      "Epoch: 245570 | training loss: 1.9334e-03 | validation loss: 1.5397e-03\n",
      "Epoch: 245580 | training loss: 1.9929e-03 | validation loss: 1.5938e-03\n",
      "Epoch: 245590 | training loss: 1.9311e-03 | validation loss: 1.5289e-03\n",
      "Epoch: 245600 | training loss: 1.9346e-03 | validation loss: 1.5272e-03\n",
      "Epoch: 245610 | training loss: 1.9315e-03 | validation loss: 1.5408e-03\n",
      "Epoch: 245620 | training loss: 1.9289e-03 | validation loss: 1.5320e-03\n",
      "Epoch: 245630 | training loss: 1.9288e-03 | validation loss: 1.5328e-03\n",
      "Epoch: 245640 | training loss: 1.9288e-03 | validation loss: 1.5341e-03\n",
      "Epoch: 245650 | training loss: 1.9287e-03 | validation loss: 1.5329e-03\n",
      "Epoch: 245660 | training loss: 1.9287e-03 | validation loss: 1.5332e-03\n",
      "Epoch: 245670 | training loss: 1.9287e-03 | validation loss: 1.5334e-03\n",
      "Epoch: 245680 | training loss: 1.9287e-03 | validation loss: 1.5328e-03\n",
      "Epoch: 245690 | training loss: 1.9288e-03 | validation loss: 1.5321e-03\n",
      "Epoch: 245700 | training loss: 1.9371e-03 | validation loss: 1.5293e-03\n",
      "Epoch: 245710 | training loss: 2.5731e-03 | validation loss: 1.8938e-03\n",
      "Epoch: 245720 | training loss: 2.0041e-03 | validation loss: 1.5655e-03\n",
      "Epoch: 245730 | training loss: 1.9973e-03 | validation loss: 1.6045e-03\n",
      "Epoch: 245740 | training loss: 1.9625e-03 | validation loss: 1.5783e-03\n",
      "Epoch: 245750 | training loss: 1.9378e-03 | validation loss: 1.5519e-03\n",
      "Epoch: 245760 | training loss: 1.9461e-03 | validation loss: 1.5575e-03\n",
      "Epoch: 245770 | training loss: 2.1076e-03 | validation loss: 1.6717e-03\n",
      "Epoch: 245780 | training loss: 2.1654e-03 | validation loss: 1.7064e-03\n",
      "Epoch: 245790 | training loss: 2.0432e-03 | validation loss: 1.5448e-03\n",
      "Epoch: 245800 | training loss: 1.9755e-03 | validation loss: 1.5832e-03\n",
      "Epoch: 245810 | training loss: 1.9417e-03 | validation loss: 1.5251e-03\n",
      "Epoch: 245820 | training loss: 1.9286e-03 | validation loss: 1.5342e-03\n",
      "Epoch: 245830 | training loss: 1.9316e-03 | validation loss: 1.5410e-03\n",
      "Epoch: 245840 | training loss: 1.9284e-03 | validation loss: 1.5322e-03\n",
      "Epoch: 245850 | training loss: 1.9293e-03 | validation loss: 1.5293e-03\n",
      "Epoch: 245860 | training loss: 1.9347e-03 | validation loss: 1.5257e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 245870 | training loss: 2.0430e-03 | validation loss: 1.5457e-03\n",
      "Epoch: 245880 | training loss: 2.5670e-03 | validation loss: 1.7455e-03\n",
      "Epoch: 245890 | training loss: 2.1130e-03 | validation loss: 1.6687e-03\n",
      "Epoch: 245900 | training loss: 1.9623e-03 | validation loss: 1.5291e-03\n",
      "Epoch: 245910 | training loss: 1.9323e-03 | validation loss: 1.5422e-03\n",
      "Epoch: 245920 | training loss: 1.9291e-03 | validation loss: 1.5290e-03\n",
      "Epoch: 245930 | training loss: 1.9291e-03 | validation loss: 1.5362e-03\n",
      "Epoch: 245940 | training loss: 1.9295e-03 | validation loss: 1.5293e-03\n",
      "Epoch: 245950 | training loss: 1.9289e-03 | validation loss: 1.5357e-03\n",
      "Epoch: 245960 | training loss: 1.9282e-03 | validation loss: 1.5330e-03\n",
      "Epoch: 245970 | training loss: 1.9284e-03 | validation loss: 1.5311e-03\n",
      "Epoch: 245980 | training loss: 1.9286e-03 | validation loss: 1.5304e-03\n",
      "Epoch: 245990 | training loss: 1.9313e-03 | validation loss: 1.5276e-03\n",
      "Epoch: 246000 | training loss: 2.0027e-03 | validation loss: 1.5375e-03\n",
      "Epoch: 246010 | training loss: 2.9260e-03 | validation loss: 1.9046e-03\n",
      "Epoch: 246020 | training loss: 2.1971e-03 | validation loss: 1.7192e-03\n",
      "Epoch: 246030 | training loss: 1.9369e-03 | validation loss: 1.5307e-03\n",
      "Epoch: 246040 | training loss: 1.9424e-03 | validation loss: 1.5223e-03\n",
      "Epoch: 246050 | training loss: 1.9422e-03 | validation loss: 1.5548e-03\n",
      "Epoch: 246060 | training loss: 1.9333e-03 | validation loss: 1.5256e-03\n",
      "Epoch: 246070 | training loss: 1.9298e-03 | validation loss: 1.5388e-03\n",
      "Epoch: 246080 | training loss: 1.9288e-03 | validation loss: 1.5288e-03\n",
      "Epoch: 246090 | training loss: 1.9284e-03 | validation loss: 1.5352e-03\n",
      "Epoch: 246100 | training loss: 1.9280e-03 | validation loss: 1.5316e-03\n",
      "Epoch: 246110 | training loss: 1.9280e-03 | validation loss: 1.5316e-03\n",
      "Epoch: 246120 | training loss: 1.9280e-03 | validation loss: 1.5325e-03\n",
      "Epoch: 246130 | training loss: 1.9280e-03 | validation loss: 1.5325e-03\n",
      "Epoch: 246140 | training loss: 1.9303e-03 | validation loss: 1.5318e-03\n",
      "Epoch: 246150 | training loss: 2.0834e-03 | validation loss: 1.6124e-03\n",
      "Epoch: 246160 | training loss: 1.9764e-03 | validation loss: 1.5894e-03\n",
      "Epoch: 246170 | training loss: 2.0720e-03 | validation loss: 1.6271e-03\n",
      "Epoch: 246180 | training loss: 2.4747e-03 | validation loss: 1.8817e-03\n",
      "Epoch: 246190 | training loss: 2.0695e-03 | validation loss: 1.5523e-03\n",
      "Epoch: 246200 | training loss: 1.9781e-03 | validation loss: 1.5903e-03\n",
      "Epoch: 246210 | training loss: 1.9422e-03 | validation loss: 1.5279e-03\n",
      "Epoch: 246220 | training loss: 1.9286e-03 | validation loss: 1.5362e-03\n",
      "Epoch: 246230 | training loss: 1.9293e-03 | validation loss: 1.5360e-03\n",
      "Epoch: 246240 | training loss: 1.9296e-03 | validation loss: 1.5269e-03\n",
      "Epoch: 246250 | training loss: 1.9282e-03 | validation loss: 1.5299e-03\n",
      "Epoch: 246260 | training loss: 1.9277e-03 | validation loss: 1.5315e-03\n",
      "Epoch: 246270 | training loss: 1.9277e-03 | validation loss: 1.5319e-03\n",
      "Epoch: 246280 | training loss: 1.9278e-03 | validation loss: 1.5330e-03\n",
      "Epoch: 246290 | training loss: 1.9311e-03 | validation loss: 1.5410e-03\n",
      "Epoch: 246300 | training loss: 2.2790e-03 | validation loss: 1.7786e-03\n",
      "Epoch: 246310 | training loss: 2.0022e-03 | validation loss: 1.5311e-03\n",
      "Epoch: 246320 | training loss: 2.1672e-03 | validation loss: 1.6938e-03\n",
      "Epoch: 246330 | training loss: 2.0020e-03 | validation loss: 1.6057e-03\n",
      "Epoch: 246340 | training loss: 1.9450e-03 | validation loss: 1.5586e-03\n",
      "Epoch: 246350 | training loss: 1.9397e-03 | validation loss: 1.5393e-03\n",
      "Epoch: 246360 | training loss: 1.9280e-03 | validation loss: 1.5290e-03\n",
      "Epoch: 246370 | training loss: 1.9295e-03 | validation loss: 1.5314e-03\n",
      "Epoch: 246380 | training loss: 1.9276e-03 | validation loss: 1.5308e-03\n",
      "Epoch: 246390 | training loss: 1.9278e-03 | validation loss: 1.5323e-03\n",
      "Epoch: 246400 | training loss: 1.9276e-03 | validation loss: 1.5325e-03\n",
      "Epoch: 246410 | training loss: 1.9275e-03 | validation loss: 1.5310e-03\n",
      "Epoch: 246420 | training loss: 1.9275e-03 | validation loss: 1.5318e-03\n",
      "Epoch: 246430 | training loss: 1.9275e-03 | validation loss: 1.5316e-03\n",
      "Epoch: 246440 | training loss: 1.9274e-03 | validation loss: 1.5315e-03\n",
      "Epoch: 246450 | training loss: 1.9274e-03 | validation loss: 1.5316e-03\n",
      "Epoch: 246460 | training loss: 1.9274e-03 | validation loss: 1.5316e-03\n",
      "Epoch: 246470 | training loss: 1.9274e-03 | validation loss: 1.5316e-03\n",
      "Epoch: 246480 | training loss: 1.9274e-03 | validation loss: 1.5315e-03\n",
      "Epoch: 246490 | training loss: 1.9274e-03 | validation loss: 1.5315e-03\n",
      "Epoch: 246500 | training loss: 1.9273e-03 | validation loss: 1.5314e-03\n",
      "Epoch: 246510 | training loss: 1.9274e-03 | validation loss: 1.5310e-03\n",
      "Epoch: 246520 | training loss: 1.9317e-03 | validation loss: 1.5291e-03\n",
      "Epoch: 246530 | training loss: 2.5276e-03 | validation loss: 1.7818e-03\n",
      "Epoch: 246540 | training loss: 2.4482e-03 | validation loss: 1.8466e-03\n",
      "Epoch: 246550 | training loss: 2.2116e-03 | validation loss: 1.6461e-03\n",
      "Epoch: 246560 | training loss: 2.0364e-03 | validation loss: 1.5567e-03\n",
      "Epoch: 246570 | training loss: 1.9498e-03 | validation loss: 1.5378e-03\n",
      "Epoch: 246580 | training loss: 1.9391e-03 | validation loss: 1.5436e-03\n",
      "Epoch: 246590 | training loss: 1.9286e-03 | validation loss: 1.5310e-03\n",
      "Epoch: 246600 | training loss: 1.9285e-03 | validation loss: 1.5266e-03\n",
      "Epoch: 246610 | training loss: 1.9274e-03 | validation loss: 1.5313e-03\n",
      "Epoch: 246620 | training loss: 1.9273e-03 | validation loss: 1.5316e-03\n",
      "Epoch: 246630 | training loss: 1.9272e-03 | validation loss: 1.5299e-03\n",
      "Epoch: 246640 | training loss: 1.9272e-03 | validation loss: 1.5315e-03\n",
      "Epoch: 246650 | training loss: 1.9271e-03 | validation loss: 1.5308e-03\n",
      "Epoch: 246660 | training loss: 1.9271e-03 | validation loss: 1.5314e-03\n",
      "Epoch: 246670 | training loss: 1.9271e-03 | validation loss: 1.5311e-03\n",
      "Epoch: 246680 | training loss: 1.9271e-03 | validation loss: 1.5312e-03\n",
      "Epoch: 246690 | training loss: 1.9270e-03 | validation loss: 1.5311e-03\n",
      "Epoch: 246700 | training loss: 1.9270e-03 | validation loss: 1.5311e-03\n",
      "Epoch: 246710 | training loss: 1.9270e-03 | validation loss: 1.5310e-03\n",
      "Epoch: 246720 | training loss: 1.9270e-03 | validation loss: 1.5309e-03\n",
      "Epoch: 246730 | training loss: 1.9270e-03 | validation loss: 1.5305e-03\n",
      "Epoch: 246740 | training loss: 1.9280e-03 | validation loss: 1.5271e-03\n",
      "Epoch: 246750 | training loss: 2.0547e-03 | validation loss: 1.5453e-03\n",
      "Epoch: 246760 | training loss: 2.4238e-03 | validation loss: 1.6963e-03\n",
      "Epoch: 246770 | training loss: 2.2561e-03 | validation loss: 1.6147e-03\n",
      "Epoch: 246780 | training loss: 2.0217e-03 | validation loss: 1.5541e-03\n",
      "Epoch: 246790 | training loss: 1.9779e-03 | validation loss: 1.5487e-03\n",
      "Epoch: 246800 | training loss: 1.9388e-03 | validation loss: 1.5298e-03\n",
      "Epoch: 246810 | training loss: 1.9289e-03 | validation loss: 1.5250e-03\n",
      "Epoch: 246820 | training loss: 1.9274e-03 | validation loss: 1.5296e-03\n",
      "Epoch: 246830 | training loss: 1.9273e-03 | validation loss: 1.5344e-03\n",
      "Epoch: 246840 | training loss: 1.9270e-03 | validation loss: 1.5333e-03\n",
      "Epoch: 246850 | training loss: 1.9268e-03 | validation loss: 1.5306e-03\n",
      "Epoch: 246860 | training loss: 1.9268e-03 | validation loss: 1.5304e-03\n",
      "Epoch: 246870 | training loss: 1.9268e-03 | validation loss: 1.5304e-03\n",
      "Epoch: 246880 | training loss: 1.9267e-03 | validation loss: 1.5310e-03\n",
      "Epoch: 246890 | training loss: 1.9267e-03 | validation loss: 1.5306e-03\n",
      "Epoch: 246900 | training loss: 1.9267e-03 | validation loss: 1.5306e-03\n",
      "Epoch: 246910 | training loss: 1.9267e-03 | validation loss: 1.5307e-03\n",
      "Epoch: 246920 | training loss: 1.9267e-03 | validation loss: 1.5306e-03\n",
      "Epoch: 246930 | training loss: 1.9266e-03 | validation loss: 1.5306e-03\n",
      "Epoch: 246940 | training loss: 1.9266e-03 | validation loss: 1.5306e-03\n",
      "Epoch: 246950 | training loss: 1.9266e-03 | validation loss: 1.5307e-03\n",
      "Epoch: 246960 | training loss: 1.9266e-03 | validation loss: 1.5309e-03\n",
      "Epoch: 246970 | training loss: 1.9276e-03 | validation loss: 1.5335e-03\n",
      "Epoch: 246980 | training loss: 2.0584e-03 | validation loss: 1.6257e-03\n",
      "Epoch: 246990 | training loss: 2.5328e-03 | validation loss: 1.9402e-03\n",
      "Epoch: 247000 | training loss: 2.2221e-03 | validation loss: 1.7271e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 247010 | training loss: 1.9581e-03 | validation loss: 1.5418e-03\n",
      "Epoch: 247020 | training loss: 1.9545e-03 | validation loss: 1.5206e-03\n",
      "Epoch: 247030 | training loss: 1.9410e-03 | validation loss: 1.5203e-03\n",
      "Epoch: 247040 | training loss: 1.9281e-03 | validation loss: 1.5274e-03\n",
      "Epoch: 247050 | training loss: 1.9286e-03 | validation loss: 1.5344e-03\n",
      "Epoch: 247060 | training loss: 1.9270e-03 | validation loss: 1.5306e-03\n",
      "Epoch: 247070 | training loss: 1.9268e-03 | validation loss: 1.5279e-03\n",
      "Epoch: 247080 | training loss: 1.9264e-03 | validation loss: 1.5303e-03\n",
      "Epoch: 247090 | training loss: 1.9264e-03 | validation loss: 1.5310e-03\n",
      "Epoch: 247100 | training loss: 1.9264e-03 | validation loss: 1.5301e-03\n",
      "Epoch: 247110 | training loss: 1.9264e-03 | validation loss: 1.5306e-03\n",
      "Epoch: 247120 | training loss: 1.9264e-03 | validation loss: 1.5301e-03\n",
      "Epoch: 247130 | training loss: 1.9263e-03 | validation loss: 1.5304e-03\n",
      "Epoch: 247140 | training loss: 1.9263e-03 | validation loss: 1.5302e-03\n",
      "Epoch: 247150 | training loss: 1.9263e-03 | validation loss: 1.5303e-03\n",
      "Epoch: 247160 | training loss: 1.9263e-03 | validation loss: 1.5303e-03\n",
      "Epoch: 247170 | training loss: 1.9263e-03 | validation loss: 1.5302e-03\n",
      "Epoch: 247180 | training loss: 1.9262e-03 | validation loss: 1.5302e-03\n",
      "Epoch: 247190 | training loss: 1.9262e-03 | validation loss: 1.5302e-03\n",
      "Epoch: 247200 | training loss: 1.9262e-03 | validation loss: 1.5301e-03\n",
      "Epoch: 247210 | training loss: 1.9262e-03 | validation loss: 1.5294e-03\n",
      "Epoch: 247220 | training loss: 1.9309e-03 | validation loss: 1.5232e-03\n",
      "Epoch: 247230 | training loss: 2.9699e-03 | validation loss: 1.9061e-03\n",
      "Epoch: 247240 | training loss: 2.8600e-03 | validation loss: 2.1264e-03\n",
      "Epoch: 247250 | training loss: 2.2055e-03 | validation loss: 1.7779e-03\n",
      "Epoch: 247260 | training loss: 1.9948e-03 | validation loss: 1.6103e-03\n",
      "Epoch: 247270 | training loss: 1.9343e-03 | validation loss: 1.5440e-03\n",
      "Epoch: 247280 | training loss: 1.9262e-03 | validation loss: 1.5290e-03\n",
      "Epoch: 247290 | training loss: 1.9267e-03 | validation loss: 1.5276e-03\n",
      "Epoch: 247300 | training loss: 1.9265e-03 | validation loss: 1.5277e-03\n",
      "Epoch: 247310 | training loss: 1.9262e-03 | validation loss: 1.5282e-03\n",
      "Epoch: 247320 | training loss: 1.9261e-03 | validation loss: 1.5292e-03\n",
      "Epoch: 247330 | training loss: 1.9260e-03 | validation loss: 1.5300e-03\n",
      "Epoch: 247340 | training loss: 1.9260e-03 | validation loss: 1.5302e-03\n",
      "Epoch: 247350 | training loss: 1.9260e-03 | validation loss: 1.5302e-03\n",
      "Epoch: 247360 | training loss: 1.9260e-03 | validation loss: 1.5300e-03\n",
      "Epoch: 247370 | training loss: 1.9259e-03 | validation loss: 1.5297e-03\n",
      "Epoch: 247380 | training loss: 1.9259e-03 | validation loss: 1.5296e-03\n",
      "Epoch: 247390 | training loss: 1.9259e-03 | validation loss: 1.5296e-03\n",
      "Epoch: 247400 | training loss: 1.9259e-03 | validation loss: 1.5297e-03\n",
      "Epoch: 247410 | training loss: 1.9259e-03 | validation loss: 1.5296e-03\n",
      "Epoch: 247420 | training loss: 1.9259e-03 | validation loss: 1.5296e-03\n",
      "Epoch: 247430 | training loss: 1.9258e-03 | validation loss: 1.5296e-03\n",
      "Epoch: 247440 | training loss: 1.9258e-03 | validation loss: 1.5296e-03\n",
      "Epoch: 247450 | training loss: 1.9258e-03 | validation loss: 1.5296e-03\n",
      "Epoch: 247460 | training loss: 1.9260e-03 | validation loss: 1.5302e-03\n",
      "Epoch: 247470 | training loss: 1.9638e-03 | validation loss: 1.5578e-03\n",
      "Epoch: 247480 | training loss: 3.3211e-03 | validation loss: 2.3250e-03\n",
      "Epoch: 247490 | training loss: 2.1160e-03 | validation loss: 1.5838e-03\n",
      "Epoch: 247500 | training loss: 2.0657e-03 | validation loss: 1.5461e-03\n",
      "Epoch: 247510 | training loss: 1.9390e-03 | validation loss: 1.5307e-03\n",
      "Epoch: 247520 | training loss: 1.9422e-03 | validation loss: 1.5467e-03\n",
      "Epoch: 247530 | training loss: 1.9273e-03 | validation loss: 1.5248e-03\n",
      "Epoch: 247540 | training loss: 1.9273e-03 | validation loss: 1.5240e-03\n",
      "Epoch: 247550 | training loss: 1.9266e-03 | validation loss: 1.5311e-03\n",
      "Epoch: 247560 | training loss: 1.9259e-03 | validation loss: 1.5275e-03\n",
      "Epoch: 247570 | training loss: 1.9257e-03 | validation loss: 1.5289e-03\n",
      "Epoch: 247580 | training loss: 1.9256e-03 | validation loss: 1.5293e-03\n",
      "Epoch: 247590 | training loss: 1.9256e-03 | validation loss: 1.5295e-03\n",
      "Epoch: 247600 | training loss: 1.9256e-03 | validation loss: 1.5293e-03\n",
      "Epoch: 247610 | training loss: 1.9256e-03 | validation loss: 1.5294e-03\n",
      "Epoch: 247620 | training loss: 1.9255e-03 | validation loss: 1.5291e-03\n",
      "Epoch: 247630 | training loss: 1.9255e-03 | validation loss: 1.5291e-03\n",
      "Epoch: 247640 | training loss: 1.9255e-03 | validation loss: 1.5291e-03\n",
      "Epoch: 247650 | training loss: 1.9255e-03 | validation loss: 1.5289e-03\n",
      "Epoch: 247660 | training loss: 1.9256e-03 | validation loss: 1.5279e-03\n",
      "Epoch: 247670 | training loss: 1.9335e-03 | validation loss: 1.5225e-03\n",
      "Epoch: 247680 | training loss: 2.7145e-03 | validation loss: 1.8054e-03\n",
      "Epoch: 247690 | training loss: 2.5988e-03 | validation loss: 1.9526e-03\n",
      "Epoch: 247700 | training loss: 1.9346e-03 | validation loss: 1.5439e-03\n",
      "Epoch: 247710 | training loss: 1.9897e-03 | validation loss: 1.5393e-03\n",
      "Epoch: 247720 | training loss: 1.9568e-03 | validation loss: 1.5237e-03\n",
      "Epoch: 247730 | training loss: 1.9275e-03 | validation loss: 1.5236e-03\n",
      "Epoch: 247740 | training loss: 1.9287e-03 | validation loss: 1.5345e-03\n",
      "Epoch: 247750 | training loss: 1.9260e-03 | validation loss: 1.5333e-03\n",
      "Epoch: 247760 | training loss: 1.9258e-03 | validation loss: 1.5288e-03\n",
      "Epoch: 247770 | training loss: 1.9254e-03 | validation loss: 1.5281e-03\n",
      "Epoch: 247780 | training loss: 1.9253e-03 | validation loss: 1.5298e-03\n",
      "Epoch: 247790 | training loss: 1.9253e-03 | validation loss: 1.5291e-03\n",
      "Epoch: 247800 | training loss: 1.9253e-03 | validation loss: 1.5289e-03\n",
      "Epoch: 247810 | training loss: 1.9252e-03 | validation loss: 1.5292e-03\n",
      "Epoch: 247820 | training loss: 1.9252e-03 | validation loss: 1.5290e-03\n",
      "Epoch: 247830 | training loss: 1.9252e-03 | validation loss: 1.5291e-03\n",
      "Epoch: 247840 | training loss: 1.9252e-03 | validation loss: 1.5290e-03\n",
      "Epoch: 247850 | training loss: 1.9252e-03 | validation loss: 1.5290e-03\n",
      "Epoch: 247860 | training loss: 1.9252e-03 | validation loss: 1.5290e-03\n",
      "Epoch: 247870 | training loss: 1.9251e-03 | validation loss: 1.5289e-03\n",
      "Epoch: 247880 | training loss: 1.9251e-03 | validation loss: 1.5289e-03\n",
      "Epoch: 247890 | training loss: 1.9251e-03 | validation loss: 1.5288e-03\n",
      "Epoch: 247900 | training loss: 1.9256e-03 | validation loss: 1.5282e-03\n",
      "Epoch: 247910 | training loss: 1.9788e-03 | validation loss: 1.5485e-03\n",
      "Epoch: 247920 | training loss: 3.4145e-03 | validation loss: 2.1138e-03\n",
      "Epoch: 247930 | training loss: 2.3812e-03 | validation loss: 1.9197e-03\n",
      "Epoch: 247940 | training loss: 2.0440e-03 | validation loss: 1.6500e-03\n",
      "Epoch: 247950 | training loss: 1.9299e-03 | validation loss: 1.5238e-03\n",
      "Epoch: 247960 | training loss: 1.9394e-03 | validation loss: 1.5184e-03\n",
      "Epoch: 247970 | training loss: 1.9297e-03 | validation loss: 1.5284e-03\n",
      "Epoch: 247980 | training loss: 1.9280e-03 | validation loss: 1.5311e-03\n",
      "Epoch: 247990 | training loss: 1.9260e-03 | validation loss: 1.5248e-03\n",
      "Epoch: 248000 | training loss: 1.9251e-03 | validation loss: 1.5278e-03\n",
      "Epoch: 248010 | training loss: 1.9250e-03 | validation loss: 1.5284e-03\n",
      "Epoch: 248020 | training loss: 1.9250e-03 | validation loss: 1.5276e-03\n",
      "Epoch: 248030 | training loss: 1.9249e-03 | validation loss: 1.5285e-03\n",
      "Epoch: 248040 | training loss: 1.9249e-03 | validation loss: 1.5282e-03\n",
      "Epoch: 248050 | training loss: 1.9249e-03 | validation loss: 1.5284e-03\n",
      "Epoch: 248060 | training loss: 1.9248e-03 | validation loss: 1.5285e-03\n",
      "Epoch: 248070 | training loss: 1.9248e-03 | validation loss: 1.5284e-03\n",
      "Epoch: 248080 | training loss: 1.9248e-03 | validation loss: 1.5283e-03\n",
      "Epoch: 248090 | training loss: 1.9248e-03 | validation loss: 1.5283e-03\n",
      "Epoch: 248100 | training loss: 1.9248e-03 | validation loss: 1.5279e-03\n",
      "Epoch: 248110 | training loss: 1.9252e-03 | validation loss: 1.5258e-03\n",
      "Epoch: 248120 | training loss: 1.9637e-03 | validation loss: 1.5220e-03\n",
      "Epoch: 248130 | training loss: 3.7817e-03 | validation loss: 2.2580e-03\n",
      "Epoch: 248140 | training loss: 1.9844e-03 | validation loss: 1.5677e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 248150 | training loss: 2.0978e-03 | validation loss: 1.6498e-03\n",
      "Epoch: 248160 | training loss: 1.9681e-03 | validation loss: 1.5696e-03\n",
      "Epoch: 248170 | training loss: 1.9250e-03 | validation loss: 1.5297e-03\n",
      "Epoch: 248180 | training loss: 1.9324e-03 | validation loss: 1.5262e-03\n",
      "Epoch: 248190 | training loss: 1.9260e-03 | validation loss: 1.5278e-03\n",
      "Epoch: 248200 | training loss: 1.9253e-03 | validation loss: 1.5321e-03\n",
      "Epoch: 248210 | training loss: 1.9247e-03 | validation loss: 1.5290e-03\n",
      "Epoch: 248220 | training loss: 1.9248e-03 | validation loss: 1.5266e-03\n",
      "Epoch: 248230 | training loss: 1.9246e-03 | validation loss: 1.5287e-03\n",
      "Epoch: 248240 | training loss: 1.9246e-03 | validation loss: 1.5285e-03\n",
      "Epoch: 248250 | training loss: 1.9245e-03 | validation loss: 1.5280e-03\n",
      "Epoch: 248260 | training loss: 1.9245e-03 | validation loss: 1.5283e-03\n",
      "Epoch: 248270 | training loss: 1.9245e-03 | validation loss: 1.5282e-03\n",
      "Epoch: 248280 | training loss: 1.9245e-03 | validation loss: 1.5281e-03\n",
      "Epoch: 248290 | training loss: 1.9245e-03 | validation loss: 1.5282e-03\n",
      "Epoch: 248300 | training loss: 1.9245e-03 | validation loss: 1.5281e-03\n",
      "Epoch: 248310 | training loss: 1.9244e-03 | validation loss: 1.5281e-03\n",
      "Epoch: 248320 | training loss: 1.9244e-03 | validation loss: 1.5281e-03\n",
      "Epoch: 248330 | training loss: 1.9244e-03 | validation loss: 1.5280e-03\n",
      "Epoch: 248340 | training loss: 1.9244e-03 | validation loss: 1.5276e-03\n",
      "Epoch: 248350 | training loss: 1.9256e-03 | validation loss: 1.5248e-03\n",
      "Epoch: 248360 | training loss: 2.0954e-03 | validation loss: 1.5671e-03\n",
      "Epoch: 248370 | training loss: 2.1355e-03 | validation loss: 1.5774e-03\n",
      "Epoch: 248380 | training loss: 2.3911e-03 | validation loss: 1.6825e-03\n",
      "Epoch: 248390 | training loss: 2.0897e-03 | validation loss: 1.5680e-03\n",
      "Epoch: 248400 | training loss: 1.9640e-03 | validation loss: 1.5282e-03\n",
      "Epoch: 248410 | training loss: 1.9278e-03 | validation loss: 1.5239e-03\n",
      "Epoch: 248420 | training loss: 1.9248e-03 | validation loss: 1.5303e-03\n",
      "Epoch: 248430 | training loss: 1.9265e-03 | validation loss: 1.5338e-03\n",
      "Epoch: 248440 | training loss: 1.9250e-03 | validation loss: 1.5310e-03\n",
      "Epoch: 248450 | training loss: 1.9242e-03 | validation loss: 1.5279e-03\n",
      "Epoch: 248460 | training loss: 1.9243e-03 | validation loss: 1.5270e-03\n",
      "Epoch: 248470 | training loss: 1.9242e-03 | validation loss: 1.5280e-03\n",
      "Epoch: 248480 | training loss: 1.9242e-03 | validation loss: 1.5283e-03\n",
      "Epoch: 248490 | training loss: 1.9242e-03 | validation loss: 1.5278e-03\n",
      "Epoch: 248500 | training loss: 1.9241e-03 | validation loss: 1.5278e-03\n",
      "Epoch: 248510 | training loss: 1.9241e-03 | validation loss: 1.5279e-03\n",
      "Epoch: 248520 | training loss: 1.9241e-03 | validation loss: 1.5278e-03\n",
      "Epoch: 248530 | training loss: 1.9241e-03 | validation loss: 1.5278e-03\n",
      "Epoch: 248540 | training loss: 1.9241e-03 | validation loss: 1.5277e-03\n",
      "Epoch: 248550 | training loss: 1.9241e-03 | validation loss: 1.5272e-03\n",
      "Epoch: 248560 | training loss: 1.9270e-03 | validation loss: 1.5230e-03\n",
      "Epoch: 248570 | training loss: 2.4084e-03 | validation loss: 1.7765e-03\n",
      "Epoch: 248580 | training loss: 2.1627e-03 | validation loss: 1.7210e-03\n",
      "Epoch: 248590 | training loss: 2.0289e-03 | validation loss: 1.6291e-03\n",
      "Epoch: 248600 | training loss: 1.9613e-03 | validation loss: 1.5571e-03\n",
      "Epoch: 248610 | training loss: 1.9374e-03 | validation loss: 1.5501e-03\n",
      "Epoch: 248620 | training loss: 1.9273e-03 | validation loss: 1.5268e-03\n",
      "Epoch: 248630 | training loss: 1.9240e-03 | validation loss: 1.5269e-03\n",
      "Epoch: 248640 | training loss: 1.9245e-03 | validation loss: 1.5291e-03\n",
      "Epoch: 248650 | training loss: 1.9250e-03 | validation loss: 1.5306e-03\n",
      "Epoch: 248660 | training loss: 1.9298e-03 | validation loss: 1.5397e-03\n",
      "Epoch: 248670 | training loss: 2.0455e-03 | validation loss: 1.6321e-03\n",
      "Epoch: 248680 | training loss: 2.5090e-03 | validation loss: 1.9076e-03\n",
      "Epoch: 248690 | training loss: 2.0588e-03 | validation loss: 1.5492e-03\n",
      "Epoch: 248700 | training loss: 1.9332e-03 | validation loss: 1.5440e-03\n",
      "Epoch: 248710 | training loss: 1.9241e-03 | validation loss: 1.5291e-03\n",
      "Epoch: 248720 | training loss: 1.9250e-03 | validation loss: 1.5230e-03\n",
      "Epoch: 248730 | training loss: 1.9241e-03 | validation loss: 1.5295e-03\n",
      "Epoch: 248740 | training loss: 1.9238e-03 | validation loss: 1.5276e-03\n",
      "Epoch: 248750 | training loss: 1.9241e-03 | validation loss: 1.5250e-03\n",
      "Epoch: 248760 | training loss: 1.9240e-03 | validation loss: 1.5292e-03\n",
      "Epoch: 248770 | training loss: 1.9238e-03 | validation loss: 1.5281e-03\n",
      "Epoch: 248780 | training loss: 1.9237e-03 | validation loss: 1.5270e-03\n",
      "Epoch: 248790 | training loss: 1.9237e-03 | validation loss: 1.5264e-03\n",
      "Epoch: 248800 | training loss: 1.9242e-03 | validation loss: 1.5245e-03\n",
      "Epoch: 248810 | training loss: 1.9481e-03 | validation loss: 1.5200e-03\n",
      "Epoch: 248820 | training loss: 3.1744e-03 | validation loss: 1.9991e-03\n",
      "Epoch: 248830 | training loss: 2.4745e-03 | validation loss: 1.8780e-03\n",
      "Epoch: 248840 | training loss: 1.9448e-03 | validation loss: 1.5619e-03\n",
      "Epoch: 248850 | training loss: 1.9776e-03 | validation loss: 1.5304e-03\n",
      "Epoch: 248860 | training loss: 1.9322e-03 | validation loss: 1.5177e-03\n",
      "Epoch: 248870 | training loss: 1.9293e-03 | validation loss: 1.5366e-03\n",
      "Epoch: 248880 | training loss: 1.9252e-03 | validation loss: 1.5325e-03\n",
      "Epoch: 248890 | training loss: 1.9241e-03 | validation loss: 1.5235e-03\n",
      "Epoch: 248900 | training loss: 1.9236e-03 | validation loss: 1.5279e-03\n",
      "Epoch: 248910 | training loss: 1.9236e-03 | validation loss: 1.5275e-03\n",
      "Epoch: 248920 | training loss: 1.9235e-03 | validation loss: 1.5264e-03\n",
      "Epoch: 248930 | training loss: 1.9235e-03 | validation loss: 1.5273e-03\n",
      "Epoch: 248940 | training loss: 1.9234e-03 | validation loss: 1.5267e-03\n",
      "Epoch: 248950 | training loss: 1.9234e-03 | validation loss: 1.5269e-03\n",
      "Epoch: 248960 | training loss: 1.9234e-03 | validation loss: 1.5270e-03\n",
      "Epoch: 248970 | training loss: 1.9234e-03 | validation loss: 1.5268e-03\n",
      "Epoch: 248980 | training loss: 1.9234e-03 | validation loss: 1.5267e-03\n",
      "Epoch: 248990 | training loss: 1.9234e-03 | validation loss: 1.5267e-03\n",
      "Epoch: 249000 | training loss: 1.9241e-03 | validation loss: 1.5268e-03\n",
      "Epoch: 249010 | training loss: 1.9807e-03 | validation loss: 1.5577e-03\n",
      "Epoch: 249020 | training loss: 2.7778e-03 | validation loss: 1.8506e-03\n",
      "Epoch: 249030 | training loss: 2.3629e-03 | validation loss: 1.8945e-03\n",
      "Epoch: 249040 | training loss: 2.0751e-03 | validation loss: 1.6091e-03\n",
      "Epoch: 249050 | training loss: 1.9572e-03 | validation loss: 1.5674e-03\n",
      "Epoch: 249060 | training loss: 1.9386e-03 | validation loss: 1.5526e-03\n",
      "Epoch: 249070 | training loss: 1.9306e-03 | validation loss: 1.5287e-03\n",
      "Epoch: 249080 | training loss: 1.9260e-03 | validation loss: 1.5359e-03\n",
      "Epoch: 249090 | training loss: 1.9241e-03 | validation loss: 1.5256e-03\n",
      "Epoch: 249100 | training loss: 1.9234e-03 | validation loss: 1.5286e-03\n",
      "Epoch: 249110 | training loss: 1.9232e-03 | validation loss: 1.5258e-03\n",
      "Epoch: 249120 | training loss: 1.9232e-03 | validation loss: 1.5255e-03\n",
      "Epoch: 249130 | training loss: 1.9231e-03 | validation loss: 1.5265e-03\n",
      "Epoch: 249140 | training loss: 1.9231e-03 | validation loss: 1.5270e-03\n",
      "Epoch: 249150 | training loss: 1.9232e-03 | validation loss: 1.5276e-03\n",
      "Epoch: 249160 | training loss: 1.9242e-03 | validation loss: 1.5311e-03\n",
      "Epoch: 249170 | training loss: 1.9702e-03 | validation loss: 1.5765e-03\n",
      "Epoch: 249180 | training loss: 3.2528e-03 | validation loss: 2.3132e-03\n",
      "Epoch: 249190 | training loss: 2.2125e-03 | validation loss: 1.6115e-03\n",
      "Epoch: 249200 | training loss: 1.9555e-03 | validation loss: 1.5199e-03\n",
      "Epoch: 249210 | training loss: 1.9699e-03 | validation loss: 1.5699e-03\n",
      "Epoch: 249220 | training loss: 1.9239e-03 | validation loss: 1.5272e-03\n",
      "Epoch: 249230 | training loss: 1.9297e-03 | validation loss: 1.5202e-03\n",
      "Epoch: 249240 | training loss: 1.9246e-03 | validation loss: 1.5323e-03\n",
      "Epoch: 249250 | training loss: 1.9230e-03 | validation loss: 1.5258e-03\n",
      "Epoch: 249260 | training loss: 1.9229e-03 | validation loss: 1.5256e-03\n",
      "Epoch: 249270 | training loss: 1.9229e-03 | validation loss: 1.5267e-03\n",
      "Epoch: 249280 | training loss: 1.9229e-03 | validation loss: 1.5262e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 249290 | training loss: 1.9229e-03 | validation loss: 1.5259e-03\n",
      "Epoch: 249300 | training loss: 1.9229e-03 | validation loss: 1.5266e-03\n",
      "Epoch: 249310 | training loss: 1.9228e-03 | validation loss: 1.5261e-03\n",
      "Epoch: 249320 | training loss: 1.9228e-03 | validation loss: 1.5260e-03\n",
      "Epoch: 249330 | training loss: 1.9228e-03 | validation loss: 1.5258e-03\n",
      "Epoch: 249340 | training loss: 1.9228e-03 | validation loss: 1.5253e-03\n",
      "Epoch: 249350 | training loss: 1.9241e-03 | validation loss: 1.5226e-03\n",
      "Epoch: 249360 | training loss: 2.0128e-03 | validation loss: 1.5361e-03\n",
      "Epoch: 249370 | training loss: 3.1547e-03 | validation loss: 2.0028e-03\n",
      "Epoch: 249380 | training loss: 1.9471e-03 | validation loss: 1.5157e-03\n",
      "Epoch: 249390 | training loss: 2.0312e-03 | validation loss: 1.6154e-03\n",
      "Epoch: 249400 | training loss: 1.9607e-03 | validation loss: 1.5702e-03\n",
      "Epoch: 249410 | training loss: 1.9247e-03 | validation loss: 1.5213e-03\n",
      "Epoch: 249420 | training loss: 1.9296e-03 | validation loss: 1.5188e-03\n",
      "Epoch: 249430 | training loss: 1.9229e-03 | validation loss: 1.5285e-03\n",
      "Epoch: 249440 | training loss: 1.9233e-03 | validation loss: 1.5293e-03\n",
      "Epoch: 249450 | training loss: 1.9229e-03 | validation loss: 1.5242e-03\n",
      "Epoch: 249460 | training loss: 1.9226e-03 | validation loss: 1.5265e-03\n",
      "Epoch: 249470 | training loss: 1.9226e-03 | validation loss: 1.5261e-03\n",
      "Epoch: 249480 | training loss: 1.9226e-03 | validation loss: 1.5258e-03\n",
      "Epoch: 249490 | training loss: 1.9225e-03 | validation loss: 1.5261e-03\n",
      "Epoch: 249500 | training loss: 1.9225e-03 | validation loss: 1.5259e-03\n",
      "Epoch: 249510 | training loss: 1.9225e-03 | validation loss: 1.5259e-03\n",
      "Epoch: 249520 | training loss: 1.9225e-03 | validation loss: 1.5264e-03\n",
      "Epoch: 249530 | training loss: 1.9236e-03 | validation loss: 1.5300e-03\n",
      "Epoch: 249540 | training loss: 2.0639e-03 | validation loss: 1.6611e-03\n",
      "Epoch: 249550 | training loss: 2.0195e-03 | validation loss: 1.5839e-03\n",
      "Epoch: 249560 | training loss: 1.9563e-03 | validation loss: 1.5655e-03\n",
      "Epoch: 249570 | training loss: 1.9623e-03 | validation loss: 1.5595e-03\n",
      "Epoch: 249580 | training loss: 1.9365e-03 | validation loss: 1.5316e-03\n",
      "Epoch: 249590 | training loss: 1.9441e-03 | validation loss: 1.5168e-03\n",
      "Epoch: 249600 | training loss: 2.1839e-03 | validation loss: 1.5847e-03\n",
      "Epoch: 249610 | training loss: 1.9967e-03 | validation loss: 1.5256e-03\n",
      "Epoch: 249620 | training loss: 1.9721e-03 | validation loss: 1.5768e-03\n",
      "Epoch: 249630 | training loss: 1.9510e-03 | validation loss: 1.5180e-03\n",
      "Epoch: 249640 | training loss: 1.9387e-03 | validation loss: 1.5496e-03\n",
      "Epoch: 249650 | training loss: 1.9265e-03 | validation loss: 1.5193e-03\n",
      "Epoch: 249660 | training loss: 1.9226e-03 | validation loss: 1.5231e-03\n",
      "Epoch: 249670 | training loss: 1.9235e-03 | validation loss: 1.5304e-03\n",
      "Epoch: 249680 | training loss: 1.9241e-03 | validation loss: 1.5318e-03\n",
      "Epoch: 249690 | training loss: 1.9298e-03 | validation loss: 1.5399e-03\n",
      "Epoch: 249700 | training loss: 2.0308e-03 | validation loss: 1.6190e-03\n",
      "Epoch: 249710 | training loss: 2.5279e-03 | validation loss: 1.9122e-03\n",
      "Epoch: 249720 | training loss: 2.1233e-03 | validation loss: 1.5728e-03\n",
      "Epoch: 249730 | training loss: 1.9880e-03 | validation loss: 1.5842e-03\n",
      "Epoch: 249740 | training loss: 1.9448e-03 | validation loss: 1.5188e-03\n",
      "Epoch: 249750 | training loss: 1.9321e-03 | validation loss: 1.5421e-03\n",
      "Epoch: 249760 | training loss: 1.9264e-03 | validation loss: 1.5195e-03\n",
      "Epoch: 249770 | training loss: 1.9226e-03 | validation loss: 1.5283e-03\n",
      "Epoch: 249780 | training loss: 1.9225e-03 | validation loss: 1.5277e-03\n",
      "Epoch: 249790 | training loss: 1.9222e-03 | validation loss: 1.5242e-03\n",
      "Epoch: 249800 | training loss: 1.9225e-03 | validation loss: 1.5229e-03\n",
      "Epoch: 249810 | training loss: 1.9252e-03 | validation loss: 1.5204e-03\n",
      "Epoch: 249820 | training loss: 1.9857e-03 | validation loss: 1.5275e-03\n",
      "Epoch: 249830 | training loss: 2.8605e-03 | validation loss: 1.8724e-03\n",
      "Epoch: 249840 | training loss: 2.2361e-03 | validation loss: 1.7397e-03\n",
      "Epoch: 249850 | training loss: 1.9769e-03 | validation loss: 1.5281e-03\n",
      "Epoch: 249860 | training loss: 1.9232e-03 | validation loss: 1.5233e-03\n",
      "Epoch: 249870 | training loss: 1.9256e-03 | validation loss: 1.5363e-03\n",
      "Epoch: 249880 | training loss: 1.9249e-03 | validation loss: 1.5187e-03\n",
      "Epoch: 249890 | training loss: 1.9231e-03 | validation loss: 1.5309e-03\n",
      "Epoch: 249900 | training loss: 1.9221e-03 | validation loss: 1.5231e-03\n",
      "Epoch: 249910 | training loss: 1.9219e-03 | validation loss: 1.5244e-03\n",
      "Epoch: 249920 | training loss: 1.9220e-03 | validation loss: 1.5270e-03\n",
      "Epoch: 249930 | training loss: 1.9218e-03 | validation loss: 1.5253e-03\n",
      "Epoch: 249940 | training loss: 1.9220e-03 | validation loss: 1.5251e-03\n",
      "Epoch: 249950 | training loss: 1.9244e-03 | validation loss: 1.5291e-03\n",
      "Epoch: 249960 | training loss: 2.0402e-03 | validation loss: 1.6280e-03\n",
      "Epoch: 249970 | training loss: 1.9248e-03 | validation loss: 1.5191e-03\n",
      "Epoch: 249980 | training loss: 2.0265e-03 | validation loss: 1.5757e-03\n",
      "Epoch: 249990 | training loss: 2.4717e-03 | validation loss: 1.6958e-03\n",
      "Epoch: 250000 | training loss: 2.0180e-03 | validation loss: 1.5994e-03\n",
      "Epoch: 250010 | training loss: 1.9574e-03 | validation loss: 1.5169e-03\n",
      "Epoch: 250020 | training loss: 1.9373e-03 | validation loss: 1.5511e-03\n",
      "Epoch: 250030 | training loss: 1.9247e-03 | validation loss: 1.5209e-03\n",
      "Epoch: 250040 | training loss: 1.9219e-03 | validation loss: 1.5231e-03\n",
      "Epoch: 250050 | training loss: 1.9232e-03 | validation loss: 1.5300e-03\n",
      "Epoch: 250060 | training loss: 1.9222e-03 | validation loss: 1.5227e-03\n",
      "Epoch: 250070 | training loss: 1.9223e-03 | validation loss: 1.5215e-03\n",
      "Epoch: 250080 | training loss: 1.9222e-03 | validation loss: 1.5218e-03\n",
      "Epoch: 250090 | training loss: 1.9256e-03 | validation loss: 1.5186e-03\n",
      "Epoch: 250100 | training loss: 2.0137e-03 | validation loss: 1.5312e-03\n",
      "Epoch: 250110 | training loss: 2.7535e-03 | validation loss: 1.8185e-03\n",
      "Epoch: 250120 | training loss: 2.1301e-03 | validation loss: 1.6725e-03\n",
      "Epoch: 250130 | training loss: 1.9410e-03 | validation loss: 1.5324e-03\n",
      "Epoch: 250140 | training loss: 1.9253e-03 | validation loss: 1.5167e-03\n",
      "Epoch: 250150 | training loss: 1.9262e-03 | validation loss: 1.5341e-03\n",
      "Epoch: 250160 | training loss: 1.9242e-03 | validation loss: 1.5222e-03\n",
      "Epoch: 250170 | training loss: 1.9224e-03 | validation loss: 1.5266e-03\n",
      "Epoch: 250180 | training loss: 1.9215e-03 | validation loss: 1.5247e-03\n",
      "Epoch: 250190 | training loss: 1.9215e-03 | validation loss: 1.5232e-03\n",
      "Epoch: 250200 | training loss: 1.9215e-03 | validation loss: 1.5258e-03\n",
      "Epoch: 250210 | training loss: 1.9214e-03 | validation loss: 1.5252e-03\n",
      "Epoch: 250220 | training loss: 1.9214e-03 | validation loss: 1.5245e-03\n",
      "Epoch: 250230 | training loss: 1.9213e-03 | validation loss: 1.5245e-03\n",
      "Epoch: 250240 | training loss: 1.9214e-03 | validation loss: 1.5252e-03\n",
      "Epoch: 250250 | training loss: 1.9265e-03 | validation loss: 1.5335e-03\n",
      "Epoch: 250260 | training loss: 2.4481e-03 | validation loss: 1.8527e-03\n",
      "Epoch: 250270 | training loss: 2.2700e-03 | validation loss: 1.6725e-03\n",
      "Epoch: 250280 | training loss: 1.9930e-03 | validation loss: 1.5971e-03\n",
      "Epoch: 250290 | training loss: 1.9957e-03 | validation loss: 1.5736e-03\n",
      "Epoch: 250300 | training loss: 1.9622e-03 | validation loss: 1.5444e-03\n",
      "Epoch: 250310 | training loss: 1.9299e-03 | validation loss: 1.5306e-03\n",
      "Epoch: 250320 | training loss: 1.9215e-03 | validation loss: 1.5267e-03\n",
      "Epoch: 250330 | training loss: 1.9223e-03 | validation loss: 1.5231e-03\n",
      "Epoch: 250340 | training loss: 1.9217e-03 | validation loss: 1.5253e-03\n",
      "Epoch: 250350 | training loss: 1.9212e-03 | validation loss: 1.5241e-03\n",
      "Epoch: 250360 | training loss: 1.9212e-03 | validation loss: 1.5246e-03\n",
      "Epoch: 250370 | training loss: 1.9211e-03 | validation loss: 1.5241e-03\n",
      "Epoch: 250380 | training loss: 1.9211e-03 | validation loss: 1.5244e-03\n",
      "Epoch: 250390 | training loss: 1.9211e-03 | validation loss: 1.5242e-03\n",
      "Epoch: 250400 | training loss: 1.9211e-03 | validation loss: 1.5241e-03\n",
      "Epoch: 250410 | training loss: 1.9211e-03 | validation loss: 1.5242e-03\n",
      "Epoch: 250420 | training loss: 1.9210e-03 | validation loss: 1.5241e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 250430 | training loss: 1.9210e-03 | validation loss: 1.5240e-03\n",
      "Epoch: 250440 | training loss: 1.9210e-03 | validation loss: 1.5235e-03\n",
      "Epoch: 250450 | training loss: 1.9228e-03 | validation loss: 1.5186e-03\n",
      "Epoch: 250460 | training loss: 2.2132e-03 | validation loss: 1.6136e-03\n",
      "Epoch: 250470 | training loss: 2.2368e-03 | validation loss: 1.8014e-03\n",
      "Epoch: 250480 | training loss: 2.0271e-03 | validation loss: 1.5841e-03\n",
      "Epoch: 250490 | training loss: 1.9565e-03 | validation loss: 1.5725e-03\n",
      "Epoch: 250500 | training loss: 1.9368e-03 | validation loss: 1.5493e-03\n",
      "Epoch: 250510 | training loss: 1.9305e-03 | validation loss: 1.5314e-03\n",
      "Epoch: 250520 | training loss: 1.9241e-03 | validation loss: 1.5335e-03\n",
      "Epoch: 250530 | training loss: 1.9217e-03 | validation loss: 1.5260e-03\n",
      "Epoch: 250540 | training loss: 1.9210e-03 | validation loss: 1.5250e-03\n",
      "Epoch: 250550 | training loss: 1.9208e-03 | validation loss: 1.5236e-03\n",
      "Epoch: 250560 | training loss: 1.9208e-03 | validation loss: 1.5235e-03\n",
      "Epoch: 250570 | training loss: 1.9208e-03 | validation loss: 1.5228e-03\n",
      "Epoch: 250580 | training loss: 1.9208e-03 | validation loss: 1.5234e-03\n",
      "Epoch: 250590 | training loss: 1.9208e-03 | validation loss: 1.5237e-03\n",
      "Epoch: 250600 | training loss: 1.9207e-03 | validation loss: 1.5237e-03\n",
      "Epoch: 250610 | training loss: 1.9207e-03 | validation loss: 1.5237e-03\n",
      "Epoch: 250620 | training loss: 1.9208e-03 | validation loss: 1.5244e-03\n",
      "Epoch: 250630 | training loss: 1.9240e-03 | validation loss: 1.5314e-03\n",
      "Epoch: 250640 | training loss: 2.2384e-03 | validation loss: 1.7399e-03\n",
      "Epoch: 250650 | training loss: 1.9293e-03 | validation loss: 1.5174e-03\n",
      "Epoch: 250660 | training loss: 2.2263e-03 | validation loss: 1.7318e-03\n",
      "Epoch: 250670 | training loss: 2.0210e-03 | validation loss: 1.6060e-03\n",
      "Epoch: 250680 | training loss: 1.9220e-03 | validation loss: 1.5275e-03\n",
      "Epoch: 250690 | training loss: 1.9302e-03 | validation loss: 1.5176e-03\n",
      "Epoch: 250700 | training loss: 1.9252e-03 | validation loss: 1.5184e-03\n",
      "Epoch: 250710 | training loss: 1.9208e-03 | validation loss: 1.5251e-03\n",
      "Epoch: 250720 | training loss: 1.9213e-03 | validation loss: 1.5267e-03\n",
      "Epoch: 250730 | training loss: 1.9206e-03 | validation loss: 1.5226e-03\n",
      "Epoch: 250740 | training loss: 1.9206e-03 | validation loss: 1.5229e-03\n",
      "Epoch: 250750 | training loss: 1.9206e-03 | validation loss: 1.5241e-03\n",
      "Epoch: 250760 | training loss: 1.9205e-03 | validation loss: 1.5230e-03\n",
      "Epoch: 250770 | training loss: 1.9205e-03 | validation loss: 1.5236e-03\n",
      "Epoch: 250780 | training loss: 1.9205e-03 | validation loss: 1.5233e-03\n",
      "Epoch: 250790 | training loss: 1.9205e-03 | validation loss: 1.5234e-03\n",
      "Epoch: 250800 | training loss: 1.9204e-03 | validation loss: 1.5233e-03\n",
      "Epoch: 250810 | training loss: 1.9204e-03 | validation loss: 1.5233e-03\n",
      "Epoch: 250820 | training loss: 1.9204e-03 | validation loss: 1.5233e-03\n",
      "Epoch: 250830 | training loss: 1.9204e-03 | validation loss: 1.5233e-03\n",
      "Epoch: 250840 | training loss: 1.9204e-03 | validation loss: 1.5233e-03\n",
      "Epoch: 250850 | training loss: 1.9203e-03 | validation loss: 1.5233e-03\n",
      "Epoch: 250860 | training loss: 1.9203e-03 | validation loss: 1.5234e-03\n",
      "Epoch: 250870 | training loss: 1.9206e-03 | validation loss: 1.5250e-03\n",
      "Epoch: 250880 | training loss: 1.9630e-03 | validation loss: 1.5661e-03\n",
      "Epoch: 250890 | training loss: 4.4023e-03 | validation loss: 2.8994e-03\n",
      "Epoch: 250900 | training loss: 2.3258e-03 | validation loss: 1.7887e-03\n",
      "Epoch: 250910 | training loss: 2.0336e-03 | validation loss: 1.6118e-03\n",
      "Epoch: 250920 | training loss: 1.9628e-03 | validation loss: 1.5635e-03\n",
      "Epoch: 250930 | training loss: 1.9345e-03 | validation loss: 1.5426e-03\n",
      "Epoch: 250940 | training loss: 1.9236e-03 | validation loss: 1.5311e-03\n",
      "Epoch: 250950 | training loss: 1.9205e-03 | validation loss: 1.5250e-03\n",
      "Epoch: 250960 | training loss: 1.9202e-03 | validation loss: 1.5225e-03\n",
      "Epoch: 250970 | training loss: 1.9205e-03 | validation loss: 1.5217e-03\n",
      "Epoch: 250980 | training loss: 1.9204e-03 | validation loss: 1.5219e-03\n",
      "Epoch: 250990 | training loss: 1.9202e-03 | validation loss: 1.5227e-03\n",
      "Epoch: 251000 | training loss: 1.9201e-03 | validation loss: 1.5234e-03\n",
      "Epoch: 251010 | training loss: 1.9201e-03 | validation loss: 1.5234e-03\n",
      "Epoch: 251020 | training loss: 1.9201e-03 | validation loss: 1.5230e-03\n",
      "Epoch: 251030 | training loss: 1.9201e-03 | validation loss: 1.5230e-03\n",
      "Epoch: 251040 | training loss: 1.9201e-03 | validation loss: 1.5231e-03\n",
      "Epoch: 251050 | training loss: 1.9200e-03 | validation loss: 1.5230e-03\n",
      "Epoch: 251060 | training loss: 1.9200e-03 | validation loss: 1.5230e-03\n",
      "Epoch: 251070 | training loss: 1.9200e-03 | validation loss: 1.5230e-03\n",
      "Epoch: 251080 | training loss: 1.9200e-03 | validation loss: 1.5229e-03\n",
      "Epoch: 251090 | training loss: 1.9200e-03 | validation loss: 1.5228e-03\n",
      "Epoch: 251100 | training loss: 1.9202e-03 | validation loss: 1.5213e-03\n",
      "Epoch: 251110 | training loss: 1.9746e-03 | validation loss: 1.5334e-03\n",
      "Epoch: 251120 | training loss: 2.0527e-03 | validation loss: 1.5380e-03\n",
      "Epoch: 251130 | training loss: 2.0033e-03 | validation loss: 1.5800e-03\n",
      "Epoch: 251140 | training loss: 1.9448e-03 | validation loss: 1.5151e-03\n",
      "Epoch: 251150 | training loss: 1.9274e-03 | validation loss: 1.5315e-03\n",
      "Epoch: 251160 | training loss: 1.9224e-03 | validation loss: 1.5161e-03\n",
      "Epoch: 251170 | training loss: 1.9203e-03 | validation loss: 1.5231e-03\n",
      "Epoch: 251180 | training loss: 1.9204e-03 | validation loss: 1.5237e-03\n",
      "Epoch: 251190 | training loss: 1.9201e-03 | validation loss: 1.5209e-03\n",
      "Epoch: 251200 | training loss: 1.9201e-03 | validation loss: 1.5200e-03\n",
      "Epoch: 251210 | training loss: 1.9211e-03 | validation loss: 1.5179e-03\n",
      "Epoch: 251220 | training loss: 1.9477e-03 | validation loss: 1.5141e-03\n",
      "Epoch: 251230 | training loss: 2.7219e-03 | validation loss: 1.7958e-03\n",
      "Epoch: 251240 | training loss: 2.2305e-03 | validation loss: 1.7417e-03\n",
      "Epoch: 251250 | training loss: 2.0383e-03 | validation loss: 1.5379e-03\n",
      "Epoch: 251260 | training loss: 1.9224e-03 | validation loss: 1.5302e-03\n",
      "Epoch: 251270 | training loss: 1.9251e-03 | validation loss: 1.5344e-03\n",
      "Epoch: 251280 | training loss: 1.9255e-03 | validation loss: 1.5161e-03\n",
      "Epoch: 251290 | training loss: 1.9223e-03 | validation loss: 1.5297e-03\n",
      "Epoch: 251300 | training loss: 1.9207e-03 | validation loss: 1.5187e-03\n",
      "Epoch: 251310 | training loss: 1.9200e-03 | validation loss: 1.5246e-03\n",
      "Epoch: 251320 | training loss: 1.9196e-03 | validation loss: 1.5216e-03\n",
      "Epoch: 251330 | training loss: 1.9196e-03 | validation loss: 1.5216e-03\n",
      "Epoch: 251340 | training loss: 1.9196e-03 | validation loss: 1.5227e-03\n",
      "Epoch: 251350 | training loss: 1.9196e-03 | validation loss: 1.5229e-03\n",
      "Epoch: 251360 | training loss: 1.9197e-03 | validation loss: 1.5235e-03\n",
      "Epoch: 251370 | training loss: 1.9210e-03 | validation loss: 1.5276e-03\n",
      "Epoch: 251380 | training loss: 1.9768e-03 | validation loss: 1.5789e-03\n",
      "Epoch: 251390 | training loss: 3.2083e-03 | validation loss: 2.2808e-03\n",
      "Epoch: 251400 | training loss: 2.1607e-03 | validation loss: 1.5926e-03\n",
      "Epoch: 251410 | training loss: 1.9539e-03 | validation loss: 1.5121e-03\n",
      "Epoch: 251420 | training loss: 1.9644e-03 | validation loss: 1.5622e-03\n",
      "Epoch: 251430 | training loss: 1.9215e-03 | validation loss: 1.5278e-03\n",
      "Epoch: 251440 | training loss: 1.9234e-03 | validation loss: 1.5175e-03\n",
      "Epoch: 251450 | training loss: 1.9218e-03 | validation loss: 1.5260e-03\n",
      "Epoch: 251460 | training loss: 1.9200e-03 | validation loss: 1.5218e-03\n",
      "Epoch: 251470 | training loss: 1.9195e-03 | validation loss: 1.5224e-03\n",
      "Epoch: 251480 | training loss: 1.9194e-03 | validation loss: 1.5219e-03\n",
      "Epoch: 251490 | training loss: 1.9194e-03 | validation loss: 1.5224e-03\n",
      "Epoch: 251500 | training loss: 1.9193e-03 | validation loss: 1.5218e-03\n",
      "Epoch: 251510 | training loss: 1.9193e-03 | validation loss: 1.5219e-03\n",
      "Epoch: 251520 | training loss: 1.9193e-03 | validation loss: 1.5222e-03\n",
      "Epoch: 251530 | training loss: 1.9193e-03 | validation loss: 1.5222e-03\n",
      "Epoch: 251540 | training loss: 1.9193e-03 | validation loss: 1.5223e-03\n",
      "Epoch: 251550 | training loss: 1.9195e-03 | validation loss: 1.5233e-03\n",
      "Epoch: 251560 | training loss: 1.9300e-03 | validation loss: 1.5359e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 251570 | training loss: 2.7139e-03 | validation loss: 1.9943e-03\n",
      "Epoch: 251580 | training loss: 2.4312e-03 | validation loss: 1.7603e-03\n",
      "Epoch: 251590 | training loss: 2.0121e-03 | validation loss: 1.6270e-03\n",
      "Epoch: 251600 | training loss: 1.9241e-03 | validation loss: 1.5226e-03\n",
      "Epoch: 251610 | training loss: 1.9375e-03 | validation loss: 1.5185e-03\n",
      "Epoch: 251620 | training loss: 1.9292e-03 | validation loss: 1.5299e-03\n",
      "Epoch: 251630 | training loss: 1.9216e-03 | validation loss: 1.5210e-03\n",
      "Epoch: 251640 | training loss: 1.9192e-03 | validation loss: 1.5204e-03\n",
      "Epoch: 251650 | training loss: 1.9194e-03 | validation loss: 1.5237e-03\n",
      "Epoch: 251660 | training loss: 1.9192e-03 | validation loss: 1.5217e-03\n",
      "Epoch: 251670 | training loss: 1.9191e-03 | validation loss: 1.5218e-03\n",
      "Epoch: 251680 | training loss: 1.9191e-03 | validation loss: 1.5218e-03\n",
      "Epoch: 251690 | training loss: 1.9190e-03 | validation loss: 1.5215e-03\n",
      "Epoch: 251700 | training loss: 1.9190e-03 | validation loss: 1.5216e-03\n",
      "Epoch: 251710 | training loss: 1.9190e-03 | validation loss: 1.5217e-03\n",
      "Epoch: 251720 | training loss: 1.9190e-03 | validation loss: 1.5218e-03\n",
      "Epoch: 251730 | training loss: 1.9190e-03 | validation loss: 1.5223e-03\n",
      "Epoch: 251740 | training loss: 1.9200e-03 | validation loss: 1.5271e-03\n",
      "Epoch: 251750 | training loss: 2.0282e-03 | validation loss: 1.6278e-03\n",
      "Epoch: 251760 | training loss: 2.4667e-03 | validation loss: 1.8842e-03\n",
      "Epoch: 251770 | training loss: 2.1758e-03 | validation loss: 1.7578e-03\n",
      "Epoch: 251780 | training loss: 2.0221e-03 | validation loss: 1.6140e-03\n",
      "Epoch: 251790 | training loss: 1.9314e-03 | validation loss: 1.5438e-03\n",
      "Epoch: 251800 | training loss: 1.9220e-03 | validation loss: 1.5295e-03\n",
      "Epoch: 251810 | training loss: 1.9202e-03 | validation loss: 1.5164e-03\n",
      "Epoch: 251820 | training loss: 1.9202e-03 | validation loss: 1.5174e-03\n",
      "Epoch: 251830 | training loss: 1.9194e-03 | validation loss: 1.5192e-03\n",
      "Epoch: 251840 | training loss: 1.9188e-03 | validation loss: 1.5205e-03\n",
      "Epoch: 251850 | training loss: 1.9188e-03 | validation loss: 1.5226e-03\n",
      "Epoch: 251860 | training loss: 1.9188e-03 | validation loss: 1.5213e-03\n",
      "Epoch: 251870 | training loss: 1.9187e-03 | validation loss: 1.5207e-03\n",
      "Epoch: 251880 | training loss: 1.9187e-03 | validation loss: 1.5212e-03\n",
      "Epoch: 251890 | training loss: 1.9187e-03 | validation loss: 1.5212e-03\n",
      "Epoch: 251900 | training loss: 1.9187e-03 | validation loss: 1.5210e-03\n",
      "Epoch: 251910 | training loss: 1.9187e-03 | validation loss: 1.5210e-03\n",
      "Epoch: 251920 | training loss: 1.9187e-03 | validation loss: 1.5206e-03\n",
      "Epoch: 251930 | training loss: 1.9200e-03 | validation loss: 1.5187e-03\n",
      "Epoch: 251940 | training loss: 2.0337e-03 | validation loss: 1.5501e-03\n",
      "Epoch: 251950 | training loss: 2.7399e-03 | validation loss: 1.8116e-03\n",
      "Epoch: 251960 | training loss: 2.0807e-03 | validation loss: 1.5774e-03\n",
      "Epoch: 251970 | training loss: 1.9757e-03 | validation loss: 1.5901e-03\n",
      "Epoch: 251980 | training loss: 1.9679e-03 | validation loss: 1.5774e-03\n",
      "Epoch: 251990 | training loss: 1.9192e-03 | validation loss: 1.5246e-03\n",
      "Epoch: 252000 | training loss: 1.9245e-03 | validation loss: 1.5129e-03\n",
      "Epoch: 252010 | training loss: 1.9191e-03 | validation loss: 1.5178e-03\n",
      "Epoch: 252020 | training loss: 1.9193e-03 | validation loss: 1.5249e-03\n",
      "Epoch: 252030 | training loss: 1.9186e-03 | validation loss: 1.5210e-03\n",
      "Epoch: 252040 | training loss: 1.9185e-03 | validation loss: 1.5204e-03\n",
      "Epoch: 252050 | training loss: 1.9185e-03 | validation loss: 1.5214e-03\n",
      "Epoch: 252060 | training loss: 1.9184e-03 | validation loss: 1.5207e-03\n",
      "Epoch: 252070 | training loss: 1.9184e-03 | validation loss: 1.5212e-03\n",
      "Epoch: 252080 | training loss: 1.9184e-03 | validation loss: 1.5208e-03\n",
      "Epoch: 252090 | training loss: 1.9184e-03 | validation loss: 1.5209e-03\n",
      "Epoch: 252100 | training loss: 1.9184e-03 | validation loss: 1.5209e-03\n",
      "Epoch: 252110 | training loss: 1.9183e-03 | validation loss: 1.5208e-03\n",
      "Epoch: 252120 | training loss: 1.9183e-03 | validation loss: 1.5208e-03\n",
      "Epoch: 252130 | training loss: 1.9183e-03 | validation loss: 1.5207e-03\n",
      "Epoch: 252140 | training loss: 1.9183e-03 | validation loss: 1.5204e-03\n",
      "Epoch: 252150 | training loss: 1.9187e-03 | validation loss: 1.5186e-03\n",
      "Epoch: 252160 | training loss: 1.9557e-03 | validation loss: 1.5161e-03\n",
      "Epoch: 252170 | training loss: 3.9520e-03 | validation loss: 2.3446e-03\n",
      "Epoch: 252180 | training loss: 1.9519e-03 | validation loss: 1.5403e-03\n",
      "Epoch: 252190 | training loss: 2.0394e-03 | validation loss: 1.6308e-03\n",
      "Epoch: 252200 | training loss: 1.9941e-03 | validation loss: 1.5991e-03\n",
      "Epoch: 252210 | training loss: 1.9390e-03 | validation loss: 1.5494e-03\n",
      "Epoch: 252220 | training loss: 1.9189e-03 | validation loss: 1.5255e-03\n",
      "Epoch: 252230 | training loss: 1.9197e-03 | validation loss: 1.5155e-03\n",
      "Epoch: 252240 | training loss: 1.9193e-03 | validation loss: 1.5167e-03\n",
      "Epoch: 252250 | training loss: 1.9181e-03 | validation loss: 1.5207e-03\n",
      "Epoch: 252260 | training loss: 1.9183e-03 | validation loss: 1.5227e-03\n",
      "Epoch: 252270 | training loss: 1.9181e-03 | validation loss: 1.5203e-03\n",
      "Epoch: 252280 | training loss: 1.9181e-03 | validation loss: 1.5199e-03\n",
      "Epoch: 252290 | training loss: 1.9181e-03 | validation loss: 1.5209e-03\n",
      "Epoch: 252300 | training loss: 1.9180e-03 | validation loss: 1.5203e-03\n",
      "Epoch: 252310 | training loss: 1.9180e-03 | validation loss: 1.5203e-03\n",
      "Epoch: 252320 | training loss: 1.9185e-03 | validation loss: 1.5192e-03\n",
      "Epoch: 252330 | training loss: 1.9611e-03 | validation loss: 1.5353e-03\n",
      "Epoch: 252340 | training loss: 2.0998e-03 | validation loss: 1.6222e-03\n",
      "Epoch: 252350 | training loss: 2.0089e-03 | validation loss: 1.5516e-03\n",
      "Epoch: 252360 | training loss: 1.9202e-03 | validation loss: 1.5281e-03\n",
      "Epoch: 252370 | training loss: 1.9320e-03 | validation loss: 1.5356e-03\n",
      "Epoch: 252380 | training loss: 1.9209e-03 | validation loss: 1.5257e-03\n",
      "Epoch: 252390 | training loss: 1.9187e-03 | validation loss: 1.5203e-03\n",
      "Epoch: 252400 | training loss: 1.9185e-03 | validation loss: 1.5193e-03\n",
      "Epoch: 252410 | training loss: 1.9180e-03 | validation loss: 1.5212e-03\n",
      "Epoch: 252420 | training loss: 1.9179e-03 | validation loss: 1.5207e-03\n",
      "Epoch: 252430 | training loss: 1.9179e-03 | validation loss: 1.5206e-03\n",
      "Epoch: 252440 | training loss: 1.9209e-03 | validation loss: 1.5289e-03\n",
      "Epoch: 252450 | training loss: 2.2815e-03 | validation loss: 1.7742e-03\n",
      "Epoch: 252460 | training loss: 2.0561e-03 | validation loss: 1.5384e-03\n",
      "Epoch: 252470 | training loss: 2.1067e-03 | validation loss: 1.6452e-03\n",
      "Epoch: 252480 | training loss: 2.0011e-03 | validation loss: 1.5906e-03\n",
      "Epoch: 252490 | training loss: 1.9300e-03 | validation loss: 1.5435e-03\n",
      "Epoch: 252500 | training loss: 1.9283e-03 | validation loss: 1.5324e-03\n",
      "Epoch: 252510 | training loss: 1.9228e-03 | validation loss: 1.5246e-03\n",
      "Epoch: 252520 | training loss: 1.9178e-03 | validation loss: 1.5216e-03\n",
      "Epoch: 252530 | training loss: 1.9183e-03 | validation loss: 1.5204e-03\n",
      "Epoch: 252540 | training loss: 1.9178e-03 | validation loss: 1.5183e-03\n",
      "Epoch: 252550 | training loss: 1.9177e-03 | validation loss: 1.5197e-03\n",
      "Epoch: 252560 | training loss: 1.9177e-03 | validation loss: 1.5208e-03\n",
      "Epoch: 252570 | training loss: 1.9176e-03 | validation loss: 1.5196e-03\n",
      "Epoch: 252580 | training loss: 1.9176e-03 | validation loss: 1.5199e-03\n",
      "Epoch: 252590 | training loss: 1.9176e-03 | validation loss: 1.5199e-03\n",
      "Epoch: 252600 | training loss: 1.9176e-03 | validation loss: 1.5198e-03\n",
      "Epoch: 252610 | training loss: 1.9176e-03 | validation loss: 1.5198e-03\n",
      "Epoch: 252620 | training loss: 1.9175e-03 | validation loss: 1.5198e-03\n",
      "Epoch: 252630 | training loss: 1.9175e-03 | validation loss: 1.5199e-03\n",
      "Epoch: 252640 | training loss: 1.9175e-03 | validation loss: 1.5198e-03\n",
      "Epoch: 252650 | training loss: 1.9175e-03 | validation loss: 1.5198e-03\n",
      "Epoch: 252660 | training loss: 1.9175e-03 | validation loss: 1.5198e-03\n",
      "Epoch: 252670 | training loss: 1.9175e-03 | validation loss: 1.5199e-03\n",
      "Epoch: 252680 | training loss: 1.9177e-03 | validation loss: 1.5212e-03\n",
      "Epoch: 252690 | training loss: 1.9678e-03 | validation loss: 1.5630e-03\n",
      "Epoch: 252700 | training loss: 4.0289e-03 | validation loss: 2.6962e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 252710 | training loss: 2.3290e-03 | validation loss: 1.7654e-03\n",
      "Epoch: 252720 | training loss: 2.0428e-03 | validation loss: 1.5736e-03\n",
      "Epoch: 252730 | training loss: 1.9420e-03 | validation loss: 1.5093e-03\n",
      "Epoch: 252740 | training loss: 1.9318e-03 | validation loss: 1.5163e-03\n",
      "Epoch: 252750 | training loss: 1.9247e-03 | validation loss: 1.5236e-03\n",
      "Epoch: 252760 | training loss: 1.9199e-03 | validation loss: 1.5261e-03\n",
      "Epoch: 252770 | training loss: 1.9183e-03 | validation loss: 1.5244e-03\n",
      "Epoch: 252780 | training loss: 1.9174e-03 | validation loss: 1.5201e-03\n",
      "Epoch: 252790 | training loss: 1.9174e-03 | validation loss: 1.5187e-03\n",
      "Epoch: 252800 | training loss: 1.9173e-03 | validation loss: 1.5198e-03\n",
      "Epoch: 252810 | training loss: 1.9173e-03 | validation loss: 1.5198e-03\n",
      "Epoch: 252820 | training loss: 1.9172e-03 | validation loss: 1.5193e-03\n",
      "Epoch: 252830 | training loss: 1.9172e-03 | validation loss: 1.5197e-03\n",
      "Epoch: 252840 | training loss: 1.9172e-03 | validation loss: 1.5194e-03\n",
      "Epoch: 252850 | training loss: 1.9172e-03 | validation loss: 1.5195e-03\n",
      "Epoch: 252860 | training loss: 1.9172e-03 | validation loss: 1.5194e-03\n",
      "Epoch: 252870 | training loss: 1.9172e-03 | validation loss: 1.5194e-03\n",
      "Epoch: 252880 | training loss: 1.9171e-03 | validation loss: 1.5194e-03\n",
      "Epoch: 252890 | training loss: 1.9171e-03 | validation loss: 1.5194e-03\n",
      "Epoch: 252900 | training loss: 1.9171e-03 | validation loss: 1.5194e-03\n",
      "Epoch: 252910 | training loss: 1.9171e-03 | validation loss: 1.5193e-03\n",
      "Epoch: 252920 | training loss: 1.9171e-03 | validation loss: 1.5193e-03\n",
      "Epoch: 252930 | training loss: 1.9171e-03 | validation loss: 1.5189e-03\n",
      "Epoch: 252940 | training loss: 1.9182e-03 | validation loss: 1.5152e-03\n",
      "Epoch: 252950 | training loss: 2.1247e-03 | validation loss: 1.5602e-03\n",
      "Epoch: 252960 | training loss: 2.0061e-03 | validation loss: 1.6048e-03\n",
      "Epoch: 252970 | training loss: 2.0863e-03 | validation loss: 1.5784e-03\n",
      "Epoch: 252980 | training loss: 1.9419e-03 | validation loss: 1.5246e-03\n",
      "Epoch: 252990 | training loss: 1.9219e-03 | validation loss: 1.5309e-03\n",
      "Epoch: 253000 | training loss: 1.9246e-03 | validation loss: 1.5362e-03\n",
      "Epoch: 253010 | training loss: 1.9204e-03 | validation loss: 1.5297e-03\n",
      "Epoch: 253020 | training loss: 1.9179e-03 | validation loss: 1.5226e-03\n",
      "Epoch: 253030 | training loss: 1.9173e-03 | validation loss: 1.5199e-03\n",
      "Epoch: 253040 | training loss: 1.9169e-03 | validation loss: 1.5198e-03\n",
      "Epoch: 253050 | training loss: 1.9169e-03 | validation loss: 1.5190e-03\n",
      "Epoch: 253060 | training loss: 1.9169e-03 | validation loss: 1.5181e-03\n",
      "Epoch: 253070 | training loss: 1.9169e-03 | validation loss: 1.5186e-03\n",
      "Epoch: 253080 | training loss: 1.9168e-03 | validation loss: 1.5187e-03\n",
      "Epoch: 253090 | training loss: 1.9168e-03 | validation loss: 1.5190e-03\n",
      "Epoch: 253100 | training loss: 1.9168e-03 | validation loss: 1.5189e-03\n",
      "Epoch: 253110 | training loss: 1.9168e-03 | validation loss: 1.5188e-03\n",
      "Epoch: 253120 | training loss: 1.9168e-03 | validation loss: 1.5188e-03\n",
      "Epoch: 253130 | training loss: 1.9167e-03 | validation loss: 1.5188e-03\n",
      "Epoch: 253140 | training loss: 1.9167e-03 | validation loss: 1.5188e-03\n",
      "Epoch: 253150 | training loss: 1.9167e-03 | validation loss: 1.5188e-03\n",
      "Epoch: 253160 | training loss: 1.9167e-03 | validation loss: 1.5188e-03\n",
      "Epoch: 253170 | training loss: 1.9167e-03 | validation loss: 1.5189e-03\n",
      "Epoch: 253180 | training loss: 1.9170e-03 | validation loss: 1.5203e-03\n",
      "Epoch: 253190 | training loss: 1.9576e-03 | validation loss: 1.5559e-03\n",
      "Epoch: 253200 | training loss: 4.0597e-03 | validation loss: 2.7104e-03\n",
      "Epoch: 253210 | training loss: 2.0509e-03 | validation loss: 1.5952e-03\n",
      "Epoch: 253220 | training loss: 2.0048e-03 | validation loss: 1.5294e-03\n",
      "Epoch: 253230 | training loss: 1.9813e-03 | validation loss: 1.5156e-03\n",
      "Epoch: 253240 | training loss: 1.9455e-03 | validation loss: 1.5092e-03\n",
      "Epoch: 253250 | training loss: 1.9228e-03 | validation loss: 1.5106e-03\n",
      "Epoch: 253260 | training loss: 1.9175e-03 | validation loss: 1.5179e-03\n",
      "Epoch: 253270 | training loss: 1.9176e-03 | validation loss: 1.5218e-03\n",
      "Epoch: 253280 | training loss: 1.9166e-03 | validation loss: 1.5199e-03\n",
      "Epoch: 253290 | training loss: 1.9166e-03 | validation loss: 1.5179e-03\n",
      "Epoch: 253300 | training loss: 1.9165e-03 | validation loss: 1.5186e-03\n",
      "Epoch: 253310 | training loss: 1.9165e-03 | validation loss: 1.5192e-03\n",
      "Epoch: 253320 | training loss: 1.9165e-03 | validation loss: 1.5183e-03\n",
      "Epoch: 253330 | training loss: 1.9164e-03 | validation loss: 1.5185e-03\n",
      "Epoch: 253340 | training loss: 1.9164e-03 | validation loss: 1.5187e-03\n",
      "Epoch: 253350 | training loss: 1.9164e-03 | validation loss: 1.5185e-03\n",
      "Epoch: 253360 | training loss: 1.9164e-03 | validation loss: 1.5185e-03\n",
      "Epoch: 253370 | training loss: 1.9164e-03 | validation loss: 1.5185e-03\n",
      "Epoch: 253380 | training loss: 1.9164e-03 | validation loss: 1.5185e-03\n",
      "Epoch: 253390 | training loss: 1.9163e-03 | validation loss: 1.5185e-03\n",
      "Epoch: 253400 | training loss: 1.9163e-03 | validation loss: 1.5185e-03\n",
      "Epoch: 253410 | training loss: 1.9163e-03 | validation loss: 1.5184e-03\n",
      "Epoch: 253420 | training loss: 1.9163e-03 | validation loss: 1.5184e-03\n",
      "Epoch: 253430 | training loss: 1.9163e-03 | validation loss: 1.5185e-03\n",
      "Epoch: 253440 | training loss: 1.9163e-03 | validation loss: 1.5188e-03\n",
      "Epoch: 253450 | training loss: 1.9176e-03 | validation loss: 1.5237e-03\n",
      "Epoch: 253460 | training loss: 2.2396e-03 | validation loss: 1.7469e-03\n",
      "Epoch: 253470 | training loss: 2.2926e-03 | validation loss: 1.6336e-03\n",
      "Epoch: 253480 | training loss: 1.9982e-03 | validation loss: 1.5321e-03\n",
      "Epoch: 253490 | training loss: 1.9690e-03 | validation loss: 1.5105e-03\n",
      "Epoch: 253500 | training loss: 1.9583e-03 | validation loss: 1.5091e-03\n",
      "Epoch: 253510 | training loss: 1.9361e-03 | validation loss: 1.5077e-03\n",
      "Epoch: 253520 | training loss: 1.9239e-03 | validation loss: 1.5088e-03\n",
      "Epoch: 253530 | training loss: 1.9189e-03 | validation loss: 1.5114e-03\n",
      "Epoch: 253540 | training loss: 1.9171e-03 | validation loss: 1.5140e-03\n",
      "Epoch: 253550 | training loss: 1.9164e-03 | validation loss: 1.5157e-03\n",
      "Epoch: 253560 | training loss: 1.9161e-03 | validation loss: 1.5170e-03\n",
      "Epoch: 253570 | training loss: 1.9161e-03 | validation loss: 1.5180e-03\n",
      "Epoch: 253580 | training loss: 1.9161e-03 | validation loss: 1.5184e-03\n",
      "Epoch: 253590 | training loss: 1.9160e-03 | validation loss: 1.5184e-03\n",
      "Epoch: 253600 | training loss: 1.9160e-03 | validation loss: 1.5181e-03\n",
      "Epoch: 253610 | training loss: 1.9160e-03 | validation loss: 1.5179e-03\n",
      "Epoch: 253620 | training loss: 1.9160e-03 | validation loss: 1.5179e-03\n",
      "Epoch: 253630 | training loss: 1.9160e-03 | validation loss: 1.5180e-03\n",
      "Epoch: 253640 | training loss: 1.9159e-03 | validation loss: 1.5180e-03\n",
      "Epoch: 253650 | training loss: 1.9159e-03 | validation loss: 1.5179e-03\n",
      "Epoch: 253660 | training loss: 1.9159e-03 | validation loss: 1.5180e-03\n",
      "Epoch: 253670 | training loss: 1.9164e-03 | validation loss: 1.5189e-03\n",
      "Epoch: 253680 | training loss: 1.9880e-03 | validation loss: 1.5713e-03\n",
      "Epoch: 253690 | training loss: 2.0858e-03 | validation loss: 1.5549e-03\n",
      "Epoch: 253700 | training loss: 2.0139e-03 | validation loss: 1.6245e-03\n",
      "Epoch: 253710 | training loss: 1.9586e-03 | validation loss: 1.5617e-03\n",
      "Epoch: 253720 | training loss: 1.9304e-03 | validation loss: 1.5354e-03\n",
      "Epoch: 253730 | training loss: 1.9181e-03 | validation loss: 1.5247e-03\n",
      "Epoch: 253740 | training loss: 1.9158e-03 | validation loss: 1.5176e-03\n",
      "Epoch: 253750 | training loss: 1.9166e-03 | validation loss: 1.5141e-03\n",
      "Epoch: 253760 | training loss: 1.9162e-03 | validation loss: 1.5186e-03\n",
      "Epoch: 253770 | training loss: 1.9158e-03 | validation loss: 1.5181e-03\n",
      "Epoch: 253780 | training loss: 1.9158e-03 | validation loss: 1.5173e-03\n",
      "Epoch: 253790 | training loss: 1.9159e-03 | validation loss: 1.5159e-03\n",
      "Epoch: 253800 | training loss: 1.9179e-03 | validation loss: 1.5128e-03\n",
      "Epoch: 253810 | training loss: 1.9870e-03 | validation loss: 1.5198e-03\n",
      "Epoch: 253820 | training loss: 3.0397e-03 | validation loss: 1.9358e-03\n",
      "Epoch: 253830 | training loss: 2.1172e-03 | validation loss: 1.6607e-03\n",
      "Epoch: 253840 | training loss: 1.9290e-03 | validation loss: 1.5372e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 253850 | training loss: 1.9610e-03 | validation loss: 1.5181e-03\n",
      "Epoch: 253860 | training loss: 1.9211e-03 | validation loss: 1.5290e-03\n",
      "Epoch: 253870 | training loss: 1.9159e-03 | validation loss: 1.5186e-03\n",
      "Epoch: 253880 | training loss: 1.9167e-03 | validation loss: 1.5140e-03\n",
      "Epoch: 253890 | training loss: 1.9162e-03 | validation loss: 1.5209e-03\n",
      "Epoch: 253900 | training loss: 1.9158e-03 | validation loss: 1.5158e-03\n",
      "Epoch: 253910 | training loss: 1.9156e-03 | validation loss: 1.5182e-03\n",
      "Epoch: 253920 | training loss: 1.9155e-03 | validation loss: 1.5175e-03\n",
      "Epoch: 253930 | training loss: 1.9155e-03 | validation loss: 1.5169e-03\n",
      "Epoch: 253940 | training loss: 1.9155e-03 | validation loss: 1.5175e-03\n",
      "Epoch: 253950 | training loss: 1.9155e-03 | validation loss: 1.5177e-03\n",
      "Epoch: 253960 | training loss: 1.9155e-03 | validation loss: 1.5181e-03\n",
      "Epoch: 253970 | training loss: 1.9161e-03 | validation loss: 1.5204e-03\n",
      "Epoch: 253980 | training loss: 1.9419e-03 | validation loss: 1.5480e-03\n",
      "Epoch: 253990 | training loss: 3.1902e-03 | validation loss: 2.2588e-03\n",
      "Epoch: 254000 | training loss: 2.4638e-03 | validation loss: 1.7026e-03\n",
      "Epoch: 254010 | training loss: 1.9256e-03 | validation loss: 1.5072e-03\n",
      "Epoch: 254020 | training loss: 1.9752e-03 | validation loss: 1.5739e-03\n",
      "Epoch: 254030 | training loss: 1.9199e-03 | validation loss: 1.5283e-03\n",
      "Epoch: 254040 | training loss: 1.9236e-03 | validation loss: 1.5101e-03\n",
      "Epoch: 254050 | training loss: 1.9153e-03 | validation loss: 1.5176e-03\n",
      "Epoch: 254060 | training loss: 1.9162e-03 | validation loss: 1.5211e-03\n",
      "Epoch: 254070 | training loss: 1.9157e-03 | validation loss: 1.5152e-03\n",
      "Epoch: 254080 | training loss: 1.9153e-03 | validation loss: 1.5184e-03\n",
      "Epoch: 254090 | training loss: 1.9153e-03 | validation loss: 1.5167e-03\n",
      "Epoch: 254100 | training loss: 1.9152e-03 | validation loss: 1.5176e-03\n",
      "Epoch: 254110 | training loss: 1.9152e-03 | validation loss: 1.5169e-03\n",
      "Epoch: 254120 | training loss: 1.9152e-03 | validation loss: 1.5175e-03\n",
      "Epoch: 254130 | training loss: 1.9152e-03 | validation loss: 1.5176e-03\n",
      "Epoch: 254140 | training loss: 1.9160e-03 | validation loss: 1.5205e-03\n",
      "Epoch: 254150 | training loss: 1.9938e-03 | validation loss: 1.6006e-03\n",
      "Epoch: 254160 | training loss: 1.9620e-03 | validation loss: 1.5751e-03\n",
      "Epoch: 254170 | training loss: 2.0396e-03 | validation loss: 1.6366e-03\n",
      "Epoch: 254180 | training loss: 1.9314e-03 | validation loss: 1.5291e-03\n",
      "Epoch: 254190 | training loss: 1.9249e-03 | validation loss: 1.5073e-03\n",
      "Epoch: 254200 | training loss: 1.9504e-03 | validation loss: 1.5081e-03\n",
      "Epoch: 254210 | training loss: 2.1630e-03 | validation loss: 1.5724e-03\n",
      "Epoch: 254220 | training loss: 1.9828e-03 | validation loss: 1.5193e-03\n",
      "Epoch: 254230 | training loss: 1.9880e-03 | validation loss: 1.5875e-03\n",
      "Epoch: 254240 | training loss: 1.9480e-03 | validation loss: 1.5097e-03\n",
      "Epoch: 254250 | training loss: 1.9171e-03 | validation loss: 1.5225e-03\n",
      "Epoch: 254260 | training loss: 1.9200e-03 | validation loss: 1.5279e-03\n",
      "Epoch: 254270 | training loss: 1.9151e-03 | validation loss: 1.5157e-03\n",
      "Epoch: 254280 | training loss: 1.9172e-03 | validation loss: 1.5118e-03\n",
      "Epoch: 254290 | training loss: 1.9317e-03 | validation loss: 1.5087e-03\n",
      "Epoch: 254300 | training loss: 2.1883e-03 | validation loss: 1.5847e-03\n",
      "Epoch: 254310 | training loss: 2.0079e-03 | validation loss: 1.5294e-03\n",
      "Epoch: 254320 | training loss: 1.9386e-03 | validation loss: 1.5421e-03\n",
      "Epoch: 254330 | training loss: 1.9188e-03 | validation loss: 1.5112e-03\n",
      "Epoch: 254340 | training loss: 1.9170e-03 | validation loss: 1.5235e-03\n",
      "Epoch: 254350 | training loss: 1.9178e-03 | validation loss: 1.5113e-03\n",
      "Epoch: 254360 | training loss: 1.9179e-03 | validation loss: 1.5245e-03\n",
      "Epoch: 254370 | training loss: 1.9154e-03 | validation loss: 1.5140e-03\n",
      "Epoch: 254380 | training loss: 1.9153e-03 | validation loss: 1.5144e-03\n",
      "Epoch: 254390 | training loss: 1.9148e-03 | validation loss: 1.5163e-03\n",
      "Epoch: 254400 | training loss: 1.9148e-03 | validation loss: 1.5174e-03\n",
      "Epoch: 254410 | training loss: 1.9154e-03 | validation loss: 1.5198e-03\n",
      "Epoch: 254420 | training loss: 1.9399e-03 | validation loss: 1.5465e-03\n",
      "Epoch: 254430 | training loss: 3.0501e-03 | validation loss: 2.1861e-03\n",
      "Epoch: 254440 | training loss: 2.4579e-03 | validation loss: 1.6975e-03\n",
      "Epoch: 254450 | training loss: 1.9205e-03 | validation loss: 1.5197e-03\n",
      "Epoch: 254460 | training loss: 1.9808e-03 | validation loss: 1.5811e-03\n",
      "Epoch: 254470 | training loss: 1.9155e-03 | validation loss: 1.5143e-03\n",
      "Epoch: 254480 | training loss: 1.9221e-03 | validation loss: 1.5085e-03\n",
      "Epoch: 254490 | training loss: 1.9166e-03 | validation loss: 1.5237e-03\n",
      "Epoch: 254500 | training loss: 1.9146e-03 | validation loss: 1.5160e-03\n",
      "Epoch: 254510 | training loss: 1.9147e-03 | validation loss: 1.5151e-03\n",
      "Epoch: 254520 | training loss: 1.9147e-03 | validation loss: 1.5179e-03\n",
      "Epoch: 254530 | training loss: 1.9146e-03 | validation loss: 1.5154e-03\n",
      "Epoch: 254540 | training loss: 1.9145e-03 | validation loss: 1.5167e-03\n",
      "Epoch: 254550 | training loss: 1.9145e-03 | validation loss: 1.5163e-03\n",
      "Epoch: 254560 | training loss: 1.9145e-03 | validation loss: 1.5160e-03\n",
      "Epoch: 254570 | training loss: 1.9145e-03 | validation loss: 1.5160e-03\n",
      "Epoch: 254580 | training loss: 1.9155e-03 | validation loss: 1.5143e-03\n",
      "Epoch: 254590 | training loss: 2.0338e-03 | validation loss: 1.5685e-03\n",
      "Epoch: 254600 | training loss: 1.9600e-03 | validation loss: 1.5442e-03\n",
      "Epoch: 254610 | training loss: 1.9849e-03 | validation loss: 1.5334e-03\n",
      "Epoch: 254620 | training loss: 1.9623e-03 | validation loss: 1.5230e-03\n",
      "Epoch: 254630 | training loss: 1.9390e-03 | validation loss: 1.5063e-03\n",
      "Epoch: 254640 | training loss: 2.0798e-03 | validation loss: 1.5444e-03\n",
      "Epoch: 254650 | training loss: 2.1570e-03 | validation loss: 1.5744e-03\n",
      "Epoch: 254660 | training loss: 2.0297e-03 | validation loss: 1.6172e-03\n",
      "Epoch: 254670 | training loss: 1.9508e-03 | validation loss: 1.5098e-03\n",
      "Epoch: 254680 | training loss: 1.9173e-03 | validation loss: 1.5234e-03\n",
      "Epoch: 254690 | training loss: 1.9174e-03 | validation loss: 1.5237e-03\n",
      "Epoch: 254700 | training loss: 1.9166e-03 | validation loss: 1.5109e-03\n",
      "Epoch: 254710 | training loss: 1.9173e-03 | validation loss: 1.5106e-03\n",
      "Epoch: 254720 | training loss: 1.9200e-03 | validation loss: 1.5091e-03\n",
      "Epoch: 254730 | training loss: 1.9669e-03 | validation loss: 1.5127e-03\n",
      "Epoch: 254740 | training loss: 2.4978e-03 | validation loss: 1.7047e-03\n",
      "Epoch: 254750 | training loss: 2.0218e-03 | validation loss: 1.6074e-03\n",
      "Epoch: 254760 | training loss: 1.9559e-03 | validation loss: 1.5130e-03\n",
      "Epoch: 254770 | training loss: 1.9281e-03 | validation loss: 1.5352e-03\n",
      "Epoch: 254780 | training loss: 1.9161e-03 | validation loss: 1.5113e-03\n",
      "Epoch: 254790 | training loss: 1.9144e-03 | validation loss: 1.5142e-03\n",
      "Epoch: 254800 | training loss: 1.9163e-03 | validation loss: 1.5219e-03\n",
      "Epoch: 254810 | training loss: 1.9142e-03 | validation loss: 1.5146e-03\n",
      "Epoch: 254820 | training loss: 1.9149e-03 | validation loss: 1.5128e-03\n",
      "Epoch: 254830 | training loss: 1.9162e-03 | validation loss: 1.5115e-03\n",
      "Epoch: 254840 | training loss: 1.9350e-03 | validation loss: 1.5092e-03\n",
      "Epoch: 254850 | training loss: 2.3525e-03 | validation loss: 1.6527e-03\n",
      "Epoch: 254860 | training loss: 1.9180e-03 | validation loss: 1.5278e-03\n",
      "Epoch: 254870 | training loss: 1.9456e-03 | validation loss: 1.5075e-03\n",
      "Epoch: 254880 | training loss: 1.9478e-03 | validation loss: 1.5532e-03\n",
      "Epoch: 254890 | training loss: 1.9310e-03 | validation loss: 1.5093e-03\n",
      "Epoch: 254900 | training loss: 1.9205e-03 | validation loss: 1.5276e-03\n",
      "Epoch: 254910 | training loss: 1.9158e-03 | validation loss: 1.5115e-03\n",
      "Epoch: 254920 | training loss: 1.9141e-03 | validation loss: 1.5177e-03\n",
      "Epoch: 254930 | training loss: 1.9142e-03 | validation loss: 1.5164e-03\n",
      "Epoch: 254940 | training loss: 1.9140e-03 | validation loss: 1.5145e-03\n",
      "Epoch: 254950 | training loss: 1.9141e-03 | validation loss: 1.5149e-03\n",
      "Epoch: 254960 | training loss: 1.9144e-03 | validation loss: 1.5147e-03\n",
      "Epoch: 254970 | training loss: 1.9207e-03 | validation loss: 1.5149e-03\n",
      "Epoch: 254980 | training loss: 2.1151e-03 | validation loss: 1.5939e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 254990 | training loss: 2.4155e-03 | validation loss: 1.6731e-03\n",
      "Epoch: 255000 | training loss: 2.1071e-03 | validation loss: 1.7060e-03\n",
      "Epoch: 255010 | training loss: 1.9231e-03 | validation loss: 1.5064e-03\n",
      "Epoch: 255020 | training loss: 1.9313e-03 | validation loss: 1.5153e-03\n",
      "Epoch: 255030 | training loss: 1.9146e-03 | validation loss: 1.5201e-03\n",
      "Epoch: 255040 | training loss: 1.9161e-03 | validation loss: 1.5185e-03\n",
      "Epoch: 255050 | training loss: 1.9141e-03 | validation loss: 1.5137e-03\n",
      "Epoch: 255060 | training loss: 1.9139e-03 | validation loss: 1.5169e-03\n",
      "Epoch: 255070 | training loss: 1.9139e-03 | validation loss: 1.5147e-03\n",
      "Epoch: 255080 | training loss: 1.9137e-03 | validation loss: 1.5140e-03\n",
      "Epoch: 255090 | training loss: 1.9137e-03 | validation loss: 1.5155e-03\n",
      "Epoch: 255100 | training loss: 1.9136e-03 | validation loss: 1.5153e-03\n",
      "Epoch: 255110 | training loss: 1.9137e-03 | validation loss: 1.5166e-03\n",
      "Epoch: 255120 | training loss: 1.9177e-03 | validation loss: 1.5256e-03\n",
      "Epoch: 255130 | training loss: 2.2805e-03 | validation loss: 1.7746e-03\n",
      "Epoch: 255140 | training loss: 2.0280e-03 | validation loss: 1.5378e-03\n",
      "Epoch: 255150 | training loss: 2.0838e-03 | validation loss: 1.6508e-03\n",
      "Epoch: 255160 | training loss: 2.0147e-03 | validation loss: 1.6262e-03\n",
      "Epoch: 255170 | training loss: 1.9312e-03 | validation loss: 1.5447e-03\n",
      "Epoch: 255180 | training loss: 1.9154e-03 | validation loss: 1.5143e-03\n",
      "Epoch: 255190 | training loss: 1.9176e-03 | validation loss: 1.5075e-03\n",
      "Epoch: 255200 | training loss: 1.9142e-03 | validation loss: 1.5111e-03\n",
      "Epoch: 255210 | training loss: 1.9138e-03 | validation loss: 1.5178e-03\n",
      "Epoch: 255220 | training loss: 1.9136e-03 | validation loss: 1.5165e-03\n",
      "Epoch: 255230 | training loss: 1.9135e-03 | validation loss: 1.5139e-03\n",
      "Epoch: 255240 | training loss: 1.9134e-03 | validation loss: 1.5148e-03\n",
      "Epoch: 255250 | training loss: 1.9134e-03 | validation loss: 1.5153e-03\n",
      "Epoch: 255260 | training loss: 1.9134e-03 | validation loss: 1.5146e-03\n",
      "Epoch: 255270 | training loss: 1.9134e-03 | validation loss: 1.5150e-03\n",
      "Epoch: 255280 | training loss: 1.9133e-03 | validation loss: 1.5147e-03\n",
      "Epoch: 255290 | training loss: 1.9133e-03 | validation loss: 1.5149e-03\n",
      "Epoch: 255300 | training loss: 1.9133e-03 | validation loss: 1.5147e-03\n",
      "Epoch: 255310 | training loss: 1.9148e-03 | validation loss: 1.5147e-03\n",
      "Epoch: 255320 | training loss: 2.0675e-03 | validation loss: 1.5921e-03\n",
      "Epoch: 255330 | training loss: 2.4368e-03 | validation loss: 1.6850e-03\n",
      "Epoch: 255340 | training loss: 1.9451e-03 | validation loss: 1.5604e-03\n",
      "Epoch: 255350 | training loss: 1.9560e-03 | validation loss: 1.5702e-03\n",
      "Epoch: 255360 | training loss: 1.9480e-03 | validation loss: 1.5257e-03\n",
      "Epoch: 255370 | training loss: 1.9242e-03 | validation loss: 1.5367e-03\n",
      "Epoch: 255380 | training loss: 1.9153e-03 | validation loss: 1.5163e-03\n",
      "Epoch: 255390 | training loss: 1.9133e-03 | validation loss: 1.5162e-03\n",
      "Epoch: 255400 | training loss: 1.9134e-03 | validation loss: 1.5128e-03\n",
      "Epoch: 255410 | training loss: 1.9133e-03 | validation loss: 1.5151e-03\n",
      "Epoch: 255420 | training loss: 1.9132e-03 | validation loss: 1.5135e-03\n",
      "Epoch: 255430 | training loss: 1.9131e-03 | validation loss: 1.5148e-03\n",
      "Epoch: 255440 | training loss: 1.9131e-03 | validation loss: 1.5150e-03\n",
      "Epoch: 255450 | training loss: 1.9131e-03 | validation loss: 1.5151e-03\n",
      "Epoch: 255460 | training loss: 1.9133e-03 | validation loss: 1.5164e-03\n",
      "Epoch: 255470 | training loss: 1.9178e-03 | validation loss: 1.5252e-03\n",
      "Epoch: 255480 | training loss: 2.1600e-03 | validation loss: 1.6948e-03\n",
      "Epoch: 255490 | training loss: 2.0228e-03 | validation loss: 1.5984e-03\n",
      "Epoch: 255500 | training loss: 2.0701e-03 | validation loss: 1.6398e-03\n",
      "Epoch: 255510 | training loss: 1.9708e-03 | validation loss: 1.5267e-03\n",
      "Epoch: 255520 | training loss: 1.9231e-03 | validation loss: 1.5069e-03\n",
      "Epoch: 255530 | training loss: 1.9256e-03 | validation loss: 1.5287e-03\n",
      "Epoch: 255540 | training loss: 1.9133e-03 | validation loss: 1.5133e-03\n",
      "Epoch: 255550 | training loss: 1.9135e-03 | validation loss: 1.5136e-03\n",
      "Epoch: 255560 | training loss: 1.9135e-03 | validation loss: 1.5163e-03\n",
      "Epoch: 255570 | training loss: 1.9131e-03 | validation loss: 1.5130e-03\n",
      "Epoch: 255580 | training loss: 1.9130e-03 | validation loss: 1.5155e-03\n",
      "Epoch: 255590 | training loss: 1.9129e-03 | validation loss: 1.5137e-03\n",
      "Epoch: 255600 | training loss: 1.9128e-03 | validation loss: 1.5145e-03\n",
      "Epoch: 255610 | training loss: 1.9128e-03 | validation loss: 1.5145e-03\n",
      "Epoch: 255620 | training loss: 1.9128e-03 | validation loss: 1.5142e-03\n",
      "Epoch: 255630 | training loss: 1.9128e-03 | validation loss: 1.5141e-03\n",
      "Epoch: 255640 | training loss: 1.9128e-03 | validation loss: 1.5139e-03\n",
      "Epoch: 255650 | training loss: 1.9130e-03 | validation loss: 1.5128e-03\n",
      "Epoch: 255660 | training loss: 1.9233e-03 | validation loss: 1.5089e-03\n",
      "Epoch: 255670 | training loss: 2.6861e-03 | validation loss: 1.8016e-03\n",
      "Epoch: 255680 | training loss: 2.4367e-03 | validation loss: 1.8414e-03\n",
      "Epoch: 255690 | training loss: 1.9522e-03 | validation loss: 1.5076e-03\n",
      "Epoch: 255700 | training loss: 2.0036e-03 | validation loss: 1.5384e-03\n",
      "Epoch: 255710 | training loss: 1.9177e-03 | validation loss: 1.5131e-03\n",
      "Epoch: 255720 | training loss: 1.9207e-03 | validation loss: 1.5234e-03\n",
      "Epoch: 255730 | training loss: 1.9143e-03 | validation loss: 1.5186e-03\n",
      "Epoch: 255740 | training loss: 1.9139e-03 | validation loss: 1.5118e-03\n",
      "Epoch: 255750 | training loss: 1.9126e-03 | validation loss: 1.5144e-03\n",
      "Epoch: 255760 | training loss: 1.9128e-03 | validation loss: 1.5150e-03\n",
      "Epoch: 255770 | training loss: 1.9127e-03 | validation loss: 1.5136e-03\n",
      "Epoch: 255780 | training loss: 1.9126e-03 | validation loss: 1.5145e-03\n",
      "Epoch: 255790 | training loss: 1.9126e-03 | validation loss: 1.5139e-03\n",
      "Epoch: 255800 | training loss: 1.9125e-03 | validation loss: 1.5141e-03\n",
      "Epoch: 255810 | training loss: 1.9125e-03 | validation loss: 1.5138e-03\n",
      "Epoch: 255820 | training loss: 1.9125e-03 | validation loss: 1.5135e-03\n",
      "Epoch: 255830 | training loss: 1.9134e-03 | validation loss: 1.5104e-03\n",
      "Epoch: 255840 | training loss: 2.0257e-03 | validation loss: 1.5441e-03\n",
      "Epoch: 255850 | training loss: 2.1476e-03 | validation loss: 1.5796e-03\n",
      "Epoch: 255860 | training loss: 2.0403e-03 | validation loss: 1.5964e-03\n",
      "Epoch: 255870 | training loss: 1.9781e-03 | validation loss: 1.5198e-03\n",
      "Epoch: 255880 | training loss: 1.9399e-03 | validation loss: 1.5304e-03\n",
      "Epoch: 255890 | training loss: 1.9207e-03 | validation loss: 1.5057e-03\n",
      "Epoch: 255900 | training loss: 1.9130e-03 | validation loss: 1.5142e-03\n",
      "Epoch: 255910 | training loss: 1.9130e-03 | validation loss: 1.5175e-03\n",
      "Epoch: 255920 | training loss: 1.9127e-03 | validation loss: 1.5144e-03\n",
      "Epoch: 255930 | training loss: 1.9127e-03 | validation loss: 1.5119e-03\n",
      "Epoch: 255940 | training loss: 1.9145e-03 | validation loss: 1.5084e-03\n",
      "Epoch: 255950 | training loss: 1.9567e-03 | validation loss: 1.5085e-03\n",
      "Epoch: 255960 | training loss: 2.8271e-03 | validation loss: 1.8387e-03\n",
      "Epoch: 255970 | training loss: 2.2493e-03 | validation loss: 1.7435e-03\n",
      "Epoch: 255980 | training loss: 1.9943e-03 | validation loss: 1.5215e-03\n",
      "Epoch: 255990 | training loss: 1.9135e-03 | validation loss: 1.5186e-03\n",
      "Epoch: 256000 | training loss: 1.9157e-03 | validation loss: 1.5219e-03\n",
      "Epoch: 256010 | training loss: 1.9159e-03 | validation loss: 1.5078e-03\n",
      "Epoch: 256020 | training loss: 1.9139e-03 | validation loss: 1.5188e-03\n",
      "Epoch: 256030 | training loss: 1.9126e-03 | validation loss: 1.5113e-03\n",
      "Epoch: 256040 | training loss: 1.9122e-03 | validation loss: 1.5139e-03\n",
      "Epoch: 256050 | training loss: 1.9122e-03 | validation loss: 1.5144e-03\n",
      "Epoch: 256060 | training loss: 1.9122e-03 | validation loss: 1.5126e-03\n",
      "Epoch: 256070 | training loss: 1.9122e-03 | validation loss: 1.5126e-03\n",
      "Epoch: 256080 | training loss: 1.9121e-03 | validation loss: 1.5125e-03\n",
      "Epoch: 256090 | training loss: 1.9125e-03 | validation loss: 1.5111e-03\n",
      "Epoch: 256100 | training loss: 1.9252e-03 | validation loss: 1.5066e-03\n",
      "Epoch: 256110 | training loss: 2.5663e-03 | validation loss: 1.7388e-03\n",
      "Epoch: 256120 | training loss: 2.1713e-03 | validation loss: 1.7008e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 256130 | training loss: 2.0875e-03 | validation loss: 1.5544e-03\n",
      "Epoch: 256140 | training loss: 1.9245e-03 | validation loss: 1.5021e-03\n",
      "Epoch: 256150 | training loss: 1.9373e-03 | validation loss: 1.5439e-03\n",
      "Epoch: 256160 | training loss: 1.9131e-03 | validation loss: 1.5155e-03\n",
      "Epoch: 256170 | training loss: 1.9145e-03 | validation loss: 1.5071e-03\n",
      "Epoch: 256180 | training loss: 1.9132e-03 | validation loss: 1.5190e-03\n",
      "Epoch: 256190 | training loss: 1.9122e-03 | validation loss: 1.5111e-03\n",
      "Epoch: 256200 | training loss: 1.9120e-03 | validation loss: 1.5145e-03\n",
      "Epoch: 256210 | training loss: 1.9119e-03 | validation loss: 1.5124e-03\n",
      "Epoch: 256220 | training loss: 1.9119e-03 | validation loss: 1.5140e-03\n",
      "Epoch: 256230 | training loss: 1.9119e-03 | validation loss: 1.5127e-03\n",
      "Epoch: 256240 | training loss: 1.9118e-03 | validation loss: 1.5131e-03\n",
      "Epoch: 256250 | training loss: 1.9118e-03 | validation loss: 1.5133e-03\n",
      "Epoch: 256260 | training loss: 1.9118e-03 | validation loss: 1.5130e-03\n",
      "Epoch: 256270 | training loss: 1.9124e-03 | validation loss: 1.5123e-03\n",
      "Epoch: 256280 | training loss: 1.9513e-03 | validation loss: 1.5280e-03\n",
      "Epoch: 256290 | training loss: 2.1810e-03 | validation loss: 1.6711e-03\n",
      "Epoch: 256300 | training loss: 2.2046e-03 | validation loss: 1.7101e-03\n",
      "Epoch: 256310 | training loss: 1.9538e-03 | validation loss: 1.5659e-03\n",
      "Epoch: 256320 | training loss: 1.9742e-03 | validation loss: 1.5280e-03\n",
      "Epoch: 256330 | training loss: 1.9469e-03 | validation loss: 1.5597e-03\n",
      "Epoch: 256340 | training loss: 1.9182e-03 | validation loss: 1.5066e-03\n",
      "Epoch: 256350 | training loss: 1.9134e-03 | validation loss: 1.5075e-03\n",
      "Epoch: 256360 | training loss: 1.9136e-03 | validation loss: 1.5177e-03\n",
      "Epoch: 256370 | training loss: 1.9153e-03 | validation loss: 1.5226e-03\n",
      "Epoch: 256380 | training loss: 1.9278e-03 | validation loss: 1.5372e-03\n",
      "Epoch: 256390 | training loss: 2.1172e-03 | validation loss: 1.6697e-03\n",
      "Epoch: 256400 | training loss: 2.1180e-03 | validation loss: 1.6661e-03\n",
      "Epoch: 256410 | training loss: 2.0036e-03 | validation loss: 1.5226e-03\n",
      "Epoch: 256420 | training loss: 1.9508e-03 | validation loss: 1.5552e-03\n",
      "Epoch: 256430 | training loss: 1.9307e-03 | validation loss: 1.5048e-03\n",
      "Epoch: 256440 | training loss: 1.9183e-03 | validation loss: 1.5256e-03\n",
      "Epoch: 256450 | training loss: 1.9116e-03 | validation loss: 1.5118e-03\n",
      "Epoch: 256460 | training loss: 1.9131e-03 | validation loss: 1.5085e-03\n",
      "Epoch: 256470 | training loss: 1.9116e-03 | validation loss: 1.5114e-03\n",
      "Epoch: 256480 | training loss: 1.9115e-03 | validation loss: 1.5126e-03\n",
      "Epoch: 256490 | training loss: 1.9115e-03 | validation loss: 1.5132e-03\n",
      "Epoch: 256500 | training loss: 1.9123e-03 | validation loss: 1.5163e-03\n",
      "Epoch: 256510 | training loss: 1.9681e-03 | validation loss: 1.5680e-03\n",
      "Epoch: 256520 | training loss: 3.5832e-03 | validation loss: 2.4718e-03\n",
      "Epoch: 256530 | training loss: 1.9370e-03 | validation loss: 1.5318e-03\n",
      "Epoch: 256540 | training loss: 2.0567e-03 | validation loss: 1.5374e-03\n",
      "Epoch: 256550 | training loss: 1.9537e-03 | validation loss: 1.5031e-03\n",
      "Epoch: 256560 | training loss: 1.9128e-03 | validation loss: 1.5136e-03\n",
      "Epoch: 256570 | training loss: 1.9193e-03 | validation loss: 1.5308e-03\n",
      "Epoch: 256580 | training loss: 1.9116e-03 | validation loss: 1.5149e-03\n",
      "Epoch: 256590 | training loss: 1.9123e-03 | validation loss: 1.5083e-03\n",
      "Epoch: 256600 | training loss: 1.9113e-03 | validation loss: 1.5130e-03\n",
      "Epoch: 256610 | training loss: 1.9114e-03 | validation loss: 1.5138e-03\n",
      "Epoch: 256620 | training loss: 1.9113e-03 | validation loss: 1.5116e-03\n",
      "Epoch: 256630 | training loss: 1.9112e-03 | validation loss: 1.5128e-03\n",
      "Epoch: 256640 | training loss: 1.9112e-03 | validation loss: 1.5122e-03\n",
      "Epoch: 256650 | training loss: 1.9112e-03 | validation loss: 1.5125e-03\n",
      "Epoch: 256660 | training loss: 1.9112e-03 | validation loss: 1.5123e-03\n",
      "Epoch: 256670 | training loss: 1.9112e-03 | validation loss: 1.5124e-03\n",
      "Epoch: 256680 | training loss: 1.9111e-03 | validation loss: 1.5123e-03\n",
      "Epoch: 256690 | training loss: 1.9111e-03 | validation loss: 1.5122e-03\n",
      "Epoch: 256700 | training loss: 1.9113e-03 | validation loss: 1.5116e-03\n",
      "Epoch: 256710 | training loss: 1.9386e-03 | validation loss: 1.5215e-03\n",
      "Epoch: 256720 | training loss: 2.3702e-03 | validation loss: 1.7882e-03\n",
      "Epoch: 256730 | training loss: 2.1603e-03 | validation loss: 1.6704e-03\n",
      "Epoch: 256740 | training loss: 1.9341e-03 | validation loss: 1.5086e-03\n",
      "Epoch: 256750 | training loss: 1.9292e-03 | validation loss: 1.5015e-03\n",
      "Epoch: 256760 | training loss: 1.9116e-03 | validation loss: 1.5121e-03\n",
      "Epoch: 256770 | training loss: 1.9149e-03 | validation loss: 1.5231e-03\n",
      "Epoch: 256780 | training loss: 1.9314e-03 | validation loss: 1.5426e-03\n",
      "Epoch: 256790 | training loss: 2.2023e-03 | validation loss: 1.7248e-03\n",
      "Epoch: 256800 | training loss: 1.9555e-03 | validation loss: 1.5584e-03\n",
      "Epoch: 256810 | training loss: 1.9266e-03 | validation loss: 1.5029e-03\n",
      "Epoch: 256820 | training loss: 1.9163e-03 | validation loss: 1.5226e-03\n",
      "Epoch: 256830 | training loss: 1.9154e-03 | validation loss: 1.5055e-03\n",
      "Epoch: 256840 | training loss: 1.9160e-03 | validation loss: 1.5232e-03\n",
      "Epoch: 256850 | training loss: 1.9136e-03 | validation loss: 1.5068e-03\n",
      "Epoch: 256860 | training loss: 1.9109e-03 | validation loss: 1.5116e-03\n",
      "Epoch: 256870 | training loss: 1.9116e-03 | validation loss: 1.5155e-03\n",
      "Epoch: 256880 | training loss: 1.9119e-03 | validation loss: 1.5164e-03\n",
      "Epoch: 256890 | training loss: 1.9165e-03 | validation loss: 1.5238e-03\n",
      "Epoch: 256900 | training loss: 2.0149e-03 | validation loss: 1.6020e-03\n",
      "Epoch: 256910 | training loss: 2.6119e-03 | validation loss: 1.9513e-03\n",
      "Epoch: 256920 | training loss: 2.1151e-03 | validation loss: 1.5625e-03\n",
      "Epoch: 256930 | training loss: 1.9472e-03 | validation loss: 1.5486e-03\n",
      "Epoch: 256940 | training loss: 1.9138e-03 | validation loss: 1.5058e-03\n",
      "Epoch: 256950 | training loss: 1.9110e-03 | validation loss: 1.5144e-03\n",
      "Epoch: 256960 | training loss: 1.9110e-03 | validation loss: 1.5100e-03\n",
      "Epoch: 256970 | training loss: 1.9114e-03 | validation loss: 1.5148e-03\n",
      "Epoch: 256980 | training loss: 1.9115e-03 | validation loss: 1.5092e-03\n",
      "Epoch: 256990 | training loss: 1.9107e-03 | validation loss: 1.5123e-03\n",
      "Epoch: 257000 | training loss: 1.9109e-03 | validation loss: 1.5135e-03\n",
      "Epoch: 257010 | training loss: 1.9108e-03 | validation loss: 1.5132e-03\n",
      "Epoch: 257020 | training loss: 1.9115e-03 | validation loss: 1.5154e-03\n",
      "Epoch: 257030 | training loss: 1.9276e-03 | validation loss: 1.5344e-03\n",
      "Epoch: 257040 | training loss: 2.5181e-03 | validation loss: 1.8934e-03\n",
      "Epoch: 257050 | training loss: 2.0555e-03 | validation loss: 1.5368e-03\n",
      "Epoch: 257060 | training loss: 2.0949e-03 | validation loss: 1.6487e-03\n",
      "Epoch: 257070 | training loss: 1.9261e-03 | validation loss: 1.5078e-03\n",
      "Epoch: 257080 | training loss: 1.9171e-03 | validation loss: 1.5031e-03\n",
      "Epoch: 257090 | training loss: 1.9190e-03 | validation loss: 1.5284e-03\n",
      "Epoch: 257100 | training loss: 1.9135e-03 | validation loss: 1.5056e-03\n",
      "Epoch: 257110 | training loss: 1.9113e-03 | validation loss: 1.5161e-03\n",
      "Epoch: 257120 | training loss: 1.9108e-03 | validation loss: 1.5092e-03\n",
      "Epoch: 257130 | training loss: 1.9106e-03 | validation loss: 1.5133e-03\n",
      "Epoch: 257140 | training loss: 1.9105e-03 | validation loss: 1.5107e-03\n",
      "Epoch: 257150 | training loss: 1.9104e-03 | validation loss: 1.5116e-03\n",
      "Epoch: 257160 | training loss: 1.9104e-03 | validation loss: 1.5124e-03\n",
      "Epoch: 257170 | training loss: 1.9106e-03 | validation loss: 1.5135e-03\n",
      "Epoch: 257180 | training loss: 1.9218e-03 | validation loss: 1.5298e-03\n",
      "Epoch: 257190 | training loss: 2.3617e-03 | validation loss: 1.8857e-03\n",
      "Epoch: 257200 | training loss: 2.0574e-03 | validation loss: 1.5980e-03\n",
      "Epoch: 257210 | training loss: 2.0501e-03 | validation loss: 1.6149e-03\n",
      "Epoch: 257220 | training loss: 2.0801e-03 | validation loss: 1.6560e-03\n",
      "Epoch: 257230 | training loss: 1.9368e-03 | validation loss: 1.5151e-03\n",
      "Epoch: 257240 | training loss: 1.9317e-03 | validation loss: 1.5027e-03\n",
      "Epoch: 257250 | training loss: 1.9125e-03 | validation loss: 1.5132e-03\n",
      "Epoch: 257260 | training loss: 1.9216e-03 | validation loss: 1.5299e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 257270 | training loss: 1.9697e-03 | validation loss: 1.5717e-03\n",
      "Epoch: 257280 | training loss: 2.2640e-03 | validation loss: 1.7558e-03\n",
      "Epoch: 257290 | training loss: 1.9121e-03 | validation loss: 1.5057e-03\n",
      "Epoch: 257300 | training loss: 1.9322e-03 | validation loss: 1.5038e-03\n",
      "Epoch: 257310 | training loss: 1.9365e-03 | validation loss: 1.5431e-03\n",
      "Epoch: 257320 | training loss: 1.9112e-03 | validation loss: 1.5075e-03\n",
      "Epoch: 257330 | training loss: 1.9169e-03 | validation loss: 1.5046e-03\n",
      "Epoch: 257340 | training loss: 1.9143e-03 | validation loss: 1.5054e-03\n",
      "Epoch: 257350 | training loss: 1.9219e-03 | validation loss: 1.5039e-03\n",
      "Epoch: 257360 | training loss: 2.0541e-03 | validation loss: 1.5362e-03\n",
      "Epoch: 257370 | training loss: 2.3546e-03 | validation loss: 1.6521e-03\n",
      "Epoch: 257380 | training loss: 2.0728e-03 | validation loss: 1.6325e-03\n",
      "Epoch: 257390 | training loss: 1.9704e-03 | validation loss: 1.5157e-03\n",
      "Epoch: 257400 | training loss: 1.9344e-03 | validation loss: 1.5376e-03\n",
      "Epoch: 257410 | training loss: 1.9184e-03 | validation loss: 1.5064e-03\n",
      "Epoch: 257420 | training loss: 1.9106e-03 | validation loss: 1.5131e-03\n",
      "Epoch: 257430 | training loss: 1.9111e-03 | validation loss: 1.5153e-03\n",
      "Epoch: 257440 | training loss: 1.9102e-03 | validation loss: 1.5094e-03\n",
      "Epoch: 257450 | training loss: 1.9109e-03 | validation loss: 1.5080e-03\n",
      "Epoch: 257460 | training loss: 1.9147e-03 | validation loss: 1.5059e-03\n",
      "Epoch: 257470 | training loss: 1.9894e-03 | validation loss: 1.5197e-03\n",
      "Epoch: 257480 | training loss: 2.7249e-03 | validation loss: 1.8108e-03\n",
      "Epoch: 257490 | training loss: 2.1778e-03 | validation loss: 1.6963e-03\n",
      "Epoch: 257500 | training loss: 1.9808e-03 | validation loss: 1.5163e-03\n",
      "Epoch: 257510 | training loss: 1.9212e-03 | validation loss: 1.5271e-03\n",
      "Epoch: 257520 | training loss: 1.9116e-03 | validation loss: 1.5073e-03\n",
      "Epoch: 257530 | training loss: 1.9107e-03 | validation loss: 1.5143e-03\n",
      "Epoch: 257540 | training loss: 1.9108e-03 | validation loss: 1.5078e-03\n",
      "Epoch: 257550 | training loss: 1.9106e-03 | validation loss: 1.5143e-03\n",
      "Epoch: 257560 | training loss: 1.9098e-03 | validation loss: 1.5101e-03\n",
      "Epoch: 257570 | training loss: 1.9100e-03 | validation loss: 1.5095e-03\n",
      "Epoch: 257580 | training loss: 1.9099e-03 | validation loss: 1.5109e-03\n",
      "Epoch: 257590 | training loss: 1.9175e-03 | validation loss: 1.5244e-03\n",
      "Epoch: 257600 | training loss: 2.5105e-03 | validation loss: 2.0130e-03\n",
      "Epoch: 257610 | training loss: 2.1540e-03 | validation loss: 1.6683e-03\n",
      "Epoch: 257620 | training loss: 1.9926e-03 | validation loss: 1.5384e-03\n",
      "Epoch: 257630 | training loss: 1.9522e-03 | validation loss: 1.5081e-03\n",
      "Epoch: 257640 | training loss: 1.9144e-03 | validation loss: 1.5134e-03\n",
      "Epoch: 257650 | training loss: 1.9179e-03 | validation loss: 1.5265e-03\n",
      "Epoch: 257660 | training loss: 1.9340e-03 | validation loss: 1.5443e-03\n",
      "Epoch: 257670 | training loss: 2.0760e-03 | validation loss: 1.6448e-03\n",
      "Epoch: 257680 | training loss: 2.1222e-03 | validation loss: 1.6705e-03\n",
      "Epoch: 257690 | training loss: 2.0061e-03 | validation loss: 1.5180e-03\n",
      "Epoch: 257700 | training loss: 1.9231e-03 | validation loss: 1.5307e-03\n",
      "Epoch: 257710 | training loss: 1.9124e-03 | validation loss: 1.5181e-03\n",
      "Epoch: 257720 | training loss: 1.9169e-03 | validation loss: 1.5033e-03\n",
      "Epoch: 257730 | training loss: 1.9126e-03 | validation loss: 1.5049e-03\n",
      "Epoch: 257740 | training loss: 1.9111e-03 | validation loss: 1.5060e-03\n",
      "Epoch: 257750 | training loss: 1.9183e-03 | validation loss: 1.5029e-03\n",
      "Epoch: 257760 | training loss: 2.0921e-03 | validation loss: 1.5465e-03\n",
      "Epoch: 257770 | training loss: 2.2576e-03 | validation loss: 1.6119e-03\n",
      "Epoch: 257780 | training loss: 1.9648e-03 | validation loss: 1.5598e-03\n",
      "Epoch: 257790 | training loss: 1.9097e-03 | validation loss: 1.5079e-03\n",
      "Epoch: 257800 | training loss: 1.9120e-03 | validation loss: 1.5068e-03\n",
      "Epoch: 257810 | training loss: 1.9113e-03 | validation loss: 1.5158e-03\n",
      "Epoch: 257820 | training loss: 1.9097e-03 | validation loss: 1.5078e-03\n",
      "Epoch: 257830 | training loss: 1.9094e-03 | validation loss: 1.5096e-03\n",
      "Epoch: 257840 | training loss: 1.9099e-03 | validation loss: 1.5128e-03\n",
      "Epoch: 257850 | training loss: 1.9094e-03 | validation loss: 1.5090e-03\n",
      "Epoch: 257860 | training loss: 1.9095e-03 | validation loss: 1.5086e-03\n",
      "Epoch: 257870 | training loss: 1.9094e-03 | validation loss: 1.5087e-03\n",
      "Epoch: 257880 | training loss: 1.9102e-03 | validation loss: 1.5071e-03\n",
      "Epoch: 257890 | training loss: 1.9305e-03 | validation loss: 1.5041e-03\n",
      "Epoch: 257900 | training loss: 2.6815e-03 | validation loss: 1.7878e-03\n",
      "Epoch: 257910 | training loss: 2.2157e-03 | validation loss: 1.7228e-03\n",
      "Epoch: 257920 | training loss: 2.0582e-03 | validation loss: 1.5403e-03\n",
      "Epoch: 257930 | training loss: 1.9107e-03 | validation loss: 1.5056e-03\n",
      "Epoch: 257940 | training loss: 1.9277e-03 | validation loss: 1.5371e-03\n",
      "Epoch: 257950 | training loss: 1.9162e-03 | validation loss: 1.5028e-03\n",
      "Epoch: 257960 | training loss: 1.9099e-03 | validation loss: 1.5138e-03\n",
      "Epoch: 257970 | training loss: 1.9091e-03 | validation loss: 1.5092e-03\n",
      "Epoch: 257980 | training loss: 1.9091e-03 | validation loss: 1.5104e-03\n",
      "Epoch: 257990 | training loss: 1.9091e-03 | validation loss: 1.5093e-03\n",
      "Epoch: 258000 | training loss: 1.9091e-03 | validation loss: 1.5107e-03\n",
      "Epoch: 258010 | training loss: 1.9091e-03 | validation loss: 1.5093e-03\n",
      "Epoch: 258020 | training loss: 1.9090e-03 | validation loss: 1.5098e-03\n",
      "Epoch: 258030 | training loss: 1.9091e-03 | validation loss: 1.5106e-03\n",
      "Epoch: 258040 | training loss: 1.9100e-03 | validation loss: 1.5138e-03\n",
      "Epoch: 258050 | training loss: 1.9831e-03 | validation loss: 1.5875e-03\n",
      "Epoch: 258060 | training loss: 1.9493e-03 | validation loss: 1.5628e-03\n",
      "Epoch: 258070 | training loss: 2.0809e-03 | validation loss: 1.6758e-03\n",
      "Epoch: 258080 | training loss: 2.1025e-03 | validation loss: 1.6636e-03\n",
      "Epoch: 258090 | training loss: 1.9223e-03 | validation loss: 1.5186e-03\n",
      "Epoch: 258100 | training loss: 1.9520e-03 | validation loss: 1.5018e-03\n",
      "Epoch: 258110 | training loss: 1.9155e-03 | validation loss: 1.5015e-03\n",
      "Epoch: 258120 | training loss: 1.9097e-03 | validation loss: 1.5133e-03\n",
      "Epoch: 258130 | training loss: 1.9132e-03 | validation loss: 1.5206e-03\n",
      "Epoch: 258140 | training loss: 1.9784e-03 | validation loss: 1.5755e-03\n",
      "Epoch: 258150 | training loss: 2.6608e-03 | validation loss: 1.9810e-03\n",
      "Epoch: 258160 | training loss: 2.1570e-03 | validation loss: 1.5723e-03\n",
      "Epoch: 258170 | training loss: 1.9952e-03 | validation loss: 1.5843e-03\n",
      "Epoch: 258180 | training loss: 1.9335e-03 | validation loss: 1.5019e-03\n",
      "Epoch: 258190 | training loss: 1.9170e-03 | validation loss: 1.5239e-03\n",
      "Epoch: 258200 | training loss: 1.9128e-03 | validation loss: 1.5043e-03\n",
      "Epoch: 258210 | training loss: 1.9108e-03 | validation loss: 1.5154e-03\n",
      "Epoch: 258220 | training loss: 1.9090e-03 | validation loss: 1.5076e-03\n",
      "Epoch: 258230 | training loss: 1.9090e-03 | validation loss: 1.5077e-03\n",
      "Epoch: 258240 | training loss: 1.9087e-03 | validation loss: 1.5097e-03\n",
      "Epoch: 258250 | training loss: 1.9089e-03 | validation loss: 1.5110e-03\n",
      "Epoch: 258260 | training loss: 1.9103e-03 | validation loss: 1.5146e-03\n",
      "Epoch: 258270 | training loss: 1.9490e-03 | validation loss: 1.5520e-03\n",
      "Epoch: 258280 | training loss: 2.9583e-03 | validation loss: 2.1355e-03\n",
      "Epoch: 258290 | training loss: 2.3139e-03 | validation loss: 1.6354e-03\n",
      "Epoch: 258300 | training loss: 1.9536e-03 | validation loss: 1.5474e-03\n",
      "Epoch: 258310 | training loss: 1.9226e-03 | validation loss: 1.5332e-03\n",
      "Epoch: 258320 | training loss: 1.9255e-03 | validation loss: 1.5022e-03\n",
      "Epoch: 258330 | training loss: 1.9130e-03 | validation loss: 1.5168e-03\n",
      "Epoch: 258340 | training loss: 1.9094e-03 | validation loss: 1.5095e-03\n",
      "Epoch: 258350 | training loss: 1.9088e-03 | validation loss: 1.5086e-03\n",
      "Epoch: 258360 | training loss: 1.9086e-03 | validation loss: 1.5098e-03\n",
      "Epoch: 258370 | training loss: 1.9085e-03 | validation loss: 1.5096e-03\n",
      "Epoch: 258380 | training loss: 1.9085e-03 | validation loss: 1.5083e-03\n",
      "Epoch: 258390 | training loss: 1.9085e-03 | validation loss: 1.5097e-03\n",
      "Epoch: 258400 | training loss: 1.9085e-03 | validation loss: 1.5096e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 258410 | training loss: 1.9084e-03 | validation loss: 1.5094e-03\n",
      "Epoch: 258420 | training loss: 1.9086e-03 | validation loss: 1.5098e-03\n",
      "Epoch: 258430 | training loss: 1.9144e-03 | validation loss: 1.5161e-03\n",
      "Epoch: 258440 | training loss: 2.2355e-03 | validation loss: 1.7351e-03\n",
      "Epoch: 258450 | training loss: 2.7398e-03 | validation loss: 1.8583e-03\n",
      "Epoch: 258460 | training loss: 2.0527e-03 | validation loss: 1.6135e-03\n",
      "Epoch: 258470 | training loss: 1.9751e-03 | validation loss: 1.5098e-03\n",
      "Epoch: 258480 | training loss: 1.9391e-03 | validation loss: 1.5502e-03\n",
      "Epoch: 258490 | training loss: 1.9227e-03 | validation loss: 1.5083e-03\n",
      "Epoch: 258500 | training loss: 1.9145e-03 | validation loss: 1.5239e-03\n",
      "Epoch: 258510 | training loss: 1.9097e-03 | validation loss: 1.5073e-03\n",
      "Epoch: 258520 | training loss: 1.9083e-03 | validation loss: 1.5082e-03\n",
      "Epoch: 258530 | training loss: 1.9087e-03 | validation loss: 1.5105e-03\n",
      "Epoch: 258540 | training loss: 1.9083e-03 | validation loss: 1.5093e-03\n",
      "Epoch: 258550 | training loss: 1.9082e-03 | validation loss: 1.5091e-03\n",
      "Epoch: 258560 | training loss: 1.9082e-03 | validation loss: 1.5090e-03\n",
      "Epoch: 258570 | training loss: 1.9083e-03 | validation loss: 1.5097e-03\n",
      "Epoch: 258580 | training loss: 1.9136e-03 | validation loss: 1.5205e-03\n",
      "Epoch: 258590 | training loss: 2.5280e-03 | validation loss: 1.9081e-03\n",
      "Epoch: 258600 | training loss: 2.4458e-03 | validation loss: 1.6769e-03\n",
      "Epoch: 258610 | training loss: 1.9532e-03 | validation loss: 1.5308e-03\n",
      "Epoch: 258620 | training loss: 1.9925e-03 | validation loss: 1.5695e-03\n",
      "Epoch: 258630 | training loss: 1.9365e-03 | validation loss: 1.5369e-03\n",
      "Epoch: 258640 | training loss: 1.9088e-03 | validation loss: 1.5131e-03\n",
      "Epoch: 258650 | training loss: 1.9121e-03 | validation loss: 1.5088e-03\n",
      "Epoch: 258660 | training loss: 1.9090e-03 | validation loss: 1.5084e-03\n",
      "Epoch: 258670 | training loss: 1.9083e-03 | validation loss: 1.5104e-03\n",
      "Epoch: 258680 | training loss: 1.9081e-03 | validation loss: 1.5086e-03\n",
      "Epoch: 258690 | training loss: 1.9081e-03 | validation loss: 1.5076e-03\n",
      "Epoch: 258700 | training loss: 1.9080e-03 | validation loss: 1.5091e-03\n",
      "Epoch: 258710 | training loss: 1.9080e-03 | validation loss: 1.5085e-03\n",
      "Epoch: 258720 | training loss: 1.9080e-03 | validation loss: 1.5083e-03\n",
      "Epoch: 258730 | training loss: 1.9079e-03 | validation loss: 1.5086e-03\n",
      "Epoch: 258740 | training loss: 1.9079e-03 | validation loss: 1.5084e-03\n",
      "Epoch: 258750 | training loss: 1.9079e-03 | validation loss: 1.5085e-03\n",
      "Epoch: 258760 | training loss: 1.9079e-03 | validation loss: 1.5085e-03\n",
      "Epoch: 258770 | training loss: 1.9079e-03 | validation loss: 1.5084e-03\n",
      "Epoch: 258780 | training loss: 1.9079e-03 | validation loss: 1.5084e-03\n",
      "Epoch: 258790 | training loss: 1.9078e-03 | validation loss: 1.5084e-03\n",
      "Epoch: 258800 | training loss: 1.9078e-03 | validation loss: 1.5083e-03\n",
      "Epoch: 258810 | training loss: 1.9078e-03 | validation loss: 1.5078e-03\n",
      "Epoch: 258820 | training loss: 1.9104e-03 | validation loss: 1.5048e-03\n",
      "Epoch: 258830 | training loss: 2.2863e-03 | validation loss: 1.6361e-03\n",
      "Epoch: 258840 | training loss: 2.1240e-03 | validation loss: 1.6468e-03\n",
      "Epoch: 258850 | training loss: 1.9829e-03 | validation loss: 1.5061e-03\n",
      "Epoch: 258860 | training loss: 1.9483e-03 | validation loss: 1.5088e-03\n",
      "Epoch: 258870 | training loss: 1.9264e-03 | validation loss: 1.5204e-03\n",
      "Epoch: 258880 | training loss: 1.9190e-03 | validation loss: 1.5206e-03\n",
      "Epoch: 258890 | training loss: 1.9130e-03 | validation loss: 1.5108e-03\n",
      "Epoch: 258900 | training loss: 1.9100e-03 | validation loss: 1.5075e-03\n",
      "Epoch: 258910 | training loss: 1.9081e-03 | validation loss: 1.5093e-03\n",
      "Epoch: 258920 | training loss: 1.9077e-03 | validation loss: 1.5085e-03\n",
      "Epoch: 258930 | training loss: 1.9077e-03 | validation loss: 1.5080e-03\n",
      "Epoch: 258940 | training loss: 1.9077e-03 | validation loss: 1.5085e-03\n",
      "Epoch: 258950 | training loss: 1.9076e-03 | validation loss: 1.5080e-03\n",
      "Epoch: 258960 | training loss: 1.9076e-03 | validation loss: 1.5083e-03\n",
      "Epoch: 258970 | training loss: 1.9076e-03 | validation loss: 1.5081e-03\n",
      "Epoch: 258980 | training loss: 1.9076e-03 | validation loss: 1.5081e-03\n",
      "Epoch: 258990 | training loss: 1.9075e-03 | validation loss: 1.5081e-03\n",
      "Epoch: 259000 | training loss: 1.9075e-03 | validation loss: 1.5081e-03\n",
      "Epoch: 259010 | training loss: 1.9075e-03 | validation loss: 1.5081e-03\n",
      "Epoch: 259020 | training loss: 1.9075e-03 | validation loss: 1.5081e-03\n",
      "Epoch: 259030 | training loss: 1.9075e-03 | validation loss: 1.5084e-03\n",
      "Epoch: 259040 | training loss: 1.9078e-03 | validation loss: 1.5109e-03\n",
      "Epoch: 259050 | training loss: 1.9526e-03 | validation loss: 1.5674e-03\n",
      "Epoch: 259060 | training loss: 3.0679e-03 | validation loss: 2.2832e-03\n",
      "Epoch: 259070 | training loss: 2.2165e-03 | validation loss: 1.7645e-03\n",
      "Epoch: 259080 | training loss: 2.0413e-03 | validation loss: 1.6135e-03\n",
      "Epoch: 259090 | training loss: 1.9533e-03 | validation loss: 1.5654e-03\n",
      "Epoch: 259100 | training loss: 1.9206e-03 | validation loss: 1.5294e-03\n",
      "Epoch: 259110 | training loss: 1.9132e-03 | validation loss: 1.5132e-03\n",
      "Epoch: 259120 | training loss: 1.9094e-03 | validation loss: 1.5151e-03\n",
      "Epoch: 259130 | training loss: 1.9080e-03 | validation loss: 1.5094e-03\n",
      "Epoch: 259140 | training loss: 1.9075e-03 | validation loss: 1.5095e-03\n",
      "Epoch: 259150 | training loss: 1.9073e-03 | validation loss: 1.5078e-03\n",
      "Epoch: 259160 | training loss: 1.9073e-03 | validation loss: 1.5079e-03\n",
      "Epoch: 259170 | training loss: 1.9073e-03 | validation loss: 1.5070e-03\n",
      "Epoch: 259180 | training loss: 1.9072e-03 | validation loss: 1.5072e-03\n",
      "Epoch: 259190 | training loss: 1.9072e-03 | validation loss: 1.5075e-03\n",
      "Epoch: 259200 | training loss: 1.9072e-03 | validation loss: 1.5076e-03\n",
      "Epoch: 259210 | training loss: 1.9072e-03 | validation loss: 1.5078e-03\n",
      "Epoch: 259220 | training loss: 1.9075e-03 | validation loss: 1.5096e-03\n",
      "Epoch: 259230 | training loss: 1.9269e-03 | validation loss: 1.5323e-03\n",
      "Epoch: 259240 | training loss: 3.2623e-03 | validation loss: 2.2925e-03\n",
      "Epoch: 259250 | training loss: 2.5309e-03 | validation loss: 1.7246e-03\n",
      "Epoch: 259260 | training loss: 1.9869e-03 | validation loss: 1.5143e-03\n",
      "Epoch: 259270 | training loss: 1.9211e-03 | validation loss: 1.5258e-03\n",
      "Epoch: 259280 | training loss: 1.9363e-03 | validation loss: 1.5397e-03\n",
      "Epoch: 259290 | training loss: 1.9080e-03 | validation loss: 1.5110e-03\n",
      "Epoch: 259300 | training loss: 1.9104e-03 | validation loss: 1.5029e-03\n",
      "Epoch: 259310 | training loss: 1.9072e-03 | validation loss: 1.5063e-03\n",
      "Epoch: 259320 | training loss: 1.9076e-03 | validation loss: 1.5101e-03\n",
      "Epoch: 259330 | training loss: 1.9071e-03 | validation loss: 1.5066e-03\n",
      "Epoch: 259340 | training loss: 1.9070e-03 | validation loss: 1.5072e-03\n",
      "Epoch: 259350 | training loss: 1.9070e-03 | validation loss: 1.5077e-03\n",
      "Epoch: 259360 | training loss: 1.9070e-03 | validation loss: 1.5070e-03\n",
      "Epoch: 259370 | training loss: 1.9070e-03 | validation loss: 1.5074e-03\n",
      "Epoch: 259380 | training loss: 1.9069e-03 | validation loss: 1.5072e-03\n",
      "Epoch: 259390 | training loss: 1.9069e-03 | validation loss: 1.5072e-03\n",
      "Epoch: 259400 | training loss: 1.9069e-03 | validation loss: 1.5073e-03\n",
      "Epoch: 259410 | training loss: 1.9069e-03 | validation loss: 1.5072e-03\n",
      "Epoch: 259420 | training loss: 1.9069e-03 | validation loss: 1.5072e-03\n",
      "Epoch: 259430 | training loss: 1.9069e-03 | validation loss: 1.5071e-03\n",
      "Epoch: 259440 | training loss: 1.9068e-03 | validation loss: 1.5070e-03\n",
      "Epoch: 259450 | training loss: 1.9069e-03 | validation loss: 1.5061e-03\n",
      "Epoch: 259460 | training loss: 1.9185e-03 | validation loss: 1.5014e-03\n",
      "Epoch: 259470 | training loss: 3.5514e-03 | validation loss: 2.1717e-03\n",
      "Epoch: 259480 | training loss: 2.5213e-03 | validation loss: 1.8912e-03\n",
      "Epoch: 259490 | training loss: 2.2148e-03 | validation loss: 1.7137e-03\n",
      "Epoch: 259500 | training loss: 2.0167e-03 | validation loss: 1.5932e-03\n",
      "Epoch: 259510 | training loss: 1.9403e-03 | validation loss: 1.5419e-03\n",
      "Epoch: 259520 | training loss: 1.9134e-03 | validation loss: 1.5189e-03\n",
      "Epoch: 259530 | training loss: 1.9069e-03 | validation loss: 1.5086e-03\n",
      "Epoch: 259540 | training loss: 1.9072e-03 | validation loss: 1.5050e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 259550 | training loss: 1.9073e-03 | validation loss: 1.5049e-03\n",
      "Epoch: 259560 | training loss: 1.9067e-03 | validation loss: 1.5063e-03\n",
      "Epoch: 259570 | training loss: 1.9067e-03 | validation loss: 1.5077e-03\n",
      "Epoch: 259580 | training loss: 1.9067e-03 | validation loss: 1.5075e-03\n",
      "Epoch: 259590 | training loss: 1.9066e-03 | validation loss: 1.5068e-03\n",
      "Epoch: 259600 | training loss: 1.9066e-03 | validation loss: 1.5069e-03\n",
      "Epoch: 259610 | training loss: 1.9066e-03 | validation loss: 1.5071e-03\n",
      "Epoch: 259620 | training loss: 1.9066e-03 | validation loss: 1.5069e-03\n",
      "Epoch: 259630 | training loss: 1.9066e-03 | validation loss: 1.5070e-03\n",
      "Epoch: 259640 | training loss: 1.9065e-03 | validation loss: 1.5069e-03\n",
      "Epoch: 259650 | training loss: 1.9065e-03 | validation loss: 1.5069e-03\n",
      "Epoch: 259660 | training loss: 1.9065e-03 | validation loss: 1.5070e-03\n",
      "Epoch: 259670 | training loss: 1.9066e-03 | validation loss: 1.5082e-03\n",
      "Epoch: 259680 | training loss: 1.9317e-03 | validation loss: 1.5435e-03\n",
      "Epoch: 259690 | training loss: 2.4284e-03 | validation loss: 1.9401e-03\n",
      "Epoch: 259700 | training loss: 2.1537e-03 | validation loss: 1.6684e-03\n",
      "Epoch: 259710 | training loss: 1.9903e-03 | validation loss: 1.5952e-03\n",
      "Epoch: 259720 | training loss: 1.9339e-03 | validation loss: 1.5301e-03\n",
      "Epoch: 259730 | training loss: 1.9157e-03 | validation loss: 1.5177e-03\n",
      "Epoch: 259740 | training loss: 1.9108e-03 | validation loss: 1.5174e-03\n",
      "Epoch: 259750 | training loss: 1.9075e-03 | validation loss: 1.5089e-03\n",
      "Epoch: 259760 | training loss: 1.9069e-03 | validation loss: 1.5053e-03\n",
      "Epoch: 259770 | training loss: 1.9078e-03 | validation loss: 1.5024e-03\n",
      "Epoch: 259780 | training loss: 1.9260e-03 | validation loss: 1.4977e-03\n",
      "Epoch: 259790 | training loss: 2.4081e-03 | validation loss: 1.6598e-03\n",
      "Epoch: 259800 | training loss: 1.9568e-03 | validation loss: 1.5599e-03\n",
      "Epoch: 259810 | training loss: 2.0015e-03 | validation loss: 1.5153e-03\n",
      "Epoch: 259820 | training loss: 1.9591e-03 | validation loss: 1.5601e-03\n",
      "Epoch: 259830 | training loss: 1.9211e-03 | validation loss: 1.4989e-03\n",
      "Epoch: 259840 | training loss: 1.9100e-03 | validation loss: 1.5155e-03\n",
      "Epoch: 259850 | training loss: 1.9077e-03 | validation loss: 1.5021e-03\n",
      "Epoch: 259860 | training loss: 1.9072e-03 | validation loss: 1.5103e-03\n",
      "Epoch: 259870 | training loss: 1.9066e-03 | validation loss: 1.5040e-03\n",
      "Epoch: 259880 | training loss: 1.9062e-03 | validation loss: 1.5064e-03\n",
      "Epoch: 259890 | training loss: 1.9063e-03 | validation loss: 1.5075e-03\n",
      "Epoch: 259900 | training loss: 1.9062e-03 | validation loss: 1.5073e-03\n",
      "Epoch: 259910 | training loss: 1.9063e-03 | validation loss: 1.5081e-03\n",
      "Epoch: 259920 | training loss: 1.9095e-03 | validation loss: 1.5149e-03\n",
      "Epoch: 259930 | training loss: 2.0424e-03 | validation loss: 1.6166e-03\n",
      "Epoch: 259940 | training loss: 2.5735e-03 | validation loss: 1.9218e-03\n",
      "Epoch: 259950 | training loss: 1.9138e-03 | validation loss: 1.5170e-03\n",
      "Epoch: 259960 | training loss: 1.9980e-03 | validation loss: 1.5180e-03\n",
      "Epoch: 259970 | training loss: 1.9241e-03 | validation loss: 1.5235e-03\n",
      "Epoch: 259980 | training loss: 1.9077e-03 | validation loss: 1.5114e-03\n",
      "Epoch: 259990 | training loss: 1.9107e-03 | validation loss: 1.5032e-03\n",
      "Epoch: 260000 | training loss: 1.9079e-03 | validation loss: 1.5107e-03\n",
      "Epoch: 260010 | training loss: 1.9065e-03 | validation loss: 1.5039e-03\n",
      "Epoch: 260020 | training loss: 1.9062e-03 | validation loss: 1.5079e-03\n",
      "Epoch: 260030 | training loss: 1.9060e-03 | validation loss: 1.5049e-03\n",
      "Epoch: 260040 | training loss: 1.9060e-03 | validation loss: 1.5069e-03\n",
      "Epoch: 260050 | training loss: 1.9059e-03 | validation loss: 1.5059e-03\n",
      "Epoch: 260060 | training loss: 1.9059e-03 | validation loss: 1.5057e-03\n",
      "Epoch: 260070 | training loss: 1.9059e-03 | validation loss: 1.5058e-03\n",
      "Epoch: 260080 | training loss: 1.9059e-03 | validation loss: 1.5057e-03\n",
      "Epoch: 260090 | training loss: 1.9059e-03 | validation loss: 1.5052e-03\n",
      "Epoch: 260100 | training loss: 1.9077e-03 | validation loss: 1.5023e-03\n",
      "Epoch: 260110 | training loss: 2.0603e-03 | validation loss: 1.5409e-03\n",
      "Epoch: 260120 | training loss: 2.4001e-03 | validation loss: 1.6677e-03\n",
      "Epoch: 260130 | training loss: 2.2056e-03 | validation loss: 1.6034e-03\n",
      "Epoch: 260140 | training loss: 1.9066e-03 | validation loss: 1.5095e-03\n",
      "Epoch: 260150 | training loss: 1.9439e-03 | validation loss: 1.5396e-03\n",
      "Epoch: 260160 | training loss: 1.9189e-03 | validation loss: 1.5222e-03\n",
      "Epoch: 260170 | training loss: 1.9063e-03 | validation loss: 1.5045e-03\n",
      "Epoch: 260180 | training loss: 1.9081e-03 | validation loss: 1.5027e-03\n",
      "Epoch: 260190 | training loss: 1.9058e-03 | validation loss: 1.5070e-03\n",
      "Epoch: 260200 | training loss: 1.9059e-03 | validation loss: 1.5072e-03\n",
      "Epoch: 260210 | training loss: 1.9058e-03 | validation loss: 1.5051e-03\n",
      "Epoch: 260220 | training loss: 1.9057e-03 | validation loss: 1.5061e-03\n",
      "Epoch: 260230 | training loss: 1.9056e-03 | validation loss: 1.5058e-03\n",
      "Epoch: 260240 | training loss: 1.9056e-03 | validation loss: 1.5057e-03\n",
      "Epoch: 260250 | training loss: 1.9056e-03 | validation loss: 1.5057e-03\n",
      "Epoch: 260260 | training loss: 1.9056e-03 | validation loss: 1.5055e-03\n",
      "Epoch: 260270 | training loss: 1.9057e-03 | validation loss: 1.5041e-03\n",
      "Epoch: 260280 | training loss: 1.9213e-03 | validation loss: 1.5003e-03\n",
      "Epoch: 260290 | training loss: 2.6481e-03 | validation loss: 1.8814e-03\n",
      "Epoch: 260300 | training loss: 1.9323e-03 | validation loss: 1.5367e-03\n",
      "Epoch: 260310 | training loss: 1.9388e-03 | validation loss: 1.5319e-03\n",
      "Epoch: 260320 | training loss: 1.9284e-03 | validation loss: 1.5319e-03\n",
      "Epoch: 260330 | training loss: 1.9131e-03 | validation loss: 1.5212e-03\n",
      "Epoch: 260340 | training loss: 1.9072e-03 | validation loss: 1.5010e-03\n",
      "Epoch: 260350 | training loss: 1.9064e-03 | validation loss: 1.5023e-03\n",
      "Epoch: 260360 | training loss: 1.9058e-03 | validation loss: 1.5061e-03\n",
      "Epoch: 260370 | training loss: 1.9065e-03 | validation loss: 1.5105e-03\n",
      "Epoch: 260380 | training loss: 1.9194e-03 | validation loss: 1.5276e-03\n",
      "Epoch: 260390 | training loss: 2.2862e-03 | validation loss: 1.7684e-03\n",
      "Epoch: 260400 | training loss: 1.9087e-03 | validation loss: 1.5034e-03\n",
      "Epoch: 260410 | training loss: 1.9537e-03 | validation loss: 1.5541e-03\n",
      "Epoch: 260420 | training loss: 1.9604e-03 | validation loss: 1.5067e-03\n",
      "Epoch: 260430 | training loss: 1.9268e-03 | validation loss: 1.5328e-03\n",
      "Epoch: 260440 | training loss: 1.9116e-03 | validation loss: 1.4984e-03\n",
      "Epoch: 260450 | training loss: 1.9076e-03 | validation loss: 1.5116e-03\n",
      "Epoch: 260460 | training loss: 1.9065e-03 | validation loss: 1.5021e-03\n",
      "Epoch: 260470 | training loss: 1.9057e-03 | validation loss: 1.5078e-03\n",
      "Epoch: 260480 | training loss: 1.9052e-03 | validation loss: 1.5049e-03\n",
      "Epoch: 260490 | training loss: 1.9053e-03 | validation loss: 1.5040e-03\n",
      "Epoch: 260500 | training loss: 1.9052e-03 | validation loss: 1.5045e-03\n",
      "Epoch: 260510 | training loss: 1.9052e-03 | validation loss: 1.5045e-03\n",
      "Epoch: 260520 | training loss: 1.9055e-03 | validation loss: 1.5032e-03\n",
      "Epoch: 260530 | training loss: 1.9157e-03 | validation loss: 1.4984e-03\n",
      "Epoch: 260540 | training loss: 2.5614e-03 | validation loss: 1.7314e-03\n",
      "Epoch: 260550 | training loss: 2.2330e-03 | validation loss: 1.7357e-03\n",
      "Epoch: 260560 | training loss: 2.0417e-03 | validation loss: 1.5345e-03\n",
      "Epoch: 260570 | training loss: 1.9608e-03 | validation loss: 1.4986e-03\n",
      "Epoch: 260580 | training loss: 1.9124e-03 | validation loss: 1.5170e-03\n",
      "Epoch: 260590 | training loss: 1.9141e-03 | validation loss: 1.5243e-03\n",
      "Epoch: 260600 | training loss: 1.9070e-03 | validation loss: 1.4994e-03\n",
      "Epoch: 260610 | training loss: 1.9053e-03 | validation loss: 1.5034e-03\n",
      "Epoch: 260620 | training loss: 1.9055e-03 | validation loss: 1.5081e-03\n",
      "Epoch: 260630 | training loss: 1.9052e-03 | validation loss: 1.5035e-03\n",
      "Epoch: 260640 | training loss: 1.9050e-03 | validation loss: 1.5057e-03\n",
      "Epoch: 260650 | training loss: 1.9050e-03 | validation loss: 1.5045e-03\n",
      "Epoch: 260660 | training loss: 1.9050e-03 | validation loss: 1.5054e-03\n",
      "Epoch: 260670 | training loss: 1.9050e-03 | validation loss: 1.5046e-03\n",
      "Epoch: 260680 | training loss: 1.9049e-03 | validation loss: 1.5049e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 260690 | training loss: 1.9049e-03 | validation loss: 1.5049e-03\n",
      "Epoch: 260700 | training loss: 1.9050e-03 | validation loss: 1.5043e-03\n",
      "Epoch: 260710 | training loss: 1.9103e-03 | validation loss: 1.5033e-03\n",
      "Epoch: 260720 | training loss: 2.4035e-03 | validation loss: 1.7962e-03\n",
      "Epoch: 260730 | training loss: 2.1079e-03 | validation loss: 1.6932e-03\n",
      "Epoch: 260740 | training loss: 2.0363e-03 | validation loss: 1.6371e-03\n",
      "Epoch: 260750 | training loss: 1.9915e-03 | validation loss: 1.5912e-03\n",
      "Epoch: 260760 | training loss: 1.9380e-03 | validation loss: 1.5419e-03\n",
      "Epoch: 260770 | training loss: 1.9119e-03 | validation loss: 1.5130e-03\n",
      "Epoch: 260780 | training loss: 1.9086e-03 | validation loss: 1.5102e-03\n",
      "Epoch: 260790 | training loss: 1.9272e-03 | validation loss: 1.5341e-03\n",
      "Epoch: 260800 | training loss: 2.3839e-03 | validation loss: 1.8273e-03\n",
      "Epoch: 260810 | training loss: 1.9332e-03 | validation loss: 1.4963e-03\n",
      "Epoch: 260820 | training loss: 1.9594e-03 | validation loss: 1.5578e-03\n",
      "Epoch: 260830 | training loss: 1.9453e-03 | validation loss: 1.4990e-03\n",
      "Epoch: 260840 | training loss: 1.9227e-03 | validation loss: 1.5291e-03\n",
      "Epoch: 260850 | training loss: 1.9116e-03 | validation loss: 1.4980e-03\n",
      "Epoch: 260860 | training loss: 1.9070e-03 | validation loss: 1.5114e-03\n",
      "Epoch: 260870 | training loss: 1.9049e-03 | validation loss: 1.5024e-03\n",
      "Epoch: 260880 | training loss: 1.9048e-03 | validation loss: 1.5030e-03\n",
      "Epoch: 260890 | training loss: 1.9048e-03 | validation loss: 1.5061e-03\n",
      "Epoch: 260900 | training loss: 1.9048e-03 | validation loss: 1.5061e-03\n",
      "Epoch: 260910 | training loss: 1.9050e-03 | validation loss: 1.5070e-03\n",
      "Epoch: 260920 | training loss: 1.9094e-03 | validation loss: 1.5149e-03\n",
      "Epoch: 260930 | training loss: 2.0568e-03 | validation loss: 1.6245e-03\n",
      "Epoch: 260940 | training loss: 2.4557e-03 | validation loss: 1.8541e-03\n",
      "Epoch: 260950 | training loss: 1.9237e-03 | validation loss: 1.5100e-03\n",
      "Epoch: 260960 | training loss: 1.9498e-03 | validation loss: 1.5001e-03\n",
      "Epoch: 260970 | training loss: 1.9381e-03 | validation loss: 1.5364e-03\n",
      "Epoch: 260980 | training loss: 1.9101e-03 | validation loss: 1.5029e-03\n",
      "Epoch: 260990 | training loss: 1.9046e-03 | validation loss: 1.5047e-03\n",
      "Epoch: 261000 | training loss: 1.9045e-03 | validation loss: 1.5043e-03\n",
      "Epoch: 261010 | training loss: 1.9044e-03 | validation loss: 1.5046e-03\n",
      "Epoch: 261020 | training loss: 1.9044e-03 | validation loss: 1.5034e-03\n",
      "Epoch: 261030 | training loss: 1.9045e-03 | validation loss: 1.5054e-03\n",
      "Epoch: 261040 | training loss: 1.9044e-03 | validation loss: 1.5037e-03\n",
      "Epoch: 261050 | training loss: 1.9044e-03 | validation loss: 1.5036e-03\n",
      "Epoch: 261060 | training loss: 1.9043e-03 | validation loss: 1.5040e-03\n",
      "Epoch: 261070 | training loss: 1.9043e-03 | validation loss: 1.5039e-03\n",
      "Epoch: 261080 | training loss: 1.9045e-03 | validation loss: 1.5029e-03\n",
      "Epoch: 261090 | training loss: 1.9133e-03 | validation loss: 1.4992e-03\n",
      "Epoch: 261100 | training loss: 2.6345e-03 | validation loss: 1.7768e-03\n",
      "Epoch: 261110 | training loss: 2.4272e-03 | validation loss: 1.8275e-03\n",
      "Epoch: 261120 | training loss: 1.9388e-03 | validation loss: 1.4955e-03\n",
      "Epoch: 261130 | training loss: 1.9945e-03 | validation loss: 1.5339e-03\n",
      "Epoch: 261140 | training loss: 1.9212e-03 | validation loss: 1.5088e-03\n",
      "Epoch: 261150 | training loss: 1.9063e-03 | validation loss: 1.5058e-03\n",
      "Epoch: 261160 | training loss: 1.9087e-03 | validation loss: 1.5106e-03\n",
      "Epoch: 261170 | training loss: 1.9042e-03 | validation loss: 1.5034e-03\n",
      "Epoch: 261180 | training loss: 1.9047e-03 | validation loss: 1.5035e-03\n",
      "Epoch: 261190 | training loss: 1.9042e-03 | validation loss: 1.5044e-03\n",
      "Epoch: 261200 | training loss: 1.9041e-03 | validation loss: 1.5043e-03\n",
      "Epoch: 261210 | training loss: 1.9041e-03 | validation loss: 1.5037e-03\n",
      "Epoch: 261220 | training loss: 1.9041e-03 | validation loss: 1.5040e-03\n",
      "Epoch: 261230 | training loss: 1.9041e-03 | validation loss: 1.5038e-03\n",
      "Epoch: 261240 | training loss: 1.9041e-03 | validation loss: 1.5039e-03\n",
      "Epoch: 261250 | training loss: 1.9041e-03 | validation loss: 1.5035e-03\n",
      "Epoch: 261260 | training loss: 1.9043e-03 | validation loss: 1.5016e-03\n",
      "Epoch: 261270 | training loss: 1.9306e-03 | validation loss: 1.4978e-03\n",
      "Epoch: 261280 | training loss: 2.8138e-03 | validation loss: 1.9107e-03\n",
      "Epoch: 261290 | training loss: 2.0698e-03 | validation loss: 1.6131e-03\n",
      "Epoch: 261300 | training loss: 1.9428e-03 | validation loss: 1.5007e-03\n",
      "Epoch: 261310 | training loss: 1.9226e-03 | validation loss: 1.5336e-03\n",
      "Epoch: 261320 | training loss: 1.9154e-03 | validation loss: 1.5083e-03\n",
      "Epoch: 261330 | training loss: 1.9093e-03 | validation loss: 1.5170e-03\n",
      "Epoch: 261340 | training loss: 1.9050e-03 | validation loss: 1.5025e-03\n",
      "Epoch: 261350 | training loss: 1.9040e-03 | validation loss: 1.5027e-03\n",
      "Epoch: 261360 | training loss: 1.9042e-03 | validation loss: 1.5042e-03\n",
      "Epoch: 261370 | training loss: 1.9039e-03 | validation loss: 1.5043e-03\n",
      "Epoch: 261380 | training loss: 1.9039e-03 | validation loss: 1.5048e-03\n",
      "Epoch: 261390 | training loss: 1.9046e-03 | validation loss: 1.5071e-03\n",
      "Epoch: 261400 | training loss: 1.9297e-03 | validation loss: 1.5353e-03\n",
      "Epoch: 261410 | training loss: 2.9785e-03 | validation loss: 2.1493e-03\n",
      "Epoch: 261420 | training loss: 2.4014e-03 | validation loss: 1.6650e-03\n",
      "Epoch: 261430 | training loss: 1.9244e-03 | validation loss: 1.5222e-03\n",
      "Epoch: 261440 | training loss: 1.9546e-03 | validation loss: 1.5491e-03\n",
      "Epoch: 261450 | training loss: 1.9133e-03 | validation loss: 1.4973e-03\n",
      "Epoch: 261460 | training loss: 1.9053e-03 | validation loss: 1.5014e-03\n",
      "Epoch: 261470 | training loss: 1.9067e-03 | validation loss: 1.5110e-03\n",
      "Epoch: 261480 | training loss: 1.9047e-03 | validation loss: 1.4999e-03\n",
      "Epoch: 261490 | training loss: 1.9039e-03 | validation loss: 1.5049e-03\n",
      "Epoch: 261500 | training loss: 1.9037e-03 | validation loss: 1.5027e-03\n",
      "Epoch: 261510 | training loss: 1.9037e-03 | validation loss: 1.5038e-03\n",
      "Epoch: 261520 | training loss: 1.9037e-03 | validation loss: 1.5027e-03\n",
      "Epoch: 261530 | training loss: 1.9036e-03 | validation loss: 1.5035e-03\n",
      "Epoch: 261540 | training loss: 1.9036e-03 | validation loss: 1.5034e-03\n",
      "Epoch: 261550 | training loss: 1.9036e-03 | validation loss: 1.5031e-03\n",
      "Epoch: 261560 | training loss: 1.9036e-03 | validation loss: 1.5030e-03\n",
      "Epoch: 261570 | training loss: 1.9036e-03 | validation loss: 1.5026e-03\n",
      "Epoch: 261580 | training loss: 1.9044e-03 | validation loss: 1.5004e-03\n",
      "Epoch: 261590 | training loss: 1.9633e-03 | validation loss: 1.5056e-03\n",
      "Epoch: 261600 | training loss: 3.5704e-03 | validation loss: 2.1753e-03\n",
      "Epoch: 261610 | training loss: 1.9073e-03 | validation loss: 1.5060e-03\n",
      "Epoch: 261620 | training loss: 2.0544e-03 | validation loss: 1.6195e-03\n",
      "Epoch: 261630 | training loss: 1.9458e-03 | validation loss: 1.5486e-03\n",
      "Epoch: 261640 | training loss: 1.9042e-03 | validation loss: 1.4999e-03\n",
      "Epoch: 261650 | training loss: 1.9117e-03 | validation loss: 1.4964e-03\n",
      "Epoch: 261660 | training loss: 1.9035e-03 | validation loss: 1.5021e-03\n",
      "Epoch: 261670 | training loss: 1.9045e-03 | validation loss: 1.5074e-03\n",
      "Epoch: 261680 | training loss: 1.9034e-03 | validation loss: 1.5026e-03\n",
      "Epoch: 261690 | training loss: 1.9035e-03 | validation loss: 1.5021e-03\n",
      "Epoch: 261700 | training loss: 1.9034e-03 | validation loss: 1.5039e-03\n",
      "Epoch: 261710 | training loss: 1.9034e-03 | validation loss: 1.5025e-03\n",
      "Epoch: 261720 | training loss: 1.9033e-03 | validation loss: 1.5033e-03\n",
      "Epoch: 261730 | training loss: 1.9033e-03 | validation loss: 1.5028e-03\n",
      "Epoch: 261740 | training loss: 1.9033e-03 | validation loss: 1.5031e-03\n",
      "Epoch: 261750 | training loss: 1.9033e-03 | validation loss: 1.5031e-03\n",
      "Epoch: 261760 | training loss: 1.9035e-03 | validation loss: 1.5048e-03\n",
      "Epoch: 261770 | training loss: 1.9387e-03 | validation loss: 1.5469e-03\n",
      "Epoch: 261780 | training loss: 2.1784e-03 | validation loss: 1.7486e-03\n",
      "Epoch: 261790 | training loss: 2.0454e-03 | validation loss: 1.6257e-03\n",
      "Epoch: 261800 | training loss: 1.9240e-03 | validation loss: 1.5149e-03\n",
      "Epoch: 261810 | training loss: 1.9033e-03 | validation loss: 1.5020e-03\n",
      "Epoch: 261820 | training loss: 1.9080e-03 | validation loss: 1.5057e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 261830 | training loss: 1.9115e-03 | validation loss: 1.5131e-03\n",
      "Epoch: 261840 | training loss: 1.9760e-03 | validation loss: 1.5713e-03\n",
      "Epoch: 261850 | training loss: 2.5407e-03 | validation loss: 1.9152e-03\n",
      "Epoch: 261860 | training loss: 2.1020e-03 | validation loss: 1.5457e-03\n",
      "Epoch: 261870 | training loss: 1.9809e-03 | validation loss: 1.5748e-03\n",
      "Epoch: 261880 | training loss: 1.9328e-03 | validation loss: 1.4953e-03\n",
      "Epoch: 261890 | training loss: 1.9139e-03 | validation loss: 1.5198e-03\n",
      "Epoch: 261900 | training loss: 1.9055e-03 | validation loss: 1.4975e-03\n",
      "Epoch: 261910 | training loss: 1.9031e-03 | validation loss: 1.5020e-03\n",
      "Epoch: 261920 | training loss: 1.9039e-03 | validation loss: 1.5066e-03\n",
      "Epoch: 261930 | training loss: 1.9031e-03 | validation loss: 1.5032e-03\n",
      "Epoch: 261940 | training loss: 1.9030e-03 | validation loss: 1.5017e-03\n",
      "Epoch: 261950 | training loss: 1.9034e-03 | validation loss: 1.5002e-03\n",
      "Epoch: 261960 | training loss: 1.9119e-03 | validation loss: 1.4952e-03\n",
      "Epoch: 261970 | training loss: 2.3234e-03 | validation loss: 1.6282e-03\n",
      "Epoch: 261980 | training loss: 1.9295e-03 | validation loss: 1.5414e-03\n",
      "Epoch: 261990 | training loss: 2.1113e-03 | validation loss: 1.5520e-03\n",
      "Epoch: 262000 | training loss: 1.9268e-03 | validation loss: 1.5208e-03\n",
      "Epoch: 262010 | training loss: 1.9153e-03 | validation loss: 1.5204e-03\n",
      "Epoch: 262020 | training loss: 1.9134e-03 | validation loss: 1.5010e-03\n",
      "Epoch: 262030 | training loss: 1.9039e-03 | validation loss: 1.5046e-03\n",
      "Epoch: 262040 | training loss: 1.9029e-03 | validation loss: 1.5021e-03\n",
      "Epoch: 262050 | training loss: 1.9030e-03 | validation loss: 1.5021e-03\n",
      "Epoch: 262060 | training loss: 1.9029e-03 | validation loss: 1.5025e-03\n",
      "Epoch: 262070 | training loss: 1.9028e-03 | validation loss: 1.5023e-03\n",
      "Epoch: 262080 | training loss: 1.9028e-03 | validation loss: 1.5018e-03\n",
      "Epoch: 262090 | training loss: 1.9028e-03 | validation loss: 1.5027e-03\n",
      "Epoch: 262100 | training loss: 1.9027e-03 | validation loss: 1.5022e-03\n",
      "Epoch: 262110 | training loss: 1.9027e-03 | validation loss: 1.5020e-03\n",
      "Epoch: 262120 | training loss: 1.9027e-03 | validation loss: 1.5018e-03\n",
      "Epoch: 262130 | training loss: 1.9029e-03 | validation loss: 1.5009e-03\n",
      "Epoch: 262140 | training loss: 1.9094e-03 | validation loss: 1.4974e-03\n",
      "Epoch: 262150 | training loss: 2.3917e-03 | validation loss: 1.6727e-03\n",
      "Epoch: 262160 | training loss: 2.0639e-03 | validation loss: 1.6129e-03\n",
      "Epoch: 262170 | training loss: 2.1053e-03 | validation loss: 1.5573e-03\n",
      "Epoch: 262180 | training loss: 1.9726e-03 | validation loss: 1.5302e-03\n",
      "Epoch: 262190 | training loss: 1.9042e-03 | validation loss: 1.5059e-03\n",
      "Epoch: 262200 | training loss: 1.9158e-03 | validation loss: 1.5134e-03\n",
      "Epoch: 262210 | training loss: 1.9027e-03 | validation loss: 1.5039e-03\n",
      "Epoch: 262220 | training loss: 1.9043e-03 | validation loss: 1.5001e-03\n",
      "Epoch: 262230 | training loss: 1.9026e-03 | validation loss: 1.5031e-03\n",
      "Epoch: 262240 | training loss: 1.9026e-03 | validation loss: 1.5023e-03\n",
      "Epoch: 262250 | training loss: 1.9026e-03 | validation loss: 1.5017e-03\n",
      "Epoch: 262260 | training loss: 1.9025e-03 | validation loss: 1.5024e-03\n",
      "Epoch: 262270 | training loss: 1.9025e-03 | validation loss: 1.5018e-03\n",
      "Epoch: 262280 | training loss: 1.9025e-03 | validation loss: 1.5020e-03\n",
      "Epoch: 262290 | training loss: 1.9025e-03 | validation loss: 1.5016e-03\n",
      "Epoch: 262300 | training loss: 1.9025e-03 | validation loss: 1.5009e-03\n",
      "Epoch: 262310 | training loss: 1.9051e-03 | validation loss: 1.4961e-03\n",
      "Epoch: 262320 | training loss: 2.1574e-03 | validation loss: 1.5949e-03\n",
      "Epoch: 262330 | training loss: 2.1324e-03 | validation loss: 1.6559e-03\n",
      "Epoch: 262340 | training loss: 1.9158e-03 | validation loss: 1.5133e-03\n",
      "Epoch: 262350 | training loss: 1.9284e-03 | validation loss: 1.5023e-03\n",
      "Epoch: 262360 | training loss: 1.9177e-03 | validation loss: 1.5032e-03\n",
      "Epoch: 262370 | training loss: 1.9060e-03 | validation loss: 1.4956e-03\n",
      "Epoch: 262380 | training loss: 1.9035e-03 | validation loss: 1.5067e-03\n",
      "Epoch: 262390 | training loss: 1.9033e-03 | validation loss: 1.5019e-03\n",
      "Epoch: 262400 | training loss: 1.9025e-03 | validation loss: 1.5007e-03\n",
      "Epoch: 262410 | training loss: 1.9023e-03 | validation loss: 1.5009e-03\n",
      "Epoch: 262420 | training loss: 1.9023e-03 | validation loss: 1.5022e-03\n",
      "Epoch: 262430 | training loss: 1.9031e-03 | validation loss: 1.5057e-03\n",
      "Epoch: 262440 | training loss: 1.9345e-03 | validation loss: 1.5385e-03\n",
      "Epoch: 262450 | training loss: 3.1047e-03 | validation loss: 2.2147e-03\n",
      "Epoch: 262460 | training loss: 2.3729e-03 | validation loss: 1.6554e-03\n",
      "Epoch: 262470 | training loss: 1.9049e-03 | validation loss: 1.4993e-03\n",
      "Epoch: 262480 | training loss: 1.9608e-03 | validation loss: 1.5522e-03\n",
      "Epoch: 262490 | training loss: 1.9055e-03 | validation loss: 1.4968e-03\n",
      "Epoch: 262500 | training loss: 1.9059e-03 | validation loss: 1.4982e-03\n",
      "Epoch: 262510 | training loss: 1.9050e-03 | validation loss: 1.5088e-03\n",
      "Epoch: 262520 | training loss: 1.9027e-03 | validation loss: 1.4985e-03\n",
      "Epoch: 262530 | training loss: 1.9022e-03 | validation loss: 1.5023e-03\n",
      "Epoch: 262540 | training loss: 1.9021e-03 | validation loss: 1.5011e-03\n",
      "Epoch: 262550 | training loss: 1.9021e-03 | validation loss: 1.5016e-03\n",
      "Epoch: 262560 | training loss: 1.9021e-03 | validation loss: 1.5009e-03\n",
      "Epoch: 262570 | training loss: 1.9020e-03 | validation loss: 1.5016e-03\n",
      "Epoch: 262580 | training loss: 1.9020e-03 | validation loss: 1.5014e-03\n",
      "Epoch: 262590 | training loss: 1.9020e-03 | validation loss: 1.5011e-03\n",
      "Epoch: 262600 | training loss: 1.9020e-03 | validation loss: 1.5010e-03\n",
      "Epoch: 262610 | training loss: 1.9020e-03 | validation loss: 1.5005e-03\n",
      "Epoch: 262620 | training loss: 1.9035e-03 | validation loss: 1.4977e-03\n",
      "Epoch: 262630 | training loss: 2.0041e-03 | validation loss: 1.5165e-03\n",
      "Epoch: 262640 | training loss: 2.9854e-03 | validation loss: 1.9180e-03\n",
      "Epoch: 262650 | training loss: 1.9611e-03 | validation loss: 1.5006e-03\n",
      "Epoch: 262660 | training loss: 1.9847e-03 | validation loss: 1.5719e-03\n",
      "Epoch: 262670 | training loss: 1.9468e-03 | validation loss: 1.5471e-03\n",
      "Epoch: 262680 | training loss: 1.9024e-03 | validation loss: 1.4987e-03\n",
      "Epoch: 262690 | training loss: 1.9090e-03 | validation loss: 1.4952e-03\n",
      "Epoch: 262700 | training loss: 1.9019e-03 | validation loss: 1.5019e-03\n",
      "Epoch: 262710 | training loss: 1.9026e-03 | validation loss: 1.5047e-03\n",
      "Epoch: 262720 | training loss: 1.9020e-03 | validation loss: 1.4995e-03\n",
      "Epoch: 262730 | training loss: 1.9018e-03 | validation loss: 1.5014e-03\n",
      "Epoch: 262740 | training loss: 1.9018e-03 | validation loss: 1.5013e-03\n",
      "Epoch: 262750 | training loss: 1.9018e-03 | validation loss: 1.5007e-03\n",
      "Epoch: 262760 | training loss: 1.9017e-03 | validation loss: 1.5012e-03\n",
      "Epoch: 262770 | training loss: 1.9017e-03 | validation loss: 1.5009e-03\n",
      "Epoch: 262780 | training loss: 1.9017e-03 | validation loss: 1.5007e-03\n",
      "Epoch: 262790 | training loss: 1.9019e-03 | validation loss: 1.4997e-03\n",
      "Epoch: 262800 | training loss: 1.9243e-03 | validation loss: 1.5011e-03\n",
      "Epoch: 262810 | training loss: 2.4193e-03 | validation loss: 1.7745e-03\n",
      "Epoch: 262820 | training loss: 1.9656e-03 | validation loss: 1.5258e-03\n",
      "Epoch: 262830 | training loss: 1.9096e-03 | validation loss: 1.5172e-03\n",
      "Epoch: 262840 | training loss: 1.9119e-03 | validation loss: 1.5117e-03\n",
      "Epoch: 262850 | training loss: 1.9119e-03 | validation loss: 1.5062e-03\n",
      "Epoch: 262860 | training loss: 1.9082e-03 | validation loss: 1.4969e-03\n",
      "Epoch: 262870 | training loss: 1.9429e-03 | validation loss: 1.4941e-03\n",
      "Epoch: 262880 | training loss: 2.3816e-03 | validation loss: 1.6458e-03\n",
      "Epoch: 262890 | training loss: 1.9377e-03 | validation loss: 1.5410e-03\n",
      "Epoch: 262900 | training loss: 1.9103e-03 | validation loss: 1.4937e-03\n",
      "Epoch: 262910 | training loss: 1.9025e-03 | validation loss: 1.5051e-03\n",
      "Epoch: 262920 | training loss: 1.9022e-03 | validation loss: 1.5042e-03\n",
      "Epoch: 262930 | training loss: 1.9053e-03 | validation loss: 1.4946e-03\n",
      "Epoch: 262940 | training loss: 1.9032e-03 | validation loss: 1.5063e-03\n",
      "Epoch: 262950 | training loss: 1.9020e-03 | validation loss: 1.5037e-03\n",
      "Epoch: 262960 | training loss: 1.9014e-03 | validation loss: 1.5003e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 262970 | training loss: 1.9016e-03 | validation loss: 1.4987e-03\n",
      "Epoch: 262980 | training loss: 1.9049e-03 | validation loss: 1.4949e-03\n",
      "Epoch: 262990 | training loss: 2.0356e-03 | validation loss: 1.5212e-03\n",
      "Epoch: 263000 | training loss: 2.5708e-03 | validation loss: 1.7340e-03\n",
      "Epoch: 263010 | training loss: 1.9154e-03 | validation loss: 1.5065e-03\n",
      "Epoch: 263020 | training loss: 1.9784e-03 | validation loss: 1.5680e-03\n",
      "Epoch: 263030 | training loss: 1.9274e-03 | validation loss: 1.5027e-03\n",
      "Epoch: 263040 | training loss: 1.9015e-03 | validation loss: 1.4986e-03\n",
      "Epoch: 263050 | training loss: 1.9041e-03 | validation loss: 1.5056e-03\n",
      "Epoch: 263060 | training loss: 1.9032e-03 | validation loss: 1.4982e-03\n",
      "Epoch: 263070 | training loss: 1.9020e-03 | validation loss: 1.5031e-03\n",
      "Epoch: 263080 | training loss: 1.9015e-03 | validation loss: 1.4988e-03\n",
      "Epoch: 263090 | training loss: 1.9013e-03 | validation loss: 1.5014e-03\n",
      "Epoch: 263100 | training loss: 1.9012e-03 | validation loss: 1.5000e-03\n",
      "Epoch: 263110 | training loss: 1.9012e-03 | validation loss: 1.4999e-03\n",
      "Epoch: 263120 | training loss: 1.9012e-03 | validation loss: 1.5004e-03\n",
      "Epoch: 263130 | training loss: 1.9012e-03 | validation loss: 1.5005e-03\n",
      "Epoch: 263140 | training loss: 1.9012e-03 | validation loss: 1.5010e-03\n",
      "Epoch: 263150 | training loss: 1.9022e-03 | validation loss: 1.5040e-03\n",
      "Epoch: 263160 | training loss: 1.9519e-03 | validation loss: 1.5477e-03\n",
      "Epoch: 263170 | training loss: 3.4147e-03 | validation loss: 2.3655e-03\n",
      "Epoch: 263180 | training loss: 2.0864e-03 | validation loss: 1.5419e-03\n",
      "Epoch: 263190 | training loss: 2.0231e-03 | validation loss: 1.5347e-03\n",
      "Epoch: 263200 | training loss: 1.9082e-03 | validation loss: 1.5109e-03\n",
      "Epoch: 263210 | training loss: 1.9212e-03 | validation loss: 1.5197e-03\n",
      "Epoch: 263220 | training loss: 1.9021e-03 | validation loss: 1.4995e-03\n",
      "Epoch: 263230 | training loss: 1.9029e-03 | validation loss: 1.4973e-03\n",
      "Epoch: 263240 | training loss: 1.9019e-03 | validation loss: 1.5032e-03\n",
      "Epoch: 263250 | training loss: 1.9010e-03 | validation loss: 1.4996e-03\n",
      "Epoch: 263260 | training loss: 1.9010e-03 | validation loss: 1.4999e-03\n",
      "Epoch: 263270 | training loss: 1.9010e-03 | validation loss: 1.5005e-03\n",
      "Epoch: 263280 | training loss: 1.9009e-03 | validation loss: 1.5000e-03\n",
      "Epoch: 263290 | training loss: 1.9009e-03 | validation loss: 1.5002e-03\n",
      "Epoch: 263300 | training loss: 1.9009e-03 | validation loss: 1.5005e-03\n",
      "Epoch: 263310 | training loss: 1.9012e-03 | validation loss: 1.5024e-03\n",
      "Epoch: 263320 | training loss: 1.9235e-03 | validation loss: 1.5358e-03\n",
      "Epoch: 263330 | training loss: 2.6169e-03 | validation loss: 2.0781e-03\n",
      "Epoch: 263340 | training loss: 1.9314e-03 | validation loss: 1.5015e-03\n",
      "Epoch: 263350 | training loss: 1.9294e-03 | validation loss: 1.5111e-03\n",
      "Epoch: 263360 | training loss: 1.9236e-03 | validation loss: 1.5052e-03\n",
      "Epoch: 263370 | training loss: 1.9081e-03 | validation loss: 1.4914e-03\n",
      "Epoch: 263380 | training loss: 1.9023e-03 | validation loss: 1.5060e-03\n",
      "Epoch: 263390 | training loss: 1.9025e-03 | validation loss: 1.5065e-03\n",
      "Epoch: 263400 | training loss: 1.9008e-03 | validation loss: 1.5008e-03\n",
      "Epoch: 263410 | training loss: 1.9009e-03 | validation loss: 1.4989e-03\n",
      "Epoch: 263420 | training loss: 1.9007e-03 | validation loss: 1.4999e-03\n",
      "Epoch: 263430 | training loss: 1.9011e-03 | validation loss: 1.5025e-03\n",
      "Epoch: 263440 | training loss: 1.9382e-03 | validation loss: 1.5416e-03\n",
      "Epoch: 263450 | training loss: 3.9112e-03 | validation loss: 2.6493e-03\n",
      "Epoch: 263460 | training loss: 1.9408e-03 | validation loss: 1.5372e-03\n",
      "Epoch: 263470 | training loss: 2.0367e-03 | validation loss: 1.5459e-03\n",
      "Epoch: 263480 | training loss: 1.9630e-03 | validation loss: 1.5085e-03\n",
      "Epoch: 263490 | training loss: 1.9076e-03 | validation loss: 1.4928e-03\n",
      "Epoch: 263500 | training loss: 1.9028e-03 | validation loss: 1.5004e-03\n",
      "Epoch: 263510 | training loss: 1.9042e-03 | validation loss: 1.5034e-03\n",
      "Epoch: 263520 | training loss: 1.9008e-03 | validation loss: 1.4990e-03\n",
      "Epoch: 263530 | training loss: 1.9009e-03 | validation loss: 1.4982e-03\n",
      "Epoch: 263540 | training loss: 1.9006e-03 | validation loss: 1.5001e-03\n",
      "Epoch: 263550 | training loss: 1.9006e-03 | validation loss: 1.5002e-03\n",
      "Epoch: 263560 | training loss: 1.9005e-03 | validation loss: 1.4988e-03\n",
      "Epoch: 263570 | training loss: 1.9005e-03 | validation loss: 1.4995e-03\n",
      "Epoch: 263580 | training loss: 1.9005e-03 | validation loss: 1.4995e-03\n",
      "Epoch: 263590 | training loss: 1.9005e-03 | validation loss: 1.4993e-03\n",
      "Epoch: 263600 | training loss: 1.9005e-03 | validation loss: 1.4994e-03\n",
      "Epoch: 263610 | training loss: 1.9004e-03 | validation loss: 1.4993e-03\n",
      "Epoch: 263620 | training loss: 1.9004e-03 | validation loss: 1.4993e-03\n",
      "Epoch: 263630 | training loss: 1.9004e-03 | validation loss: 1.4993e-03\n",
      "Epoch: 263640 | training loss: 1.9004e-03 | validation loss: 1.4993e-03\n",
      "Epoch: 263650 | training loss: 1.9004e-03 | validation loss: 1.4993e-03\n",
      "Epoch: 263660 | training loss: 1.9004e-03 | validation loss: 1.4993e-03\n",
      "Epoch: 263670 | training loss: 1.9004e-03 | validation loss: 1.4996e-03\n",
      "Epoch: 263680 | training loss: 1.9013e-03 | validation loss: 1.5026e-03\n",
      "Epoch: 263690 | training loss: 2.0350e-03 | validation loss: 1.5999e-03\n",
      "Epoch: 263700 | training loss: 2.4256e-03 | validation loss: 1.8466e-03\n",
      "Epoch: 263710 | training loss: 2.3818e-03 | validation loss: 1.7939e-03\n",
      "Epoch: 263720 | training loss: 2.0628e-03 | validation loss: 1.5982e-03\n",
      "Epoch: 263730 | training loss: 1.9636e-03 | validation loss: 1.5302e-03\n",
      "Epoch: 263740 | training loss: 1.9223e-03 | validation loss: 1.5089e-03\n",
      "Epoch: 263750 | training loss: 1.9075e-03 | validation loss: 1.5053e-03\n",
      "Epoch: 263760 | training loss: 1.9022e-03 | validation loss: 1.5019e-03\n",
      "Epoch: 263770 | training loss: 1.9005e-03 | validation loss: 1.4983e-03\n",
      "Epoch: 263780 | training loss: 1.9002e-03 | validation loss: 1.4992e-03\n",
      "Epoch: 263790 | training loss: 1.9003e-03 | validation loss: 1.4997e-03\n",
      "Epoch: 263800 | training loss: 1.9002e-03 | validation loss: 1.4988e-03\n",
      "Epoch: 263810 | training loss: 1.9001e-03 | validation loss: 1.4993e-03\n",
      "Epoch: 263820 | training loss: 1.9001e-03 | validation loss: 1.4990e-03\n",
      "Epoch: 263830 | training loss: 1.9001e-03 | validation loss: 1.4991e-03\n",
      "Epoch: 263840 | training loss: 1.9001e-03 | validation loss: 1.4990e-03\n",
      "Epoch: 263850 | training loss: 1.9001e-03 | validation loss: 1.4990e-03\n",
      "Epoch: 263860 | training loss: 1.9001e-03 | validation loss: 1.4990e-03\n",
      "Epoch: 263870 | training loss: 1.9000e-03 | validation loss: 1.4990e-03\n",
      "Epoch: 263880 | training loss: 1.9000e-03 | validation loss: 1.4990e-03\n",
      "Epoch: 263890 | training loss: 1.9000e-03 | validation loss: 1.4991e-03\n",
      "Epoch: 263900 | training loss: 1.9000e-03 | validation loss: 1.4998e-03\n",
      "Epoch: 263910 | training loss: 1.9030e-03 | validation loss: 1.5090e-03\n",
      "Epoch: 263920 | training loss: 2.3663e-03 | validation loss: 1.8738e-03\n",
      "Epoch: 263930 | training loss: 2.3667e-03 | validation loss: 1.7006e-03\n",
      "Epoch: 263940 | training loss: 2.0954e-03 | validation loss: 1.6121e-03\n",
      "Epoch: 263950 | training loss: 1.9461e-03 | validation loss: 1.5067e-03\n",
      "Epoch: 263960 | training loss: 1.9258e-03 | validation loss: 1.4907e-03\n",
      "Epoch: 263970 | training loss: 1.9085e-03 | validation loss: 1.4993e-03\n",
      "Epoch: 263980 | training loss: 1.9022e-03 | validation loss: 1.4952e-03\n",
      "Epoch: 263990 | training loss: 1.9006e-03 | validation loss: 1.4951e-03\n",
      "Epoch: 264000 | training loss: 1.9000e-03 | validation loss: 1.4988e-03\n",
      "Epoch: 264010 | training loss: 1.8999e-03 | validation loss: 1.4979e-03\n",
      "Epoch: 264020 | training loss: 1.8999e-03 | validation loss: 1.4993e-03\n",
      "Epoch: 264030 | training loss: 1.8998e-03 | validation loss: 1.4987e-03\n",
      "Epoch: 264040 | training loss: 1.8998e-03 | validation loss: 1.4982e-03\n",
      "Epoch: 264050 | training loss: 1.8998e-03 | validation loss: 1.4983e-03\n",
      "Epoch: 264060 | training loss: 1.8998e-03 | validation loss: 1.4984e-03\n",
      "Epoch: 264070 | training loss: 1.8997e-03 | validation loss: 1.4986e-03\n",
      "Epoch: 264080 | training loss: 1.8998e-03 | validation loss: 1.4990e-03\n",
      "Epoch: 264090 | training loss: 1.9012e-03 | validation loss: 1.5031e-03\n",
      "Epoch: 264100 | training loss: 2.0131e-03 | validation loss: 1.5893e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 264110 | training loss: 2.8197e-03 | validation loss: 2.0493e-03\n",
      "Epoch: 264120 | training loss: 2.0608e-03 | validation loss: 1.6190e-03\n",
      "Epoch: 264130 | training loss: 1.9261e-03 | validation loss: 1.4919e-03\n",
      "Epoch: 264140 | training loss: 1.9542e-03 | validation loss: 1.4987e-03\n",
      "Epoch: 264150 | training loss: 1.9043e-03 | validation loss: 1.4932e-03\n",
      "Epoch: 264160 | training loss: 1.9035e-03 | validation loss: 1.5068e-03\n",
      "Epoch: 264170 | training loss: 1.9010e-03 | validation loss: 1.5028e-03\n",
      "Epoch: 264180 | training loss: 1.9002e-03 | validation loss: 1.4961e-03\n",
      "Epoch: 264190 | training loss: 1.8996e-03 | validation loss: 1.4978e-03\n",
      "Epoch: 264200 | training loss: 1.8997e-03 | validation loss: 1.4994e-03\n",
      "Epoch: 264210 | training loss: 1.8996e-03 | validation loss: 1.4976e-03\n",
      "Epoch: 264220 | training loss: 1.8995e-03 | validation loss: 1.4985e-03\n",
      "Epoch: 264230 | training loss: 1.8995e-03 | validation loss: 1.4980e-03\n",
      "Epoch: 264240 | training loss: 1.8995e-03 | validation loss: 1.4983e-03\n",
      "Epoch: 264250 | training loss: 1.8995e-03 | validation loss: 1.4980e-03\n",
      "Epoch: 264260 | training loss: 1.8995e-03 | validation loss: 1.4982e-03\n",
      "Epoch: 264270 | training loss: 1.8994e-03 | validation loss: 1.4981e-03\n",
      "Epoch: 264280 | training loss: 1.8994e-03 | validation loss: 1.4981e-03\n",
      "Epoch: 264290 | training loss: 1.8994e-03 | validation loss: 1.4980e-03\n",
      "Epoch: 264300 | training loss: 1.8994e-03 | validation loss: 1.4979e-03\n",
      "Epoch: 264310 | training loss: 1.8994e-03 | validation loss: 1.4974e-03\n",
      "Epoch: 264320 | training loss: 1.9019e-03 | validation loss: 1.4940e-03\n",
      "Epoch: 264330 | training loss: 2.2128e-03 | validation loss: 1.5926e-03\n",
      "Epoch: 264340 | training loss: 1.9295e-03 | validation loss: 1.5293e-03\n",
      "Epoch: 264350 | training loss: 2.1261e-03 | validation loss: 1.5577e-03\n",
      "Epoch: 264360 | training loss: 2.0418e-03 | validation loss: 1.5295e-03\n",
      "Epoch: 264370 | training loss: 1.9399e-03 | validation loss: 1.4981e-03\n",
      "Epoch: 264380 | training loss: 1.9018e-03 | validation loss: 1.4943e-03\n",
      "Epoch: 264390 | training loss: 1.9008e-03 | validation loss: 1.5026e-03\n",
      "Epoch: 264400 | training loss: 1.9017e-03 | validation loss: 1.5039e-03\n",
      "Epoch: 264410 | training loss: 1.8994e-03 | validation loss: 1.4991e-03\n",
      "Epoch: 264420 | training loss: 1.8995e-03 | validation loss: 1.4968e-03\n",
      "Epoch: 264430 | training loss: 1.8992e-03 | validation loss: 1.4975e-03\n",
      "Epoch: 264440 | training loss: 1.8992e-03 | validation loss: 1.4986e-03\n",
      "Epoch: 264450 | training loss: 1.8992e-03 | validation loss: 1.4978e-03\n",
      "Epoch: 264460 | training loss: 1.8992e-03 | validation loss: 1.4978e-03\n",
      "Epoch: 264470 | training loss: 1.8991e-03 | validation loss: 1.4980e-03\n",
      "Epoch: 264480 | training loss: 1.8991e-03 | validation loss: 1.4978e-03\n",
      "Epoch: 264490 | training loss: 1.8991e-03 | validation loss: 1.4979e-03\n",
      "Epoch: 264500 | training loss: 1.8991e-03 | validation loss: 1.4976e-03\n",
      "Epoch: 264510 | training loss: 1.8992e-03 | validation loss: 1.4966e-03\n",
      "Epoch: 264520 | training loss: 1.9148e-03 | validation loss: 1.4937e-03\n",
      "Epoch: 264530 | training loss: 2.6715e-03 | validation loss: 1.9065e-03\n",
      "Epoch: 264540 | training loss: 2.0074e-03 | validation loss: 1.5643e-03\n",
      "Epoch: 264550 | training loss: 1.9105e-03 | validation loss: 1.4898e-03\n",
      "Epoch: 264560 | training loss: 1.8992e-03 | validation loss: 1.4991e-03\n",
      "Epoch: 264570 | training loss: 1.9010e-03 | validation loss: 1.5045e-03\n",
      "Epoch: 264580 | training loss: 1.9021e-03 | validation loss: 1.4974e-03\n",
      "Epoch: 264590 | training loss: 1.9002e-03 | validation loss: 1.4994e-03\n",
      "Epoch: 264600 | training loss: 1.8993e-03 | validation loss: 1.5001e-03\n",
      "Epoch: 264610 | training loss: 1.8998e-03 | validation loss: 1.5014e-03\n",
      "Epoch: 264620 | training loss: 1.9119e-03 | validation loss: 1.5176e-03\n",
      "Epoch: 264630 | training loss: 2.2859e-03 | validation loss: 1.7648e-03\n",
      "Epoch: 264640 | training loss: 1.9014e-03 | validation loss: 1.4919e-03\n",
      "Epoch: 264650 | training loss: 1.9735e-03 | validation loss: 1.5643e-03\n",
      "Epoch: 264660 | training loss: 1.9612e-03 | validation loss: 1.4980e-03\n",
      "Epoch: 264670 | training loss: 1.9147e-03 | validation loss: 1.5202e-03\n",
      "Epoch: 264680 | training loss: 1.9010e-03 | validation loss: 1.4932e-03\n",
      "Epoch: 264690 | training loss: 1.8992e-03 | validation loss: 1.4995e-03\n",
      "Epoch: 264700 | training loss: 1.8991e-03 | validation loss: 1.4952e-03\n",
      "Epoch: 264710 | training loss: 1.8991e-03 | validation loss: 1.4996e-03\n",
      "Epoch: 264720 | training loss: 1.8990e-03 | validation loss: 1.4957e-03\n",
      "Epoch: 264730 | training loss: 1.8987e-03 | validation loss: 1.4971e-03\n",
      "Epoch: 264740 | training loss: 1.8988e-03 | validation loss: 1.4981e-03\n",
      "Epoch: 264750 | training loss: 1.8988e-03 | validation loss: 1.4983e-03\n",
      "Epoch: 264760 | training loss: 1.8993e-03 | validation loss: 1.5002e-03\n",
      "Epoch: 264770 | training loss: 1.9116e-03 | validation loss: 1.5166e-03\n",
      "Epoch: 264780 | training loss: 2.4435e-03 | validation loss: 1.8473e-03\n",
      "Epoch: 264790 | training loss: 2.0099e-03 | validation loss: 1.5083e-03\n",
      "Epoch: 264800 | training loss: 2.0983e-03 | validation loss: 1.6421e-03\n",
      "Epoch: 264810 | training loss: 1.9127e-03 | validation loss: 1.5030e-03\n",
      "Epoch: 264820 | training loss: 1.9120e-03 | validation loss: 1.4877e-03\n",
      "Epoch: 264830 | training loss: 1.9065e-03 | validation loss: 1.5090e-03\n",
      "Epoch: 264840 | training loss: 1.9005e-03 | validation loss: 1.4978e-03\n",
      "Epoch: 264850 | training loss: 1.8992e-03 | validation loss: 1.4956e-03\n",
      "Epoch: 264860 | training loss: 1.8988e-03 | validation loss: 1.4988e-03\n",
      "Epoch: 264870 | training loss: 1.8987e-03 | validation loss: 1.4962e-03\n",
      "Epoch: 264880 | training loss: 1.8986e-03 | validation loss: 1.4975e-03\n",
      "Epoch: 264890 | training loss: 1.8985e-03 | validation loss: 1.4970e-03\n",
      "Epoch: 264900 | training loss: 1.8985e-03 | validation loss: 1.4968e-03\n",
      "Epoch: 264910 | training loss: 1.8985e-03 | validation loss: 1.4971e-03\n",
      "Epoch: 264920 | training loss: 1.8985e-03 | validation loss: 1.4971e-03\n",
      "Epoch: 264930 | training loss: 1.8985e-03 | validation loss: 1.4971e-03\n",
      "Epoch: 264940 | training loss: 1.9005e-03 | validation loss: 1.4984e-03\n",
      "Epoch: 264950 | training loss: 2.0286e-03 | validation loss: 1.5723e-03\n",
      "Epoch: 264960 | training loss: 2.7596e-03 | validation loss: 1.8064e-03\n",
      "Epoch: 264970 | training loss: 2.2124e-03 | validation loss: 1.7576e-03\n",
      "Epoch: 264980 | training loss: 1.9736e-03 | validation loss: 1.5533e-03\n",
      "Epoch: 264990 | training loss: 1.9292e-03 | validation loss: 1.5212e-03\n",
      "Epoch: 265000 | training loss: 1.9162e-03 | validation loss: 1.5262e-03\n",
      "Epoch: 265010 | training loss: 1.9049e-03 | validation loss: 1.4989e-03\n",
      "Epoch: 265020 | training loss: 1.9001e-03 | validation loss: 1.5037e-03\n",
      "Epoch: 265030 | training loss: 1.8987e-03 | validation loss: 1.4952e-03\n",
      "Epoch: 265040 | training loss: 1.8985e-03 | validation loss: 1.4977e-03\n",
      "Epoch: 265050 | training loss: 1.8984e-03 | validation loss: 1.4950e-03\n",
      "Epoch: 265060 | training loss: 1.8983e-03 | validation loss: 1.4962e-03\n",
      "Epoch: 265070 | training loss: 1.8983e-03 | validation loss: 1.4972e-03\n",
      "Epoch: 265080 | training loss: 1.8982e-03 | validation loss: 1.4971e-03\n",
      "Epoch: 265090 | training loss: 1.8982e-03 | validation loss: 1.4972e-03\n",
      "Epoch: 265100 | training loss: 1.8989e-03 | validation loss: 1.4999e-03\n",
      "Epoch: 265110 | training loss: 1.9278e-03 | validation loss: 1.5313e-03\n",
      "Epoch: 265120 | training loss: 3.1933e-03 | validation loss: 2.2586e-03\n",
      "Epoch: 265130 | training loss: 2.3887e-03 | validation loss: 1.6605e-03\n",
      "Epoch: 265140 | training loss: 1.9099e-03 | validation loss: 1.4860e-03\n",
      "Epoch: 265150 | training loss: 1.9579e-03 | validation loss: 1.5469e-03\n",
      "Epoch: 265160 | training loss: 1.8996e-03 | validation loss: 1.5005e-03\n",
      "Epoch: 265170 | training loss: 1.9067e-03 | validation loss: 1.4922e-03\n",
      "Epoch: 265180 | training loss: 1.8986e-03 | validation loss: 1.4996e-03\n",
      "Epoch: 265190 | training loss: 1.8984e-03 | validation loss: 1.4979e-03\n",
      "Epoch: 265200 | training loss: 1.8984e-03 | validation loss: 1.4944e-03\n",
      "Epoch: 265210 | training loss: 1.8982e-03 | validation loss: 1.4981e-03\n",
      "Epoch: 265220 | training loss: 1.8981e-03 | validation loss: 1.4955e-03\n",
      "Epoch: 265230 | training loss: 1.8980e-03 | validation loss: 1.4969e-03\n",
      "Epoch: 265240 | training loss: 1.8980e-03 | validation loss: 1.4961e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 265250 | training loss: 1.8980e-03 | validation loss: 1.4963e-03\n",
      "Epoch: 265260 | training loss: 1.8979e-03 | validation loss: 1.4965e-03\n",
      "Epoch: 265270 | training loss: 1.8979e-03 | validation loss: 1.4964e-03\n",
      "Epoch: 265280 | training loss: 1.8979e-03 | validation loss: 1.4963e-03\n",
      "Epoch: 265290 | training loss: 1.8979e-03 | validation loss: 1.4964e-03\n",
      "Epoch: 265300 | training loss: 1.8980e-03 | validation loss: 1.4973e-03\n",
      "Epoch: 265310 | training loss: 1.9048e-03 | validation loss: 1.5084e-03\n",
      "Epoch: 265320 | training loss: 2.7314e-03 | validation loss: 1.9989e-03\n",
      "Epoch: 265330 | training loss: 2.7340e-03 | validation loss: 1.8047e-03\n",
      "Epoch: 265340 | training loss: 1.9317e-03 | validation loss: 1.4905e-03\n",
      "Epoch: 265350 | training loss: 1.9075e-03 | validation loss: 1.5101e-03\n",
      "Epoch: 265360 | training loss: 1.9249e-03 | validation loss: 1.5270e-03\n",
      "Epoch: 265370 | training loss: 1.9123e-03 | validation loss: 1.5163e-03\n",
      "Epoch: 265380 | training loss: 1.8990e-03 | validation loss: 1.5005e-03\n",
      "Epoch: 265390 | training loss: 1.8984e-03 | validation loss: 1.4938e-03\n",
      "Epoch: 265400 | training loss: 1.8984e-03 | validation loss: 1.4939e-03\n",
      "Epoch: 265410 | training loss: 1.8977e-03 | validation loss: 1.4968e-03\n",
      "Epoch: 265420 | training loss: 1.8978e-03 | validation loss: 1.4971e-03\n",
      "Epoch: 265430 | training loss: 1.8977e-03 | validation loss: 1.4957e-03\n",
      "Epoch: 265440 | training loss: 1.8977e-03 | validation loss: 1.4960e-03\n",
      "Epoch: 265450 | training loss: 1.8977e-03 | validation loss: 1.4962e-03\n",
      "Epoch: 265460 | training loss: 1.8976e-03 | validation loss: 1.4959e-03\n",
      "Epoch: 265470 | training loss: 1.8976e-03 | validation loss: 1.4961e-03\n",
      "Epoch: 265480 | training loss: 1.8976e-03 | validation loss: 1.4959e-03\n",
      "Epoch: 265490 | training loss: 1.8976e-03 | validation loss: 1.4959e-03\n",
      "Epoch: 265500 | training loss: 1.8978e-03 | validation loss: 1.4946e-03\n",
      "Epoch: 265510 | training loss: 1.9359e-03 | validation loss: 1.5013e-03\n",
      "Epoch: 265520 | training loss: 2.1313e-03 | validation loss: 1.5765e-03\n",
      "Epoch: 265530 | training loss: 2.0811e-03 | validation loss: 1.6047e-03\n",
      "Epoch: 265540 | training loss: 1.9627e-03 | validation loss: 1.5141e-03\n",
      "Epoch: 265550 | training loss: 1.9199e-03 | validation loss: 1.4992e-03\n",
      "Epoch: 265560 | training loss: 1.9072e-03 | validation loss: 1.4998e-03\n",
      "Epoch: 265570 | training loss: 1.9005e-03 | validation loss: 1.4924e-03\n",
      "Epoch: 265580 | training loss: 1.8988e-03 | validation loss: 1.4913e-03\n",
      "Epoch: 265590 | training loss: 1.8993e-03 | validation loss: 1.4906e-03\n",
      "Epoch: 265600 | training loss: 1.9176e-03 | validation loss: 1.4876e-03\n",
      "Epoch: 265610 | training loss: 2.3328e-03 | validation loss: 1.6241e-03\n",
      "Epoch: 265620 | training loss: 1.9085e-03 | validation loss: 1.5155e-03\n",
      "Epoch: 265630 | training loss: 1.9375e-03 | validation loss: 1.4912e-03\n",
      "Epoch: 265640 | training loss: 1.9338e-03 | validation loss: 1.5363e-03\n",
      "Epoch: 265650 | training loss: 1.9148e-03 | validation loss: 1.4878e-03\n",
      "Epoch: 265660 | training loss: 1.9042e-03 | validation loss: 1.5084e-03\n",
      "Epoch: 265670 | training loss: 1.8995e-03 | validation loss: 1.4907e-03\n",
      "Epoch: 265680 | training loss: 1.8975e-03 | validation loss: 1.4971e-03\n",
      "Epoch: 265690 | training loss: 1.8975e-03 | validation loss: 1.4971e-03\n",
      "Epoch: 265700 | training loss: 1.8974e-03 | validation loss: 1.4939e-03\n",
      "Epoch: 265710 | training loss: 1.8975e-03 | validation loss: 1.4937e-03\n",
      "Epoch: 265720 | training loss: 1.8977e-03 | validation loss: 1.4930e-03\n",
      "Epoch: 265730 | training loss: 1.9021e-03 | validation loss: 1.4893e-03\n",
      "Epoch: 265740 | training loss: 2.0479e-03 | validation loss: 1.5220e-03\n",
      "Epoch: 265750 | training loss: 2.4439e-03 | validation loss: 1.6789e-03\n",
      "Epoch: 265760 | training loss: 1.9194e-03 | validation loss: 1.5141e-03\n",
      "Epoch: 265770 | training loss: 1.9360e-03 | validation loss: 1.5356e-03\n",
      "Epoch: 265780 | training loss: 1.9317e-03 | validation loss: 1.4954e-03\n",
      "Epoch: 265790 | training loss: 1.9038e-03 | validation loss: 1.5064e-03\n",
      "Epoch: 265800 | training loss: 1.8977e-03 | validation loss: 1.4924e-03\n",
      "Epoch: 265810 | training loss: 1.8972e-03 | validation loss: 1.4964e-03\n",
      "Epoch: 265820 | training loss: 1.8971e-03 | validation loss: 1.4944e-03\n",
      "Epoch: 265830 | training loss: 1.8972e-03 | validation loss: 1.4964e-03\n",
      "Epoch: 265840 | training loss: 1.8972e-03 | validation loss: 1.4941e-03\n",
      "Epoch: 265850 | training loss: 1.8971e-03 | validation loss: 1.4955e-03\n",
      "Epoch: 265860 | training loss: 1.8971e-03 | validation loss: 1.4958e-03\n",
      "Epoch: 265870 | training loss: 1.8971e-03 | validation loss: 1.4958e-03\n",
      "Epoch: 265880 | training loss: 1.8971e-03 | validation loss: 1.4965e-03\n",
      "Epoch: 265890 | training loss: 1.8999e-03 | validation loss: 1.5022e-03\n",
      "Epoch: 265900 | training loss: 2.0398e-03 | validation loss: 1.6046e-03\n",
      "Epoch: 265910 | training loss: 2.5578e-03 | validation loss: 1.9033e-03\n",
      "Epoch: 265920 | training loss: 1.9276e-03 | validation loss: 1.5292e-03\n",
      "Epoch: 265930 | training loss: 2.0094e-03 | validation loss: 1.5126e-03\n",
      "Epoch: 265940 | training loss: 1.9001e-03 | validation loss: 1.4896e-03\n",
      "Epoch: 265950 | training loss: 1.9129e-03 | validation loss: 1.5168e-03\n",
      "Epoch: 265960 | training loss: 1.8973e-03 | validation loss: 1.4932e-03\n",
      "Epoch: 265970 | training loss: 1.8979e-03 | validation loss: 1.4922e-03\n",
      "Epoch: 265980 | training loss: 1.8977e-03 | validation loss: 1.4985e-03\n",
      "Epoch: 265990 | training loss: 1.8971e-03 | validation loss: 1.4935e-03\n",
      "Epoch: 266000 | training loss: 1.8969e-03 | validation loss: 1.4960e-03\n",
      "Epoch: 266010 | training loss: 1.8968e-03 | validation loss: 1.4944e-03\n",
      "Epoch: 266020 | training loss: 1.8968e-03 | validation loss: 1.4955e-03\n",
      "Epoch: 266030 | training loss: 1.8968e-03 | validation loss: 1.4948e-03\n",
      "Epoch: 266040 | training loss: 1.8968e-03 | validation loss: 1.4954e-03\n",
      "Epoch: 266050 | training loss: 1.8992e-03 | validation loss: 1.5021e-03\n",
      "Epoch: 266060 | training loss: 2.2062e-03 | validation loss: 1.7734e-03\n",
      "Epoch: 266070 | training loss: 2.2349e-03 | validation loss: 1.6972e-03\n",
      "Epoch: 266080 | training loss: 1.9334e-03 | validation loss: 1.4940e-03\n",
      "Epoch: 266090 | training loss: 1.8996e-03 | validation loss: 1.4899e-03\n",
      "Epoch: 266100 | training loss: 1.9042e-03 | validation loss: 1.5098e-03\n",
      "Epoch: 266110 | training loss: 1.9062e-03 | validation loss: 1.5139e-03\n",
      "Epoch: 266120 | training loss: 1.9175e-03 | validation loss: 1.5250e-03\n",
      "Epoch: 266130 | training loss: 2.1102e-03 | validation loss: 1.6561e-03\n",
      "Epoch: 266140 | training loss: 2.0512e-03 | validation loss: 1.6151e-03\n",
      "Epoch: 266150 | training loss: 1.9870e-03 | validation loss: 1.5008e-03\n",
      "Epoch: 266160 | training loss: 1.9395e-03 | validation loss: 1.5404e-03\n",
      "Epoch: 266170 | training loss: 1.9113e-03 | validation loss: 1.4872e-03\n",
      "Epoch: 266180 | training loss: 1.8970e-03 | validation loss: 1.4973e-03\n",
      "Epoch: 266190 | training loss: 1.8994e-03 | validation loss: 1.5021e-03\n",
      "Epoch: 266200 | training loss: 1.8965e-03 | validation loss: 1.4940e-03\n",
      "Epoch: 266210 | training loss: 1.8974e-03 | validation loss: 1.4913e-03\n",
      "Epoch: 266220 | training loss: 1.9033e-03 | validation loss: 1.4877e-03\n",
      "Epoch: 266230 | training loss: 2.0312e-03 | validation loss: 1.5156e-03\n",
      "Epoch: 266240 | training loss: 2.4603e-03 | validation loss: 1.6823e-03\n",
      "Epoch: 266250 | training loss: 2.0312e-03 | validation loss: 1.5967e-03\n",
      "Epoch: 266260 | training loss: 1.9086e-03 | validation loss: 1.4895e-03\n",
      "Epoch: 266270 | training loss: 1.8965e-03 | validation loss: 1.4951e-03\n",
      "Epoch: 266280 | training loss: 1.8968e-03 | validation loss: 1.4956e-03\n",
      "Epoch: 266290 | training loss: 1.8964e-03 | validation loss: 1.4941e-03\n",
      "Epoch: 266300 | training loss: 1.8966e-03 | validation loss: 1.4929e-03\n",
      "Epoch: 266310 | training loss: 1.8970e-03 | validation loss: 1.4972e-03\n",
      "Epoch: 266320 | training loss: 1.8965e-03 | validation loss: 1.4934e-03\n",
      "Epoch: 266330 | training loss: 1.8965e-03 | validation loss: 1.4928e-03\n",
      "Epoch: 266340 | training loss: 1.8964e-03 | validation loss: 1.4931e-03\n",
      "Epoch: 266350 | training loss: 1.8969e-03 | validation loss: 1.4919e-03\n",
      "Epoch: 266360 | training loss: 1.9077e-03 | validation loss: 1.4881e-03\n",
      "Epoch: 266370 | training loss: 2.3682e-03 | validation loss: 1.6493e-03\n",
      "Epoch: 266380 | training loss: 1.9375e-03 | validation loss: 1.5401e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 266390 | training loss: 2.0998e-03 | validation loss: 1.5415e-03\n",
      "Epoch: 266400 | training loss: 1.9179e-03 | validation loss: 1.5182e-03\n",
      "Epoch: 266410 | training loss: 1.9059e-03 | validation loss: 1.5122e-03\n",
      "Epoch: 266420 | training loss: 1.9066e-03 | validation loss: 1.4863e-03\n",
      "Epoch: 266430 | training loss: 1.8982e-03 | validation loss: 1.5011e-03\n",
      "Epoch: 266440 | training loss: 1.8963e-03 | validation loss: 1.4925e-03\n",
      "Epoch: 266450 | training loss: 1.8962e-03 | validation loss: 1.4946e-03\n",
      "Epoch: 266460 | training loss: 1.8961e-03 | validation loss: 1.4939e-03\n",
      "Epoch: 266470 | training loss: 1.8961e-03 | validation loss: 1.4947e-03\n",
      "Epoch: 266480 | training loss: 1.8961e-03 | validation loss: 1.4932e-03\n",
      "Epoch: 266490 | training loss: 1.8961e-03 | validation loss: 1.4940e-03\n",
      "Epoch: 266500 | training loss: 1.8963e-03 | validation loss: 1.4933e-03\n",
      "Epoch: 266510 | training loss: 1.9049e-03 | validation loss: 1.4917e-03\n",
      "Epoch: 266520 | training loss: 2.3253e-03 | validation loss: 1.7314e-03\n",
      "Epoch: 266530 | training loss: 2.0834e-03 | validation loss: 1.6195e-03\n",
      "Epoch: 266540 | training loss: 1.9467e-03 | validation loss: 1.5006e-03\n",
      "Epoch: 266550 | training loss: 1.9615e-03 | validation loss: 1.4923e-03\n",
      "Epoch: 266560 | training loss: 1.9730e-03 | validation loss: 1.4940e-03\n",
      "Epoch: 266570 | training loss: 1.9761e-03 | validation loss: 1.4993e-03\n",
      "Epoch: 266580 | training loss: 1.9323e-03 | validation loss: 1.4902e-03\n",
      "Epoch: 266590 | training loss: 1.9058e-03 | validation loss: 1.4853e-03\n",
      "Epoch: 266600 | training loss: 1.9106e-03 | validation loss: 1.4851e-03\n",
      "Epoch: 266610 | training loss: 2.0295e-03 | validation loss: 1.5140e-03\n",
      "Epoch: 266620 | training loss: 2.2863e-03 | validation loss: 1.6088e-03\n",
      "Epoch: 266630 | training loss: 2.0409e-03 | validation loss: 1.6064e-03\n",
      "Epoch: 266640 | training loss: 1.9435e-03 | validation loss: 1.4921e-03\n",
      "Epoch: 266650 | training loss: 1.9050e-03 | validation loss: 1.5091e-03\n",
      "Epoch: 266660 | training loss: 1.8960e-03 | validation loss: 1.4944e-03\n",
      "Epoch: 266670 | training loss: 1.8997e-03 | validation loss: 1.4885e-03\n",
      "Epoch: 266680 | training loss: 1.8958e-03 | validation loss: 1.4934e-03\n",
      "Epoch: 266690 | training loss: 1.8967e-03 | validation loss: 1.4976e-03\n",
      "Epoch: 266700 | training loss: 1.9012e-03 | validation loss: 1.5047e-03\n",
      "Epoch: 266710 | training loss: 1.9809e-03 | validation loss: 1.5683e-03\n",
      "Epoch: 266720 | training loss: 2.6515e-03 | validation loss: 1.9577e-03\n",
      "Epoch: 266730 | training loss: 2.1417e-03 | validation loss: 1.5599e-03\n",
      "Epoch: 266740 | training loss: 1.9709e-03 | validation loss: 1.5562e-03\n",
      "Epoch: 266750 | training loss: 1.9156e-03 | validation loss: 1.4921e-03\n",
      "Epoch: 266760 | training loss: 1.9025e-03 | validation loss: 1.5024e-03\n",
      "Epoch: 266770 | training loss: 1.8993e-03 | validation loss: 1.4914e-03\n",
      "Epoch: 266780 | training loss: 1.8975e-03 | validation loss: 1.4978e-03\n",
      "Epoch: 266790 | training loss: 1.8958e-03 | validation loss: 1.4918e-03\n",
      "Epoch: 266800 | training loss: 1.8959e-03 | validation loss: 1.4930e-03\n",
      "Epoch: 266810 | training loss: 1.8956e-03 | validation loss: 1.4937e-03\n",
      "Epoch: 266820 | training loss: 1.8957e-03 | validation loss: 1.4940e-03\n",
      "Epoch: 266830 | training loss: 1.8966e-03 | validation loss: 1.4958e-03\n",
      "Epoch: 266840 | training loss: 1.9234e-03 | validation loss: 1.5188e-03\n",
      "Epoch: 266850 | training loss: 2.8274e-03 | validation loss: 2.0380e-03\n",
      "Epoch: 266860 | training loss: 2.2177e-03 | validation loss: 1.6372e-03\n",
      "Epoch: 266870 | training loss: 1.9862e-03 | validation loss: 1.5929e-03\n",
      "Epoch: 266880 | training loss: 1.9255e-03 | validation loss: 1.4892e-03\n",
      "Epoch: 266890 | training loss: 1.9093e-03 | validation loss: 1.4991e-03\n",
      "Epoch: 266900 | training loss: 1.8955e-03 | validation loss: 1.4942e-03\n",
      "Epoch: 266910 | training loss: 1.8974e-03 | validation loss: 1.4956e-03\n",
      "Epoch: 266920 | training loss: 1.8954e-03 | validation loss: 1.4935e-03\n",
      "Epoch: 266930 | training loss: 1.8957e-03 | validation loss: 1.4934e-03\n",
      "Epoch: 266940 | training loss: 1.8955e-03 | validation loss: 1.4924e-03\n",
      "Epoch: 266950 | training loss: 1.8954e-03 | validation loss: 1.4934e-03\n",
      "Epoch: 266960 | training loss: 1.8954e-03 | validation loss: 1.4937e-03\n",
      "Epoch: 266970 | training loss: 1.8954e-03 | validation loss: 1.4942e-03\n",
      "Epoch: 266980 | training loss: 1.8958e-03 | validation loss: 1.4961e-03\n",
      "Epoch: 266990 | training loss: 1.9072e-03 | validation loss: 1.5141e-03\n",
      "Epoch: 267000 | training loss: 2.4688e-03 | validation loss: 1.8868e-03\n",
      "Epoch: 267010 | training loss: 2.1085e-03 | validation loss: 1.5449e-03\n",
      "Epoch: 267020 | training loss: 1.9961e-03 | validation loss: 1.5851e-03\n",
      "Epoch: 267030 | training loss: 1.9462e-03 | validation loss: 1.5457e-03\n",
      "Epoch: 267040 | training loss: 1.9020e-03 | validation loss: 1.4909e-03\n",
      "Epoch: 267050 | training loss: 1.9000e-03 | validation loss: 1.4861e-03\n",
      "Epoch: 267060 | training loss: 1.8966e-03 | validation loss: 1.4985e-03\n",
      "Epoch: 267070 | training loss: 1.8953e-03 | validation loss: 1.4939e-03\n",
      "Epoch: 267080 | training loss: 1.8955e-03 | validation loss: 1.4911e-03\n",
      "Epoch: 267090 | training loss: 1.8953e-03 | validation loss: 1.4939e-03\n",
      "Epoch: 267100 | training loss: 1.8952e-03 | validation loss: 1.4922e-03\n",
      "Epoch: 267110 | training loss: 1.8952e-03 | validation loss: 1.4935e-03\n",
      "Epoch: 267120 | training loss: 1.8951e-03 | validation loss: 1.4926e-03\n",
      "Epoch: 267130 | training loss: 1.8952e-03 | validation loss: 1.4935e-03\n",
      "Epoch: 267140 | training loss: 1.8961e-03 | validation loss: 1.4953e-03\n",
      "Epoch: 267150 | training loss: 1.9475e-03 | validation loss: 1.5348e-03\n",
      "Epoch: 267160 | training loss: 3.1964e-03 | validation loss: 2.2381e-03\n",
      "Epoch: 267170 | training loss: 2.1635e-03 | validation loss: 1.5591e-03\n",
      "Epoch: 267180 | training loss: 1.9144e-03 | validation loss: 1.5048e-03\n",
      "Epoch: 267190 | training loss: 1.9489e-03 | validation loss: 1.5583e-03\n",
      "Epoch: 267200 | training loss: 1.8998e-03 | validation loss: 1.4916e-03\n",
      "Epoch: 267210 | training loss: 1.8969e-03 | validation loss: 1.4877e-03\n",
      "Epoch: 267220 | training loss: 1.8971e-03 | validation loss: 1.4961e-03\n",
      "Epoch: 267230 | training loss: 1.8957e-03 | validation loss: 1.4907e-03\n",
      "Epoch: 267240 | training loss: 1.8953e-03 | validation loss: 1.4954e-03\n",
      "Epoch: 267250 | training loss: 1.8951e-03 | validation loss: 1.4909e-03\n",
      "Epoch: 267260 | training loss: 1.8950e-03 | validation loss: 1.4934e-03\n",
      "Epoch: 267270 | training loss: 1.8949e-03 | validation loss: 1.4925e-03\n",
      "Epoch: 267280 | training loss: 1.8949e-03 | validation loss: 1.4921e-03\n",
      "Epoch: 267290 | training loss: 1.8949e-03 | validation loss: 1.4925e-03\n",
      "Epoch: 267300 | training loss: 1.8949e-03 | validation loss: 1.4925e-03\n",
      "Epoch: 267310 | training loss: 1.8948e-03 | validation loss: 1.4927e-03\n",
      "Epoch: 267320 | training loss: 1.8950e-03 | validation loss: 1.4941e-03\n",
      "Epoch: 267330 | training loss: 1.9048e-03 | validation loss: 1.5097e-03\n",
      "Epoch: 267340 | training loss: 2.8527e-03 | validation loss: 2.0805e-03\n",
      "Epoch: 267350 | training loss: 2.6091e-03 | validation loss: 1.7341e-03\n",
      "Epoch: 267360 | training loss: 1.9861e-03 | validation loss: 1.5094e-03\n",
      "Epoch: 267370 | training loss: 1.8980e-03 | validation loss: 1.5005e-03\n",
      "Epoch: 267380 | training loss: 1.9170e-03 | validation loss: 1.5279e-03\n",
      "Epoch: 267390 | training loss: 1.9062e-03 | validation loss: 1.5141e-03\n",
      "Epoch: 267400 | training loss: 1.8954e-03 | validation loss: 1.4964e-03\n",
      "Epoch: 267410 | training loss: 1.8955e-03 | validation loss: 1.4887e-03\n",
      "Epoch: 267420 | training loss: 1.8951e-03 | validation loss: 1.4894e-03\n",
      "Epoch: 267430 | training loss: 1.8947e-03 | validation loss: 1.4934e-03\n",
      "Epoch: 267440 | training loss: 1.8947e-03 | validation loss: 1.4931e-03\n",
      "Epoch: 267450 | training loss: 1.8947e-03 | validation loss: 1.4914e-03\n",
      "Epoch: 267460 | training loss: 1.8946e-03 | validation loss: 1.4923e-03\n",
      "Epoch: 267470 | training loss: 1.8946e-03 | validation loss: 1.4922e-03\n",
      "Epoch: 267480 | training loss: 1.8946e-03 | validation loss: 1.4920e-03\n",
      "Epoch: 267490 | training loss: 1.8946e-03 | validation loss: 1.4922e-03\n",
      "Epoch: 267500 | training loss: 1.8946e-03 | validation loss: 1.4920e-03\n",
      "Epoch: 267510 | training loss: 1.8949e-03 | validation loss: 1.4920e-03\n",
      "Epoch: 267520 | training loss: 1.9233e-03 | validation loss: 1.5077e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 267530 | training loss: 2.5814e-03 | validation loss: 1.9032e-03\n",
      "Epoch: 267540 | training loss: 1.9956e-03 | validation loss: 1.5075e-03\n",
      "Epoch: 267550 | training loss: 1.9129e-03 | validation loss: 1.5207e-03\n",
      "Epoch: 267560 | training loss: 1.9115e-03 | validation loss: 1.5059e-03\n",
      "Epoch: 267570 | training loss: 1.9007e-03 | validation loss: 1.5011e-03\n",
      "Epoch: 267580 | training loss: 1.8964e-03 | validation loss: 1.4987e-03\n",
      "Epoch: 267590 | training loss: 1.8956e-03 | validation loss: 1.4877e-03\n",
      "Epoch: 267600 | training loss: 1.8952e-03 | validation loss: 1.4881e-03\n",
      "Epoch: 267610 | training loss: 1.8950e-03 | validation loss: 1.4899e-03\n",
      "Epoch: 267620 | training loss: 1.8976e-03 | validation loss: 1.4866e-03\n",
      "Epoch: 267630 | training loss: 1.9651e-03 | validation loss: 1.4934e-03\n",
      "Epoch: 267640 | training loss: 2.8058e-03 | validation loss: 1.8204e-03\n",
      "Epoch: 267650 | training loss: 2.1730e-03 | validation loss: 1.6836e-03\n",
      "Epoch: 267660 | training loss: 1.9277e-03 | validation loss: 1.4903e-03\n",
      "Epoch: 267670 | training loss: 1.8949e-03 | validation loss: 1.4906e-03\n",
      "Epoch: 267680 | training loss: 1.8991e-03 | validation loss: 1.5007e-03\n",
      "Epoch: 267690 | training loss: 1.8975e-03 | validation loss: 1.4867e-03\n",
      "Epoch: 267700 | training loss: 1.8954e-03 | validation loss: 1.4964e-03\n",
      "Epoch: 267710 | training loss: 1.8944e-03 | validation loss: 1.4902e-03\n",
      "Epoch: 267720 | training loss: 1.8943e-03 | validation loss: 1.4911e-03\n",
      "Epoch: 267730 | training loss: 1.8944e-03 | validation loss: 1.4932e-03\n",
      "Epoch: 267740 | training loss: 1.8942e-03 | validation loss: 1.4917e-03\n",
      "Epoch: 267750 | training loss: 1.8942e-03 | validation loss: 1.4910e-03\n",
      "Epoch: 267760 | training loss: 1.8943e-03 | validation loss: 1.4902e-03\n",
      "Epoch: 267770 | training loss: 1.8970e-03 | validation loss: 1.4871e-03\n",
      "Epoch: 267780 | training loss: 2.0143e-03 | validation loss: 1.5121e-03\n",
      "Epoch: 267790 | training loss: 2.7353e-03 | validation loss: 1.8044e-03\n",
      "Epoch: 267800 | training loss: 1.8994e-03 | validation loss: 1.4941e-03\n",
      "Epoch: 267810 | training loss: 2.0040e-03 | validation loss: 1.5839e-03\n",
      "Epoch: 267820 | training loss: 1.9027e-03 | validation loss: 1.4869e-03\n",
      "Epoch: 267830 | training loss: 1.9023e-03 | validation loss: 1.4832e-03\n",
      "Epoch: 267840 | training loss: 1.8995e-03 | validation loss: 1.5043e-03\n",
      "Epoch: 267850 | training loss: 1.8946e-03 | validation loss: 1.4884e-03\n",
      "Epoch: 267860 | training loss: 1.8940e-03 | validation loss: 1.4919e-03\n",
      "Epoch: 267870 | training loss: 1.8940e-03 | validation loss: 1.4920e-03\n",
      "Epoch: 267880 | training loss: 1.8940e-03 | validation loss: 1.4911e-03\n",
      "Epoch: 267890 | training loss: 1.8940e-03 | validation loss: 1.4915e-03\n",
      "Epoch: 267900 | training loss: 1.8940e-03 | validation loss: 1.4920e-03\n",
      "Epoch: 267910 | training loss: 1.8940e-03 | validation loss: 1.4913e-03\n",
      "Epoch: 267920 | training loss: 1.8940e-03 | validation loss: 1.4918e-03\n",
      "Epoch: 267930 | training loss: 1.8957e-03 | validation loss: 1.4962e-03\n",
      "Epoch: 267940 | training loss: 2.0299e-03 | validation loss: 1.6184e-03\n",
      "Epoch: 267950 | training loss: 1.9288e-03 | validation loss: 1.5131e-03\n",
      "Epoch: 267960 | training loss: 1.9976e-03 | validation loss: 1.5993e-03\n",
      "Epoch: 267970 | training loss: 1.9806e-03 | validation loss: 1.5804e-03\n",
      "Epoch: 267980 | training loss: 2.0914e-03 | validation loss: 1.6372e-03\n",
      "Epoch: 267990 | training loss: 1.9055e-03 | validation loss: 1.5014e-03\n",
      "Epoch: 268000 | training loss: 1.9314e-03 | validation loss: 1.4842e-03\n",
      "Epoch: 268010 | training loss: 1.9044e-03 | validation loss: 1.4855e-03\n",
      "Epoch: 268020 | training loss: 1.8941e-03 | validation loss: 1.4899e-03\n",
      "Epoch: 268030 | training loss: 1.8940e-03 | validation loss: 1.4918e-03\n",
      "Epoch: 268040 | training loss: 1.8971e-03 | validation loss: 1.4999e-03\n",
      "Epoch: 268050 | training loss: 2.0383e-03 | validation loss: 1.6086e-03\n",
      "Epoch: 268060 | training loss: 2.4631e-03 | validation loss: 1.8527e-03\n",
      "Epoch: 268070 | training loss: 1.9027e-03 | validation loss: 1.5095e-03\n",
      "Epoch: 268080 | training loss: 1.9926e-03 | validation loss: 1.5084e-03\n",
      "Epoch: 268090 | training loss: 1.9048e-03 | validation loss: 1.5043e-03\n",
      "Epoch: 268100 | training loss: 1.8975e-03 | validation loss: 1.4969e-03\n",
      "Epoch: 268110 | training loss: 1.8986e-03 | validation loss: 1.4864e-03\n",
      "Epoch: 268120 | training loss: 1.8953e-03 | validation loss: 1.4967e-03\n",
      "Epoch: 268130 | training loss: 1.8940e-03 | validation loss: 1.4886e-03\n",
      "Epoch: 268140 | training loss: 1.8937e-03 | validation loss: 1.4922e-03\n",
      "Epoch: 268150 | training loss: 1.8937e-03 | validation loss: 1.4901e-03\n",
      "Epoch: 268160 | training loss: 1.8936e-03 | validation loss: 1.4915e-03\n",
      "Epoch: 268170 | training loss: 1.8936e-03 | validation loss: 1.4907e-03\n",
      "Epoch: 268180 | training loss: 1.8935e-03 | validation loss: 1.4905e-03\n",
      "Epoch: 268190 | training loss: 1.8935e-03 | validation loss: 1.4907e-03\n",
      "Epoch: 268200 | training loss: 1.8935e-03 | validation loss: 1.4908e-03\n",
      "Epoch: 268210 | training loss: 1.8935e-03 | validation loss: 1.4907e-03\n",
      "Epoch: 268220 | training loss: 1.8935e-03 | validation loss: 1.4904e-03\n",
      "Epoch: 268230 | training loss: 1.8943e-03 | validation loss: 1.4880e-03\n",
      "Epoch: 268240 | training loss: 2.0340e-03 | validation loss: 1.5191e-03\n",
      "Epoch: 268250 | training loss: 2.2738e-03 | validation loss: 1.6142e-03\n",
      "Epoch: 268260 | training loss: 2.3995e-03 | validation loss: 1.6584e-03\n",
      "Epoch: 268270 | training loss: 2.0872e-03 | validation loss: 1.5314e-03\n",
      "Epoch: 268280 | training loss: 1.9619e-03 | validation loss: 1.4920e-03\n",
      "Epoch: 268290 | training loss: 1.9151e-03 | validation loss: 1.4850e-03\n",
      "Epoch: 268300 | training loss: 1.8981e-03 | validation loss: 1.4854e-03\n",
      "Epoch: 268310 | training loss: 1.8936e-03 | validation loss: 1.4889e-03\n",
      "Epoch: 268320 | training loss: 1.8936e-03 | validation loss: 1.4927e-03\n",
      "Epoch: 268330 | training loss: 1.8937e-03 | validation loss: 1.4932e-03\n",
      "Epoch: 268340 | training loss: 1.8934e-03 | validation loss: 1.4917e-03\n",
      "Epoch: 268350 | training loss: 1.8933e-03 | validation loss: 1.4902e-03\n",
      "Epoch: 268360 | training loss: 1.8933e-03 | validation loss: 1.4902e-03\n",
      "Epoch: 268370 | training loss: 1.8933e-03 | validation loss: 1.4908e-03\n",
      "Epoch: 268380 | training loss: 1.8932e-03 | validation loss: 1.4907e-03\n",
      "Epoch: 268390 | training loss: 1.8932e-03 | validation loss: 1.4905e-03\n",
      "Epoch: 268400 | training loss: 1.8932e-03 | validation loss: 1.4906e-03\n",
      "Epoch: 268410 | training loss: 1.8932e-03 | validation loss: 1.4905e-03\n",
      "Epoch: 268420 | training loss: 1.8932e-03 | validation loss: 1.4906e-03\n",
      "Epoch: 268430 | training loss: 1.8932e-03 | validation loss: 1.4911e-03\n",
      "Epoch: 268440 | training loss: 1.8972e-03 | validation loss: 1.4999e-03\n",
      "Epoch: 268450 | training loss: 2.4390e-03 | validation loss: 1.9505e-03\n",
      "Epoch: 268460 | training loss: 2.0392e-03 | validation loss: 1.5598e-03\n",
      "Epoch: 268470 | training loss: 1.9810e-03 | validation loss: 1.5306e-03\n",
      "Epoch: 268480 | training loss: 1.9223e-03 | validation loss: 1.4996e-03\n",
      "Epoch: 268490 | training loss: 1.8996e-03 | validation loss: 1.4856e-03\n",
      "Epoch: 268500 | training loss: 1.8935e-03 | validation loss: 1.4918e-03\n",
      "Epoch: 268510 | training loss: 1.8936e-03 | validation loss: 1.4931e-03\n",
      "Epoch: 268520 | training loss: 1.8936e-03 | validation loss: 1.4918e-03\n",
      "Epoch: 268530 | training loss: 1.8931e-03 | validation loss: 1.4896e-03\n",
      "Epoch: 268540 | training loss: 1.8939e-03 | validation loss: 1.4864e-03\n",
      "Epoch: 268550 | training loss: 1.9251e-03 | validation loss: 1.4827e-03\n",
      "Epoch: 268560 | training loss: 3.0918e-03 | validation loss: 1.9375e-03\n",
      "Epoch: 268570 | training loss: 2.3196e-03 | validation loss: 1.7702e-03\n",
      "Epoch: 268580 | training loss: 1.8952e-03 | validation loss: 1.4974e-03\n",
      "Epoch: 268590 | training loss: 1.9502e-03 | validation loss: 1.4946e-03\n",
      "Epoch: 268600 | training loss: 1.8953e-03 | validation loss: 1.4978e-03\n",
      "Epoch: 268610 | training loss: 1.8968e-03 | validation loss: 1.4985e-03\n",
      "Epoch: 268620 | training loss: 1.8957e-03 | validation loss: 1.4846e-03\n",
      "Epoch: 268630 | training loss: 1.8934e-03 | validation loss: 1.4925e-03\n",
      "Epoch: 268640 | training loss: 1.8929e-03 | validation loss: 1.4894e-03\n",
      "Epoch: 268650 | training loss: 1.8929e-03 | validation loss: 1.4904e-03\n",
      "Epoch: 268660 | training loss: 1.8928e-03 | validation loss: 1.4893e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 268670 | training loss: 1.8928e-03 | validation loss: 1.4904e-03\n",
      "Epoch: 268680 | training loss: 1.8928e-03 | validation loss: 1.4895e-03\n",
      "Epoch: 268690 | training loss: 1.8928e-03 | validation loss: 1.4897e-03\n",
      "Epoch: 268700 | training loss: 1.8928e-03 | validation loss: 1.4899e-03\n",
      "Epoch: 268710 | training loss: 1.8928e-03 | validation loss: 1.4900e-03\n",
      "Epoch: 268720 | training loss: 1.8928e-03 | validation loss: 1.4905e-03\n",
      "Epoch: 268730 | training loss: 1.8941e-03 | validation loss: 1.4946e-03\n",
      "Epoch: 268740 | training loss: 1.9922e-03 | validation loss: 1.5734e-03\n",
      "Epoch: 268750 | training loss: 3.0089e-03 | validation loss: 2.1462e-03\n",
      "Epoch: 268760 | training loss: 1.9813e-03 | validation loss: 1.5770e-03\n",
      "Epoch: 268770 | training loss: 1.9522e-03 | validation loss: 1.4929e-03\n",
      "Epoch: 268780 | training loss: 1.9465e-03 | validation loss: 1.4836e-03\n",
      "Epoch: 268790 | training loss: 1.8938e-03 | validation loss: 1.4854e-03\n",
      "Epoch: 268800 | training loss: 1.8984e-03 | validation loss: 1.5037e-03\n",
      "Epoch: 268810 | training loss: 1.8933e-03 | validation loss: 1.4926e-03\n",
      "Epoch: 268820 | training loss: 1.8935e-03 | validation loss: 1.4867e-03\n",
      "Epoch: 268830 | training loss: 1.8926e-03 | validation loss: 1.4896e-03\n",
      "Epoch: 268840 | training loss: 1.8926e-03 | validation loss: 1.4911e-03\n",
      "Epoch: 268850 | training loss: 1.8926e-03 | validation loss: 1.4886e-03\n",
      "Epoch: 268860 | training loss: 1.8925e-03 | validation loss: 1.4901e-03\n",
      "Epoch: 268870 | training loss: 1.8925e-03 | validation loss: 1.4894e-03\n",
      "Epoch: 268880 | training loss: 1.8925e-03 | validation loss: 1.4898e-03\n",
      "Epoch: 268890 | training loss: 1.8925e-03 | validation loss: 1.4895e-03\n",
      "Epoch: 268900 | training loss: 1.8925e-03 | validation loss: 1.4900e-03\n",
      "Epoch: 268910 | training loss: 1.8936e-03 | validation loss: 1.4929e-03\n",
      "Epoch: 268920 | training loss: 1.9892e-03 | validation loss: 1.5799e-03\n",
      "Epoch: 268930 | training loss: 1.8946e-03 | validation loss: 1.4848e-03\n",
      "Epoch: 268940 | training loss: 1.9972e-03 | validation loss: 1.5806e-03\n",
      "Epoch: 268950 | training loss: 1.9032e-03 | validation loss: 1.5048e-03\n",
      "Epoch: 268960 | training loss: 1.8973e-03 | validation loss: 1.4901e-03\n",
      "Epoch: 268970 | training loss: 1.9003e-03 | validation loss: 1.4968e-03\n",
      "Epoch: 268980 | training loss: 1.9765e-03 | validation loss: 1.5682e-03\n",
      "Epoch: 268990 | training loss: 2.7125e-03 | validation loss: 2.0053e-03\n",
      "Epoch: 269000 | training loss: 2.1050e-03 | validation loss: 1.5423e-03\n",
      "Epoch: 269010 | training loss: 1.9055e-03 | validation loss: 1.5078e-03\n",
      "Epoch: 269020 | training loss: 1.8953e-03 | validation loss: 1.4943e-03\n",
      "Epoch: 269030 | training loss: 1.8983e-03 | validation loss: 1.4823e-03\n",
      "Epoch: 269040 | training loss: 1.8956e-03 | validation loss: 1.4979e-03\n",
      "Epoch: 269050 | training loss: 1.8934e-03 | validation loss: 1.4861e-03\n",
      "Epoch: 269060 | training loss: 1.8924e-03 | validation loss: 1.4903e-03\n",
      "Epoch: 269070 | training loss: 1.8923e-03 | validation loss: 1.4899e-03\n",
      "Epoch: 269080 | training loss: 1.8923e-03 | validation loss: 1.4878e-03\n",
      "Epoch: 269090 | training loss: 1.8922e-03 | validation loss: 1.4886e-03\n",
      "Epoch: 269100 | training loss: 1.8922e-03 | validation loss: 1.4893e-03\n",
      "Epoch: 269110 | training loss: 1.8922e-03 | validation loss: 1.4898e-03\n",
      "Epoch: 269120 | training loss: 1.8931e-03 | validation loss: 1.4931e-03\n",
      "Epoch: 269130 | training loss: 1.9424e-03 | validation loss: 1.5392e-03\n",
      "Epoch: 269140 | training loss: 3.4083e-03 | validation loss: 2.3650e-03\n",
      "Epoch: 269150 | training loss: 2.0690e-03 | validation loss: 1.5411e-03\n",
      "Epoch: 269160 | training loss: 2.0138e-03 | validation loss: 1.5021e-03\n",
      "Epoch: 269170 | training loss: 1.9030e-03 | validation loss: 1.4946e-03\n",
      "Epoch: 269180 | training loss: 1.9105e-03 | validation loss: 1.5189e-03\n",
      "Epoch: 269190 | training loss: 1.8927e-03 | validation loss: 1.4897e-03\n",
      "Epoch: 269200 | training loss: 1.8947e-03 | validation loss: 1.4827e-03\n",
      "Epoch: 269210 | training loss: 1.8926e-03 | validation loss: 1.4929e-03\n",
      "Epoch: 269220 | training loss: 1.8920e-03 | validation loss: 1.4889e-03\n",
      "Epoch: 269230 | training loss: 1.8920e-03 | validation loss: 1.4881e-03\n",
      "Epoch: 269240 | training loss: 1.8920e-03 | validation loss: 1.4896e-03\n",
      "Epoch: 269250 | training loss: 1.8920e-03 | validation loss: 1.4884e-03\n",
      "Epoch: 269260 | training loss: 1.8919e-03 | validation loss: 1.4891e-03\n",
      "Epoch: 269270 | training loss: 1.8919e-03 | validation loss: 1.4887e-03\n",
      "Epoch: 269280 | training loss: 1.8919e-03 | validation loss: 1.4886e-03\n",
      "Epoch: 269290 | training loss: 1.8919e-03 | validation loss: 1.4887e-03\n",
      "Epoch: 269300 | training loss: 1.8920e-03 | validation loss: 1.4885e-03\n",
      "Epoch: 269310 | training loss: 1.8973e-03 | validation loss: 1.4891e-03\n",
      "Epoch: 269320 | training loss: 2.2663e-03 | validation loss: 1.7092e-03\n",
      "Epoch: 269330 | training loss: 2.3538e-03 | validation loss: 1.8754e-03\n",
      "Epoch: 269340 | training loss: 1.9131e-03 | validation loss: 1.5192e-03\n",
      "Epoch: 269350 | training loss: 1.9284e-03 | validation loss: 1.4800e-03\n",
      "Epoch: 269360 | training loss: 1.9112e-03 | validation loss: 1.5041e-03\n",
      "Epoch: 269370 | training loss: 1.8997e-03 | validation loss: 1.4969e-03\n",
      "Epoch: 269380 | training loss: 1.8918e-03 | validation loss: 1.4873e-03\n",
      "Epoch: 269390 | training loss: 1.8948e-03 | validation loss: 1.4852e-03\n",
      "Epoch: 269400 | training loss: 1.9206e-03 | validation loss: 1.4816e-03\n",
      "Epoch: 269410 | training loss: 2.4030e-03 | validation loss: 1.6484e-03\n",
      "Epoch: 269420 | training loss: 1.9361e-03 | validation loss: 1.5368e-03\n",
      "Epoch: 269430 | training loss: 1.9335e-03 | validation loss: 1.4855e-03\n",
      "Epoch: 269440 | training loss: 1.9182e-03 | validation loss: 1.5198e-03\n",
      "Epoch: 269450 | training loss: 1.9028e-03 | validation loss: 1.4808e-03\n",
      "Epoch: 269460 | training loss: 1.8946e-03 | validation loss: 1.4961e-03\n",
      "Epoch: 269470 | training loss: 1.8917e-03 | validation loss: 1.4874e-03\n",
      "Epoch: 269480 | training loss: 1.8921e-03 | validation loss: 1.4858e-03\n",
      "Epoch: 269490 | training loss: 1.8919e-03 | validation loss: 1.4906e-03\n",
      "Epoch: 269500 | training loss: 1.8919e-03 | validation loss: 1.4906e-03\n",
      "Epoch: 269510 | training loss: 1.8919e-03 | validation loss: 1.4906e-03\n",
      "Epoch: 269520 | training loss: 1.8943e-03 | validation loss: 1.4957e-03\n",
      "Epoch: 269530 | training loss: 1.9635e-03 | validation loss: 1.5543e-03\n",
      "Epoch: 269540 | training loss: 2.9217e-03 | validation loss: 2.1037e-03\n",
      "Epoch: 269550 | training loss: 2.1554e-03 | validation loss: 1.5654e-03\n",
      "Epoch: 269560 | training loss: 1.9011e-03 | validation loss: 1.4913e-03\n",
      "Epoch: 269570 | training loss: 1.9107e-03 | validation loss: 1.5157e-03\n",
      "Epoch: 269580 | training loss: 1.9049e-03 | validation loss: 1.4834e-03\n",
      "Epoch: 269590 | training loss: 1.8959e-03 | validation loss: 1.4946e-03\n",
      "Epoch: 269600 | training loss: 1.8929e-03 | validation loss: 1.4880e-03\n",
      "Epoch: 269610 | training loss: 1.8921e-03 | validation loss: 1.4890e-03\n",
      "Epoch: 269620 | training loss: 1.8917e-03 | validation loss: 1.4879e-03\n",
      "Epoch: 269630 | training loss: 1.8914e-03 | validation loss: 1.4887e-03\n",
      "Epoch: 269640 | training loss: 1.8914e-03 | validation loss: 1.4879e-03\n",
      "Epoch: 269650 | training loss: 1.8914e-03 | validation loss: 1.4881e-03\n",
      "Epoch: 269660 | training loss: 1.8914e-03 | validation loss: 1.4881e-03\n",
      "Epoch: 269670 | training loss: 1.8915e-03 | validation loss: 1.4878e-03\n",
      "Epoch: 269680 | training loss: 1.8957e-03 | validation loss: 1.4880e-03\n",
      "Epoch: 269690 | training loss: 2.0939e-03 | validation loss: 1.5770e-03\n",
      "Epoch: 269700 | training loss: 2.5240e-03 | validation loss: 1.7142e-03\n",
      "Epoch: 269710 | training loss: 2.0572e-03 | validation loss: 1.6534e-03\n",
      "Epoch: 269720 | training loss: 1.9457e-03 | validation loss: 1.5519e-03\n",
      "Epoch: 269730 | training loss: 1.9095e-03 | validation loss: 1.4856e-03\n",
      "Epoch: 269740 | training loss: 1.8992e-03 | validation loss: 1.4980e-03\n",
      "Epoch: 269750 | training loss: 1.8954e-03 | validation loss: 1.4815e-03\n",
      "Epoch: 269760 | training loss: 1.8918e-03 | validation loss: 1.4897e-03\n",
      "Epoch: 269770 | training loss: 1.8917e-03 | validation loss: 1.4876e-03\n",
      "Epoch: 269780 | training loss: 1.8913e-03 | validation loss: 1.4899e-03\n",
      "Epoch: 269790 | training loss: 1.8912e-03 | validation loss: 1.4866e-03\n",
      "Epoch: 269800 | training loss: 1.8911e-03 | validation loss: 1.4874e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 269810 | training loss: 1.8911e-03 | validation loss: 1.4880e-03\n",
      "Epoch: 269820 | training loss: 1.8911e-03 | validation loss: 1.4881e-03\n",
      "Epoch: 269830 | training loss: 1.8912e-03 | validation loss: 1.4894e-03\n",
      "Epoch: 269840 | training loss: 1.8953e-03 | validation loss: 1.4976e-03\n",
      "Epoch: 269850 | training loss: 2.1427e-03 | validation loss: 1.6717e-03\n",
      "Epoch: 269860 | training loss: 1.9841e-03 | validation loss: 1.5550e-03\n",
      "Epoch: 269870 | training loss: 2.1016e-03 | validation loss: 1.6536e-03\n",
      "Epoch: 269880 | training loss: 1.9298e-03 | validation loss: 1.5105e-03\n",
      "Epoch: 269890 | training loss: 1.9114e-03 | validation loss: 1.4771e-03\n",
      "Epoch: 269900 | training loss: 1.8991e-03 | validation loss: 1.4898e-03\n",
      "Epoch: 269910 | training loss: 1.8938e-03 | validation loss: 1.4961e-03\n",
      "Epoch: 269920 | training loss: 1.8911e-03 | validation loss: 1.4875e-03\n",
      "Epoch: 269930 | training loss: 1.8914e-03 | validation loss: 1.4859e-03\n",
      "Epoch: 269940 | training loss: 1.8911e-03 | validation loss: 1.4891e-03\n",
      "Epoch: 269950 | training loss: 1.8910e-03 | validation loss: 1.4871e-03\n",
      "Epoch: 269960 | training loss: 1.8909e-03 | validation loss: 1.4877e-03\n",
      "Epoch: 269970 | training loss: 1.8909e-03 | validation loss: 1.4873e-03\n",
      "Epoch: 269980 | training loss: 1.8909e-03 | validation loss: 1.4878e-03\n",
      "Epoch: 269990 | training loss: 1.8908e-03 | validation loss: 1.4873e-03\n",
      "Epoch: 270000 | training loss: 1.8908e-03 | validation loss: 1.4874e-03\n",
      "Epoch: 270010 | training loss: 1.8908e-03 | validation loss: 1.4874e-03\n",
      "Epoch: 270020 | training loss: 1.8909e-03 | validation loss: 1.4872e-03\n",
      "Epoch: 270030 | training loss: 1.8958e-03 | validation loss: 1.4878e-03\n",
      "Epoch: 270040 | training loss: 2.2984e-03 | validation loss: 1.6853e-03\n",
      "Epoch: 270050 | training loss: 2.3436e-03 | validation loss: 1.7085e-03\n",
      "Epoch: 270060 | training loss: 1.9896e-03 | validation loss: 1.5052e-03\n",
      "Epoch: 270070 | training loss: 1.9481e-03 | validation loss: 1.5336e-03\n",
      "Epoch: 270080 | training loss: 1.8932e-03 | validation loss: 1.4816e-03\n",
      "Epoch: 270090 | training loss: 1.8930e-03 | validation loss: 1.4827e-03\n",
      "Epoch: 270100 | training loss: 1.8934e-03 | validation loss: 1.4959e-03\n",
      "Epoch: 270110 | training loss: 1.8922e-03 | validation loss: 1.4864e-03\n",
      "Epoch: 270120 | training loss: 1.8912e-03 | validation loss: 1.4910e-03\n",
      "Epoch: 270130 | training loss: 1.8908e-03 | validation loss: 1.4862e-03\n",
      "Epoch: 270140 | training loss: 1.8907e-03 | validation loss: 1.4877e-03\n",
      "Epoch: 270150 | training loss: 1.8906e-03 | validation loss: 1.4866e-03\n",
      "Epoch: 270160 | training loss: 1.8906e-03 | validation loss: 1.4868e-03\n",
      "Epoch: 270170 | training loss: 1.8906e-03 | validation loss: 1.4872e-03\n",
      "Epoch: 270180 | training loss: 1.8906e-03 | validation loss: 1.4872e-03\n",
      "Epoch: 270190 | training loss: 1.8906e-03 | validation loss: 1.4877e-03\n",
      "Epoch: 270200 | training loss: 1.8912e-03 | validation loss: 1.4905e-03\n",
      "Epoch: 270210 | training loss: 1.9312e-03 | validation loss: 1.5314e-03\n",
      "Epoch: 270220 | training loss: 3.5255e-03 | validation loss: 2.4360e-03\n",
      "Epoch: 270230 | training loss: 2.0738e-03 | validation loss: 1.5484e-03\n",
      "Epoch: 270240 | training loss: 2.0306e-03 | validation loss: 1.5103e-03\n",
      "Epoch: 270250 | training loss: 1.9049e-03 | validation loss: 1.4844e-03\n",
      "Epoch: 270260 | training loss: 1.9094e-03 | validation loss: 1.5041e-03\n",
      "Epoch: 270270 | training loss: 1.8915e-03 | validation loss: 1.4921e-03\n",
      "Epoch: 270280 | training loss: 1.8933e-03 | validation loss: 1.4876e-03\n",
      "Epoch: 270290 | training loss: 1.8907e-03 | validation loss: 1.4871e-03\n",
      "Epoch: 270300 | training loss: 1.8905e-03 | validation loss: 1.4866e-03\n",
      "Epoch: 270310 | training loss: 1.8905e-03 | validation loss: 1.4869e-03\n",
      "Epoch: 270320 | training loss: 1.8904e-03 | validation loss: 1.4873e-03\n",
      "Epoch: 270330 | training loss: 1.8904e-03 | validation loss: 1.4864e-03\n",
      "Epoch: 270340 | training loss: 1.8903e-03 | validation loss: 1.4872e-03\n",
      "Epoch: 270350 | training loss: 1.8903e-03 | validation loss: 1.4866e-03\n",
      "Epoch: 270360 | training loss: 1.8903e-03 | validation loss: 1.4868e-03\n",
      "Epoch: 270370 | training loss: 1.8903e-03 | validation loss: 1.4868e-03\n",
      "Epoch: 270380 | training loss: 1.8903e-03 | validation loss: 1.4868e-03\n",
      "Epoch: 270390 | training loss: 1.8902e-03 | validation loss: 1.4868e-03\n",
      "Epoch: 270400 | training loss: 1.8902e-03 | validation loss: 1.4867e-03\n",
      "Epoch: 270410 | training loss: 1.8902e-03 | validation loss: 1.4867e-03\n",
      "Epoch: 270420 | training loss: 1.8902e-03 | validation loss: 1.4864e-03\n",
      "Epoch: 270430 | training loss: 1.8938e-03 | validation loss: 1.4841e-03\n",
      "Epoch: 270440 | training loss: 2.7111e-03 | validation loss: 1.8301e-03\n",
      "Epoch: 270450 | training loss: 2.8814e-03 | validation loss: 2.0663e-03\n",
      "Epoch: 270460 | training loss: 2.0827e-03 | validation loss: 1.5958e-03\n",
      "Epoch: 270470 | training loss: 1.8996e-03 | validation loss: 1.4754e-03\n",
      "Epoch: 270480 | training loss: 1.9272e-03 | validation loss: 1.4996e-03\n",
      "Epoch: 270490 | training loss: 1.9052e-03 | validation loss: 1.4909e-03\n",
      "Epoch: 270500 | training loss: 1.8904e-03 | validation loss: 1.4851e-03\n",
      "Epoch: 270510 | training loss: 1.8911e-03 | validation loss: 1.4880e-03\n",
      "Epoch: 270520 | training loss: 1.8908e-03 | validation loss: 1.4876e-03\n",
      "Epoch: 270530 | training loss: 1.8903e-03 | validation loss: 1.4855e-03\n",
      "Epoch: 270540 | training loss: 1.8901e-03 | validation loss: 1.4856e-03\n",
      "Epoch: 270550 | training loss: 1.8900e-03 | validation loss: 1.4867e-03\n",
      "Epoch: 270560 | training loss: 1.8900e-03 | validation loss: 1.4864e-03\n",
      "Epoch: 270570 | training loss: 1.8900e-03 | validation loss: 1.4862e-03\n",
      "Epoch: 270580 | training loss: 1.8900e-03 | validation loss: 1.4865e-03\n",
      "Epoch: 270590 | training loss: 1.8900e-03 | validation loss: 1.4863e-03\n",
      "Epoch: 270600 | training loss: 1.8899e-03 | validation loss: 1.4864e-03\n",
      "Epoch: 270610 | training loss: 1.8899e-03 | validation loss: 1.4863e-03\n",
      "Epoch: 270620 | training loss: 1.8899e-03 | validation loss: 1.4863e-03\n",
      "Epoch: 270630 | training loss: 1.8899e-03 | validation loss: 1.4863e-03\n",
      "Epoch: 270640 | training loss: 1.8899e-03 | validation loss: 1.4863e-03\n",
      "Epoch: 270650 | training loss: 1.8899e-03 | validation loss: 1.4863e-03\n",
      "Epoch: 270660 | training loss: 1.8899e-03 | validation loss: 1.4863e-03\n",
      "Epoch: 270670 | training loss: 1.8898e-03 | validation loss: 1.4862e-03\n",
      "Epoch: 270680 | training loss: 1.8898e-03 | validation loss: 1.4860e-03\n",
      "Epoch: 270690 | training loss: 1.8904e-03 | validation loss: 1.4833e-03\n",
      "Epoch: 270700 | training loss: 2.0184e-03 | validation loss: 1.5026e-03\n",
      "Epoch: 270710 | training loss: 2.1011e-03 | validation loss: 1.5603e-03\n",
      "Epoch: 270720 | training loss: 2.1392e-03 | validation loss: 1.5682e-03\n",
      "Epoch: 270730 | training loss: 1.9537e-03 | validation loss: 1.5180e-03\n",
      "Epoch: 270740 | training loss: 1.9259e-03 | validation loss: 1.5234e-03\n",
      "Epoch: 270750 | training loss: 1.9070e-03 | validation loss: 1.5124e-03\n",
      "Epoch: 270760 | training loss: 1.8956e-03 | validation loss: 1.5002e-03\n",
      "Epoch: 270770 | training loss: 1.8915e-03 | validation loss: 1.4929e-03\n",
      "Epoch: 270780 | training loss: 1.8900e-03 | validation loss: 1.4884e-03\n",
      "Epoch: 270790 | training loss: 1.8898e-03 | validation loss: 1.4862e-03\n",
      "Epoch: 270800 | training loss: 1.8897e-03 | validation loss: 1.4860e-03\n",
      "Epoch: 270810 | training loss: 1.8897e-03 | validation loss: 1.4865e-03\n",
      "Epoch: 270820 | training loss: 1.8896e-03 | validation loss: 1.4863e-03\n",
      "Epoch: 270830 | training loss: 1.8896e-03 | validation loss: 1.4860e-03\n",
      "Epoch: 270840 | training loss: 1.8896e-03 | validation loss: 1.4860e-03\n",
      "Epoch: 270850 | training loss: 1.8896e-03 | validation loss: 1.4858e-03\n",
      "Epoch: 270860 | training loss: 1.8896e-03 | validation loss: 1.4857e-03\n",
      "Epoch: 270870 | training loss: 1.8896e-03 | validation loss: 1.4857e-03\n",
      "Epoch: 270880 | training loss: 1.8895e-03 | validation loss: 1.4858e-03\n",
      "Epoch: 270890 | training loss: 1.8895e-03 | validation loss: 1.4857e-03\n",
      "Epoch: 270900 | training loss: 1.8895e-03 | validation loss: 1.4857e-03\n",
      "Epoch: 270910 | training loss: 1.8895e-03 | validation loss: 1.4857e-03\n",
      "Epoch: 270920 | training loss: 1.8895e-03 | validation loss: 1.4857e-03\n",
      "Epoch: 270930 | training loss: 1.8895e-03 | validation loss: 1.4857e-03\n",
      "Epoch: 270940 | training loss: 1.8894e-03 | validation loss: 1.4857e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 270950 | training loss: 1.8894e-03 | validation loss: 1.4859e-03\n",
      "Epoch: 270960 | training loss: 1.8902e-03 | validation loss: 1.4884e-03\n",
      "Epoch: 270970 | training loss: 2.0618e-03 | validation loss: 1.6073e-03\n",
      "Epoch: 270980 | training loss: 2.0184e-03 | validation loss: 1.6151e-03\n",
      "Epoch: 270990 | training loss: 2.1934e-03 | validation loss: 1.7225e-03\n",
      "Epoch: 271000 | training loss: 2.0225e-03 | validation loss: 1.6124e-03\n",
      "Epoch: 271010 | training loss: 1.9476e-03 | validation loss: 1.5549e-03\n",
      "Epoch: 271020 | training loss: 1.9129e-03 | validation loss: 1.5229e-03\n",
      "Epoch: 271030 | training loss: 1.8970e-03 | validation loss: 1.5031e-03\n",
      "Epoch: 271040 | training loss: 1.8910e-03 | validation loss: 1.4919e-03\n",
      "Epoch: 271050 | training loss: 1.8897e-03 | validation loss: 1.4865e-03\n",
      "Epoch: 271060 | training loss: 1.8896e-03 | validation loss: 1.4847e-03\n",
      "Epoch: 271070 | training loss: 1.8894e-03 | validation loss: 1.4849e-03\n",
      "Epoch: 271080 | training loss: 1.8893e-03 | validation loss: 1.4858e-03\n",
      "Epoch: 271090 | training loss: 1.8893e-03 | validation loss: 1.4859e-03\n",
      "Epoch: 271100 | training loss: 1.8892e-03 | validation loss: 1.4853e-03\n",
      "Epoch: 271110 | training loss: 1.8892e-03 | validation loss: 1.4853e-03\n",
      "Epoch: 271120 | training loss: 1.8892e-03 | validation loss: 1.4856e-03\n",
      "Epoch: 271130 | training loss: 1.8892e-03 | validation loss: 1.4854e-03\n",
      "Epoch: 271140 | training loss: 1.8892e-03 | validation loss: 1.4854e-03\n",
      "Epoch: 271150 | training loss: 1.8892e-03 | validation loss: 1.4854e-03\n",
      "Epoch: 271160 | training loss: 1.8891e-03 | validation loss: 1.4854e-03\n",
      "Epoch: 271170 | training loss: 1.8891e-03 | validation loss: 1.4854e-03\n",
      "Epoch: 271180 | training loss: 1.8891e-03 | validation loss: 1.4853e-03\n",
      "Epoch: 271190 | training loss: 1.8891e-03 | validation loss: 1.4853e-03\n",
      "Epoch: 271200 | training loss: 1.8891e-03 | validation loss: 1.4853e-03\n",
      "Epoch: 271210 | training loss: 1.8891e-03 | validation loss: 1.4853e-03\n",
      "Epoch: 271220 | training loss: 1.8890e-03 | validation loss: 1.4853e-03\n",
      "Epoch: 271230 | training loss: 1.8890e-03 | validation loss: 1.4852e-03\n",
      "Epoch: 271240 | training loss: 1.8890e-03 | validation loss: 1.4851e-03\n",
      "Epoch: 271250 | training loss: 1.8891e-03 | validation loss: 1.4838e-03\n",
      "Epoch: 271260 | training loss: 1.9190e-03 | validation loss: 1.4773e-03\n",
      "Epoch: 271270 | training loss: 4.5349e-03 | validation loss: 2.5832e-03\n",
      "Epoch: 271280 | training loss: 2.5505e-03 | validation loss: 1.7755e-03\n",
      "Epoch: 271290 | training loss: 2.1588e-03 | validation loss: 1.5831e-03\n",
      "Epoch: 271300 | training loss: 1.9232e-03 | validation loss: 1.4802e-03\n",
      "Epoch: 271310 | training loss: 1.8899e-03 | validation loss: 1.4887e-03\n",
      "Epoch: 271320 | training loss: 1.8960e-03 | validation loss: 1.5002e-03\n",
      "Epoch: 271330 | training loss: 1.8930e-03 | validation loss: 1.4963e-03\n",
      "Epoch: 271340 | training loss: 1.8903e-03 | validation loss: 1.4910e-03\n",
      "Epoch: 271350 | training loss: 1.8892e-03 | validation loss: 1.4877e-03\n",
      "Epoch: 271360 | training loss: 1.8890e-03 | validation loss: 1.4861e-03\n",
      "Epoch: 271370 | training loss: 1.8889e-03 | validation loss: 1.4856e-03\n",
      "Epoch: 271380 | training loss: 1.8888e-03 | validation loss: 1.4853e-03\n",
      "Epoch: 271390 | training loss: 1.8888e-03 | validation loss: 1.4851e-03\n",
      "Epoch: 271400 | training loss: 1.8888e-03 | validation loss: 1.4849e-03\n",
      "Epoch: 271410 | training loss: 1.8888e-03 | validation loss: 1.4848e-03\n",
      "Epoch: 271420 | training loss: 1.8888e-03 | validation loss: 1.4847e-03\n",
      "Epoch: 271430 | training loss: 1.8887e-03 | validation loss: 1.4846e-03\n",
      "Epoch: 271440 | training loss: 1.8887e-03 | validation loss: 1.4846e-03\n",
      "Epoch: 271450 | training loss: 1.8887e-03 | validation loss: 1.4846e-03\n",
      "Epoch: 271460 | training loss: 1.8887e-03 | validation loss: 1.4846e-03\n",
      "Epoch: 271470 | training loss: 1.8887e-03 | validation loss: 1.4846e-03\n",
      "Epoch: 271480 | training loss: 1.8887e-03 | validation loss: 1.4846e-03\n",
      "Epoch: 271490 | training loss: 1.8887e-03 | validation loss: 1.4843e-03\n",
      "Epoch: 271500 | training loss: 1.9013e-03 | validation loss: 1.4881e-03\n",
      "Epoch: 271510 | training loss: 3.2931e-03 | validation loss: 2.1876e-03\n",
      "Epoch: 271520 | training loss: 2.3560e-03 | validation loss: 1.8238e-03\n",
      "Epoch: 271530 | training loss: 1.9595e-03 | validation loss: 1.5634e-03\n",
      "Epoch: 271540 | training loss: 1.9362e-03 | validation loss: 1.5061e-03\n",
      "Epoch: 271550 | training loss: 1.9035e-03 | validation loss: 1.4922e-03\n",
      "Epoch: 271560 | training loss: 1.8950e-03 | validation loss: 1.4997e-03\n",
      "Epoch: 271570 | training loss: 1.8896e-03 | validation loss: 1.4895e-03\n",
      "Epoch: 271580 | training loss: 1.8895e-03 | validation loss: 1.4832e-03\n",
      "Epoch: 271590 | training loss: 1.8887e-03 | validation loss: 1.4865e-03\n",
      "Epoch: 271600 | training loss: 1.8885e-03 | validation loss: 1.4838e-03\n",
      "Epoch: 271610 | training loss: 1.8885e-03 | validation loss: 1.4842e-03\n",
      "Epoch: 271620 | training loss: 1.8885e-03 | validation loss: 1.4841e-03\n",
      "Epoch: 271630 | training loss: 1.8885e-03 | validation loss: 1.4846e-03\n",
      "Epoch: 271640 | training loss: 1.8884e-03 | validation loss: 1.4843e-03\n",
      "Epoch: 271650 | training loss: 1.8884e-03 | validation loss: 1.4844e-03\n",
      "Epoch: 271660 | training loss: 1.8884e-03 | validation loss: 1.4844e-03\n",
      "Epoch: 271670 | training loss: 1.8884e-03 | validation loss: 1.4844e-03\n",
      "Epoch: 271680 | training loss: 1.8884e-03 | validation loss: 1.4844e-03\n",
      "Epoch: 271690 | training loss: 1.8884e-03 | validation loss: 1.4847e-03\n",
      "Epoch: 271700 | training loss: 1.8889e-03 | validation loss: 1.4871e-03\n",
      "Epoch: 271710 | training loss: 1.9494e-03 | validation loss: 1.5399e-03\n",
      "Epoch: 271720 | training loss: 3.7077e-03 | validation loss: 2.5114e-03\n",
      "Epoch: 271730 | training loss: 2.0031e-03 | validation loss: 1.5771e-03\n",
      "Epoch: 271740 | training loss: 1.8993e-03 | validation loss: 1.4787e-03\n",
      "Epoch: 271750 | training loss: 1.9284e-03 | validation loss: 1.4826e-03\n",
      "Epoch: 271760 | training loss: 1.9140e-03 | validation loss: 1.4801e-03\n",
      "Epoch: 271770 | training loss: 1.8928e-03 | validation loss: 1.4791e-03\n",
      "Epoch: 271780 | training loss: 1.8884e-03 | validation loss: 1.4853e-03\n",
      "Epoch: 271790 | training loss: 1.8894e-03 | validation loss: 1.4885e-03\n",
      "Epoch: 271800 | training loss: 1.8883e-03 | validation loss: 1.4853e-03\n",
      "Epoch: 271810 | training loss: 1.8883e-03 | validation loss: 1.4831e-03\n",
      "Epoch: 271820 | training loss: 1.8882e-03 | validation loss: 1.4841e-03\n",
      "Epoch: 271830 | training loss: 1.8882e-03 | validation loss: 1.4847e-03\n",
      "Epoch: 271840 | training loss: 1.8882e-03 | validation loss: 1.4839e-03\n",
      "Epoch: 271850 | training loss: 1.8881e-03 | validation loss: 1.4842e-03\n",
      "Epoch: 271860 | training loss: 1.8881e-03 | validation loss: 1.4841e-03\n",
      "Epoch: 271870 | training loss: 1.8881e-03 | validation loss: 1.4841e-03\n",
      "Epoch: 271880 | training loss: 1.8881e-03 | validation loss: 1.4841e-03\n",
      "Epoch: 271890 | training loss: 1.8881e-03 | validation loss: 1.4841e-03\n",
      "Epoch: 271900 | training loss: 1.8881e-03 | validation loss: 1.4840e-03\n",
      "Epoch: 271910 | training loss: 1.8880e-03 | validation loss: 1.4840e-03\n",
      "Epoch: 271920 | training loss: 1.8880e-03 | validation loss: 1.4840e-03\n",
      "Epoch: 271930 | training loss: 1.8880e-03 | validation loss: 1.4840e-03\n",
      "Epoch: 271940 | training loss: 1.8880e-03 | validation loss: 1.4840e-03\n",
      "Epoch: 271950 | training loss: 1.8880e-03 | validation loss: 1.4840e-03\n",
      "Epoch: 271960 | training loss: 1.8889e-03 | validation loss: 1.4845e-03\n",
      "Epoch: 271970 | training loss: 2.0914e-03 | validation loss: 1.6038e-03\n",
      "Epoch: 271980 | training loss: 3.3469e-03 | validation loss: 2.5366e-03\n",
      "Epoch: 271990 | training loss: 2.4285e-03 | validation loss: 1.7598e-03\n",
      "Epoch: 272000 | training loss: 1.9033e-03 | validation loss: 1.4972e-03\n",
      "Epoch: 272010 | training loss: 1.9463e-03 | validation loss: 1.5253e-03\n",
      "Epoch: 272020 | training loss: 1.8960e-03 | validation loss: 1.4777e-03\n",
      "Epoch: 272030 | training loss: 1.8906e-03 | validation loss: 1.4816e-03\n",
      "Epoch: 272040 | training loss: 1.8911e-03 | validation loss: 1.4932e-03\n",
      "Epoch: 272050 | training loss: 1.8884e-03 | validation loss: 1.4820e-03\n",
      "Epoch: 272060 | training loss: 1.8880e-03 | validation loss: 1.4845e-03\n",
      "Epoch: 272070 | training loss: 1.8879e-03 | validation loss: 1.4823e-03\n",
      "Epoch: 272080 | training loss: 1.8879e-03 | validation loss: 1.4838e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 272090 | training loss: 1.8878e-03 | validation loss: 1.4828e-03\n",
      "Epoch: 272100 | training loss: 1.8878e-03 | validation loss: 1.4835e-03\n",
      "Epoch: 272110 | training loss: 1.8878e-03 | validation loss: 1.4836e-03\n",
      "Epoch: 272120 | training loss: 1.8877e-03 | validation loss: 1.4835e-03\n",
      "Epoch: 272130 | training loss: 1.8877e-03 | validation loss: 1.4835e-03\n",
      "Epoch: 272140 | training loss: 1.8877e-03 | validation loss: 1.4839e-03\n",
      "Epoch: 272150 | training loss: 1.8884e-03 | validation loss: 1.4870e-03\n",
      "Epoch: 272160 | training loss: 1.9483e-03 | validation loss: 1.5433e-03\n",
      "Epoch: 272170 | training loss: 3.4620e-03 | validation loss: 2.4028e-03\n",
      "Epoch: 272180 | training loss: 1.9178e-03 | validation loss: 1.5212e-03\n",
      "Epoch: 272190 | training loss: 1.9646e-03 | validation loss: 1.4896e-03\n",
      "Epoch: 272200 | training loss: 1.9501e-03 | validation loss: 1.4842e-03\n",
      "Epoch: 272210 | training loss: 1.8932e-03 | validation loss: 1.4769e-03\n",
      "Epoch: 272220 | training loss: 1.8905e-03 | validation loss: 1.4904e-03\n",
      "Epoch: 272230 | training loss: 1.8901e-03 | validation loss: 1.4899e-03\n",
      "Epoch: 272240 | training loss: 1.8877e-03 | validation loss: 1.4819e-03\n",
      "Epoch: 272250 | training loss: 1.8879e-03 | validation loss: 1.4815e-03\n",
      "Epoch: 272260 | training loss: 1.8876e-03 | validation loss: 1.4845e-03\n",
      "Epoch: 272270 | training loss: 1.8875e-03 | validation loss: 1.4833e-03\n",
      "Epoch: 272280 | training loss: 1.8875e-03 | validation loss: 1.4830e-03\n",
      "Epoch: 272290 | training loss: 1.8875e-03 | validation loss: 1.4835e-03\n",
      "Epoch: 272300 | training loss: 1.8875e-03 | validation loss: 1.4831e-03\n",
      "Epoch: 272310 | training loss: 1.8875e-03 | validation loss: 1.4833e-03\n",
      "Epoch: 272320 | training loss: 1.8875e-03 | validation loss: 1.4832e-03\n",
      "Epoch: 272330 | training loss: 1.8874e-03 | validation loss: 1.4832e-03\n",
      "Epoch: 272340 | training loss: 1.8874e-03 | validation loss: 1.4832e-03\n",
      "Epoch: 272350 | training loss: 1.8874e-03 | validation loss: 1.4832e-03\n",
      "Epoch: 272360 | training loss: 1.8874e-03 | validation loss: 1.4832e-03\n",
      "Epoch: 272370 | training loss: 1.8874e-03 | validation loss: 1.4832e-03\n",
      "Epoch: 272380 | training loss: 1.8874e-03 | validation loss: 1.4834e-03\n",
      "Epoch: 272390 | training loss: 1.8878e-03 | validation loss: 1.4858e-03\n",
      "Epoch: 272400 | training loss: 1.9562e-03 | validation loss: 1.5461e-03\n",
      "Epoch: 272410 | training loss: 3.5780e-03 | validation loss: 2.4482e-03\n",
      "Epoch: 272420 | training loss: 2.3229e-03 | validation loss: 1.7780e-03\n",
      "Epoch: 272430 | training loss: 1.9731e-03 | validation loss: 1.5699e-03\n",
      "Epoch: 272440 | training loss: 1.9068e-03 | validation loss: 1.5151e-03\n",
      "Epoch: 272450 | training loss: 1.8895e-03 | validation loss: 1.4908e-03\n",
      "Epoch: 272460 | training loss: 1.8880e-03 | validation loss: 1.4798e-03\n",
      "Epoch: 272470 | training loss: 1.8894e-03 | validation loss: 1.4772e-03\n",
      "Epoch: 272480 | training loss: 1.8884e-03 | validation loss: 1.4788e-03\n",
      "Epoch: 272490 | training loss: 1.8874e-03 | validation loss: 1.4821e-03\n",
      "Epoch: 272500 | training loss: 1.8872e-03 | validation loss: 1.4839e-03\n",
      "Epoch: 272510 | training loss: 1.8872e-03 | validation loss: 1.4839e-03\n",
      "Epoch: 272520 | training loss: 1.8872e-03 | validation loss: 1.4829e-03\n",
      "Epoch: 272530 | training loss: 1.8872e-03 | validation loss: 1.4826e-03\n",
      "Epoch: 272540 | training loss: 1.8871e-03 | validation loss: 1.4831e-03\n",
      "Epoch: 272550 | training loss: 1.8871e-03 | validation loss: 1.4829e-03\n",
      "Epoch: 272560 | training loss: 1.8871e-03 | validation loss: 1.4828e-03\n",
      "Epoch: 272570 | training loss: 1.8871e-03 | validation loss: 1.4829e-03\n",
      "Epoch: 272580 | training loss: 1.8871e-03 | validation loss: 1.4828e-03\n",
      "Epoch: 272590 | training loss: 1.8871e-03 | validation loss: 1.4829e-03\n",
      "Epoch: 272600 | training loss: 1.8870e-03 | validation loss: 1.4828e-03\n",
      "Epoch: 272610 | training loss: 1.8870e-03 | validation loss: 1.4828e-03\n",
      "Epoch: 272620 | training loss: 1.8870e-03 | validation loss: 1.4828e-03\n",
      "Epoch: 272630 | training loss: 1.8870e-03 | validation loss: 1.4833e-03\n",
      "Epoch: 272640 | training loss: 1.8937e-03 | validation loss: 1.4927e-03\n",
      "Epoch: 272650 | training loss: 2.6008e-03 | validation loss: 2.0134e-03\n",
      "Epoch: 272660 | training loss: 1.9572e-03 | validation loss: 1.4871e-03\n",
      "Epoch: 272670 | training loss: 1.9381e-03 | validation loss: 1.4895e-03\n",
      "Epoch: 272680 | training loss: 1.9100e-03 | validation loss: 1.4893e-03\n",
      "Epoch: 272690 | training loss: 1.9040e-03 | validation loss: 1.4951e-03\n",
      "Epoch: 272700 | training loss: 1.9437e-03 | validation loss: 1.5348e-03\n",
      "Epoch: 272710 | training loss: 2.3577e-03 | validation loss: 1.7948e-03\n",
      "Epoch: 272720 | training loss: 1.9361e-03 | validation loss: 1.4782e-03\n",
      "Epoch: 272730 | training loss: 1.8924e-03 | validation loss: 1.4937e-03\n",
      "Epoch: 272740 | training loss: 1.8871e-03 | validation loss: 1.4842e-03\n",
      "Epoch: 272750 | training loss: 1.8921e-03 | validation loss: 1.4763e-03\n",
      "Epoch: 272760 | training loss: 1.8917e-03 | validation loss: 1.4934e-03\n",
      "Epoch: 272770 | training loss: 1.8868e-03 | validation loss: 1.4832e-03\n",
      "Epoch: 272780 | training loss: 1.8878e-03 | validation loss: 1.4788e-03\n",
      "Epoch: 272790 | training loss: 1.8901e-03 | validation loss: 1.4768e-03\n",
      "Epoch: 272800 | training loss: 1.9172e-03 | validation loss: 1.4759e-03\n",
      "Epoch: 272810 | training loss: 2.3717e-03 | validation loss: 1.6343e-03\n",
      "Epoch: 272820 | training loss: 1.9112e-03 | validation loss: 1.5139e-03\n",
      "Epoch: 272830 | training loss: 1.9076e-03 | validation loss: 1.4762e-03\n",
      "Epoch: 272840 | training loss: 1.8998e-03 | validation loss: 1.5008e-03\n",
      "Epoch: 272850 | training loss: 1.8909e-03 | validation loss: 1.4764e-03\n",
      "Epoch: 272860 | training loss: 1.8869e-03 | validation loss: 1.4841e-03\n",
      "Epoch: 272870 | training loss: 1.8873e-03 | validation loss: 1.4855e-03\n",
      "Epoch: 272880 | training loss: 1.8878e-03 | validation loss: 1.4787e-03\n",
      "Epoch: 272890 | training loss: 1.8866e-03 | validation loss: 1.4816e-03\n",
      "Epoch: 272900 | training loss: 1.8868e-03 | validation loss: 1.4837e-03\n",
      "Epoch: 272910 | training loss: 1.8875e-03 | validation loss: 1.4860e-03\n",
      "Epoch: 272920 | training loss: 1.8998e-03 | validation loss: 1.5015e-03\n",
      "Epoch: 272930 | training loss: 2.2838e-03 | validation loss: 1.7462e-03\n",
      "Epoch: 272940 | training loss: 1.8891e-03 | validation loss: 1.4789e-03\n",
      "Epoch: 272950 | training loss: 1.9704e-03 | validation loss: 1.5568e-03\n",
      "Epoch: 272960 | training loss: 1.9530e-03 | validation loss: 1.4881e-03\n",
      "Epoch: 272970 | training loss: 1.9004e-03 | validation loss: 1.4973e-03\n",
      "Epoch: 272980 | training loss: 1.8878e-03 | validation loss: 1.4828e-03\n",
      "Epoch: 272990 | training loss: 1.8867e-03 | validation loss: 1.4808e-03\n",
      "Epoch: 273000 | training loss: 1.8865e-03 | validation loss: 1.4828e-03\n",
      "Epoch: 273010 | training loss: 1.8866e-03 | validation loss: 1.4823e-03\n",
      "Epoch: 273020 | training loss: 1.8866e-03 | validation loss: 1.4814e-03\n",
      "Epoch: 273030 | training loss: 1.8865e-03 | validation loss: 1.4824e-03\n",
      "Epoch: 273040 | training loss: 1.8864e-03 | validation loss: 1.4824e-03\n",
      "Epoch: 273050 | training loss: 1.8864e-03 | validation loss: 1.4822e-03\n",
      "Epoch: 273060 | training loss: 1.8864e-03 | validation loss: 1.4824e-03\n",
      "Epoch: 273070 | training loss: 1.8866e-03 | validation loss: 1.4835e-03\n",
      "Epoch: 273080 | training loss: 1.8960e-03 | validation loss: 1.4960e-03\n",
      "Epoch: 273090 | training loss: 2.6209e-03 | validation loss: 1.9231e-03\n",
      "Epoch: 273100 | training loss: 2.3871e-03 | validation loss: 1.6914e-03\n",
      "Epoch: 273110 | training loss: 1.9244e-03 | validation loss: 1.5276e-03\n",
      "Epoch: 273120 | training loss: 1.9613e-03 | validation loss: 1.5266e-03\n",
      "Epoch: 273130 | training loss: 1.9116e-03 | validation loss: 1.4954e-03\n",
      "Epoch: 273140 | training loss: 1.8868e-03 | validation loss: 1.4854e-03\n",
      "Epoch: 273150 | training loss: 1.8895e-03 | validation loss: 1.4814e-03\n",
      "Epoch: 273160 | training loss: 1.8870e-03 | validation loss: 1.4832e-03\n",
      "Epoch: 273170 | training loss: 1.8865e-03 | validation loss: 1.4815e-03\n",
      "Epoch: 273180 | training loss: 1.8863e-03 | validation loss: 1.4825e-03\n",
      "Epoch: 273190 | training loss: 1.8862e-03 | validation loss: 1.4816e-03\n",
      "Epoch: 273200 | training loss: 1.8862e-03 | validation loss: 1.4817e-03\n",
      "Epoch: 273210 | training loss: 1.8862e-03 | validation loss: 1.4819e-03\n",
      "Epoch: 273220 | training loss: 1.8861e-03 | validation loss: 1.4819e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 273230 | training loss: 1.8861e-03 | validation loss: 1.4821e-03\n",
      "Epoch: 273240 | training loss: 1.8862e-03 | validation loss: 1.4835e-03\n",
      "Epoch: 273250 | training loss: 1.8927e-03 | validation loss: 1.4978e-03\n",
      "Epoch: 273260 | training loss: 2.4155e-03 | validation loss: 1.9050e-03\n",
      "Epoch: 273270 | training loss: 2.1805e-03 | validation loss: 1.5959e-03\n",
      "Epoch: 273280 | training loss: 1.9652e-03 | validation loss: 1.5378e-03\n",
      "Epoch: 273290 | training loss: 1.9102e-03 | validation loss: 1.4880e-03\n",
      "Epoch: 273300 | training loss: 1.8995e-03 | validation loss: 1.5047e-03\n",
      "Epoch: 273310 | training loss: 1.8901e-03 | validation loss: 1.4901e-03\n",
      "Epoch: 273320 | training loss: 1.8860e-03 | validation loss: 1.4808e-03\n",
      "Epoch: 273330 | training loss: 1.8865e-03 | validation loss: 1.4793e-03\n",
      "Epoch: 273340 | training loss: 1.8861e-03 | validation loss: 1.4811e-03\n",
      "Epoch: 273350 | training loss: 1.8861e-03 | validation loss: 1.4811e-03\n",
      "Epoch: 273360 | training loss: 1.8859e-03 | validation loss: 1.4816e-03\n",
      "Epoch: 273370 | training loss: 1.8859e-03 | validation loss: 1.4814e-03\n",
      "Epoch: 273380 | training loss: 1.8859e-03 | validation loss: 1.4820e-03\n",
      "Epoch: 273390 | training loss: 1.8861e-03 | validation loss: 1.4829e-03\n",
      "Epoch: 273400 | training loss: 1.8917e-03 | validation loss: 1.4923e-03\n",
      "Epoch: 273410 | training loss: 2.1967e-03 | validation loss: 1.6930e-03\n",
      "Epoch: 273420 | training loss: 1.9009e-03 | validation loss: 1.4987e-03\n",
      "Epoch: 273430 | training loss: 2.1196e-03 | validation loss: 1.6461e-03\n",
      "Epoch: 273440 | training loss: 1.9039e-03 | validation loss: 1.4783e-03\n",
      "Epoch: 273450 | training loss: 1.9128e-03 | validation loss: 1.4766e-03\n",
      "Epoch: 273460 | training loss: 1.8919e-03 | validation loss: 1.4909e-03\n",
      "Epoch: 273470 | training loss: 1.8865e-03 | validation loss: 1.4846e-03\n",
      "Epoch: 273480 | training loss: 1.8874e-03 | validation loss: 1.4781e-03\n",
      "Epoch: 273490 | training loss: 1.8863e-03 | validation loss: 1.4835e-03\n",
      "Epoch: 273500 | training loss: 1.8859e-03 | validation loss: 1.4803e-03\n",
      "Epoch: 273510 | training loss: 1.8858e-03 | validation loss: 1.4816e-03\n",
      "Epoch: 273520 | training loss: 1.8857e-03 | validation loss: 1.4806e-03\n",
      "Epoch: 273530 | training loss: 1.8857e-03 | validation loss: 1.4814e-03\n",
      "Epoch: 273540 | training loss: 1.8857e-03 | validation loss: 1.4809e-03\n",
      "Epoch: 273550 | training loss: 1.8856e-03 | validation loss: 1.4808e-03\n",
      "Epoch: 273560 | training loss: 1.8856e-03 | validation loss: 1.4809e-03\n",
      "Epoch: 273570 | training loss: 1.8856e-03 | validation loss: 1.4810e-03\n",
      "Epoch: 273580 | training loss: 1.8856e-03 | validation loss: 1.4812e-03\n",
      "Epoch: 273590 | training loss: 1.8858e-03 | validation loss: 1.4823e-03\n",
      "Epoch: 273600 | training loss: 1.8999e-03 | validation loss: 1.4990e-03\n",
      "Epoch: 273610 | training loss: 3.2733e-03 | validation loss: 2.2739e-03\n",
      "Epoch: 273620 | training loss: 2.5815e-03 | validation loss: 1.7458e-03\n",
      "Epoch: 273630 | training loss: 2.0989e-03 | validation loss: 1.5872e-03\n",
      "Epoch: 273640 | training loss: 1.9210e-03 | validation loss: 1.5060e-03\n",
      "Epoch: 273650 | training loss: 1.8863e-03 | validation loss: 1.4792e-03\n",
      "Epoch: 273660 | training loss: 1.8878e-03 | validation loss: 1.4817e-03\n",
      "Epoch: 273670 | training loss: 1.8892e-03 | validation loss: 1.4853e-03\n",
      "Epoch: 273680 | training loss: 1.8867e-03 | validation loss: 1.4815e-03\n",
      "Epoch: 273690 | training loss: 1.8855e-03 | validation loss: 1.4811e-03\n",
      "Epoch: 273700 | training loss: 1.8856e-03 | validation loss: 1.4807e-03\n",
      "Epoch: 273710 | training loss: 1.8854e-03 | validation loss: 1.4809e-03\n",
      "Epoch: 273720 | training loss: 1.8854e-03 | validation loss: 1.4808e-03\n",
      "Epoch: 273730 | training loss: 1.8854e-03 | validation loss: 1.4808e-03\n",
      "Epoch: 273740 | training loss: 1.8854e-03 | validation loss: 1.4807e-03\n",
      "Epoch: 273750 | training loss: 1.8854e-03 | validation loss: 1.4807e-03\n",
      "Epoch: 273760 | training loss: 1.8853e-03 | validation loss: 1.4806e-03\n",
      "Epoch: 273770 | training loss: 1.8853e-03 | validation loss: 1.4805e-03\n",
      "Epoch: 273780 | training loss: 1.8854e-03 | validation loss: 1.4797e-03\n",
      "Epoch: 273790 | training loss: 1.8879e-03 | validation loss: 1.4746e-03\n",
      "Epoch: 273800 | training loss: 2.1921e-03 | validation loss: 1.5863e-03\n",
      "Epoch: 273810 | training loss: 2.0967e-03 | validation loss: 1.6713e-03\n",
      "Epoch: 273820 | training loss: 1.9508e-03 | validation loss: 1.4914e-03\n",
      "Epoch: 273830 | training loss: 1.9142e-03 | validation loss: 1.5076e-03\n",
      "Epoch: 273840 | training loss: 1.8920e-03 | validation loss: 1.4736e-03\n",
      "Epoch: 273850 | training loss: 1.8908e-03 | validation loss: 1.4741e-03\n",
      "Epoch: 273860 | training loss: 1.8883e-03 | validation loss: 1.4812e-03\n",
      "Epoch: 273870 | training loss: 1.8859e-03 | validation loss: 1.4767e-03\n",
      "Epoch: 273880 | training loss: 1.8853e-03 | validation loss: 1.4819e-03\n",
      "Epoch: 273890 | training loss: 1.8853e-03 | validation loss: 1.4808e-03\n",
      "Epoch: 273900 | training loss: 1.8852e-03 | validation loss: 1.4800e-03\n",
      "Epoch: 273910 | training loss: 1.8851e-03 | validation loss: 1.4803e-03\n",
      "Epoch: 273920 | training loss: 1.8851e-03 | validation loss: 1.4804e-03\n",
      "Epoch: 273930 | training loss: 1.8851e-03 | validation loss: 1.4805e-03\n",
      "Epoch: 273940 | training loss: 1.8851e-03 | validation loss: 1.4806e-03\n",
      "Epoch: 273950 | training loss: 1.8856e-03 | validation loss: 1.4831e-03\n",
      "Epoch: 273960 | training loss: 1.9213e-03 | validation loss: 1.5192e-03\n",
      "Epoch: 273970 | training loss: 3.5809e-03 | validation loss: 2.4501e-03\n",
      "Epoch: 273980 | training loss: 2.1099e-03 | validation loss: 1.5473e-03\n",
      "Epoch: 273990 | training loss: 2.0591e-03 | validation loss: 1.5188e-03\n",
      "Epoch: 274000 | training loss: 1.8892e-03 | validation loss: 1.4719e-03\n",
      "Epoch: 274010 | training loss: 1.9038e-03 | validation loss: 1.4998e-03\n",
      "Epoch: 274020 | training loss: 1.8888e-03 | validation loss: 1.4891e-03\n",
      "Epoch: 274030 | training loss: 1.8869e-03 | validation loss: 1.4788e-03\n",
      "Epoch: 274040 | training loss: 1.8852e-03 | validation loss: 1.4782e-03\n",
      "Epoch: 274050 | training loss: 1.8854e-03 | validation loss: 1.4816e-03\n",
      "Epoch: 274060 | training loss: 1.8850e-03 | validation loss: 1.4799e-03\n",
      "Epoch: 274070 | training loss: 1.8849e-03 | validation loss: 1.4799e-03\n",
      "Epoch: 274080 | training loss: 1.8849e-03 | validation loss: 1.4801e-03\n",
      "Epoch: 274090 | training loss: 1.8849e-03 | validation loss: 1.4799e-03\n",
      "Epoch: 274100 | training loss: 1.8848e-03 | validation loss: 1.4799e-03\n",
      "Epoch: 274110 | training loss: 1.8848e-03 | validation loss: 1.4800e-03\n",
      "Epoch: 274120 | training loss: 1.8848e-03 | validation loss: 1.4798e-03\n",
      "Epoch: 274130 | training loss: 1.8848e-03 | validation loss: 1.4799e-03\n",
      "Epoch: 274140 | training loss: 1.8848e-03 | validation loss: 1.4799e-03\n",
      "Epoch: 274150 | training loss: 1.8848e-03 | validation loss: 1.4799e-03\n",
      "Epoch: 274160 | training loss: 1.8848e-03 | validation loss: 1.4800e-03\n",
      "Epoch: 274170 | training loss: 1.8849e-03 | validation loss: 1.4809e-03\n",
      "Epoch: 274180 | training loss: 1.8952e-03 | validation loss: 1.4935e-03\n",
      "Epoch: 274190 | training loss: 2.9900e-03 | validation loss: 2.1190e-03\n",
      "Epoch: 274200 | training loss: 2.6307e-03 | validation loss: 1.7897e-03\n",
      "Epoch: 274210 | training loss: 2.0975e-03 | validation loss: 1.6456e-03\n",
      "Epoch: 274220 | training loss: 1.9687e-03 | validation loss: 1.5634e-03\n",
      "Epoch: 274230 | training loss: 1.9097e-03 | validation loss: 1.4974e-03\n",
      "Epoch: 274240 | training loss: 1.8947e-03 | validation loss: 1.4826e-03\n",
      "Epoch: 274250 | training loss: 1.8874e-03 | validation loss: 1.4859e-03\n",
      "Epoch: 274260 | training loss: 1.8851e-03 | validation loss: 1.4810e-03\n",
      "Epoch: 274270 | training loss: 1.8847e-03 | validation loss: 1.4787e-03\n",
      "Epoch: 274280 | training loss: 1.8847e-03 | validation loss: 1.4804e-03\n",
      "Epoch: 274290 | training loss: 1.8847e-03 | validation loss: 1.4789e-03\n",
      "Epoch: 274300 | training loss: 1.8846e-03 | validation loss: 1.4799e-03\n",
      "Epoch: 274310 | training loss: 1.8845e-03 | validation loss: 1.4795e-03\n",
      "Epoch: 274320 | training loss: 1.8845e-03 | validation loss: 1.4796e-03\n",
      "Epoch: 274330 | training loss: 1.8845e-03 | validation loss: 1.4796e-03\n",
      "Epoch: 274340 | training loss: 1.8845e-03 | validation loss: 1.4795e-03\n",
      "Epoch: 274350 | training loss: 1.8845e-03 | validation loss: 1.4795e-03\n",
      "Epoch: 274360 | training loss: 1.8845e-03 | validation loss: 1.4794e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 274370 | training loss: 1.8845e-03 | validation loss: 1.4790e-03\n",
      "Epoch: 274380 | training loss: 1.8852e-03 | validation loss: 1.4760e-03\n",
      "Epoch: 274390 | training loss: 1.9737e-03 | validation loss: 1.4845e-03\n",
      "Epoch: 274400 | training loss: 2.5831e-03 | validation loss: 1.7172e-03\n",
      "Epoch: 274410 | training loss: 2.1497e-03 | validation loss: 1.5662e-03\n",
      "Epoch: 274420 | training loss: 2.0108e-03 | validation loss: 1.5422e-03\n",
      "Epoch: 274430 | training loss: 1.9207e-03 | validation loss: 1.4890e-03\n",
      "Epoch: 274440 | training loss: 1.8964e-03 | validation loss: 1.4700e-03\n",
      "Epoch: 274450 | training loss: 1.8864e-03 | validation loss: 1.4738e-03\n",
      "Epoch: 274460 | training loss: 1.8850e-03 | validation loss: 1.4807e-03\n",
      "Epoch: 274470 | training loss: 1.8845e-03 | validation loss: 1.4796e-03\n",
      "Epoch: 274480 | training loss: 1.8845e-03 | validation loss: 1.4808e-03\n",
      "Epoch: 274490 | training loss: 1.8843e-03 | validation loss: 1.4801e-03\n",
      "Epoch: 274500 | training loss: 1.8843e-03 | validation loss: 1.4787e-03\n",
      "Epoch: 274510 | training loss: 1.8843e-03 | validation loss: 1.4787e-03\n",
      "Epoch: 274520 | training loss: 1.8842e-03 | validation loss: 1.4790e-03\n",
      "Epoch: 274530 | training loss: 1.8842e-03 | validation loss: 1.4790e-03\n",
      "Epoch: 274540 | training loss: 1.8842e-03 | validation loss: 1.4789e-03\n",
      "Epoch: 274550 | training loss: 1.8842e-03 | validation loss: 1.4790e-03\n",
      "Epoch: 274560 | training loss: 1.8842e-03 | validation loss: 1.4790e-03\n",
      "Epoch: 274570 | training loss: 1.8842e-03 | validation loss: 1.4791e-03\n",
      "Epoch: 274580 | training loss: 1.8843e-03 | validation loss: 1.4802e-03\n",
      "Epoch: 274590 | training loss: 1.8968e-03 | validation loss: 1.4952e-03\n",
      "Epoch: 274600 | training loss: 3.1139e-03 | validation loss: 2.1894e-03\n",
      "Epoch: 274610 | training loss: 2.6377e-03 | validation loss: 1.7539e-03\n",
      "Epoch: 274620 | training loss: 1.9841e-03 | validation loss: 1.5103e-03\n",
      "Epoch: 274630 | training loss: 1.9039e-03 | validation loss: 1.5106e-03\n",
      "Epoch: 274640 | training loss: 1.9109e-03 | validation loss: 1.5159e-03\n",
      "Epoch: 274650 | training loss: 1.8893e-03 | validation loss: 1.4894e-03\n",
      "Epoch: 274660 | training loss: 1.8857e-03 | validation loss: 1.4743e-03\n",
      "Epoch: 274670 | training loss: 1.8853e-03 | validation loss: 1.4744e-03\n",
      "Epoch: 274680 | training loss: 1.8842e-03 | validation loss: 1.4808e-03\n",
      "Epoch: 274690 | training loss: 1.8841e-03 | validation loss: 1.4806e-03\n",
      "Epoch: 274700 | training loss: 1.8841e-03 | validation loss: 1.4777e-03\n",
      "Epoch: 274710 | training loss: 1.8840e-03 | validation loss: 1.4791e-03\n",
      "Epoch: 274720 | training loss: 1.8840e-03 | validation loss: 1.4789e-03\n",
      "Epoch: 274730 | training loss: 1.8839e-03 | validation loss: 1.4787e-03\n",
      "Epoch: 274740 | training loss: 1.8839e-03 | validation loss: 1.4788e-03\n",
      "Epoch: 274750 | training loss: 1.8839e-03 | validation loss: 1.4787e-03\n",
      "Epoch: 274760 | training loss: 1.8839e-03 | validation loss: 1.4787e-03\n",
      "Epoch: 274770 | training loss: 1.8839e-03 | validation loss: 1.4787e-03\n",
      "Epoch: 274780 | training loss: 1.8839e-03 | validation loss: 1.4787e-03\n",
      "Epoch: 274790 | training loss: 1.8838e-03 | validation loss: 1.4787e-03\n",
      "Epoch: 274800 | training loss: 1.8838e-03 | validation loss: 1.4787e-03\n",
      "Epoch: 274810 | training loss: 1.8838e-03 | validation loss: 1.4787e-03\n",
      "Epoch: 274820 | training loss: 1.8838e-03 | validation loss: 1.4788e-03\n",
      "Epoch: 274830 | training loss: 1.8839e-03 | validation loss: 1.4802e-03\n",
      "Epoch: 274840 | training loss: 1.9142e-03 | validation loss: 1.5161e-03\n",
      "Epoch: 274850 | training loss: 4.5365e-03 | validation loss: 2.9854e-03\n",
      "Epoch: 274860 | training loss: 2.5148e-03 | validation loss: 1.9873e-03\n",
      "Epoch: 274870 | training loss: 2.1586e-03 | validation loss: 1.7198e-03\n",
      "Epoch: 274880 | training loss: 1.9463e-03 | validation loss: 1.5491e-03\n",
      "Epoch: 274890 | training loss: 1.8862e-03 | validation loss: 1.4856e-03\n",
      "Epoch: 274900 | training loss: 1.8854e-03 | validation loss: 1.4730e-03\n",
      "Epoch: 274910 | training loss: 1.8863e-03 | validation loss: 1.4719e-03\n",
      "Epoch: 274920 | training loss: 1.8850e-03 | validation loss: 1.4734e-03\n",
      "Epoch: 274930 | training loss: 1.8842e-03 | validation loss: 1.4750e-03\n",
      "Epoch: 274940 | training loss: 1.8838e-03 | validation loss: 1.4762e-03\n",
      "Epoch: 274950 | training loss: 1.8837e-03 | validation loss: 1.4769e-03\n",
      "Epoch: 274960 | training loss: 1.8836e-03 | validation loss: 1.4774e-03\n",
      "Epoch: 274970 | training loss: 1.8836e-03 | validation loss: 1.4778e-03\n",
      "Epoch: 274980 | training loss: 1.8836e-03 | validation loss: 1.4781e-03\n",
      "Epoch: 274990 | training loss: 1.8836e-03 | validation loss: 1.4782e-03\n",
      "Epoch: 275000 | training loss: 1.8835e-03 | validation loss: 1.4782e-03\n",
      "Epoch: 275010 | training loss: 1.8835e-03 | validation loss: 1.4781e-03\n",
      "Epoch: 275020 | training loss: 1.8835e-03 | validation loss: 1.4780e-03\n",
      "Epoch: 275030 | training loss: 1.8835e-03 | validation loss: 1.4780e-03\n",
      "Epoch: 275040 | training loss: 1.8835e-03 | validation loss: 1.4780e-03\n",
      "Epoch: 275050 | training loss: 1.8835e-03 | validation loss: 1.4780e-03\n",
      "Epoch: 275060 | training loss: 1.8921e-03 | validation loss: 1.4821e-03\n",
      "Epoch: 275070 | training loss: 2.8352e-03 | validation loss: 2.0030e-03\n",
      "Epoch: 275080 | training loss: 2.2223e-03 | validation loss: 1.7147e-03\n",
      "Epoch: 275090 | training loss: 1.9021e-03 | validation loss: 1.4725e-03\n",
      "Epoch: 275100 | training loss: 1.9060e-03 | validation loss: 1.4706e-03\n",
      "Epoch: 275110 | training loss: 1.8963e-03 | validation loss: 1.4925e-03\n",
      "Epoch: 275120 | training loss: 1.8848e-03 | validation loss: 1.4738e-03\n",
      "Epoch: 275130 | training loss: 1.8845e-03 | validation loss: 1.4737e-03\n",
      "Epoch: 275140 | training loss: 1.8840e-03 | validation loss: 1.4789e-03\n",
      "Epoch: 275150 | training loss: 1.8835e-03 | validation loss: 1.4764e-03\n",
      "Epoch: 275160 | training loss: 1.8834e-03 | validation loss: 1.4792e-03\n",
      "Epoch: 275170 | training loss: 1.8833e-03 | validation loss: 1.4779e-03\n",
      "Epoch: 275180 | training loss: 1.8833e-03 | validation loss: 1.4774e-03\n",
      "Epoch: 275190 | training loss: 1.8833e-03 | validation loss: 1.4779e-03\n",
      "Epoch: 275200 | training loss: 1.8833e-03 | validation loss: 1.4780e-03\n",
      "Epoch: 275210 | training loss: 1.8832e-03 | validation loss: 1.4779e-03\n",
      "Epoch: 275220 | training loss: 1.8832e-03 | validation loss: 1.4782e-03\n",
      "Epoch: 275230 | training loss: 1.8838e-03 | validation loss: 1.4805e-03\n",
      "Epoch: 275240 | training loss: 1.9228e-03 | validation loss: 1.5172e-03\n",
      "Epoch: 275250 | training loss: 3.6382e-03 | validation loss: 2.4678e-03\n",
      "Epoch: 275260 | training loss: 2.0304e-03 | validation loss: 1.5074e-03\n",
      "Epoch: 275270 | training loss: 2.0768e-03 | validation loss: 1.5246e-03\n",
      "Epoch: 275280 | training loss: 1.8950e-03 | validation loss: 1.4721e-03\n",
      "Epoch: 275290 | training loss: 1.8941e-03 | validation loss: 1.4946e-03\n",
      "Epoch: 275300 | training loss: 1.8905e-03 | validation loss: 1.4903e-03\n",
      "Epoch: 275310 | training loss: 1.8835e-03 | validation loss: 1.4755e-03\n",
      "Epoch: 275320 | training loss: 1.8840e-03 | validation loss: 1.4749e-03\n",
      "Epoch: 275330 | training loss: 1.8833e-03 | validation loss: 1.4796e-03\n",
      "Epoch: 275340 | training loss: 1.8831e-03 | validation loss: 1.4779e-03\n",
      "Epoch: 275350 | training loss: 1.8831e-03 | validation loss: 1.4770e-03\n",
      "Epoch: 275360 | training loss: 1.8831e-03 | validation loss: 1.4782e-03\n",
      "Epoch: 275370 | training loss: 1.8830e-03 | validation loss: 1.4773e-03\n",
      "Epoch: 275380 | training loss: 1.8830e-03 | validation loss: 1.4778e-03\n",
      "Epoch: 275390 | training loss: 1.8830e-03 | validation loss: 1.4775e-03\n",
      "Epoch: 275400 | training loss: 1.8830e-03 | validation loss: 1.4776e-03\n",
      "Epoch: 275410 | training loss: 1.8830e-03 | validation loss: 1.4776e-03\n",
      "Epoch: 275420 | training loss: 1.8829e-03 | validation loss: 1.4775e-03\n",
      "Epoch: 275430 | training loss: 1.8829e-03 | validation loss: 1.4775e-03\n",
      "Epoch: 275440 | training loss: 1.8829e-03 | validation loss: 1.4774e-03\n",
      "Epoch: 275450 | training loss: 1.8829e-03 | validation loss: 1.4771e-03\n",
      "Epoch: 275460 | training loss: 1.8836e-03 | validation loss: 1.4750e-03\n",
      "Epoch: 275470 | training loss: 1.9671e-03 | validation loss: 1.4888e-03\n",
      "Epoch: 275480 | training loss: 3.2666e-03 | validation loss: 2.0309e-03\n",
      "Epoch: 275490 | training loss: 2.2447e-03 | validation loss: 1.5910e-03\n",
      "Epoch: 275500 | training loss: 1.9059e-03 | validation loss: 1.4715e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 275510 | training loss: 1.8846e-03 | validation loss: 1.4815e-03\n",
      "Epoch: 275520 | training loss: 1.8945e-03 | validation loss: 1.4943e-03\n",
      "Epoch: 275530 | training loss: 1.8918e-03 | validation loss: 1.4915e-03\n",
      "Epoch: 275540 | training loss: 1.8843e-03 | validation loss: 1.4821e-03\n",
      "Epoch: 275550 | training loss: 1.8829e-03 | validation loss: 1.4764e-03\n",
      "Epoch: 275560 | training loss: 1.8832e-03 | validation loss: 1.4756e-03\n",
      "Epoch: 275570 | training loss: 1.8827e-03 | validation loss: 1.4774e-03\n",
      "Epoch: 275580 | training loss: 1.8828e-03 | validation loss: 1.4782e-03\n",
      "Epoch: 275590 | training loss: 1.8827e-03 | validation loss: 1.4772e-03\n",
      "Epoch: 275600 | training loss: 1.8827e-03 | validation loss: 1.4771e-03\n",
      "Epoch: 275610 | training loss: 1.8827e-03 | validation loss: 1.4774e-03\n",
      "Epoch: 275620 | training loss: 1.8826e-03 | validation loss: 1.4771e-03\n",
      "Epoch: 275630 | training loss: 1.8826e-03 | validation loss: 1.4772e-03\n",
      "Epoch: 275640 | training loss: 1.8827e-03 | validation loss: 1.4762e-03\n",
      "Epoch: 275650 | training loss: 1.9018e-03 | validation loss: 1.4751e-03\n",
      "Epoch: 275660 | training loss: 2.5096e-03 | validation loss: 1.8034e-03\n",
      "Epoch: 275670 | training loss: 2.1319e-03 | validation loss: 1.6276e-03\n",
      "Epoch: 275680 | training loss: 1.9672e-03 | validation loss: 1.5023e-03\n",
      "Epoch: 275690 | training loss: 1.9130e-03 | validation loss: 1.4922e-03\n",
      "Epoch: 275700 | training loss: 1.8938e-03 | validation loss: 1.4719e-03\n",
      "Epoch: 275710 | training loss: 1.8865e-03 | validation loss: 1.4789e-03\n",
      "Epoch: 275720 | training loss: 1.8835e-03 | validation loss: 1.4748e-03\n",
      "Epoch: 275730 | training loss: 1.8828e-03 | validation loss: 1.4746e-03\n",
      "Epoch: 275740 | training loss: 1.8825e-03 | validation loss: 1.4766e-03\n",
      "Epoch: 275750 | training loss: 1.8825e-03 | validation loss: 1.4776e-03\n",
      "Epoch: 275760 | training loss: 1.8826e-03 | validation loss: 1.4787e-03\n",
      "Epoch: 275770 | training loss: 1.8867e-03 | validation loss: 1.4871e-03\n",
      "Epoch: 275780 | training loss: 2.1363e-03 | validation loss: 1.6644e-03\n",
      "Epoch: 275790 | training loss: 1.9337e-03 | validation loss: 1.5207e-03\n",
      "Epoch: 275800 | training loss: 2.0948e-03 | validation loss: 1.6311e-03\n",
      "Epoch: 275810 | training loss: 1.9053e-03 | validation loss: 1.4716e-03\n",
      "Epoch: 275820 | training loss: 1.9084e-03 | validation loss: 1.4726e-03\n",
      "Epoch: 275830 | training loss: 1.8888e-03 | validation loss: 1.4900e-03\n",
      "Epoch: 275840 | training loss: 1.8831e-03 | validation loss: 1.4805e-03\n",
      "Epoch: 275850 | training loss: 1.8840e-03 | validation loss: 1.4726e-03\n",
      "Epoch: 275860 | training loss: 1.8829e-03 | validation loss: 1.4791e-03\n",
      "Epoch: 275870 | training loss: 1.8824e-03 | validation loss: 1.4755e-03\n",
      "Epoch: 275880 | training loss: 1.8823e-03 | validation loss: 1.4773e-03\n",
      "Epoch: 275890 | training loss: 1.8823e-03 | validation loss: 1.4761e-03\n",
      "Epoch: 275900 | training loss: 1.8823e-03 | validation loss: 1.4769e-03\n",
      "Epoch: 275910 | training loss: 1.8822e-03 | validation loss: 1.4763e-03\n",
      "Epoch: 275920 | training loss: 1.8822e-03 | validation loss: 1.4764e-03\n",
      "Epoch: 275930 | training loss: 1.8822e-03 | validation loss: 1.4765e-03\n",
      "Epoch: 275940 | training loss: 1.8822e-03 | validation loss: 1.4766e-03\n",
      "Epoch: 275950 | training loss: 1.8822e-03 | validation loss: 1.4770e-03\n",
      "Epoch: 275960 | training loss: 1.8827e-03 | validation loss: 1.4794e-03\n",
      "Epoch: 275970 | training loss: 1.9219e-03 | validation loss: 1.5180e-03\n",
      "Epoch: 275980 | training loss: 3.6561e-03 | validation loss: 2.4871e-03\n",
      "Epoch: 275990 | training loss: 2.0190e-03 | validation loss: 1.5165e-03\n",
      "Epoch: 276000 | training loss: 2.0669e-03 | validation loss: 1.5127e-03\n",
      "Epoch: 276010 | training loss: 1.9090e-03 | validation loss: 1.4660e-03\n",
      "Epoch: 276020 | training loss: 1.8864e-03 | validation loss: 1.4869e-03\n",
      "Epoch: 276030 | training loss: 1.8911e-03 | validation loss: 1.4954e-03\n",
      "Epoch: 276040 | training loss: 1.8822e-03 | validation loss: 1.4763e-03\n",
      "Epoch: 276050 | training loss: 1.8832e-03 | validation loss: 1.4725e-03\n",
      "Epoch: 276060 | training loss: 1.8820e-03 | validation loss: 1.4769e-03\n",
      "Epoch: 276070 | training loss: 1.8821e-03 | validation loss: 1.4777e-03\n",
      "Epoch: 276080 | training loss: 1.8820e-03 | validation loss: 1.4753e-03\n",
      "Epoch: 276090 | training loss: 1.8820e-03 | validation loss: 1.4768e-03\n",
      "Epoch: 276100 | training loss: 1.8820e-03 | validation loss: 1.4760e-03\n",
      "Epoch: 276110 | training loss: 1.8819e-03 | validation loss: 1.4763e-03\n",
      "Epoch: 276120 | training loss: 1.8819e-03 | validation loss: 1.4761e-03\n",
      "Epoch: 276130 | training loss: 1.8819e-03 | validation loss: 1.4763e-03\n",
      "Epoch: 276140 | training loss: 1.8819e-03 | validation loss: 1.4763e-03\n",
      "Epoch: 276150 | training loss: 1.8821e-03 | validation loss: 1.4771e-03\n",
      "Epoch: 276160 | training loss: 1.8975e-03 | validation loss: 1.4948e-03\n",
      "Epoch: 276170 | training loss: 2.4589e-03 | validation loss: 1.9007e-03\n",
      "Epoch: 276180 | training loss: 1.9100e-03 | validation loss: 1.4685e-03\n",
      "Epoch: 276190 | training loss: 1.9347e-03 | validation loss: 1.4888e-03\n",
      "Epoch: 276200 | training loss: 1.8997e-03 | validation loss: 1.4785e-03\n",
      "Epoch: 276210 | training loss: 1.8831e-03 | validation loss: 1.4801e-03\n",
      "Epoch: 276220 | training loss: 1.9052e-03 | validation loss: 1.5095e-03\n",
      "Epoch: 276230 | training loss: 2.4482e-03 | validation loss: 1.8499e-03\n",
      "Epoch: 276240 | training loss: 1.9824e-03 | validation loss: 1.4849e-03\n",
      "Epoch: 276250 | training loss: 2.0046e-03 | validation loss: 1.5730e-03\n",
      "Epoch: 276260 | training loss: 1.9297e-03 | validation loss: 1.4718e-03\n",
      "Epoch: 276270 | training loss: 1.8900e-03 | validation loss: 1.4902e-03\n",
      "Epoch: 276280 | training loss: 1.8828e-03 | validation loss: 1.4731e-03\n",
      "Epoch: 276290 | training loss: 1.8820e-03 | validation loss: 1.4783e-03\n",
      "Epoch: 276300 | training loss: 1.8821e-03 | validation loss: 1.4735e-03\n",
      "Epoch: 276310 | training loss: 1.8821e-03 | validation loss: 1.4782e-03\n",
      "Epoch: 276320 | training loss: 1.8817e-03 | validation loss: 1.4747e-03\n",
      "Epoch: 276330 | training loss: 1.8817e-03 | validation loss: 1.4748e-03\n",
      "Epoch: 276340 | training loss: 1.8816e-03 | validation loss: 1.4757e-03\n",
      "Epoch: 276350 | training loss: 1.8816e-03 | validation loss: 1.4762e-03\n",
      "Epoch: 276360 | training loss: 1.8818e-03 | validation loss: 1.4776e-03\n",
      "Epoch: 276370 | training loss: 1.8906e-03 | validation loss: 1.4910e-03\n",
      "Epoch: 276380 | training loss: 2.4668e-03 | validation loss: 1.8485e-03\n",
      "Epoch: 276390 | training loss: 2.1330e-03 | validation loss: 1.5358e-03\n",
      "Epoch: 276400 | training loss: 2.0530e-03 | validation loss: 1.5968e-03\n",
      "Epoch: 276410 | training loss: 1.9266e-03 | validation loss: 1.5293e-03\n",
      "Epoch: 276420 | training loss: 1.8953e-03 | validation loss: 1.4778e-03\n",
      "Epoch: 276430 | training loss: 1.8888e-03 | validation loss: 1.4672e-03\n",
      "Epoch: 276440 | training loss: 1.8829e-03 | validation loss: 1.4790e-03\n",
      "Epoch: 276450 | training loss: 1.8825e-03 | validation loss: 1.4801e-03\n",
      "Epoch: 276460 | training loss: 1.8819e-03 | validation loss: 1.4729e-03\n",
      "Epoch: 276470 | training loss: 1.8815e-03 | validation loss: 1.4767e-03\n",
      "Epoch: 276480 | training loss: 1.8814e-03 | validation loss: 1.4754e-03\n",
      "Epoch: 276490 | training loss: 1.8814e-03 | validation loss: 1.4755e-03\n",
      "Epoch: 276500 | training loss: 1.8814e-03 | validation loss: 1.4754e-03\n",
      "Epoch: 276510 | training loss: 1.8814e-03 | validation loss: 1.4756e-03\n",
      "Epoch: 276520 | training loss: 1.8813e-03 | validation loss: 1.4753e-03\n",
      "Epoch: 276530 | training loss: 1.8813e-03 | validation loss: 1.4755e-03\n",
      "Epoch: 276540 | training loss: 1.8813e-03 | validation loss: 1.4755e-03\n",
      "Epoch: 276550 | training loss: 1.8813e-03 | validation loss: 1.4755e-03\n",
      "Epoch: 276560 | training loss: 1.8813e-03 | validation loss: 1.4756e-03\n",
      "Epoch: 276570 | training loss: 1.8865e-03 | validation loss: 1.4788e-03\n",
      "Epoch: 276580 | training loss: 2.4837e-03 | validation loss: 1.8373e-03\n",
      "Epoch: 276590 | training loss: 2.3663e-03 | validation loss: 1.8741e-03\n",
      "Epoch: 276600 | training loss: 1.9685e-03 | validation loss: 1.5610e-03\n",
      "Epoch: 276610 | training loss: 1.9262e-03 | validation loss: 1.4786e-03\n",
      "Epoch: 276620 | training loss: 1.8953e-03 | validation loss: 1.4940e-03\n",
      "Epoch: 276630 | training loss: 1.8847e-03 | validation loss: 1.4701e-03\n",
      "Epoch: 276640 | training loss: 1.8831e-03 | validation loss: 1.4722e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 276650 | training loss: 1.8821e-03 | validation loss: 1.4747e-03\n",
      "Epoch: 276660 | training loss: 1.8815e-03 | validation loss: 1.4733e-03\n",
      "Epoch: 276670 | training loss: 1.8812e-03 | validation loss: 1.4742e-03\n",
      "Epoch: 276680 | training loss: 1.8812e-03 | validation loss: 1.4755e-03\n",
      "Epoch: 276690 | training loss: 1.8811e-03 | validation loss: 1.4744e-03\n",
      "Epoch: 276700 | training loss: 1.8811e-03 | validation loss: 1.4746e-03\n",
      "Epoch: 276710 | training loss: 1.8811e-03 | validation loss: 1.4748e-03\n",
      "Epoch: 276720 | training loss: 1.8811e-03 | validation loss: 1.4746e-03\n",
      "Epoch: 276730 | training loss: 1.8812e-03 | validation loss: 1.4733e-03\n",
      "Epoch: 276740 | training loss: 1.8918e-03 | validation loss: 1.4675e-03\n",
      "Epoch: 276750 | training loss: 2.7517e-03 | validation loss: 1.7862e-03\n",
      "Epoch: 276760 | training loss: 2.5073e-03 | validation loss: 1.8715e-03\n",
      "Epoch: 276770 | training loss: 1.8881e-03 | validation loss: 1.4769e-03\n",
      "Epoch: 276780 | training loss: 1.9621e-03 | validation loss: 1.4873e-03\n",
      "Epoch: 276790 | training loss: 1.8936e-03 | validation loss: 1.4716e-03\n",
      "Epoch: 276800 | training loss: 1.8858e-03 | validation loss: 1.4862e-03\n",
      "Epoch: 276810 | training loss: 1.8835e-03 | validation loss: 1.4822e-03\n",
      "Epoch: 276820 | training loss: 1.8818e-03 | validation loss: 1.4720e-03\n",
      "Epoch: 276830 | training loss: 1.8810e-03 | validation loss: 1.4738e-03\n",
      "Epoch: 276840 | training loss: 1.8811e-03 | validation loss: 1.4763e-03\n",
      "Epoch: 276850 | training loss: 1.8809e-03 | validation loss: 1.4740e-03\n",
      "Epoch: 276860 | training loss: 1.8809e-03 | validation loss: 1.4753e-03\n",
      "Epoch: 276870 | training loss: 1.8808e-03 | validation loss: 1.4745e-03\n",
      "Epoch: 276880 | training loss: 1.8808e-03 | validation loss: 1.4749e-03\n",
      "Epoch: 276890 | training loss: 1.8808e-03 | validation loss: 1.4746e-03\n",
      "Epoch: 276900 | training loss: 1.8808e-03 | validation loss: 1.4748e-03\n",
      "Epoch: 276910 | training loss: 1.8808e-03 | validation loss: 1.4747e-03\n",
      "Epoch: 276920 | training loss: 1.8808e-03 | validation loss: 1.4747e-03\n",
      "Epoch: 276930 | training loss: 1.8808e-03 | validation loss: 1.4746e-03\n",
      "Epoch: 276940 | training loss: 1.8807e-03 | validation loss: 1.4745e-03\n",
      "Epoch: 276950 | training loss: 1.8808e-03 | validation loss: 1.4738e-03\n",
      "Epoch: 276960 | training loss: 1.8849e-03 | validation loss: 1.4697e-03\n",
      "Epoch: 276970 | training loss: 2.3978e-03 | validation loss: 1.6491e-03\n",
      "Epoch: 276980 | training loss: 2.3009e-03 | validation loss: 1.7564e-03\n",
      "Epoch: 276990 | training loss: 1.9081e-03 | validation loss: 1.4787e-03\n",
      "Epoch: 277000 | training loss: 1.9516e-03 | validation loss: 1.4773e-03\n",
      "Epoch: 277010 | training loss: 1.9258e-03 | validation loss: 1.4686e-03\n",
      "Epoch: 277020 | training loss: 1.8917e-03 | validation loss: 1.4668e-03\n",
      "Epoch: 277030 | training loss: 1.8808e-03 | validation loss: 1.4747e-03\n",
      "Epoch: 277040 | training loss: 1.8821e-03 | validation loss: 1.4796e-03\n",
      "Epoch: 277050 | training loss: 1.8811e-03 | validation loss: 1.4776e-03\n",
      "Epoch: 277060 | training loss: 1.8806e-03 | validation loss: 1.4738e-03\n",
      "Epoch: 277070 | training loss: 1.8806e-03 | validation loss: 1.4735e-03\n",
      "Epoch: 277080 | training loss: 1.8806e-03 | validation loss: 1.4751e-03\n",
      "Epoch: 277090 | training loss: 1.8805e-03 | validation loss: 1.4746e-03\n",
      "Epoch: 277100 | training loss: 1.8805e-03 | validation loss: 1.4742e-03\n",
      "Epoch: 277110 | training loss: 1.8805e-03 | validation loss: 1.4746e-03\n",
      "Epoch: 277120 | training loss: 1.8805e-03 | validation loss: 1.4743e-03\n",
      "Epoch: 277130 | training loss: 1.8805e-03 | validation loss: 1.4745e-03\n",
      "Epoch: 277140 | training loss: 1.8805e-03 | validation loss: 1.4744e-03\n",
      "Epoch: 277150 | training loss: 1.8804e-03 | validation loss: 1.4747e-03\n",
      "Epoch: 277160 | training loss: 1.8811e-03 | validation loss: 1.4771e-03\n",
      "Epoch: 277170 | training loss: 1.9690e-03 | validation loss: 1.5621e-03\n",
      "Epoch: 277180 | training loss: 1.8854e-03 | validation loss: 1.4813e-03\n",
      "Epoch: 277190 | training loss: 1.9726e-03 | validation loss: 1.5563e-03\n",
      "Epoch: 277200 | training loss: 1.9245e-03 | validation loss: 1.5155e-03\n",
      "Epoch: 277210 | training loss: 1.8866e-03 | validation loss: 1.4828e-03\n",
      "Epoch: 277220 | training loss: 1.8805e-03 | validation loss: 1.4737e-03\n",
      "Epoch: 277230 | training loss: 1.8824e-03 | validation loss: 1.4744e-03\n",
      "Epoch: 277240 | training loss: 1.8871e-03 | validation loss: 1.4859e-03\n",
      "Epoch: 277250 | training loss: 2.2222e-03 | validation loss: 1.7155e-03\n",
      "Epoch: 277260 | training loss: 1.8846e-03 | validation loss: 1.4686e-03\n",
      "Epoch: 277270 | training loss: 2.0997e-03 | validation loss: 1.6317e-03\n",
      "Epoch: 277280 | training loss: 1.8969e-03 | validation loss: 1.4677e-03\n",
      "Epoch: 277290 | training loss: 1.9002e-03 | validation loss: 1.4687e-03\n",
      "Epoch: 277300 | training loss: 1.8888e-03 | validation loss: 1.4893e-03\n",
      "Epoch: 277310 | training loss: 1.8802e-03 | validation loss: 1.4739e-03\n",
      "Epoch: 277320 | training loss: 1.8809e-03 | validation loss: 1.4711e-03\n",
      "Epoch: 277330 | training loss: 1.8807e-03 | validation loss: 1.4765e-03\n",
      "Epoch: 277340 | training loss: 1.8804e-03 | validation loss: 1.4724e-03\n",
      "Epoch: 277350 | training loss: 1.8802e-03 | validation loss: 1.4750e-03\n",
      "Epoch: 277360 | training loss: 1.8802e-03 | validation loss: 1.4733e-03\n",
      "Epoch: 277370 | training loss: 1.8801e-03 | validation loss: 1.4738e-03\n",
      "Epoch: 277380 | training loss: 1.8801e-03 | validation loss: 1.4741e-03\n",
      "Epoch: 277390 | training loss: 1.8801e-03 | validation loss: 1.4738e-03\n",
      "Epoch: 277400 | training loss: 1.8801e-03 | validation loss: 1.4737e-03\n",
      "Epoch: 277410 | training loss: 1.8801e-03 | validation loss: 1.4734e-03\n",
      "Epoch: 277420 | training loss: 1.8802e-03 | validation loss: 1.4723e-03\n",
      "Epoch: 277430 | training loss: 1.8897e-03 | validation loss: 1.4675e-03\n",
      "Epoch: 277440 | training loss: 2.7361e-03 | validation loss: 1.7881e-03\n",
      "Epoch: 277450 | training loss: 2.5578e-03 | validation loss: 1.8995e-03\n",
      "Epoch: 277460 | training loss: 1.8938e-03 | validation loss: 1.4937e-03\n",
      "Epoch: 277470 | training loss: 1.9382e-03 | validation loss: 1.4733e-03\n",
      "Epoch: 277480 | training loss: 1.9120e-03 | validation loss: 1.4645e-03\n",
      "Epoch: 277490 | training loss: 1.8803e-03 | validation loss: 1.4711e-03\n",
      "Epoch: 277500 | training loss: 1.8837e-03 | validation loss: 1.4848e-03\n",
      "Epoch: 277510 | training loss: 1.8804e-03 | validation loss: 1.4761e-03\n",
      "Epoch: 277520 | training loss: 1.8804e-03 | validation loss: 1.4712e-03\n",
      "Epoch: 277530 | training loss: 1.8799e-03 | validation loss: 1.4734e-03\n",
      "Epoch: 277540 | training loss: 1.8799e-03 | validation loss: 1.4748e-03\n",
      "Epoch: 277550 | training loss: 1.8799e-03 | validation loss: 1.4728e-03\n",
      "Epoch: 277560 | training loss: 1.8799e-03 | validation loss: 1.4739e-03\n",
      "Epoch: 277570 | training loss: 1.8798e-03 | validation loss: 1.4734e-03\n",
      "Epoch: 277580 | training loss: 1.8798e-03 | validation loss: 1.4736e-03\n",
      "Epoch: 277590 | training loss: 1.8798e-03 | validation loss: 1.4734e-03\n",
      "Epoch: 277600 | training loss: 1.8798e-03 | validation loss: 1.4735e-03\n",
      "Epoch: 277610 | training loss: 1.8798e-03 | validation loss: 1.4732e-03\n",
      "Epoch: 277620 | training loss: 1.8805e-03 | validation loss: 1.4721e-03\n",
      "Epoch: 277630 | training loss: 1.9588e-03 | validation loss: 1.5089e-03\n",
      "Epoch: 277640 | training loss: 1.9067e-03 | validation loss: 1.5007e-03\n",
      "Epoch: 277650 | training loss: 2.0163e-03 | validation loss: 1.5516e-03\n",
      "Epoch: 277660 | training loss: 1.9056e-03 | validation loss: 1.4843e-03\n",
      "Epoch: 277670 | training loss: 1.8800e-03 | validation loss: 1.4753e-03\n",
      "Epoch: 277680 | training loss: 1.8849e-03 | validation loss: 1.4844e-03\n",
      "Epoch: 277690 | training loss: 1.8981e-03 | validation loss: 1.5015e-03\n",
      "Epoch: 277700 | training loss: 2.5693e-03 | validation loss: 1.9128e-03\n",
      "Epoch: 277710 | training loss: 2.1440e-03 | validation loss: 1.5381e-03\n",
      "Epoch: 277720 | training loss: 2.0372e-03 | validation loss: 1.5896e-03\n",
      "Epoch: 277730 | training loss: 1.8805e-03 | validation loss: 1.4689e-03\n",
      "Epoch: 277740 | training loss: 1.8954e-03 | validation loss: 1.4660e-03\n",
      "Epoch: 277750 | training loss: 1.8874e-03 | validation loss: 1.4875e-03\n",
      "Epoch: 277760 | training loss: 1.8808e-03 | validation loss: 1.4703e-03\n",
      "Epoch: 277770 | training loss: 1.8797e-03 | validation loss: 1.4744e-03\n",
      "Epoch: 277780 | training loss: 1.8796e-03 | validation loss: 1.4722e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 277790 | training loss: 1.8796e-03 | validation loss: 1.4738e-03\n",
      "Epoch: 277800 | training loss: 1.8796e-03 | validation loss: 1.4722e-03\n",
      "Epoch: 277810 | training loss: 1.8795e-03 | validation loss: 1.4735e-03\n",
      "Epoch: 277820 | training loss: 1.8795e-03 | validation loss: 1.4732e-03\n",
      "Epoch: 277830 | training loss: 1.8795e-03 | validation loss: 1.4729e-03\n",
      "Epoch: 277840 | training loss: 1.8794e-03 | validation loss: 1.4726e-03\n",
      "Epoch: 277850 | training loss: 1.8795e-03 | validation loss: 1.4719e-03\n",
      "Epoch: 277860 | training loss: 1.8827e-03 | validation loss: 1.4680e-03\n",
      "Epoch: 277870 | training loss: 2.1237e-03 | validation loss: 1.5362e-03\n",
      "Epoch: 277880 | training loss: 1.9606e-03 | validation loss: 1.4941e-03\n",
      "Epoch: 277890 | training loss: 2.1937e-03 | validation loss: 1.5599e-03\n",
      "Epoch: 277900 | training loss: 1.8944e-03 | validation loss: 1.4657e-03\n",
      "Epoch: 277910 | training loss: 1.9066e-03 | validation loss: 1.5046e-03\n",
      "Epoch: 277920 | training loss: 1.8895e-03 | validation loss: 1.4941e-03\n",
      "Epoch: 277930 | training loss: 1.8809e-03 | validation loss: 1.4682e-03\n",
      "Epoch: 277940 | training loss: 1.8806e-03 | validation loss: 1.4683e-03\n",
      "Epoch: 277950 | training loss: 1.8799e-03 | validation loss: 1.4769e-03\n",
      "Epoch: 277960 | training loss: 1.8793e-03 | validation loss: 1.4726e-03\n",
      "Epoch: 277970 | training loss: 1.8793e-03 | validation loss: 1.4721e-03\n",
      "Epoch: 277980 | training loss: 1.8793e-03 | validation loss: 1.4735e-03\n",
      "Epoch: 277990 | training loss: 1.8792e-03 | validation loss: 1.4723e-03\n",
      "Epoch: 278000 | training loss: 1.8792e-03 | validation loss: 1.4731e-03\n",
      "Epoch: 278010 | training loss: 1.8792e-03 | validation loss: 1.4725e-03\n",
      "Epoch: 278020 | training loss: 1.8792e-03 | validation loss: 1.4727e-03\n",
      "Epoch: 278030 | training loss: 1.8792e-03 | validation loss: 1.4727e-03\n",
      "Epoch: 278040 | training loss: 1.8792e-03 | validation loss: 1.4726e-03\n",
      "Epoch: 278050 | training loss: 1.8792e-03 | validation loss: 1.4721e-03\n",
      "Epoch: 278060 | training loss: 1.8875e-03 | validation loss: 1.4734e-03\n",
      "Epoch: 278070 | training loss: 2.4841e-03 | validation loss: 1.8369e-03\n",
      "Epoch: 278080 | training loss: 2.1856e-03 | validation loss: 1.7265e-03\n",
      "Epoch: 278090 | training loss: 1.9590e-03 | validation loss: 1.5454e-03\n",
      "Epoch: 278100 | training loss: 1.9356e-03 | validation loss: 1.4955e-03\n",
      "Epoch: 278110 | training loss: 1.8950e-03 | validation loss: 1.4992e-03\n",
      "Epoch: 278120 | training loss: 1.8850e-03 | validation loss: 1.4860e-03\n",
      "Epoch: 278130 | training loss: 1.8795e-03 | validation loss: 1.4736e-03\n",
      "Epoch: 278140 | training loss: 1.8795e-03 | validation loss: 1.4710e-03\n",
      "Epoch: 278150 | training loss: 1.8792e-03 | validation loss: 1.4704e-03\n",
      "Epoch: 278160 | training loss: 1.8848e-03 | validation loss: 1.4661e-03\n",
      "Epoch: 278170 | training loss: 2.3195e-03 | validation loss: 1.6059e-03\n",
      "Epoch: 278180 | training loss: 2.0005e-03 | validation loss: 1.5813e-03\n",
      "Epoch: 278190 | training loss: 2.1119e-03 | validation loss: 1.5398e-03\n",
      "Epoch: 278200 | training loss: 1.9154e-03 | validation loss: 1.4710e-03\n",
      "Epoch: 278210 | training loss: 1.8935e-03 | validation loss: 1.4924e-03\n",
      "Epoch: 278220 | training loss: 1.8868e-03 | validation loss: 1.4849e-03\n",
      "Epoch: 278230 | training loss: 1.8813e-03 | validation loss: 1.4671e-03\n",
      "Epoch: 278240 | training loss: 1.8792e-03 | validation loss: 1.4699e-03\n",
      "Epoch: 278250 | training loss: 1.8795e-03 | validation loss: 1.4752e-03\n",
      "Epoch: 278260 | training loss: 1.8790e-03 | validation loss: 1.4712e-03\n",
      "Epoch: 278270 | training loss: 1.8788e-03 | validation loss: 1.4727e-03\n",
      "Epoch: 278280 | training loss: 1.8788e-03 | validation loss: 1.4719e-03\n",
      "Epoch: 278290 | training loss: 1.8788e-03 | validation loss: 1.4723e-03\n",
      "Epoch: 278300 | training loss: 1.8788e-03 | validation loss: 1.4719e-03\n",
      "Epoch: 278310 | training loss: 1.8788e-03 | validation loss: 1.4722e-03\n",
      "Epoch: 278320 | training loss: 1.8788e-03 | validation loss: 1.4721e-03\n",
      "Epoch: 278330 | training loss: 1.8787e-03 | validation loss: 1.4720e-03\n",
      "Epoch: 278340 | training loss: 1.8787e-03 | validation loss: 1.4720e-03\n",
      "Epoch: 278350 | training loss: 1.8787e-03 | validation loss: 1.4718e-03\n",
      "Epoch: 278360 | training loss: 1.8788e-03 | validation loss: 1.4709e-03\n",
      "Epoch: 278370 | training loss: 1.8854e-03 | validation loss: 1.4664e-03\n",
      "Epoch: 278380 | training loss: 2.5462e-03 | validation loss: 1.7089e-03\n",
      "Epoch: 278390 | training loss: 2.4213e-03 | validation loss: 1.8225e-03\n",
      "Epoch: 278400 | training loss: 1.9010e-03 | validation loss: 1.4770e-03\n",
      "Epoch: 278410 | training loss: 1.9606e-03 | validation loss: 1.4766e-03\n",
      "Epoch: 278420 | training loss: 1.9129e-03 | validation loss: 1.4636e-03\n",
      "Epoch: 278430 | training loss: 1.8791e-03 | validation loss: 1.4699e-03\n",
      "Epoch: 278440 | training loss: 1.8821e-03 | validation loss: 1.4818e-03\n",
      "Epoch: 278450 | training loss: 1.8795e-03 | validation loss: 1.4757e-03\n",
      "Epoch: 278460 | training loss: 1.8789e-03 | validation loss: 1.4702e-03\n",
      "Epoch: 278470 | training loss: 1.8786e-03 | validation loss: 1.4707e-03\n",
      "Epoch: 278480 | training loss: 1.8786e-03 | validation loss: 1.4733e-03\n",
      "Epoch: 278490 | training loss: 1.8785e-03 | validation loss: 1.4715e-03\n",
      "Epoch: 278500 | training loss: 1.8785e-03 | validation loss: 1.4718e-03\n",
      "Epoch: 278510 | training loss: 1.8785e-03 | validation loss: 1.4720e-03\n",
      "Epoch: 278520 | training loss: 1.8785e-03 | validation loss: 1.4717e-03\n",
      "Epoch: 278530 | training loss: 1.8785e-03 | validation loss: 1.4718e-03\n",
      "Epoch: 278540 | training loss: 1.8784e-03 | validation loss: 1.4717e-03\n",
      "Epoch: 278550 | training loss: 1.8784e-03 | validation loss: 1.4715e-03\n",
      "Epoch: 278560 | training loss: 1.8791e-03 | validation loss: 1.4701e-03\n",
      "Epoch: 278570 | training loss: 1.9670e-03 | validation loss: 1.5093e-03\n",
      "Epoch: 278580 | training loss: 1.8803e-03 | validation loss: 1.4751e-03\n",
      "Epoch: 278590 | training loss: 1.9720e-03 | validation loss: 1.5142e-03\n",
      "Epoch: 278600 | training loss: 1.9220e-03 | validation loss: 1.4868e-03\n",
      "Epoch: 278610 | training loss: 1.8843e-03 | validation loss: 1.4702e-03\n",
      "Epoch: 278620 | training loss: 1.8786e-03 | validation loss: 1.4737e-03\n",
      "Epoch: 278630 | training loss: 1.8817e-03 | validation loss: 1.4814e-03\n",
      "Epoch: 278640 | training loss: 1.9531e-03 | validation loss: 1.5438e-03\n",
      "Epoch: 278650 | training loss: 3.0240e-03 | validation loss: 2.1607e-03\n",
      "Epoch: 278660 | training loss: 1.9319e-03 | validation loss: 1.4762e-03\n",
      "Epoch: 278670 | training loss: 1.9809e-03 | validation loss: 1.4868e-03\n",
      "Epoch: 278680 | training loss: 1.8924e-03 | validation loss: 1.4917e-03\n",
      "Epoch: 278690 | training loss: 1.8863e-03 | validation loss: 1.4850e-03\n",
      "Epoch: 278700 | training loss: 1.8839e-03 | validation loss: 1.4652e-03\n",
      "Epoch: 278710 | training loss: 1.8785e-03 | validation loss: 1.4728e-03\n",
      "Epoch: 278720 | training loss: 1.8783e-03 | validation loss: 1.4724e-03\n",
      "Epoch: 278730 | training loss: 1.8783e-03 | validation loss: 1.4702e-03\n",
      "Epoch: 278740 | training loss: 1.8782e-03 | validation loss: 1.4723e-03\n",
      "Epoch: 278750 | training loss: 1.8782e-03 | validation loss: 1.4707e-03\n",
      "Epoch: 278760 | training loss: 1.8781e-03 | validation loss: 1.4713e-03\n",
      "Epoch: 278770 | training loss: 1.8781e-03 | validation loss: 1.4715e-03\n",
      "Epoch: 278780 | training loss: 1.8781e-03 | validation loss: 1.4710e-03\n",
      "Epoch: 278790 | training loss: 1.8781e-03 | validation loss: 1.4710e-03\n",
      "Epoch: 278800 | training loss: 1.8781e-03 | validation loss: 1.4709e-03\n",
      "Epoch: 278810 | training loss: 1.8781e-03 | validation loss: 1.4704e-03\n",
      "Epoch: 278820 | training loss: 1.8796e-03 | validation loss: 1.4675e-03\n",
      "Epoch: 278830 | training loss: 1.9848e-03 | validation loss: 1.4868e-03\n",
      "Epoch: 278840 | training loss: 2.8888e-03 | validation loss: 1.8600e-03\n",
      "Epoch: 278850 | training loss: 1.9684e-03 | validation loss: 1.4751e-03\n",
      "Epoch: 278860 | training loss: 1.9432e-03 | validation loss: 1.5249e-03\n",
      "Epoch: 278870 | training loss: 1.9252e-03 | validation loss: 1.5251e-03\n",
      "Epoch: 278880 | training loss: 1.8796e-03 | validation loss: 1.4770e-03\n",
      "Epoch: 278890 | training loss: 1.8843e-03 | validation loss: 1.4629e-03\n",
      "Epoch: 278900 | training loss: 1.8781e-03 | validation loss: 1.4695e-03\n",
      "Epoch: 278910 | training loss: 1.8788e-03 | validation loss: 1.4759e-03\n",
      "Epoch: 278920 | training loss: 1.8780e-03 | validation loss: 1.4699e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 278930 | training loss: 1.8779e-03 | validation loss: 1.4706e-03\n",
      "Epoch: 278940 | training loss: 1.8779e-03 | validation loss: 1.4717e-03\n",
      "Epoch: 278950 | training loss: 1.8779e-03 | validation loss: 1.4704e-03\n",
      "Epoch: 278960 | training loss: 1.8778e-03 | validation loss: 1.4714e-03\n",
      "Epoch: 278970 | training loss: 1.8778e-03 | validation loss: 1.4707e-03\n",
      "Epoch: 278980 | training loss: 1.8778e-03 | validation loss: 1.4710e-03\n",
      "Epoch: 278990 | training loss: 1.8778e-03 | validation loss: 1.4709e-03\n",
      "Epoch: 279000 | training loss: 1.8778e-03 | validation loss: 1.4706e-03\n",
      "Epoch: 279010 | training loss: 1.8784e-03 | validation loss: 1.4697e-03\n",
      "Epoch: 279020 | training loss: 1.9267e-03 | validation loss: 1.4905e-03\n",
      "Epoch: 279030 | training loss: 2.0529e-03 | validation loss: 1.5759e-03\n",
      "Epoch: 279040 | training loss: 2.0523e-03 | validation loss: 1.5831e-03\n",
      "Epoch: 279050 | training loss: 1.9059e-03 | validation loss: 1.5045e-03\n",
      "Epoch: 279060 | training loss: 1.8888e-03 | validation loss: 1.4855e-03\n",
      "Epoch: 279070 | training loss: 1.8880e-03 | validation loss: 1.4725e-03\n",
      "Epoch: 279080 | training loss: 1.9242e-03 | validation loss: 1.4665e-03\n",
      "Epoch: 279090 | training loss: 2.3668e-03 | validation loss: 1.6230e-03\n",
      "Epoch: 279100 | training loss: 1.9242e-03 | validation loss: 1.5178e-03\n",
      "Epoch: 279110 | training loss: 1.8864e-03 | validation loss: 1.4642e-03\n",
      "Epoch: 279120 | training loss: 1.8780e-03 | validation loss: 1.4734e-03\n",
      "Epoch: 279130 | training loss: 1.8795e-03 | validation loss: 1.4766e-03\n",
      "Epoch: 279140 | training loss: 1.8824e-03 | validation loss: 1.4644e-03\n",
      "Epoch: 279150 | training loss: 1.8783e-03 | validation loss: 1.4741e-03\n",
      "Epoch: 279160 | training loss: 1.8789e-03 | validation loss: 1.4756e-03\n",
      "Epoch: 279170 | training loss: 1.8783e-03 | validation loss: 1.4741e-03\n",
      "Epoch: 279180 | training loss: 1.8808e-03 | validation loss: 1.4787e-03\n",
      "Epoch: 279190 | training loss: 1.9365e-03 | validation loss: 1.5277e-03\n",
      "Epoch: 279200 | training loss: 2.7582e-03 | validation loss: 2.0074e-03\n",
      "Epoch: 279210 | training loss: 2.1767e-03 | validation loss: 1.5571e-03\n",
      "Epoch: 279220 | training loss: 1.9599e-03 | validation loss: 1.5360e-03\n",
      "Epoch: 279230 | training loss: 1.8861e-03 | validation loss: 1.4678e-03\n",
      "Epoch: 279240 | training loss: 1.8775e-03 | validation loss: 1.4714e-03\n",
      "Epoch: 279250 | training loss: 1.8775e-03 | validation loss: 1.4700e-03\n",
      "Epoch: 279260 | training loss: 1.8775e-03 | validation loss: 1.4716e-03\n",
      "Epoch: 279270 | training loss: 1.8777e-03 | validation loss: 1.4683e-03\n",
      "Epoch: 279280 | training loss: 1.8778e-03 | validation loss: 1.4728e-03\n",
      "Epoch: 279290 | training loss: 1.8774e-03 | validation loss: 1.4699e-03\n",
      "Epoch: 279300 | training loss: 1.8775e-03 | validation loss: 1.4694e-03\n",
      "Epoch: 279310 | training loss: 1.8775e-03 | validation loss: 1.4690e-03\n",
      "Epoch: 279320 | training loss: 1.8786e-03 | validation loss: 1.4671e-03\n",
      "Epoch: 279330 | training loss: 1.9098e-03 | validation loss: 1.4676e-03\n",
      "Epoch: 279340 | training loss: 2.8544e-03 | validation loss: 1.8461e-03\n",
      "Epoch: 279350 | training loss: 2.2924e-03 | validation loss: 1.7399e-03\n",
      "Epoch: 279360 | training loss: 1.9528e-03 | validation loss: 1.4780e-03\n",
      "Epoch: 279370 | training loss: 1.8843e-03 | validation loss: 1.4640e-03\n",
      "Epoch: 279380 | training loss: 1.8971e-03 | validation loss: 1.4947e-03\n",
      "Epoch: 279390 | training loss: 1.8827e-03 | validation loss: 1.4652e-03\n",
      "Epoch: 279400 | training loss: 1.8778e-03 | validation loss: 1.4730e-03\n",
      "Epoch: 279410 | training loss: 1.8772e-03 | validation loss: 1.4696e-03\n",
      "Epoch: 279420 | training loss: 1.8772e-03 | validation loss: 1.4707e-03\n",
      "Epoch: 279430 | training loss: 1.8772e-03 | validation loss: 1.4695e-03\n",
      "Epoch: 279440 | training loss: 1.8772e-03 | validation loss: 1.4710e-03\n",
      "Epoch: 279450 | training loss: 1.8771e-03 | validation loss: 1.4697e-03\n",
      "Epoch: 279460 | training loss: 1.8771e-03 | validation loss: 1.4693e-03\n",
      "Epoch: 279470 | training loss: 1.8785e-03 | validation loss: 1.4668e-03\n",
      "Epoch: 279480 | training loss: 2.0716e-03 | validation loss: 1.5586e-03\n",
      "Epoch: 279490 | training loss: 2.1824e-03 | validation loss: 1.6445e-03\n",
      "Epoch: 279500 | training loss: 1.8780e-03 | validation loss: 1.4753e-03\n",
      "Epoch: 279510 | training loss: 1.8978e-03 | validation loss: 1.4886e-03\n",
      "Epoch: 279520 | training loss: 1.8868e-03 | validation loss: 1.4680e-03\n",
      "Epoch: 279530 | training loss: 1.8864e-03 | validation loss: 1.4627e-03\n",
      "Epoch: 279540 | training loss: 1.8957e-03 | validation loss: 1.4607e-03\n",
      "Epoch: 279550 | training loss: 2.0604e-03 | validation loss: 1.5068e-03\n",
      "Epoch: 279560 | training loss: 2.1056e-03 | validation loss: 1.5251e-03\n",
      "Epoch: 279570 | training loss: 1.9905e-03 | validation loss: 1.5655e-03\n",
      "Epoch: 279580 | training loss: 1.9238e-03 | validation loss: 1.4668e-03\n",
      "Epoch: 279590 | training loss: 1.8900e-03 | validation loss: 1.4891e-03\n",
      "Epoch: 279600 | training loss: 1.8770e-03 | validation loss: 1.4683e-03\n",
      "Epoch: 279610 | training loss: 1.8802e-03 | validation loss: 1.4644e-03\n",
      "Epoch: 279620 | training loss: 1.8769e-03 | validation loss: 1.4696e-03\n",
      "Epoch: 279630 | training loss: 1.8776e-03 | validation loss: 1.4732e-03\n",
      "Epoch: 279640 | training loss: 1.8826e-03 | validation loss: 1.4812e-03\n",
      "Epoch: 279650 | training loss: 1.9850e-03 | validation loss: 1.5607e-03\n",
      "Epoch: 279660 | training loss: 2.5714e-03 | validation loss: 1.9020e-03\n",
      "Epoch: 279670 | training loss: 2.0719e-03 | validation loss: 1.5191e-03\n",
      "Epoch: 279680 | training loss: 1.9064e-03 | validation loss: 1.5009e-03\n",
      "Epoch: 279690 | training loss: 1.8783e-03 | validation loss: 1.4648e-03\n",
      "Epoch: 279700 | training loss: 1.8768e-03 | validation loss: 1.4702e-03\n",
      "Epoch: 279710 | training loss: 1.8768e-03 | validation loss: 1.4691e-03\n",
      "Epoch: 279720 | training loss: 1.8772e-03 | validation loss: 1.4716e-03\n",
      "Epoch: 279730 | training loss: 1.8774e-03 | validation loss: 1.4671e-03\n",
      "Epoch: 279740 | training loss: 1.8768e-03 | validation loss: 1.4706e-03\n",
      "Epoch: 279750 | training loss: 1.8769e-03 | validation loss: 1.4710e-03\n",
      "Epoch: 279760 | training loss: 1.8768e-03 | validation loss: 1.4704e-03\n",
      "Epoch: 279770 | training loss: 1.8769e-03 | validation loss: 1.4712e-03\n",
      "Epoch: 279780 | training loss: 1.8811e-03 | validation loss: 1.4787e-03\n",
      "Epoch: 279790 | training loss: 2.0727e-03 | validation loss: 1.6123e-03\n",
      "Epoch: 279800 | training loss: 2.2024e-03 | validation loss: 1.6860e-03\n",
      "Epoch: 279810 | training loss: 1.9333e-03 | validation loss: 1.5266e-03\n",
      "Epoch: 279820 | training loss: 1.9730e-03 | validation loss: 1.4818e-03\n",
      "Epoch: 279830 | training loss: 1.8777e-03 | validation loss: 1.4655e-03\n",
      "Epoch: 279840 | training loss: 1.8890e-03 | validation loss: 1.4903e-03\n",
      "Epoch: 279850 | training loss: 1.8797e-03 | validation loss: 1.4638e-03\n",
      "Epoch: 279860 | training loss: 1.8766e-03 | validation loss: 1.4704e-03\n",
      "Epoch: 279870 | training loss: 1.8766e-03 | validation loss: 1.4702e-03\n",
      "Epoch: 279880 | training loss: 1.8766e-03 | validation loss: 1.4686e-03\n",
      "Epoch: 279890 | training loss: 1.8765e-03 | validation loss: 1.4697e-03\n",
      "Epoch: 279900 | training loss: 1.8765e-03 | validation loss: 1.4692e-03\n",
      "Epoch: 279910 | training loss: 1.8765e-03 | validation loss: 1.4689e-03\n",
      "Epoch: 279920 | training loss: 1.8765e-03 | validation loss: 1.4696e-03\n",
      "Epoch: 279930 | training loss: 1.8764e-03 | validation loss: 1.4696e-03\n",
      "Epoch: 279940 | training loss: 1.8766e-03 | validation loss: 1.4706e-03\n",
      "Epoch: 279950 | training loss: 1.8895e-03 | validation loss: 1.4886e-03\n",
      "Epoch: 279960 | training loss: 2.4713e-03 | validation loss: 1.9468e-03\n",
      "Epoch: 279970 | training loss: 1.9176e-03 | validation loss: 1.5052e-03\n",
      "Epoch: 279980 | training loss: 2.0467e-03 | validation loss: 1.5818e-03\n",
      "Epoch: 279990 | training loss: 2.0148e-03 | validation loss: 1.5684e-03\n",
      "Epoch: 280000 | training loss: 1.8798e-03 | validation loss: 1.4622e-03\n",
      "Epoch: 280010 | training loss: 1.9070e-03 | validation loss: 1.4643e-03\n",
      "Epoch: 280020 | training loss: 1.9014e-03 | validation loss: 1.4648e-03\n",
      "Epoch: 280030 | training loss: 1.9137e-03 | validation loss: 1.4643e-03\n",
      "Epoch: 280040 | training loss: 2.0290e-03 | validation loss: 1.4955e-03\n",
      "Epoch: 280050 | training loss: 2.0259e-03 | validation loss: 1.4952e-03\n",
      "Epoch: 280060 | training loss: 1.9127e-03 | validation loss: 1.5088e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 280070 | training loss: 1.8879e-03 | validation loss: 1.4872e-03\n",
      "Epoch: 280080 | training loss: 1.8828e-03 | validation loss: 1.4624e-03\n",
      "Epoch: 280090 | training loss: 1.8941e-03 | validation loss: 1.4617e-03\n",
      "Epoch: 280100 | training loss: 1.9347e-03 | validation loss: 1.4689e-03\n",
      "Epoch: 280110 | training loss: 2.1418e-03 | validation loss: 1.5383e-03\n",
      "Epoch: 280120 | training loss: 1.8892e-03 | validation loss: 1.4626e-03\n",
      "Epoch: 280130 | training loss: 1.9330e-03 | validation loss: 1.5229e-03\n",
      "Epoch: 280140 | training loss: 1.8801e-03 | validation loss: 1.4632e-03\n",
      "Epoch: 280150 | training loss: 1.8904e-03 | validation loss: 1.4624e-03\n",
      "Epoch: 280160 | training loss: 1.8818e-03 | validation loss: 1.4630e-03\n",
      "Epoch: 280170 | training loss: 1.8860e-03 | validation loss: 1.4624e-03\n",
      "Epoch: 280180 | training loss: 1.9770e-03 | validation loss: 1.4825e-03\n",
      "Epoch: 280190 | training loss: 2.4491e-03 | validation loss: 1.6661e-03\n",
      "Epoch: 280200 | training loss: 2.0585e-03 | validation loss: 1.6024e-03\n",
      "Epoch: 280210 | training loss: 1.9422e-03 | validation loss: 1.4746e-03\n",
      "Epoch: 280220 | training loss: 1.8984e-03 | validation loss: 1.4937e-03\n",
      "Epoch: 280230 | training loss: 1.8804e-03 | validation loss: 1.4648e-03\n",
      "Epoch: 280240 | training loss: 1.8761e-03 | validation loss: 1.4674e-03\n",
      "Epoch: 280250 | training loss: 1.8780e-03 | validation loss: 1.4743e-03\n",
      "Epoch: 280260 | training loss: 1.8760e-03 | validation loss: 1.4688e-03\n",
      "Epoch: 280270 | training loss: 1.8763e-03 | validation loss: 1.4666e-03\n",
      "Epoch: 280280 | training loss: 1.8785e-03 | validation loss: 1.4646e-03\n",
      "Epoch: 280290 | training loss: 1.9210e-03 | validation loss: 1.4690e-03\n",
      "Epoch: 280300 | training loss: 2.7412e-03 | validation loss: 1.7965e-03\n",
      "Epoch: 280310 | training loss: 2.1817e-03 | validation loss: 1.6746e-03\n",
      "Epoch: 280320 | training loss: 1.9881e-03 | validation loss: 1.4883e-03\n",
      "Epoch: 280330 | training loss: 1.8890e-03 | validation loss: 1.4859e-03\n",
      "Epoch: 280340 | training loss: 1.8759e-03 | validation loss: 1.4679e-03\n",
      "Epoch: 280350 | training loss: 1.8761e-03 | validation loss: 1.4669e-03\n",
      "Epoch: 280360 | training loss: 1.8759e-03 | validation loss: 1.4695e-03\n",
      "Epoch: 280370 | training loss: 1.8758e-03 | validation loss: 1.4690e-03\n",
      "Epoch: 280380 | training loss: 1.8760e-03 | validation loss: 1.4670e-03\n",
      "Epoch: 280390 | training loss: 1.8759e-03 | validation loss: 1.4697e-03\n",
      "Epoch: 280400 | training loss: 1.8758e-03 | validation loss: 1.4691e-03\n",
      "Epoch: 280410 | training loss: 1.8758e-03 | validation loss: 1.4693e-03\n",
      "Epoch: 280420 | training loss: 1.8851e-03 | validation loss: 1.4851e-03\n",
      "Epoch: 280430 | training loss: 2.6206e-03 | validation loss: 2.0789e-03\n",
      "Epoch: 280440 | training loss: 1.9723e-03 | validation loss: 1.5493e-03\n",
      "Epoch: 280450 | training loss: 1.9228e-03 | validation loss: 1.4685e-03\n",
      "Epoch: 280460 | training loss: 1.8982e-03 | validation loss: 1.4656e-03\n",
      "Epoch: 280470 | training loss: 1.8909e-03 | validation loss: 1.4781e-03\n",
      "Epoch: 280480 | training loss: 1.8863e-03 | validation loss: 1.4795e-03\n",
      "Epoch: 280490 | training loss: 1.8932e-03 | validation loss: 1.4905e-03\n",
      "Epoch: 280500 | training loss: 2.0171e-03 | validation loss: 1.5827e-03\n",
      "Epoch: 280510 | training loss: 2.1951e-03 | validation loss: 1.6919e-03\n",
      "Epoch: 280520 | training loss: 1.9984e-03 | validation loss: 1.4868e-03\n",
      "Epoch: 280530 | training loss: 1.9073e-03 | validation loss: 1.5048e-03\n",
      "Epoch: 280540 | training loss: 1.8767e-03 | validation loss: 1.4644e-03\n",
      "Epoch: 280550 | training loss: 1.8800e-03 | validation loss: 1.4621e-03\n",
      "Epoch: 280560 | training loss: 1.8776e-03 | validation loss: 1.4741e-03\n",
      "Epoch: 280570 | training loss: 1.8786e-03 | validation loss: 1.4758e-03\n",
      "Epoch: 280580 | training loss: 1.8806e-03 | validation loss: 1.4787e-03\n",
      "Epoch: 280590 | training loss: 1.9170e-03 | validation loss: 1.5118e-03\n",
      "Epoch: 280600 | training loss: 2.4079e-03 | validation loss: 1.8118e-03\n",
      "Epoch: 280610 | training loss: 1.9288e-03 | validation loss: 1.4666e-03\n",
      "Epoch: 280620 | training loss: 1.8981e-03 | validation loss: 1.4942e-03\n",
      "Epoch: 280630 | training loss: 1.8835e-03 | validation loss: 1.4618e-03\n",
      "Epoch: 280640 | training loss: 1.8762e-03 | validation loss: 1.4715e-03\n",
      "Epoch: 280650 | training loss: 1.8760e-03 | validation loss: 1.4705e-03\n",
      "Epoch: 280660 | training loss: 1.8777e-03 | validation loss: 1.4636e-03\n",
      "Epoch: 280670 | training loss: 1.8756e-03 | validation loss: 1.4694e-03\n",
      "Epoch: 280680 | training loss: 1.8762e-03 | validation loss: 1.4712e-03\n",
      "Epoch: 280690 | training loss: 1.8767e-03 | validation loss: 1.4725e-03\n",
      "Epoch: 280700 | training loss: 1.8859e-03 | validation loss: 1.4841e-03\n",
      "Epoch: 280710 | training loss: 2.1047e-03 | validation loss: 1.6315e-03\n",
      "Epoch: 280720 | training loss: 2.0936e-03 | validation loss: 1.6215e-03\n",
      "Epoch: 280730 | training loss: 1.8908e-03 | validation loss: 1.4665e-03\n",
      "Epoch: 280740 | training loss: 1.8804e-03 | validation loss: 1.4600e-03\n",
      "Epoch: 280750 | training loss: 1.8852e-03 | validation loss: 1.4835e-03\n",
      "Epoch: 280760 | training loss: 1.8807e-03 | validation loss: 1.4627e-03\n",
      "Epoch: 280770 | training loss: 1.8769e-03 | validation loss: 1.4727e-03\n",
      "Epoch: 280780 | training loss: 1.8753e-03 | validation loss: 1.4663e-03\n",
      "Epoch: 280790 | training loss: 1.8754e-03 | validation loss: 1.4669e-03\n",
      "Epoch: 280800 | training loss: 1.8755e-03 | validation loss: 1.4691e-03\n",
      "Epoch: 280810 | training loss: 1.8752e-03 | validation loss: 1.4683e-03\n",
      "Epoch: 280820 | training loss: 1.8752e-03 | validation loss: 1.4677e-03\n",
      "Epoch: 280830 | training loss: 1.8751e-03 | validation loss: 1.4676e-03\n",
      "Epoch: 280840 | training loss: 1.8752e-03 | validation loss: 1.4674e-03\n",
      "Epoch: 280850 | training loss: 1.8775e-03 | validation loss: 1.4667e-03\n",
      "Epoch: 280860 | training loss: 2.1076e-03 | validation loss: 1.5605e-03\n",
      "Epoch: 280870 | training loss: 2.3709e-03 | validation loss: 1.6704e-03\n",
      "Epoch: 280880 | training loss: 2.0084e-03 | validation loss: 1.4883e-03\n",
      "Epoch: 280890 | training loss: 1.8853e-03 | validation loss: 1.4872e-03\n",
      "Epoch: 280900 | training loss: 1.8969e-03 | validation loss: 1.4968e-03\n",
      "Epoch: 280910 | training loss: 1.8755e-03 | validation loss: 1.4684e-03\n",
      "Epoch: 280920 | training loss: 1.8780e-03 | validation loss: 1.4612e-03\n",
      "Epoch: 280930 | training loss: 1.8750e-03 | validation loss: 1.4671e-03\n",
      "Epoch: 280940 | training loss: 1.8753e-03 | validation loss: 1.4699e-03\n",
      "Epoch: 280950 | training loss: 1.8752e-03 | validation loss: 1.4666e-03\n",
      "Epoch: 280960 | training loss: 1.8750e-03 | validation loss: 1.4683e-03\n",
      "Epoch: 280970 | training loss: 1.8750e-03 | validation loss: 1.4675e-03\n",
      "Epoch: 280980 | training loss: 1.8749e-03 | validation loss: 1.4673e-03\n",
      "Epoch: 280990 | training loss: 1.8749e-03 | validation loss: 1.4670e-03\n",
      "Epoch: 281000 | training loss: 1.8749e-03 | validation loss: 1.4672e-03\n",
      "Epoch: 281010 | training loss: 1.8749e-03 | validation loss: 1.4671e-03\n",
      "Epoch: 281020 | training loss: 1.8749e-03 | validation loss: 1.4671e-03\n",
      "Epoch: 281030 | training loss: 1.8749e-03 | validation loss: 1.4671e-03\n",
      "Epoch: 281040 | training loss: 1.8748e-03 | validation loss: 1.4672e-03\n",
      "Epoch: 281050 | training loss: 1.8748e-03 | validation loss: 1.4673e-03\n",
      "Epoch: 281060 | training loss: 1.8749e-03 | validation loss: 1.4685e-03\n",
      "Epoch: 281070 | training loss: 1.8830e-03 | validation loss: 1.4826e-03\n",
      "Epoch: 281080 | training loss: 2.8107e-03 | validation loss: 2.0515e-03\n",
      "Epoch: 281090 | training loss: 2.6161e-03 | validation loss: 1.7236e-03\n",
      "Epoch: 281100 | training loss: 2.0010e-03 | validation loss: 1.5031e-03\n",
      "Epoch: 281110 | training loss: 1.9298e-03 | validation loss: 1.4912e-03\n",
      "Epoch: 281120 | training loss: 1.8795e-03 | validation loss: 1.4696e-03\n",
      "Epoch: 281130 | training loss: 1.8774e-03 | validation loss: 1.4712e-03\n",
      "Epoch: 281140 | training loss: 1.8781e-03 | validation loss: 1.4741e-03\n",
      "Epoch: 281150 | training loss: 1.8760e-03 | validation loss: 1.4727e-03\n",
      "Epoch: 281160 | training loss: 1.8747e-03 | validation loss: 1.4678e-03\n",
      "Epoch: 281170 | training loss: 1.8748e-03 | validation loss: 1.4651e-03\n",
      "Epoch: 281180 | training loss: 1.8747e-03 | validation loss: 1.4664e-03\n",
      "Epoch: 281190 | training loss: 1.8746e-03 | validation loss: 1.4672e-03\n",
      "Epoch: 281200 | training loss: 1.8746e-03 | validation loss: 1.4670e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 281210 | training loss: 1.8746e-03 | validation loss: 1.4665e-03\n",
      "Epoch: 281220 | training loss: 1.8746e-03 | validation loss: 1.4669e-03\n",
      "Epoch: 281230 | training loss: 1.8746e-03 | validation loss: 1.4667e-03\n",
      "Epoch: 281240 | training loss: 1.8746e-03 | validation loss: 1.4667e-03\n",
      "Epoch: 281250 | training loss: 1.8745e-03 | validation loss: 1.4667e-03\n",
      "Epoch: 281260 | training loss: 1.8745e-03 | validation loss: 1.4667e-03\n",
      "Epoch: 281270 | training loss: 1.8745e-03 | validation loss: 1.4667e-03\n",
      "Epoch: 281280 | training loss: 1.8745e-03 | validation loss: 1.4669e-03\n",
      "Epoch: 281290 | training loss: 1.8752e-03 | validation loss: 1.4686e-03\n",
      "Epoch: 281300 | training loss: 1.9618e-03 | validation loss: 1.5302e-03\n",
      "Epoch: 281310 | training loss: 3.0294e-03 | validation loss: 2.1515e-03\n",
      "Epoch: 281320 | training loss: 2.0271e-03 | validation loss: 1.5524e-03\n",
      "Epoch: 281330 | training loss: 2.0022e-03 | validation loss: 1.4935e-03\n",
      "Epoch: 281340 | training loss: 1.9190e-03 | validation loss: 1.4628e-03\n",
      "Epoch: 281350 | training loss: 1.8825e-03 | validation loss: 1.4692e-03\n",
      "Epoch: 281360 | training loss: 1.8815e-03 | validation loss: 1.4742e-03\n",
      "Epoch: 281370 | training loss: 1.8757e-03 | validation loss: 1.4635e-03\n",
      "Epoch: 281380 | training loss: 1.8753e-03 | validation loss: 1.4629e-03\n",
      "Epoch: 281390 | training loss: 1.8746e-03 | validation loss: 1.4675e-03\n",
      "Epoch: 281400 | training loss: 1.8743e-03 | validation loss: 1.4661e-03\n",
      "Epoch: 281410 | training loss: 1.8743e-03 | validation loss: 1.4664e-03\n",
      "Epoch: 281420 | training loss: 1.8743e-03 | validation loss: 1.4669e-03\n",
      "Epoch: 281430 | training loss: 1.8743e-03 | validation loss: 1.4663e-03\n",
      "Epoch: 281440 | training loss: 1.8743e-03 | validation loss: 1.4663e-03\n",
      "Epoch: 281450 | training loss: 1.8743e-03 | validation loss: 1.4664e-03\n",
      "Epoch: 281460 | training loss: 1.8743e-03 | validation loss: 1.4663e-03\n",
      "Epoch: 281470 | training loss: 1.8742e-03 | validation loss: 1.4663e-03\n",
      "Epoch: 281480 | training loss: 1.8742e-03 | validation loss: 1.4663e-03\n",
      "Epoch: 281490 | training loss: 1.8742e-03 | validation loss: 1.4663e-03\n",
      "Epoch: 281500 | training loss: 1.8742e-03 | validation loss: 1.4665e-03\n",
      "Epoch: 281510 | training loss: 1.8743e-03 | validation loss: 1.4676e-03\n",
      "Epoch: 281520 | training loss: 1.8861e-03 | validation loss: 1.4854e-03\n",
      "Epoch: 281530 | training loss: 3.3678e-03 | validation loss: 2.3465e-03\n",
      "Epoch: 281540 | training loss: 2.4845e-03 | validation loss: 1.6800e-03\n",
      "Epoch: 281550 | training loss: 2.0876e-03 | validation loss: 1.5112e-03\n",
      "Epoch: 281560 | training loss: 1.9599e-03 | validation loss: 1.4794e-03\n",
      "Epoch: 281570 | training loss: 1.9084e-03 | validation loss: 1.4662e-03\n",
      "Epoch: 281580 | training loss: 1.8818e-03 | validation loss: 1.4589e-03\n",
      "Epoch: 281590 | training loss: 1.8754e-03 | validation loss: 1.4619e-03\n",
      "Epoch: 281600 | training loss: 1.8742e-03 | validation loss: 1.4667e-03\n",
      "Epoch: 281610 | training loss: 1.8745e-03 | validation loss: 1.4692e-03\n",
      "Epoch: 281620 | training loss: 1.8742e-03 | validation loss: 1.4679e-03\n",
      "Epoch: 281630 | training loss: 1.8740e-03 | validation loss: 1.4659e-03\n",
      "Epoch: 281640 | training loss: 1.8740e-03 | validation loss: 1.4653e-03\n",
      "Epoch: 281650 | training loss: 1.8740e-03 | validation loss: 1.4659e-03\n",
      "Epoch: 281660 | training loss: 1.8740e-03 | validation loss: 1.4662e-03\n",
      "Epoch: 281670 | training loss: 1.8740e-03 | validation loss: 1.4658e-03\n",
      "Epoch: 281680 | training loss: 1.8739e-03 | validation loss: 1.4659e-03\n",
      "Epoch: 281690 | training loss: 1.8739e-03 | validation loss: 1.4659e-03\n",
      "Epoch: 281700 | training loss: 1.8739e-03 | validation loss: 1.4658e-03\n",
      "Epoch: 281710 | training loss: 1.8739e-03 | validation loss: 1.4659e-03\n",
      "Epoch: 281720 | training loss: 1.8739e-03 | validation loss: 1.4658e-03\n",
      "Epoch: 281730 | training loss: 1.8739e-03 | validation loss: 1.4658e-03\n",
      "Epoch: 281740 | training loss: 1.8739e-03 | validation loss: 1.4658e-03\n",
      "Epoch: 281750 | training loss: 1.8738e-03 | validation loss: 1.4657e-03\n",
      "Epoch: 281760 | training loss: 1.8748e-03 | validation loss: 1.4656e-03\n",
      "Epoch: 281770 | training loss: 2.1290e-03 | validation loss: 1.5991e-03\n",
      "Epoch: 281780 | training loss: 2.5863e-03 | validation loss: 1.8347e-03\n",
      "Epoch: 281790 | training loss: 2.2423e-03 | validation loss: 1.6048e-03\n",
      "Epoch: 281800 | training loss: 1.8886e-03 | validation loss: 1.4698e-03\n",
      "Epoch: 281810 | training loss: 1.9072e-03 | validation loss: 1.5100e-03\n",
      "Epoch: 281820 | training loss: 1.8848e-03 | validation loss: 1.4843e-03\n",
      "Epoch: 281830 | training loss: 1.8778e-03 | validation loss: 1.4636e-03\n",
      "Epoch: 281840 | training loss: 1.8738e-03 | validation loss: 1.4647e-03\n",
      "Epoch: 281850 | training loss: 1.8743e-03 | validation loss: 1.4672e-03\n",
      "Epoch: 281860 | training loss: 1.8740e-03 | validation loss: 1.4636e-03\n",
      "Epoch: 281870 | training loss: 1.8737e-03 | validation loss: 1.4653e-03\n",
      "Epoch: 281880 | training loss: 1.8737e-03 | validation loss: 1.4652e-03\n",
      "Epoch: 281890 | training loss: 1.8737e-03 | validation loss: 1.4652e-03\n",
      "Epoch: 281900 | training loss: 1.8736e-03 | validation loss: 1.4653e-03\n",
      "Epoch: 281910 | training loss: 1.8736e-03 | validation loss: 1.4654e-03\n",
      "Epoch: 281920 | training loss: 1.8736e-03 | validation loss: 1.4653e-03\n",
      "Epoch: 281930 | training loss: 1.8736e-03 | validation loss: 1.4654e-03\n",
      "Epoch: 281940 | training loss: 1.8736e-03 | validation loss: 1.4654e-03\n",
      "Epoch: 281950 | training loss: 1.8736e-03 | validation loss: 1.4654e-03\n",
      "Epoch: 281960 | training loss: 1.8736e-03 | validation loss: 1.4656e-03\n",
      "Epoch: 281970 | training loss: 1.8737e-03 | validation loss: 1.4669e-03\n",
      "Epoch: 281980 | training loss: 1.8854e-03 | validation loss: 1.4840e-03\n",
      "Epoch: 281990 | training loss: 3.1172e-03 | validation loss: 2.2030e-03\n",
      "Epoch: 282000 | training loss: 2.6379e-03 | validation loss: 1.7436e-03\n",
      "Epoch: 282010 | training loss: 2.0064e-03 | validation loss: 1.4878e-03\n",
      "Epoch: 282020 | training loss: 1.8740e-03 | validation loss: 1.4616e-03\n",
      "Epoch: 282030 | training loss: 1.8903e-03 | validation loss: 1.4851e-03\n",
      "Epoch: 282040 | training loss: 1.8852e-03 | validation loss: 1.4809e-03\n",
      "Epoch: 282050 | training loss: 1.8739e-03 | validation loss: 1.4662e-03\n",
      "Epoch: 282060 | training loss: 1.8746e-03 | validation loss: 1.4614e-03\n",
      "Epoch: 282070 | training loss: 1.8736e-03 | validation loss: 1.4636e-03\n",
      "Epoch: 282080 | training loss: 1.8736e-03 | validation loss: 1.4668e-03\n",
      "Epoch: 282090 | training loss: 1.8734e-03 | validation loss: 1.4655e-03\n",
      "Epoch: 282100 | training loss: 1.8734e-03 | validation loss: 1.4648e-03\n",
      "Epoch: 282110 | training loss: 1.8734e-03 | validation loss: 1.4655e-03\n",
      "Epoch: 282120 | training loss: 1.8733e-03 | validation loss: 1.4650e-03\n",
      "Epoch: 282130 | training loss: 1.8733e-03 | validation loss: 1.4653e-03\n",
      "Epoch: 282140 | training loss: 1.8733e-03 | validation loss: 1.4651e-03\n",
      "Epoch: 282150 | training loss: 1.8733e-03 | validation loss: 1.4652e-03\n",
      "Epoch: 282160 | training loss: 1.8733e-03 | validation loss: 1.4651e-03\n",
      "Epoch: 282170 | training loss: 1.8733e-03 | validation loss: 1.4651e-03\n",
      "Epoch: 282180 | training loss: 1.8732e-03 | validation loss: 1.4651e-03\n",
      "Epoch: 282190 | training loss: 1.8732e-03 | validation loss: 1.4651e-03\n",
      "Epoch: 282200 | training loss: 1.8732e-03 | validation loss: 1.4651e-03\n",
      "Epoch: 282210 | training loss: 1.8732e-03 | validation loss: 1.4653e-03\n",
      "Epoch: 282220 | training loss: 1.8736e-03 | validation loss: 1.4674e-03\n",
      "Epoch: 282230 | training loss: 1.9301e-03 | validation loss: 1.5179e-03\n",
      "Epoch: 282240 | training loss: 3.8762e-03 | validation loss: 2.5906e-03\n",
      "Epoch: 282250 | training loss: 2.1459e-03 | validation loss: 1.6605e-03\n",
      "Epoch: 282260 | training loss: 1.8923e-03 | validation loss: 1.4925e-03\n",
      "Epoch: 282270 | training loss: 1.8732e-03 | validation loss: 1.4630e-03\n",
      "Epoch: 282280 | training loss: 1.8781e-03 | validation loss: 1.4585e-03\n",
      "Epoch: 282290 | training loss: 1.8798e-03 | validation loss: 1.4591e-03\n",
      "Epoch: 282300 | training loss: 1.8767e-03 | validation loss: 1.4600e-03\n",
      "Epoch: 282310 | training loss: 1.8736e-03 | validation loss: 1.4625e-03\n",
      "Epoch: 282320 | training loss: 1.8731e-03 | validation loss: 1.4662e-03\n",
      "Epoch: 282330 | training loss: 1.8732e-03 | validation loss: 1.4665e-03\n",
      "Epoch: 282340 | training loss: 1.8730e-03 | validation loss: 1.4650e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 282350 | training loss: 1.8730e-03 | validation loss: 1.4643e-03\n",
      "Epoch: 282360 | training loss: 1.8730e-03 | validation loss: 1.4650e-03\n",
      "Epoch: 282370 | training loss: 1.8730e-03 | validation loss: 1.4650e-03\n",
      "Epoch: 282380 | training loss: 1.8730e-03 | validation loss: 1.4647e-03\n",
      "Epoch: 282390 | training loss: 1.8729e-03 | validation loss: 1.4648e-03\n",
      "Epoch: 282400 | training loss: 1.8729e-03 | validation loss: 1.4647e-03\n",
      "Epoch: 282410 | training loss: 1.8729e-03 | validation loss: 1.4647e-03\n",
      "Epoch: 282420 | training loss: 1.8729e-03 | validation loss: 1.4642e-03\n",
      "Epoch: 282430 | training loss: 1.8756e-03 | validation loss: 1.4617e-03\n",
      "Epoch: 282440 | training loss: 2.2548e-03 | validation loss: 1.6743e-03\n",
      "Epoch: 282450 | training loss: 2.1519e-03 | validation loss: 1.6875e-03\n",
      "Epoch: 282460 | training loss: 1.9452e-03 | validation loss: 1.5381e-03\n",
      "Epoch: 282470 | training loss: 1.8821e-03 | validation loss: 1.4708e-03\n",
      "Epoch: 282480 | training loss: 1.8730e-03 | validation loss: 1.4643e-03\n",
      "Epoch: 282490 | training loss: 1.8753e-03 | validation loss: 1.4652e-03\n",
      "Epoch: 282500 | training loss: 1.8745e-03 | validation loss: 1.4643e-03\n",
      "Epoch: 282510 | training loss: 1.8732e-03 | validation loss: 1.4659e-03\n",
      "Epoch: 282520 | training loss: 1.8769e-03 | validation loss: 1.4750e-03\n",
      "Epoch: 282530 | training loss: 2.0237e-03 | validation loss: 1.5874e-03\n",
      "Epoch: 282540 | training loss: 2.3467e-03 | validation loss: 1.7750e-03\n",
      "Epoch: 282550 | training loss: 1.8743e-03 | validation loss: 1.4669e-03\n",
      "Epoch: 282560 | training loss: 1.9481e-03 | validation loss: 1.4717e-03\n",
      "Epoch: 282570 | training loss: 1.8967e-03 | validation loss: 1.4937e-03\n",
      "Epoch: 282580 | training loss: 1.8728e-03 | validation loss: 1.4626e-03\n",
      "Epoch: 282590 | training loss: 1.8740e-03 | validation loss: 1.4603e-03\n",
      "Epoch: 282600 | training loss: 1.8739e-03 | validation loss: 1.4686e-03\n",
      "Epoch: 282610 | training loss: 1.8732e-03 | validation loss: 1.4621e-03\n",
      "Epoch: 282620 | training loss: 1.8728e-03 | validation loss: 1.4657e-03\n",
      "Epoch: 282630 | training loss: 1.8726e-03 | validation loss: 1.4638e-03\n",
      "Epoch: 282640 | training loss: 1.8726e-03 | validation loss: 1.4637e-03\n",
      "Epoch: 282650 | training loss: 1.8726e-03 | validation loss: 1.4646e-03\n",
      "Epoch: 282660 | training loss: 1.8726e-03 | validation loss: 1.4645e-03\n",
      "Epoch: 282670 | training loss: 1.8726e-03 | validation loss: 1.4646e-03\n",
      "Epoch: 282680 | training loss: 1.8727e-03 | validation loss: 1.4656e-03\n",
      "Epoch: 282690 | training loss: 1.8776e-03 | validation loss: 1.4745e-03\n",
      "Epoch: 282700 | training loss: 2.1874e-03 | validation loss: 1.6800e-03\n",
      "Epoch: 282710 | training loss: 1.8861e-03 | validation loss: 1.4705e-03\n",
      "Epoch: 282720 | training loss: 2.1480e-03 | validation loss: 1.6590e-03\n",
      "Epoch: 282730 | training loss: 1.8833e-03 | validation loss: 1.4797e-03\n",
      "Epoch: 282740 | training loss: 1.9044e-03 | validation loss: 1.4560e-03\n",
      "Epoch: 282750 | training loss: 1.8747e-03 | validation loss: 1.4599e-03\n",
      "Epoch: 282760 | training loss: 1.8774e-03 | validation loss: 1.4773e-03\n",
      "Epoch: 282770 | training loss: 1.8726e-03 | validation loss: 1.4619e-03\n",
      "Epoch: 282780 | training loss: 1.8726e-03 | validation loss: 1.4622e-03\n",
      "Epoch: 282790 | training loss: 1.8726e-03 | validation loss: 1.4661e-03\n",
      "Epoch: 282800 | training loss: 1.8725e-03 | validation loss: 1.4629e-03\n",
      "Epoch: 282810 | training loss: 1.8724e-03 | validation loss: 1.4646e-03\n",
      "Epoch: 282820 | training loss: 1.8724e-03 | validation loss: 1.4635e-03\n",
      "Epoch: 282830 | training loss: 1.8723e-03 | validation loss: 1.4642e-03\n",
      "Epoch: 282840 | training loss: 1.8723e-03 | validation loss: 1.4638e-03\n",
      "Epoch: 282850 | training loss: 1.8723e-03 | validation loss: 1.4638e-03\n",
      "Epoch: 282860 | training loss: 1.8723e-03 | validation loss: 1.4640e-03\n",
      "Epoch: 282870 | training loss: 1.8730e-03 | validation loss: 1.4657e-03\n",
      "Epoch: 282880 | training loss: 1.9378e-03 | validation loss: 1.5215e-03\n",
      "Epoch: 282890 | training loss: 1.9962e-03 | validation loss: 1.5068e-03\n",
      "Epoch: 282900 | training loss: 2.1784e-03 | validation loss: 1.6090e-03\n",
      "Epoch: 282910 | training loss: 1.9156e-03 | validation loss: 1.5178e-03\n",
      "Epoch: 282920 | training loss: 1.8874e-03 | validation loss: 1.4882e-03\n",
      "Epoch: 282930 | training loss: 1.8771e-03 | validation loss: 1.4565e-03\n",
      "Epoch: 282940 | training loss: 1.8855e-03 | validation loss: 1.4546e-03\n",
      "Epoch: 282950 | training loss: 1.9038e-03 | validation loss: 1.4561e-03\n",
      "Epoch: 282960 | training loss: 2.0697e-03 | validation loss: 1.5061e-03\n",
      "Epoch: 282970 | training loss: 1.9934e-03 | validation loss: 1.4820e-03\n",
      "Epoch: 282980 | training loss: 1.9536e-03 | validation loss: 1.5374e-03\n",
      "Epoch: 282990 | training loss: 1.8805e-03 | validation loss: 1.4561e-03\n",
      "Epoch: 283000 | training loss: 1.8797e-03 | validation loss: 1.4565e-03\n",
      "Epoch: 283010 | training loss: 1.8744e-03 | validation loss: 1.4704e-03\n",
      "Epoch: 283020 | training loss: 1.8800e-03 | validation loss: 1.4779e-03\n",
      "Epoch: 283030 | training loss: 1.9031e-03 | validation loss: 1.4993e-03\n",
      "Epoch: 283040 | training loss: 2.1353e-03 | validation loss: 1.6520e-03\n",
      "Epoch: 283050 | training loss: 1.9399e-03 | validation loss: 1.5242e-03\n",
      "Epoch: 283060 | training loss: 1.9453e-03 | validation loss: 1.4692e-03\n",
      "Epoch: 283070 | training loss: 1.9080e-03 | validation loss: 1.5018e-03\n",
      "Epoch: 283080 | training loss: 1.8756e-03 | validation loss: 1.4581e-03\n",
      "Epoch: 283090 | training loss: 1.8759e-03 | validation loss: 1.4585e-03\n",
      "Epoch: 283100 | training loss: 1.8726e-03 | validation loss: 1.4663e-03\n",
      "Epoch: 283110 | training loss: 1.8756e-03 | validation loss: 1.4718e-03\n",
      "Epoch: 283120 | training loss: 1.8936e-03 | validation loss: 1.4901e-03\n",
      "Epoch: 283130 | training loss: 2.1632e-03 | validation loss: 1.6644e-03\n",
      "Epoch: 283140 | training loss: 1.9456e-03 | validation loss: 1.5253e-03\n",
      "Epoch: 283150 | training loss: 1.9072e-03 | validation loss: 1.4643e-03\n",
      "Epoch: 283160 | training loss: 1.8883e-03 | validation loss: 1.4814e-03\n",
      "Epoch: 283170 | training loss: 1.8833e-03 | validation loss: 1.4598e-03\n",
      "Epoch: 283180 | training loss: 1.8794e-03 | validation loss: 1.4746e-03\n",
      "Epoch: 283190 | training loss: 1.8732e-03 | validation loss: 1.4608e-03\n",
      "Epoch: 283200 | training loss: 1.8726e-03 | validation loss: 1.4608e-03\n",
      "Epoch: 283210 | training loss: 1.8719e-03 | validation loss: 1.4639e-03\n",
      "Epoch: 283220 | training loss: 1.8725e-03 | validation loss: 1.4664e-03\n",
      "Epoch: 283230 | training loss: 1.8790e-03 | validation loss: 1.4752e-03\n",
      "Epoch: 283240 | training loss: 2.0457e-03 | validation loss: 1.5898e-03\n",
      "Epoch: 283250 | training loss: 2.3133e-03 | validation loss: 1.7472e-03\n",
      "Epoch: 283260 | training loss: 1.9101e-03 | validation loss: 1.4602e-03\n",
      "Epoch: 283270 | training loss: 1.8792e-03 | validation loss: 1.4599e-03\n",
      "Epoch: 283280 | training loss: 1.8914e-03 | validation loss: 1.4846e-03\n",
      "Epoch: 283290 | training loss: 1.8826e-03 | validation loss: 1.4594e-03\n",
      "Epoch: 283300 | training loss: 1.8760e-03 | validation loss: 1.4715e-03\n",
      "Epoch: 283310 | training loss: 1.8733e-03 | validation loss: 1.4601e-03\n",
      "Epoch: 283320 | training loss: 1.8720e-03 | validation loss: 1.4649e-03\n",
      "Epoch: 283330 | training loss: 1.8716e-03 | validation loss: 1.4626e-03\n",
      "Epoch: 283340 | training loss: 1.8721e-03 | validation loss: 1.4601e-03\n",
      "Epoch: 283350 | training loss: 1.8869e-03 | validation loss: 1.4589e-03\n",
      "Epoch: 283360 | training loss: 2.3829e-03 | validation loss: 1.7246e-03\n",
      "Epoch: 283370 | training loss: 1.9752e-03 | validation loss: 1.5329e-03\n",
      "Epoch: 283380 | training loss: 1.9257e-03 | validation loss: 1.5286e-03\n",
      "Epoch: 283390 | training loss: 1.8766e-03 | validation loss: 1.4621e-03\n",
      "Epoch: 283400 | training loss: 1.8821e-03 | validation loss: 1.4550e-03\n",
      "Epoch: 283410 | training loss: 1.8817e-03 | validation loss: 1.4577e-03\n",
      "Epoch: 283420 | training loss: 1.9229e-03 | validation loss: 1.4619e-03\n",
      "Epoch: 283430 | training loss: 2.2622e-03 | validation loss: 1.5777e-03\n",
      "Epoch: 283440 | training loss: 1.8761e-03 | validation loss: 1.4734e-03\n",
      "Epoch: 283450 | training loss: 1.8804e-03 | validation loss: 1.4785e-03\n",
      "Epoch: 283460 | training loss: 1.8923e-03 | validation loss: 1.4555e-03\n",
      "Epoch: 283470 | training loss: 1.8818e-03 | validation loss: 1.4790e-03\n",
      "Epoch: 283480 | training loss: 1.8718e-03 | validation loss: 1.4653e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 283490 | training loss: 1.8736e-03 | validation loss: 1.4581e-03\n",
      "Epoch: 283500 | training loss: 1.8770e-03 | validation loss: 1.4569e-03\n",
      "Epoch: 283510 | training loss: 1.9073e-03 | validation loss: 1.4586e-03\n",
      "Epoch: 283520 | training loss: 2.2927e-03 | validation loss: 1.5952e-03\n",
      "Epoch: 283530 | training loss: 1.8738e-03 | validation loss: 1.4711e-03\n",
      "Epoch: 283540 | training loss: 1.8732e-03 | validation loss: 1.4628e-03\n",
      "Epoch: 283550 | training loss: 1.8751e-03 | validation loss: 1.4602e-03\n",
      "Epoch: 283560 | training loss: 1.8785e-03 | validation loss: 1.4734e-03\n",
      "Epoch: 283570 | training loss: 1.8776e-03 | validation loss: 1.4584e-03\n",
      "Epoch: 283580 | training loss: 1.8716e-03 | validation loss: 1.4639e-03\n",
      "Epoch: 283590 | training loss: 1.8728e-03 | validation loss: 1.4676e-03\n",
      "Epoch: 283600 | training loss: 1.8721e-03 | validation loss: 1.4659e-03\n",
      "Epoch: 283610 | training loss: 1.8738e-03 | validation loss: 1.4686e-03\n",
      "Epoch: 283620 | training loss: 1.9115e-03 | validation loss: 1.5022e-03\n",
      "Epoch: 283630 | training loss: 2.6782e-03 | validation loss: 1.9478e-03\n",
      "Epoch: 283640 | training loss: 2.1364e-03 | validation loss: 1.5404e-03\n",
      "Epoch: 283650 | training loss: 1.9972e-03 | validation loss: 1.5587e-03\n",
      "Epoch: 283660 | training loss: 1.8959e-03 | validation loss: 1.4583e-03\n",
      "Epoch: 283670 | training loss: 1.8730e-03 | validation loss: 1.4676e-03\n",
      "Epoch: 283680 | training loss: 1.8712e-03 | validation loss: 1.4617e-03\n",
      "Epoch: 283690 | training loss: 1.8712e-03 | validation loss: 1.4632e-03\n",
      "Epoch: 283700 | training loss: 1.8714e-03 | validation loss: 1.4608e-03\n",
      "Epoch: 283710 | training loss: 1.8715e-03 | validation loss: 1.4645e-03\n",
      "Epoch: 283720 | training loss: 1.8711e-03 | validation loss: 1.4614e-03\n",
      "Epoch: 283730 | training loss: 1.8713e-03 | validation loss: 1.4602e-03\n",
      "Epoch: 283740 | training loss: 1.8784e-03 | validation loss: 1.4573e-03\n",
      "Epoch: 283750 | training loss: 2.3707e-03 | validation loss: 1.7282e-03\n",
      "Epoch: 283760 | training loss: 2.1174e-03 | validation loss: 1.6125e-03\n",
      "Epoch: 283770 | training loss: 1.9284e-03 | validation loss: 1.5280e-03\n",
      "Epoch: 283780 | training loss: 1.8802e-03 | validation loss: 1.4774e-03\n",
      "Epoch: 283790 | training loss: 1.8778e-03 | validation loss: 1.4598e-03\n",
      "Epoch: 283800 | training loss: 1.8758e-03 | validation loss: 1.4554e-03\n",
      "Epoch: 283810 | training loss: 1.8924e-03 | validation loss: 1.4555e-03\n",
      "Epoch: 283820 | training loss: 2.2262e-03 | validation loss: 1.5648e-03\n",
      "Epoch: 283830 | training loss: 1.8790e-03 | validation loss: 1.4583e-03\n",
      "Epoch: 283840 | training loss: 1.8710e-03 | validation loss: 1.4618e-03\n",
      "Epoch: 283850 | training loss: 1.8717e-03 | validation loss: 1.4638e-03\n",
      "Epoch: 283860 | training loss: 1.8709e-03 | validation loss: 1.4604e-03\n",
      "Epoch: 283870 | training loss: 1.8714e-03 | validation loss: 1.4599e-03\n",
      "Epoch: 283880 | training loss: 1.8728e-03 | validation loss: 1.4679e-03\n",
      "Epoch: 283890 | training loss: 1.8718e-03 | validation loss: 1.4586e-03\n",
      "Epoch: 283900 | training loss: 1.8710e-03 | validation loss: 1.4603e-03\n",
      "Epoch: 283910 | training loss: 1.8708e-03 | validation loss: 1.4624e-03\n",
      "Epoch: 283920 | training loss: 1.8711e-03 | validation loss: 1.4638e-03\n",
      "Epoch: 283930 | training loss: 1.8751e-03 | validation loss: 1.4713e-03\n",
      "Epoch: 283940 | training loss: 2.0223e-03 | validation loss: 1.5792e-03\n",
      "Epoch: 283950 | training loss: 2.4432e-03 | validation loss: 1.8202e-03\n",
      "Epoch: 283960 | training loss: 1.8800e-03 | validation loss: 1.4694e-03\n",
      "Epoch: 283970 | training loss: 1.9410e-03 | validation loss: 1.4655e-03\n",
      "Epoch: 283980 | training loss: 1.8985e-03 | validation loss: 1.4877e-03\n",
      "Epoch: 283990 | training loss: 1.8723e-03 | validation loss: 1.4646e-03\n",
      "Epoch: 284000 | training loss: 1.8717e-03 | validation loss: 1.4580e-03\n",
      "Epoch: 284010 | training loss: 1.8716e-03 | validation loss: 1.4650e-03\n",
      "Epoch: 284020 | training loss: 1.8711e-03 | validation loss: 1.4604e-03\n",
      "Epoch: 284030 | training loss: 1.8707e-03 | validation loss: 1.4625e-03\n",
      "Epoch: 284040 | training loss: 1.8706e-03 | validation loss: 1.4615e-03\n",
      "Epoch: 284050 | training loss: 1.8706e-03 | validation loss: 1.4614e-03\n",
      "Epoch: 284060 | training loss: 1.8706e-03 | validation loss: 1.4618e-03\n",
      "Epoch: 284070 | training loss: 1.8706e-03 | validation loss: 1.4619e-03\n",
      "Epoch: 284080 | training loss: 1.8706e-03 | validation loss: 1.4620e-03\n",
      "Epoch: 284090 | training loss: 1.8707e-03 | validation loss: 1.4628e-03\n",
      "Epoch: 284100 | training loss: 1.8759e-03 | validation loss: 1.4703e-03\n",
      "Epoch: 284110 | training loss: 2.1998e-03 | validation loss: 1.6732e-03\n",
      "Epoch: 284120 | training loss: 1.9525e-03 | validation loss: 1.5566e-03\n",
      "Epoch: 284130 | training loss: 2.0968e-03 | validation loss: 1.6017e-03\n",
      "Epoch: 284140 | training loss: 1.9299e-03 | validation loss: 1.4782e-03\n",
      "Epoch: 284150 | training loss: 1.8729e-03 | validation loss: 1.4695e-03\n",
      "Epoch: 284160 | training loss: 1.8794e-03 | validation loss: 1.4705e-03\n",
      "Epoch: 284170 | training loss: 1.8731e-03 | validation loss: 1.4622e-03\n",
      "Epoch: 284180 | training loss: 1.8708e-03 | validation loss: 1.4629e-03\n",
      "Epoch: 284190 | training loss: 1.8709e-03 | validation loss: 1.4604e-03\n",
      "Epoch: 284200 | training loss: 1.8704e-03 | validation loss: 1.4619e-03\n",
      "Epoch: 284210 | training loss: 1.8704e-03 | validation loss: 1.4620e-03\n",
      "Epoch: 284220 | training loss: 1.8704e-03 | validation loss: 1.4609e-03\n",
      "Epoch: 284230 | training loss: 1.8703e-03 | validation loss: 1.4612e-03\n",
      "Epoch: 284240 | training loss: 1.8703e-03 | validation loss: 1.4612e-03\n",
      "Epoch: 284250 | training loss: 1.8703e-03 | validation loss: 1.4611e-03\n",
      "Epoch: 284260 | training loss: 1.8704e-03 | validation loss: 1.4601e-03\n",
      "Epoch: 284270 | training loss: 1.8743e-03 | validation loss: 1.4544e-03\n",
      "Epoch: 284280 | training loss: 2.2853e-03 | validation loss: 1.5953e-03\n",
      "Epoch: 284290 | training loss: 2.1503e-03 | validation loss: 1.7142e-03\n",
      "Epoch: 284300 | training loss: 1.9339e-03 | validation loss: 1.4740e-03\n",
      "Epoch: 284310 | training loss: 1.8959e-03 | validation loss: 1.4675e-03\n",
      "Epoch: 284320 | training loss: 1.8897e-03 | validation loss: 1.4610e-03\n",
      "Epoch: 284330 | training loss: 1.8815e-03 | validation loss: 1.4524e-03\n",
      "Epoch: 284340 | training loss: 1.8729e-03 | validation loss: 1.4591e-03\n",
      "Epoch: 284350 | training loss: 1.8702e-03 | validation loss: 1.4597e-03\n",
      "Epoch: 284360 | training loss: 1.8704e-03 | validation loss: 1.4631e-03\n",
      "Epoch: 284370 | training loss: 1.8703e-03 | validation loss: 1.4619e-03\n",
      "Epoch: 284380 | training loss: 1.8701e-03 | validation loss: 1.4608e-03\n",
      "Epoch: 284390 | training loss: 1.8701e-03 | validation loss: 1.4601e-03\n",
      "Epoch: 284400 | training loss: 1.8701e-03 | validation loss: 1.4610e-03\n",
      "Epoch: 284410 | training loss: 1.8701e-03 | validation loss: 1.4610e-03\n",
      "Epoch: 284420 | training loss: 1.8701e-03 | validation loss: 1.4608e-03\n",
      "Epoch: 284430 | training loss: 1.8701e-03 | validation loss: 1.4612e-03\n",
      "Epoch: 284440 | training loss: 1.8705e-03 | validation loss: 1.4627e-03\n",
      "Epoch: 284450 | training loss: 1.8946e-03 | validation loss: 1.4864e-03\n",
      "Epoch: 284460 | training loss: 3.3148e-03 | validation loss: 2.2845e-03\n",
      "Epoch: 284470 | training loss: 2.3695e-03 | validation loss: 1.6239e-03\n",
      "Epoch: 284480 | training loss: 1.9396e-03 | validation loss: 1.4731e-03\n",
      "Epoch: 284490 | training loss: 1.8983e-03 | validation loss: 1.4985e-03\n",
      "Epoch: 284500 | training loss: 1.8926e-03 | validation loss: 1.4912e-03\n",
      "Epoch: 284510 | training loss: 1.8706e-03 | validation loss: 1.4579e-03\n",
      "Epoch: 284520 | training loss: 1.8731e-03 | validation loss: 1.4550e-03\n",
      "Epoch: 284530 | training loss: 1.8704e-03 | validation loss: 1.4630e-03\n",
      "Epoch: 284540 | training loss: 1.8700e-03 | validation loss: 1.4623e-03\n",
      "Epoch: 284550 | training loss: 1.8700e-03 | validation loss: 1.4596e-03\n",
      "Epoch: 284560 | training loss: 1.8699e-03 | validation loss: 1.4613e-03\n",
      "Epoch: 284570 | training loss: 1.8699e-03 | validation loss: 1.4603e-03\n",
      "Epoch: 284580 | training loss: 1.8698e-03 | validation loss: 1.4609e-03\n",
      "Epoch: 284590 | training loss: 1.8698e-03 | validation loss: 1.4605e-03\n",
      "Epoch: 284600 | training loss: 1.8698e-03 | validation loss: 1.4606e-03\n",
      "Epoch: 284610 | training loss: 1.8698e-03 | validation loss: 1.4606e-03\n",
      "Epoch: 284620 | training loss: 1.8698e-03 | validation loss: 1.4605e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 284630 | training loss: 1.8698e-03 | validation loss: 1.4605e-03\n",
      "Epoch: 284640 | training loss: 1.8698e-03 | validation loss: 1.4604e-03\n",
      "Epoch: 284650 | training loss: 1.8698e-03 | validation loss: 1.4598e-03\n",
      "Epoch: 284660 | training loss: 1.8719e-03 | validation loss: 1.4567e-03\n",
      "Epoch: 284670 | training loss: 2.0908e-03 | validation loss: 1.5197e-03\n",
      "Epoch: 284680 | training loss: 1.9502e-03 | validation loss: 1.4760e-03\n",
      "Epoch: 284690 | training loss: 2.2816e-03 | validation loss: 1.5904e-03\n",
      "Epoch: 284700 | training loss: 1.9594e-03 | validation loss: 1.4677e-03\n",
      "Epoch: 284710 | training loss: 1.8704e-03 | validation loss: 1.4579e-03\n",
      "Epoch: 284720 | training loss: 1.8789e-03 | validation loss: 1.4759e-03\n",
      "Epoch: 284730 | training loss: 1.8760e-03 | validation loss: 1.4726e-03\n",
      "Epoch: 284740 | training loss: 1.8696e-03 | validation loss: 1.4609e-03\n",
      "Epoch: 284750 | training loss: 1.8705e-03 | validation loss: 1.4577e-03\n",
      "Epoch: 284760 | training loss: 1.8696e-03 | validation loss: 1.4602e-03\n",
      "Epoch: 284770 | training loss: 1.8697e-03 | validation loss: 1.4618e-03\n",
      "Epoch: 284780 | training loss: 1.8696e-03 | validation loss: 1.4600e-03\n",
      "Epoch: 284790 | training loss: 1.8695e-03 | validation loss: 1.4602e-03\n",
      "Epoch: 284800 | training loss: 1.8695e-03 | validation loss: 1.4605e-03\n",
      "Epoch: 284810 | training loss: 1.8695e-03 | validation loss: 1.4600e-03\n",
      "Epoch: 284820 | training loss: 1.8696e-03 | validation loss: 1.4597e-03\n",
      "Epoch: 284830 | training loss: 1.8737e-03 | validation loss: 1.4567e-03\n",
      "Epoch: 284840 | training loss: 2.2706e-03 | validation loss: 1.6795e-03\n",
      "Epoch: 284850 | training loss: 2.0890e-03 | validation loss: 1.6451e-03\n",
      "Epoch: 284860 | training loss: 1.9019e-03 | validation loss: 1.4966e-03\n",
      "Epoch: 284870 | training loss: 1.8718e-03 | validation loss: 1.4546e-03\n",
      "Epoch: 284880 | training loss: 1.8790e-03 | validation loss: 1.4632e-03\n",
      "Epoch: 284890 | training loss: 1.8714e-03 | validation loss: 1.4579e-03\n",
      "Epoch: 284900 | training loss: 1.8699e-03 | validation loss: 1.4593e-03\n",
      "Epoch: 284910 | training loss: 1.8702e-03 | validation loss: 1.4592e-03\n",
      "Epoch: 284920 | training loss: 1.8717e-03 | validation loss: 1.4547e-03\n",
      "Epoch: 284930 | training loss: 1.9185e-03 | validation loss: 1.4563e-03\n",
      "Epoch: 284940 | training loss: 2.7690e-03 | validation loss: 1.7851e-03\n",
      "Epoch: 284950 | training loss: 2.1848e-03 | validation loss: 1.6768e-03\n",
      "Epoch: 284960 | training loss: 1.9403e-03 | validation loss: 1.4696e-03\n",
      "Epoch: 284970 | training loss: 1.8706e-03 | validation loss: 1.4642e-03\n",
      "Epoch: 284980 | training loss: 1.8714e-03 | validation loss: 1.4640e-03\n",
      "Epoch: 284990 | training loss: 1.8714e-03 | validation loss: 1.4560e-03\n",
      "Epoch: 285000 | training loss: 1.8701e-03 | validation loss: 1.4638e-03\n",
      "Epoch: 285010 | training loss: 1.8693e-03 | validation loss: 1.4584e-03\n",
      "Epoch: 285020 | training loss: 1.8693e-03 | validation loss: 1.4592e-03\n",
      "Epoch: 285030 | training loss: 1.8694e-03 | validation loss: 1.4612e-03\n",
      "Epoch: 285040 | training loss: 1.8692e-03 | validation loss: 1.4597e-03\n",
      "Epoch: 285050 | training loss: 1.8692e-03 | validation loss: 1.4590e-03\n",
      "Epoch: 285060 | training loss: 1.8693e-03 | validation loss: 1.4583e-03\n",
      "Epoch: 285070 | training loss: 1.8716e-03 | validation loss: 1.4554e-03\n",
      "Epoch: 285080 | training loss: 1.9628e-03 | validation loss: 1.4720e-03\n",
      "Epoch: 285090 | training loss: 2.9183e-03 | validation loss: 1.8652e-03\n",
      "Epoch: 285100 | training loss: 1.9401e-03 | validation loss: 1.5149e-03\n",
      "Epoch: 285110 | training loss: 1.9434e-03 | validation loss: 1.5333e-03\n",
      "Epoch: 285120 | training loss: 1.8959e-03 | validation loss: 1.4525e-03\n",
      "Epoch: 285130 | training loss: 1.8706e-03 | validation loss: 1.4548e-03\n",
      "Epoch: 285140 | training loss: 1.8745e-03 | validation loss: 1.4728e-03\n",
      "Epoch: 285150 | training loss: 1.8709e-03 | validation loss: 1.4546e-03\n",
      "Epoch: 285160 | training loss: 1.8693e-03 | validation loss: 1.4621e-03\n",
      "Epoch: 285170 | training loss: 1.8691e-03 | validation loss: 1.4589e-03\n",
      "Epoch: 285180 | training loss: 1.8690e-03 | validation loss: 1.4602e-03\n",
      "Epoch: 285190 | training loss: 1.8690e-03 | validation loss: 1.4586e-03\n",
      "Epoch: 285200 | training loss: 1.8690e-03 | validation loss: 1.4599e-03\n",
      "Epoch: 285210 | training loss: 1.8690e-03 | validation loss: 1.4591e-03\n",
      "Epoch: 285220 | training loss: 1.8700e-03 | validation loss: 1.4576e-03\n",
      "Epoch: 285230 | training loss: 1.9306e-03 | validation loss: 1.4839e-03\n",
      "Epoch: 285240 | training loss: 1.9548e-03 | validation loss: 1.5042e-03\n",
      "Epoch: 285250 | training loss: 1.9309e-03 | validation loss: 1.4944e-03\n",
      "Epoch: 285260 | training loss: 1.9040e-03 | validation loss: 1.5061e-03\n",
      "Epoch: 285270 | training loss: 1.9881e-03 | validation loss: 1.5688e-03\n",
      "Epoch: 285280 | training loss: 2.1445e-03 | validation loss: 1.6540e-03\n",
      "Epoch: 285290 | training loss: 1.9089e-03 | validation loss: 1.4525e-03\n",
      "Epoch: 285300 | training loss: 1.8738e-03 | validation loss: 1.4532e-03\n",
      "Epoch: 285310 | training loss: 1.8876e-03 | validation loss: 1.4858e-03\n",
      "Epoch: 285320 | training loss: 1.8702e-03 | validation loss: 1.4638e-03\n",
      "Epoch: 285330 | training loss: 1.8698e-03 | validation loss: 1.4556e-03\n",
      "Epoch: 285340 | training loss: 1.8773e-03 | validation loss: 1.4526e-03\n",
      "Epoch: 285350 | training loss: 1.9915e-03 | validation loss: 1.4776e-03\n",
      "Epoch: 285360 | training loss: 2.4010e-03 | validation loss: 1.6362e-03\n",
      "Epoch: 285370 | training loss: 2.0505e-03 | validation loss: 1.5915e-03\n",
      "Epoch: 285380 | training loss: 1.9310e-03 | validation loss: 1.4653e-03\n",
      "Epoch: 285390 | training loss: 1.8923e-03 | validation loss: 1.4856e-03\n",
      "Epoch: 285400 | training loss: 1.8789e-03 | validation loss: 1.4532e-03\n",
      "Epoch: 285410 | training loss: 1.8718e-03 | validation loss: 1.4668e-03\n",
      "Epoch: 285420 | training loss: 1.8687e-03 | validation loss: 1.4585e-03\n",
      "Epoch: 285430 | training loss: 1.8695e-03 | validation loss: 1.4565e-03\n",
      "Epoch: 285440 | training loss: 1.8688e-03 | validation loss: 1.4578e-03\n",
      "Epoch: 285450 | training loss: 1.8686e-03 | validation loss: 1.4584e-03\n",
      "Epoch: 285460 | training loss: 1.8687e-03 | validation loss: 1.4578e-03\n",
      "Epoch: 285470 | training loss: 1.8733e-03 | validation loss: 1.4540e-03\n",
      "Epoch: 285480 | training loss: 2.1698e-03 | validation loss: 1.5482e-03\n",
      "Epoch: 285490 | training loss: 1.8832e-03 | validation loss: 1.4610e-03\n",
      "Epoch: 285500 | training loss: 2.1630e-03 | validation loss: 1.5409e-03\n",
      "Epoch: 285510 | training loss: 1.8698e-03 | validation loss: 1.4551e-03\n",
      "Epoch: 285520 | training loss: 1.9067e-03 | validation loss: 1.5019e-03\n",
      "Epoch: 285530 | training loss: 1.8691e-03 | validation loss: 1.4618e-03\n",
      "Epoch: 285540 | training loss: 1.8737e-03 | validation loss: 1.4532e-03\n",
      "Epoch: 285550 | training loss: 1.8687e-03 | validation loss: 1.4605e-03\n",
      "Epoch: 285560 | training loss: 1.8687e-03 | validation loss: 1.4611e-03\n",
      "Epoch: 285570 | training loss: 1.8687e-03 | validation loss: 1.4570e-03\n",
      "Epoch: 285580 | training loss: 1.8685e-03 | validation loss: 1.4601e-03\n",
      "Epoch: 285590 | training loss: 1.8684e-03 | validation loss: 1.4583e-03\n",
      "Epoch: 285600 | training loss: 1.8684e-03 | validation loss: 1.4593e-03\n",
      "Epoch: 285610 | training loss: 1.8684e-03 | validation loss: 1.4586e-03\n",
      "Epoch: 285620 | training loss: 1.8684e-03 | validation loss: 1.4595e-03\n",
      "Epoch: 285630 | training loss: 1.8708e-03 | validation loss: 1.4650e-03\n",
      "Epoch: 285640 | training loss: 2.0795e-03 | validation loss: 1.6479e-03\n",
      "Epoch: 285650 | training loss: 2.0183e-03 | validation loss: 1.5423e-03\n",
      "Epoch: 285660 | training loss: 1.8897e-03 | validation loss: 1.4810e-03\n",
      "Epoch: 285670 | training loss: 1.9005e-03 | validation loss: 1.4864e-03\n",
      "Epoch: 285680 | training loss: 1.8703e-03 | validation loss: 1.4564e-03\n",
      "Epoch: 285690 | training loss: 1.8762e-03 | validation loss: 1.4508e-03\n",
      "Epoch: 285700 | training loss: 1.9413e-03 | validation loss: 1.4602e-03\n",
      "Epoch: 285710 | training loss: 2.5864e-03 | validation loss: 1.7083e-03\n",
      "Epoch: 285720 | training loss: 2.1055e-03 | validation loss: 1.6302e-03\n",
      "Epoch: 285730 | training loss: 1.9492e-03 | validation loss: 1.4656e-03\n",
      "Epoch: 285740 | training loss: 1.8923e-03 | validation loss: 1.4875e-03\n",
      "Epoch: 285750 | training loss: 1.8768e-03 | validation loss: 1.4518e-03\n",
      "Epoch: 285760 | training loss: 1.8725e-03 | validation loss: 1.4679e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 285770 | training loss: 1.8700e-03 | validation loss: 1.4544e-03\n",
      "Epoch: 285780 | training loss: 1.8682e-03 | validation loss: 1.4593e-03\n",
      "Epoch: 285790 | training loss: 1.8685e-03 | validation loss: 1.4607e-03\n",
      "Epoch: 285800 | training loss: 1.8681e-03 | validation loss: 1.4588e-03\n",
      "Epoch: 285810 | training loss: 1.8681e-03 | validation loss: 1.4580e-03\n",
      "Epoch: 285820 | training loss: 1.8682e-03 | validation loss: 1.4571e-03\n",
      "Epoch: 285830 | training loss: 1.8716e-03 | validation loss: 1.4532e-03\n",
      "Epoch: 285840 | training loss: 2.1026e-03 | validation loss: 1.5180e-03\n",
      "Epoch: 285850 | training loss: 1.9911e-03 | validation loss: 1.4943e-03\n",
      "Epoch: 285860 | training loss: 2.1312e-03 | validation loss: 1.5241e-03\n",
      "Epoch: 285870 | training loss: 1.8864e-03 | validation loss: 1.4638e-03\n",
      "Epoch: 285880 | training loss: 1.9007e-03 | validation loss: 1.4987e-03\n",
      "Epoch: 285890 | training loss: 1.8729e-03 | validation loss: 1.4703e-03\n",
      "Epoch: 285900 | training loss: 1.8723e-03 | validation loss: 1.4511e-03\n",
      "Epoch: 285910 | training loss: 1.8680e-03 | validation loss: 1.4572e-03\n",
      "Epoch: 285920 | training loss: 1.8686e-03 | validation loss: 1.4622e-03\n",
      "Epoch: 285930 | training loss: 1.8681e-03 | validation loss: 1.4563e-03\n",
      "Epoch: 285940 | training loss: 1.8679e-03 | validation loss: 1.4587e-03\n",
      "Epoch: 285950 | training loss: 1.8679e-03 | validation loss: 1.4580e-03\n",
      "Epoch: 285960 | training loss: 1.8679e-03 | validation loss: 1.4580e-03\n",
      "Epoch: 285970 | training loss: 1.8679e-03 | validation loss: 1.4580e-03\n",
      "Epoch: 285980 | training loss: 1.8679e-03 | validation loss: 1.4582e-03\n",
      "Epoch: 285990 | training loss: 1.8678e-03 | validation loss: 1.4579e-03\n",
      "Epoch: 286000 | training loss: 1.8678e-03 | validation loss: 1.4580e-03\n",
      "Epoch: 286010 | training loss: 1.8678e-03 | validation loss: 1.4580e-03\n",
      "Epoch: 286020 | training loss: 1.8678e-03 | validation loss: 1.4582e-03\n",
      "Epoch: 286030 | training loss: 1.8696e-03 | validation loss: 1.4610e-03\n",
      "Epoch: 286040 | training loss: 2.2034e-03 | validation loss: 1.6980e-03\n",
      "Epoch: 286050 | training loss: 2.7728e-03 | validation loss: 1.8842e-03\n",
      "Epoch: 286060 | training loss: 2.1443e-03 | validation loss: 1.6269e-03\n",
      "Epoch: 286070 | training loss: 1.9623e-03 | validation loss: 1.4739e-03\n",
      "Epoch: 286080 | training loss: 1.8942e-03 | validation loss: 1.4812e-03\n",
      "Epoch: 286090 | training loss: 1.8740e-03 | validation loss: 1.4500e-03\n",
      "Epoch: 286100 | training loss: 1.8683e-03 | validation loss: 1.4606e-03\n",
      "Epoch: 286110 | training loss: 1.8680e-03 | validation loss: 1.4598e-03\n",
      "Epoch: 286120 | training loss: 1.8682e-03 | validation loss: 1.4550e-03\n",
      "Epoch: 286130 | training loss: 1.8679e-03 | validation loss: 1.4558e-03\n",
      "Epoch: 286140 | training loss: 1.8678e-03 | validation loss: 1.4563e-03\n",
      "Epoch: 286150 | training loss: 1.8682e-03 | validation loss: 1.4548e-03\n",
      "Epoch: 286160 | training loss: 1.8817e-03 | validation loss: 1.4500e-03\n",
      "Epoch: 286170 | training loss: 2.4870e-03 | validation loss: 1.6653e-03\n",
      "Epoch: 286180 | training loss: 2.0798e-03 | validation loss: 1.6161e-03\n",
      "Epoch: 286190 | training loss: 2.0467e-03 | validation loss: 1.4975e-03\n",
      "Epoch: 286200 | training loss: 1.8678e-03 | validation loss: 1.4591e-03\n",
      "Epoch: 286210 | training loss: 1.8882e-03 | validation loss: 1.4842e-03\n",
      "Epoch: 286220 | training loss: 1.8742e-03 | validation loss: 1.4520e-03\n",
      "Epoch: 286230 | training loss: 1.8678e-03 | validation loss: 1.4595e-03\n",
      "Epoch: 286240 | training loss: 1.8676e-03 | validation loss: 1.4585e-03\n",
      "Epoch: 286250 | training loss: 1.8676e-03 | validation loss: 1.4563e-03\n",
      "Epoch: 286260 | training loss: 1.8675e-03 | validation loss: 1.4580e-03\n",
      "Epoch: 286270 | training loss: 1.8675e-03 | validation loss: 1.4575e-03\n",
      "Epoch: 286280 | training loss: 1.8675e-03 | validation loss: 1.4570e-03\n",
      "Epoch: 286290 | training loss: 1.8674e-03 | validation loss: 1.4578e-03\n",
      "Epoch: 286300 | training loss: 1.8674e-03 | validation loss: 1.4576e-03\n",
      "Epoch: 286310 | training loss: 1.8674e-03 | validation loss: 1.4575e-03\n",
      "Epoch: 286320 | training loss: 1.8674e-03 | validation loss: 1.4575e-03\n",
      "Epoch: 286330 | training loss: 1.8674e-03 | validation loss: 1.4582e-03\n",
      "Epoch: 286340 | training loss: 1.8705e-03 | validation loss: 1.4651e-03\n",
      "Epoch: 286350 | training loss: 2.1897e-03 | validation loss: 1.6780e-03\n",
      "Epoch: 286360 | training loss: 1.8934e-03 | validation loss: 1.4482e-03\n",
      "Epoch: 286370 | training loss: 2.1481e-03 | validation loss: 1.6442e-03\n",
      "Epoch: 286380 | training loss: 1.9725e-03 | validation loss: 1.5477e-03\n",
      "Epoch: 286390 | training loss: 1.8806e-03 | validation loss: 1.4818e-03\n",
      "Epoch: 286400 | training loss: 1.8736e-03 | validation loss: 1.4588e-03\n",
      "Epoch: 286410 | training loss: 1.8713e-03 | validation loss: 1.4507e-03\n",
      "Epoch: 286420 | training loss: 1.8684e-03 | validation loss: 1.4544e-03\n",
      "Epoch: 286430 | training loss: 1.8676e-03 | validation loss: 1.4601e-03\n",
      "Epoch: 286440 | training loss: 1.8674e-03 | validation loss: 1.4588e-03\n",
      "Epoch: 286450 | training loss: 1.8673e-03 | validation loss: 1.4560e-03\n",
      "Epoch: 286460 | training loss: 1.8672e-03 | validation loss: 1.4574e-03\n",
      "Epoch: 286470 | training loss: 1.8672e-03 | validation loss: 1.4574e-03\n",
      "Epoch: 286480 | training loss: 1.8672e-03 | validation loss: 1.4569e-03\n",
      "Epoch: 286490 | training loss: 1.8672e-03 | validation loss: 1.4573e-03\n",
      "Epoch: 286500 | training loss: 1.8671e-03 | validation loss: 1.4570e-03\n",
      "Epoch: 286510 | training loss: 1.8671e-03 | validation loss: 1.4572e-03\n",
      "Epoch: 286520 | training loss: 1.8671e-03 | validation loss: 1.4571e-03\n",
      "Epoch: 286530 | training loss: 1.8671e-03 | validation loss: 1.4571e-03\n",
      "Epoch: 286540 | training loss: 1.8671e-03 | validation loss: 1.4571e-03\n",
      "Epoch: 286550 | training loss: 1.8671e-03 | validation loss: 1.4571e-03\n",
      "Epoch: 286560 | training loss: 1.8671e-03 | validation loss: 1.4572e-03\n",
      "Epoch: 286570 | training loss: 1.8694e-03 | validation loss: 1.4597e-03\n",
      "Epoch: 286580 | training loss: 2.1519e-03 | validation loss: 1.6476e-03\n",
      "Epoch: 286590 | training loss: 2.9482e-03 | validation loss: 1.9579e-03\n",
      "Epoch: 286600 | training loss: 2.1825e-03 | validation loss: 1.6514e-03\n",
      "Epoch: 286610 | training loss: 1.9608e-03 | validation loss: 1.4753e-03\n",
      "Epoch: 286620 | training loss: 1.8907e-03 | validation loss: 1.4648e-03\n",
      "Epoch: 286630 | training loss: 1.8744e-03 | validation loss: 1.4577e-03\n",
      "Epoch: 286640 | training loss: 1.8695e-03 | validation loss: 1.4525e-03\n",
      "Epoch: 286650 | training loss: 1.8677e-03 | validation loss: 1.4562e-03\n",
      "Epoch: 286660 | training loss: 1.8673e-03 | validation loss: 1.4564e-03\n",
      "Epoch: 286670 | training loss: 1.8672e-03 | validation loss: 1.4545e-03\n",
      "Epoch: 286680 | training loss: 1.8670e-03 | validation loss: 1.4570e-03\n",
      "Epoch: 286690 | training loss: 1.8670e-03 | validation loss: 1.4574e-03\n",
      "Epoch: 286700 | training loss: 1.8669e-03 | validation loss: 1.4576e-03\n",
      "Epoch: 286710 | training loss: 1.8672e-03 | validation loss: 1.4592e-03\n",
      "Epoch: 286720 | training loss: 1.8755e-03 | validation loss: 1.4720e-03\n",
      "Epoch: 286730 | training loss: 2.2809e-03 | validation loss: 1.7354e-03\n",
      "Epoch: 286740 | training loss: 1.8913e-03 | validation loss: 1.4474e-03\n",
      "Epoch: 286750 | training loss: 2.0812e-03 | validation loss: 1.6087e-03\n",
      "Epoch: 286760 | training loss: 1.8831e-03 | validation loss: 1.4490e-03\n",
      "Epoch: 286770 | training loss: 1.8816e-03 | validation loss: 1.4502e-03\n",
      "Epoch: 286780 | training loss: 1.8767e-03 | validation loss: 1.4727e-03\n",
      "Epoch: 286790 | training loss: 1.8674e-03 | validation loss: 1.4545e-03\n",
      "Epoch: 286800 | training loss: 1.8668e-03 | validation loss: 1.4558e-03\n",
      "Epoch: 286810 | training loss: 1.8669e-03 | validation loss: 1.4578e-03\n",
      "Epoch: 286820 | training loss: 1.8668e-03 | validation loss: 1.4556e-03\n",
      "Epoch: 286830 | training loss: 1.8667e-03 | validation loss: 1.4568e-03\n",
      "Epoch: 286840 | training loss: 1.8667e-03 | validation loss: 1.4567e-03\n",
      "Epoch: 286850 | training loss: 1.8667e-03 | validation loss: 1.4560e-03\n",
      "Epoch: 286860 | training loss: 1.8666e-03 | validation loss: 1.4564e-03\n",
      "Epoch: 286870 | training loss: 1.8666e-03 | validation loss: 1.4566e-03\n",
      "Epoch: 286880 | training loss: 1.8666e-03 | validation loss: 1.4568e-03\n",
      "Epoch: 286890 | training loss: 1.8668e-03 | validation loss: 1.4580e-03\n",
      "Epoch: 286900 | training loss: 1.8742e-03 | validation loss: 1.4696e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 286910 | training loss: 2.3902e-03 | validation loss: 1.7902e-03\n",
      "Epoch: 286920 | training loss: 2.0509e-03 | validation loss: 1.4955e-03\n",
      "Epoch: 286930 | training loss: 2.0693e-03 | validation loss: 1.5984e-03\n",
      "Epoch: 286940 | training loss: 1.9161e-03 | validation loss: 1.5099e-03\n",
      "Epoch: 286950 | training loss: 1.8771e-03 | validation loss: 1.4546e-03\n",
      "Epoch: 286960 | training loss: 1.8764e-03 | validation loss: 1.4477e-03\n",
      "Epoch: 286970 | training loss: 1.8677e-03 | validation loss: 1.4603e-03\n",
      "Epoch: 286980 | training loss: 1.8673e-03 | validation loss: 1.4611e-03\n",
      "Epoch: 286990 | training loss: 1.8671e-03 | validation loss: 1.4531e-03\n",
      "Epoch: 287000 | training loss: 1.8665e-03 | validation loss: 1.4577e-03\n",
      "Epoch: 287010 | training loss: 1.8664e-03 | validation loss: 1.4560e-03\n",
      "Epoch: 287020 | training loss: 1.8664e-03 | validation loss: 1.4562e-03\n",
      "Epoch: 287030 | training loss: 1.8664e-03 | validation loss: 1.4562e-03\n",
      "Epoch: 287040 | training loss: 1.8664e-03 | validation loss: 1.4563e-03\n",
      "Epoch: 287050 | training loss: 1.8664e-03 | validation loss: 1.4560e-03\n",
      "Epoch: 287060 | training loss: 1.8664e-03 | validation loss: 1.4563e-03\n",
      "Epoch: 287070 | training loss: 1.8663e-03 | validation loss: 1.4563e-03\n",
      "Epoch: 287080 | training loss: 1.8664e-03 | validation loss: 1.4565e-03\n",
      "Epoch: 287090 | training loss: 1.8678e-03 | validation loss: 1.4597e-03\n",
      "Epoch: 287100 | training loss: 2.0045e-03 | validation loss: 1.5744e-03\n",
      "Epoch: 287110 | training loss: 1.9424e-03 | validation loss: 1.4685e-03\n",
      "Epoch: 287120 | training loss: 2.1912e-03 | validation loss: 1.5750e-03\n",
      "Epoch: 287130 | training loss: 1.9197e-03 | validation loss: 1.4940e-03\n",
      "Epoch: 287140 | training loss: 1.9182e-03 | validation loss: 1.5152e-03\n",
      "Epoch: 287150 | training loss: 1.8806e-03 | validation loss: 1.4485e-03\n",
      "Epoch: 287160 | training loss: 1.8703e-03 | validation loss: 1.4491e-03\n",
      "Epoch: 287170 | training loss: 1.8687e-03 | validation loss: 1.4602e-03\n",
      "Epoch: 287180 | training loss: 1.8740e-03 | validation loss: 1.4702e-03\n",
      "Epoch: 287190 | training loss: 1.9215e-03 | validation loss: 1.5120e-03\n",
      "Epoch: 287200 | training loss: 2.3279e-03 | validation loss: 1.7627e-03\n",
      "Epoch: 287210 | training loss: 1.9032e-03 | validation loss: 1.4507e-03\n",
      "Epoch: 287220 | training loss: 1.8674e-03 | validation loss: 1.4592e-03\n",
      "Epoch: 287230 | training loss: 1.8691e-03 | validation loss: 1.4631e-03\n",
      "Epoch: 287240 | training loss: 1.8753e-03 | validation loss: 1.4495e-03\n",
      "Epoch: 287250 | training loss: 1.8695e-03 | validation loss: 1.4640e-03\n",
      "Epoch: 287260 | training loss: 1.8673e-03 | validation loss: 1.4602e-03\n",
      "Epoch: 287270 | training loss: 1.8661e-03 | validation loss: 1.4552e-03\n",
      "Epoch: 287280 | training loss: 1.8668e-03 | validation loss: 1.4529e-03\n",
      "Epoch: 287290 | training loss: 1.8760e-03 | validation loss: 1.4492e-03\n",
      "Epoch: 287300 | training loss: 2.1696e-03 | validation loss: 1.5415e-03\n",
      "Epoch: 287310 | training loss: 1.9200e-03 | validation loss: 1.4642e-03\n",
      "Epoch: 287320 | training loss: 1.9018e-03 | validation loss: 1.4483e-03\n",
      "Epoch: 287330 | training loss: 1.9316e-03 | validation loss: 1.5124e-03\n",
      "Epoch: 287340 | training loss: 1.8882e-03 | validation loss: 1.4556e-03\n",
      "Epoch: 287350 | training loss: 1.8692e-03 | validation loss: 1.4605e-03\n",
      "Epoch: 287360 | training loss: 1.8662e-03 | validation loss: 1.4546e-03\n",
      "Epoch: 287370 | training loss: 1.8660e-03 | validation loss: 1.4567e-03\n",
      "Epoch: 287380 | training loss: 1.8661e-03 | validation loss: 1.4542e-03\n",
      "Epoch: 287390 | training loss: 1.8661e-03 | validation loss: 1.4572e-03\n",
      "Epoch: 287400 | training loss: 1.8659e-03 | validation loss: 1.4549e-03\n",
      "Epoch: 287410 | training loss: 1.8659e-03 | validation loss: 1.4548e-03\n",
      "Epoch: 287420 | training loss: 1.8659e-03 | validation loss: 1.4554e-03\n",
      "Epoch: 287430 | training loss: 1.8658e-03 | validation loss: 1.4554e-03\n",
      "Epoch: 287440 | training loss: 1.8658e-03 | validation loss: 1.4553e-03\n",
      "Epoch: 287450 | training loss: 1.8659e-03 | validation loss: 1.4547e-03\n",
      "Epoch: 287460 | training loss: 1.8716e-03 | validation loss: 1.4512e-03\n",
      "Epoch: 287470 | training loss: 2.7257e-03 | validation loss: 1.7909e-03\n",
      "Epoch: 287480 | training loss: 2.8506e-03 | validation loss: 2.0318e-03\n",
      "Epoch: 287490 | training loss: 2.0200e-03 | validation loss: 1.5544e-03\n",
      "Epoch: 287500 | training loss: 1.8876e-03 | validation loss: 1.4689e-03\n",
      "Epoch: 287510 | training loss: 1.8665e-03 | validation loss: 1.4571e-03\n",
      "Epoch: 287520 | training loss: 1.8669e-03 | validation loss: 1.4560e-03\n",
      "Epoch: 287530 | training loss: 1.8686e-03 | validation loss: 1.4538e-03\n",
      "Epoch: 287540 | training loss: 1.8677e-03 | validation loss: 1.4542e-03\n",
      "Epoch: 287550 | training loss: 1.8661e-03 | validation loss: 1.4551e-03\n",
      "Epoch: 287560 | training loss: 1.8657e-03 | validation loss: 1.4555e-03\n",
      "Epoch: 287570 | training loss: 1.8658e-03 | validation loss: 1.4561e-03\n",
      "Epoch: 287580 | training loss: 1.8656e-03 | validation loss: 1.4554e-03\n",
      "Epoch: 287590 | training loss: 1.8656e-03 | validation loss: 1.4552e-03\n",
      "Epoch: 287600 | training loss: 1.8656e-03 | validation loss: 1.4553e-03\n",
      "Epoch: 287610 | training loss: 1.8656e-03 | validation loss: 1.4553e-03\n",
      "Epoch: 287620 | training loss: 1.8656e-03 | validation loss: 1.4552e-03\n",
      "Epoch: 287630 | training loss: 1.8656e-03 | validation loss: 1.4553e-03\n",
      "Epoch: 287640 | training loss: 1.8656e-03 | validation loss: 1.4552e-03\n",
      "Epoch: 287650 | training loss: 1.8655e-03 | validation loss: 1.4553e-03\n",
      "Epoch: 287660 | training loss: 1.8656e-03 | validation loss: 1.4563e-03\n",
      "Epoch: 287670 | training loss: 1.8722e-03 | validation loss: 1.4710e-03\n",
      "Epoch: 287680 | training loss: 2.6596e-03 | validation loss: 2.0966e-03\n",
      "Epoch: 287690 | training loss: 2.0463e-03 | validation loss: 1.5023e-03\n",
      "Epoch: 287700 | training loss: 1.9964e-03 | validation loss: 1.5385e-03\n",
      "Epoch: 287710 | training loss: 1.9106e-03 | validation loss: 1.4555e-03\n",
      "Epoch: 287720 | training loss: 1.8791e-03 | validation loss: 1.4591e-03\n",
      "Epoch: 287730 | training loss: 1.8694e-03 | validation loss: 1.4508e-03\n",
      "Epoch: 287740 | training loss: 1.8662e-03 | validation loss: 1.4546e-03\n",
      "Epoch: 287750 | training loss: 1.8656e-03 | validation loss: 1.4530e-03\n",
      "Epoch: 287760 | training loss: 1.8657e-03 | validation loss: 1.4573e-03\n",
      "Epoch: 287770 | training loss: 1.8654e-03 | validation loss: 1.4550e-03\n",
      "Epoch: 287780 | training loss: 1.8654e-03 | validation loss: 1.4538e-03\n",
      "Epoch: 287790 | training loss: 1.8654e-03 | validation loss: 1.4538e-03\n",
      "Epoch: 287800 | training loss: 1.8654e-03 | validation loss: 1.4534e-03\n",
      "Epoch: 287810 | training loss: 1.8675e-03 | validation loss: 1.4503e-03\n",
      "Epoch: 287820 | training loss: 1.9656e-03 | validation loss: 1.4662e-03\n",
      "Epoch: 287830 | training loss: 2.8298e-03 | validation loss: 1.8151e-03\n",
      "Epoch: 287840 | training loss: 1.8811e-03 | validation loss: 1.4639e-03\n",
      "Epoch: 287850 | training loss: 1.9784e-03 | validation loss: 1.5416e-03\n",
      "Epoch: 287860 | training loss: 1.8731e-03 | validation loss: 1.4511e-03\n",
      "Epoch: 287870 | training loss: 1.8750e-03 | validation loss: 1.4519e-03\n",
      "Epoch: 287880 | training loss: 1.8703e-03 | validation loss: 1.4647e-03\n",
      "Epoch: 287890 | training loss: 1.8655e-03 | validation loss: 1.4522e-03\n",
      "Epoch: 287900 | training loss: 1.8653e-03 | validation loss: 1.4537e-03\n",
      "Epoch: 287910 | training loss: 1.8653e-03 | validation loss: 1.4560e-03\n",
      "Epoch: 287920 | training loss: 1.8652e-03 | validation loss: 1.4537e-03\n",
      "Epoch: 287930 | training loss: 1.8652e-03 | validation loss: 1.4547e-03\n",
      "Epoch: 287940 | training loss: 1.8651e-03 | validation loss: 1.4546e-03\n",
      "Epoch: 287950 | training loss: 1.8651e-03 | validation loss: 1.4541e-03\n",
      "Epoch: 287960 | training loss: 1.8651e-03 | validation loss: 1.4544e-03\n",
      "Epoch: 287970 | training loss: 1.8651e-03 | validation loss: 1.4546e-03\n",
      "Epoch: 287980 | training loss: 1.8651e-03 | validation loss: 1.4547e-03\n",
      "Epoch: 287990 | training loss: 1.8652e-03 | validation loss: 1.4556e-03\n",
      "Epoch: 288000 | training loss: 1.8698e-03 | validation loss: 1.4637e-03\n",
      "Epoch: 288010 | training loss: 2.2207e-03 | validation loss: 1.6896e-03\n",
      "Epoch: 288020 | training loss: 1.8771e-03 | validation loss: 1.4464e-03\n",
      "Epoch: 288030 | training loss: 2.1733e-03 | validation loss: 1.6624e-03\n",
      "Epoch: 288040 | training loss: 1.9005e-03 | validation loss: 1.4934e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 288050 | training loss: 1.8789e-03 | validation loss: 1.4474e-03\n",
      "Epoch: 288060 | training loss: 1.8775e-03 | validation loss: 1.4480e-03\n",
      "Epoch: 288070 | training loss: 1.8656e-03 | validation loss: 1.4578e-03\n",
      "Epoch: 288080 | training loss: 1.8664e-03 | validation loss: 1.4591e-03\n",
      "Epoch: 288090 | training loss: 1.8654e-03 | validation loss: 1.4524e-03\n",
      "Epoch: 288100 | training loss: 1.8649e-03 | validation loss: 1.4542e-03\n",
      "Epoch: 288110 | training loss: 1.8649e-03 | validation loss: 1.4550e-03\n",
      "Epoch: 288120 | training loss: 1.8649e-03 | validation loss: 1.4537e-03\n",
      "Epoch: 288130 | training loss: 1.8649e-03 | validation loss: 1.4547e-03\n",
      "Epoch: 288140 | training loss: 1.8649e-03 | validation loss: 1.4540e-03\n",
      "Epoch: 288150 | training loss: 1.8648e-03 | validation loss: 1.4543e-03\n",
      "Epoch: 288160 | training loss: 1.8648e-03 | validation loss: 1.4544e-03\n",
      "Epoch: 288170 | training loss: 1.8653e-03 | validation loss: 1.4564e-03\n",
      "Epoch: 288180 | training loss: 1.9209e-03 | validation loss: 1.5169e-03\n",
      "Epoch: 288190 | training loss: 1.9630e-03 | validation loss: 1.5571e-03\n",
      "Epoch: 288200 | training loss: 2.0094e-03 | validation loss: 1.5768e-03\n",
      "Epoch: 288210 | training loss: 1.9066e-03 | validation loss: 1.4842e-03\n",
      "Epoch: 288220 | training loss: 1.8670e-03 | validation loss: 1.4581e-03\n",
      "Epoch: 288230 | training loss: 1.8669e-03 | validation loss: 1.4570e-03\n",
      "Epoch: 288240 | training loss: 1.8738e-03 | validation loss: 1.4654e-03\n",
      "Epoch: 288250 | training loss: 1.9661e-03 | validation loss: 1.5404e-03\n",
      "Epoch: 288260 | training loss: 2.4548e-03 | validation loss: 1.8346e-03\n",
      "Epoch: 288270 | training loss: 2.0620e-03 | validation loss: 1.4998e-03\n",
      "Epoch: 288280 | training loss: 1.9297e-03 | validation loss: 1.5159e-03\n",
      "Epoch: 288290 | training loss: 1.8872e-03 | validation loss: 1.4472e-03\n",
      "Epoch: 288300 | training loss: 1.8744e-03 | validation loss: 1.4694e-03\n",
      "Epoch: 288310 | training loss: 1.8688e-03 | validation loss: 1.4481e-03\n",
      "Epoch: 288320 | training loss: 1.8651e-03 | validation loss: 1.4565e-03\n",
      "Epoch: 288330 | training loss: 1.8650e-03 | validation loss: 1.4563e-03\n",
      "Epoch: 288340 | training loss: 1.8647e-03 | validation loss: 1.4525e-03\n",
      "Epoch: 288350 | training loss: 1.8650e-03 | validation loss: 1.4513e-03\n",
      "Epoch: 288360 | training loss: 1.8676e-03 | validation loss: 1.4488e-03\n",
      "Epoch: 288370 | training loss: 1.9246e-03 | validation loss: 1.4548e-03\n",
      "Epoch: 288380 | training loss: 2.7808e-03 | validation loss: 1.7932e-03\n",
      "Epoch: 288390 | training loss: 2.1749e-03 | validation loss: 1.6632e-03\n",
      "Epoch: 288400 | training loss: 1.9322e-03 | validation loss: 1.4649e-03\n",
      "Epoch: 288410 | training loss: 1.8668e-03 | validation loss: 1.4570e-03\n",
      "Epoch: 288420 | training loss: 1.8655e-03 | validation loss: 1.4563e-03\n",
      "Epoch: 288430 | training loss: 1.8658e-03 | validation loss: 1.4518e-03\n",
      "Epoch: 288440 | training loss: 1.8648e-03 | validation loss: 1.4551e-03\n",
      "Epoch: 288450 | training loss: 1.8644e-03 | validation loss: 1.4537e-03\n",
      "Epoch: 288460 | training loss: 1.8646e-03 | validation loss: 1.4522e-03\n",
      "Epoch: 288470 | training loss: 1.8645e-03 | validation loss: 1.4548e-03\n",
      "Epoch: 288480 | training loss: 1.8644e-03 | validation loss: 1.4540e-03\n",
      "Epoch: 288490 | training loss: 1.8644e-03 | validation loss: 1.4536e-03\n",
      "Epoch: 288500 | training loss: 1.8644e-03 | validation loss: 1.4533e-03\n",
      "Epoch: 288510 | training loss: 1.8644e-03 | validation loss: 1.4529e-03\n",
      "Epoch: 288520 | training loss: 1.8656e-03 | validation loss: 1.4503e-03\n",
      "Epoch: 288530 | training loss: 1.9959e-03 | validation loss: 1.4810e-03\n",
      "Epoch: 288540 | training loss: 2.5253e-03 | validation loss: 1.6951e-03\n",
      "Epoch: 288550 | training loss: 2.2764e-03 | validation loss: 1.5874e-03\n",
      "Epoch: 288560 | training loss: 1.8935e-03 | validation loss: 1.4476e-03\n",
      "Epoch: 288570 | training loss: 1.8691e-03 | validation loss: 1.4623e-03\n",
      "Epoch: 288580 | training loss: 1.8814e-03 | validation loss: 1.4757e-03\n",
      "Epoch: 288590 | training loss: 1.8702e-03 | validation loss: 1.4643e-03\n",
      "Epoch: 288600 | training loss: 1.8642e-03 | validation loss: 1.4530e-03\n",
      "Epoch: 288610 | training loss: 1.8652e-03 | validation loss: 1.4506e-03\n",
      "Epoch: 288620 | training loss: 1.8642e-03 | validation loss: 1.4531e-03\n",
      "Epoch: 288630 | training loss: 1.8643e-03 | validation loss: 1.4546e-03\n",
      "Epoch: 288640 | training loss: 1.8642e-03 | validation loss: 1.4530e-03\n",
      "Epoch: 288650 | training loss: 1.8642e-03 | validation loss: 1.4531e-03\n",
      "Epoch: 288660 | training loss: 1.8641e-03 | validation loss: 1.4535e-03\n",
      "Epoch: 288670 | training loss: 1.8641e-03 | validation loss: 1.4531e-03\n",
      "Epoch: 288680 | training loss: 1.8641e-03 | validation loss: 1.4533e-03\n",
      "Epoch: 288690 | training loss: 1.8641e-03 | validation loss: 1.4531e-03\n",
      "Epoch: 288700 | training loss: 1.8641e-03 | validation loss: 1.4528e-03\n",
      "Epoch: 288710 | training loss: 1.8659e-03 | validation loss: 1.4499e-03\n",
      "Epoch: 288720 | training loss: 2.2082e-03 | validation loss: 1.6332e-03\n",
      "Epoch: 288730 | training loss: 2.2056e-03 | validation loss: 1.7094e-03\n",
      "Epoch: 288740 | training loss: 1.9697e-03 | validation loss: 1.5578e-03\n",
      "Epoch: 288750 | training loss: 1.8963e-03 | validation loss: 1.4738e-03\n",
      "Epoch: 288760 | training loss: 1.8712e-03 | validation loss: 1.4665e-03\n",
      "Epoch: 288770 | training loss: 1.8649e-03 | validation loss: 1.4570e-03\n",
      "Epoch: 288780 | training loss: 1.8643e-03 | validation loss: 1.4504e-03\n",
      "Epoch: 288790 | training loss: 1.8652e-03 | validation loss: 1.4486e-03\n",
      "Epoch: 288800 | training loss: 1.8697e-03 | validation loss: 1.4457e-03\n",
      "Epoch: 288810 | training loss: 1.9703e-03 | validation loss: 1.4650e-03\n",
      "Epoch: 288820 | training loss: 2.5056e-03 | validation loss: 1.6710e-03\n",
      "Epoch: 288830 | training loss: 2.0399e-03 | validation loss: 1.5856e-03\n",
      "Epoch: 288840 | training loss: 1.8887e-03 | validation loss: 1.4457e-03\n",
      "Epoch: 288850 | training loss: 1.8648e-03 | validation loss: 1.4563e-03\n",
      "Epoch: 288860 | training loss: 1.8639e-03 | validation loss: 1.4529e-03\n",
      "Epoch: 288870 | training loss: 1.8639e-03 | validation loss: 1.4535e-03\n",
      "Epoch: 288880 | training loss: 1.8642e-03 | validation loss: 1.4505e-03\n",
      "Epoch: 288890 | training loss: 1.8645e-03 | validation loss: 1.4559e-03\n",
      "Epoch: 288900 | training loss: 1.8639e-03 | validation loss: 1.4516e-03\n",
      "Epoch: 288910 | training loss: 1.8640e-03 | validation loss: 1.4511e-03\n",
      "Epoch: 288920 | training loss: 1.8639e-03 | validation loss: 1.4515e-03\n",
      "Epoch: 288930 | training loss: 1.8642e-03 | validation loss: 1.4505e-03\n",
      "Epoch: 288940 | training loss: 1.8712e-03 | validation loss: 1.4463e-03\n",
      "Epoch: 288950 | training loss: 2.1795e-03 | validation loss: 1.5426e-03\n",
      "Epoch: 288960 | training loss: 1.8906e-03 | validation loss: 1.4589e-03\n",
      "Epoch: 288970 | training loss: 1.9986e-03 | validation loss: 1.4749e-03\n",
      "Epoch: 288980 | training loss: 1.9290e-03 | validation loss: 1.5043e-03\n",
      "Epoch: 288990 | training loss: 1.8655e-03 | validation loss: 1.4591e-03\n",
      "Epoch: 289000 | training loss: 1.8715e-03 | validation loss: 1.4490e-03\n",
      "Epoch: 289010 | training loss: 1.8678e-03 | validation loss: 1.4584e-03\n",
      "Epoch: 289020 | training loss: 1.8646e-03 | validation loss: 1.4512e-03\n",
      "Epoch: 289030 | training loss: 1.8639e-03 | validation loss: 1.4538e-03\n",
      "Epoch: 289040 | training loss: 1.8637e-03 | validation loss: 1.4515e-03\n",
      "Epoch: 289050 | training loss: 1.8637e-03 | validation loss: 1.4534e-03\n",
      "Epoch: 289060 | training loss: 1.8636e-03 | validation loss: 1.4520e-03\n",
      "Epoch: 289070 | training loss: 1.8636e-03 | validation loss: 1.4522e-03\n",
      "Epoch: 289080 | training loss: 1.8636e-03 | validation loss: 1.4527e-03\n",
      "Epoch: 289090 | training loss: 1.8635e-03 | validation loss: 1.4527e-03\n",
      "Epoch: 289100 | training loss: 1.8636e-03 | validation loss: 1.4532e-03\n",
      "Epoch: 289110 | training loss: 1.8654e-03 | validation loss: 1.4572e-03\n",
      "Epoch: 289120 | training loss: 1.9653e-03 | validation loss: 1.5311e-03\n",
      "Epoch: 289130 | training loss: 2.9269e-03 | validation loss: 2.0777e-03\n",
      "Epoch: 289140 | training loss: 1.8899e-03 | validation loss: 1.4551e-03\n",
      "Epoch: 289150 | training loss: 1.9782e-03 | validation loss: 1.4928e-03\n",
      "Epoch: 289160 | training loss: 1.8823e-03 | validation loss: 1.4684e-03\n",
      "Epoch: 289170 | training loss: 1.8719e-03 | validation loss: 1.4564e-03\n",
      "Epoch: 289180 | training loss: 1.8671e-03 | validation loss: 1.4568e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 289190 | training loss: 1.8647e-03 | validation loss: 1.4518e-03\n",
      "Epoch: 289200 | training loss: 1.8635e-03 | validation loss: 1.4527e-03\n",
      "Epoch: 289210 | training loss: 1.8637e-03 | validation loss: 1.4524e-03\n",
      "Epoch: 289220 | training loss: 1.8635e-03 | validation loss: 1.4526e-03\n",
      "Epoch: 289230 | training loss: 1.8634e-03 | validation loss: 1.4523e-03\n",
      "Epoch: 289240 | training loss: 1.8633e-03 | validation loss: 1.4520e-03\n",
      "Epoch: 289250 | training loss: 1.8633e-03 | validation loss: 1.4520e-03\n",
      "Epoch: 289260 | training loss: 1.8633e-03 | validation loss: 1.4515e-03\n",
      "Epoch: 289270 | training loss: 1.8637e-03 | validation loss: 1.4495e-03\n",
      "Epoch: 289280 | training loss: 1.8852e-03 | validation loss: 1.4443e-03\n",
      "Epoch: 289290 | training loss: 2.8000e-03 | validation loss: 1.8648e-03\n",
      "Epoch: 289300 | training loss: 2.0115e-03 | validation loss: 1.5817e-03\n",
      "Epoch: 289310 | training loss: 1.9849e-03 | validation loss: 1.5360e-03\n",
      "Epoch: 289320 | training loss: 1.8930e-03 | validation loss: 1.4938e-03\n",
      "Epoch: 289330 | training loss: 1.8678e-03 | validation loss: 1.4462e-03\n",
      "Epoch: 289340 | training loss: 1.8680e-03 | validation loss: 1.4543e-03\n",
      "Epoch: 289350 | training loss: 1.8648e-03 | validation loss: 1.4470e-03\n",
      "Epoch: 289360 | training loss: 1.8633e-03 | validation loss: 1.4532e-03\n",
      "Epoch: 289370 | training loss: 1.8634e-03 | validation loss: 1.4542e-03\n",
      "Epoch: 289380 | training loss: 1.8632e-03 | validation loss: 1.4512e-03\n",
      "Epoch: 289390 | training loss: 1.8632e-03 | validation loss: 1.4508e-03\n",
      "Epoch: 289400 | training loss: 1.8635e-03 | validation loss: 1.4500e-03\n",
      "Epoch: 289410 | training loss: 1.8724e-03 | validation loss: 1.4457e-03\n",
      "Epoch: 289420 | training loss: 2.3177e-03 | validation loss: 1.6014e-03\n",
      "Epoch: 289430 | training loss: 1.9084e-03 | validation loss: 1.4992e-03\n",
      "Epoch: 289440 | training loss: 2.0996e-03 | validation loss: 1.5176e-03\n",
      "Epoch: 289450 | training loss: 1.8671e-03 | validation loss: 1.4569e-03\n",
      "Epoch: 289460 | training loss: 1.8888e-03 | validation loss: 1.4793e-03\n",
      "Epoch: 289470 | training loss: 1.8689e-03 | validation loss: 1.4476e-03\n",
      "Epoch: 289480 | training loss: 1.8631e-03 | validation loss: 1.4510e-03\n",
      "Epoch: 289490 | training loss: 1.8640e-03 | validation loss: 1.4549e-03\n",
      "Epoch: 289500 | training loss: 1.8635e-03 | validation loss: 1.4496e-03\n",
      "Epoch: 289510 | training loss: 1.8632e-03 | validation loss: 1.4533e-03\n",
      "Epoch: 289520 | training loss: 1.8630e-03 | validation loss: 1.4506e-03\n",
      "Epoch: 289530 | training loss: 1.8630e-03 | validation loss: 1.4521e-03\n",
      "Epoch: 289540 | training loss: 1.8629e-03 | validation loss: 1.4515e-03\n",
      "Epoch: 289550 | training loss: 1.8629e-03 | validation loss: 1.4513e-03\n",
      "Epoch: 289560 | training loss: 1.8629e-03 | validation loss: 1.4515e-03\n",
      "Epoch: 289570 | training loss: 1.8629e-03 | validation loss: 1.4516e-03\n",
      "Epoch: 289580 | training loss: 1.8629e-03 | validation loss: 1.4518e-03\n",
      "Epoch: 289590 | training loss: 1.8631e-03 | validation loss: 1.4530e-03\n",
      "Epoch: 289600 | training loss: 1.8754e-03 | validation loss: 1.4687e-03\n",
      "Epoch: 289610 | training loss: 2.9307e-03 | validation loss: 2.0768e-03\n",
      "Epoch: 289620 | training loss: 2.6254e-03 | validation loss: 1.7376e-03\n",
      "Epoch: 289630 | training loss: 1.8865e-03 | validation loss: 1.4482e-03\n",
      "Epoch: 289640 | training loss: 1.8974e-03 | validation loss: 1.4851e-03\n",
      "Epoch: 289650 | training loss: 1.8962e-03 | validation loss: 1.4849e-03\n",
      "Epoch: 289660 | training loss: 1.8640e-03 | validation loss: 1.4557e-03\n",
      "Epoch: 289670 | training loss: 1.8659e-03 | validation loss: 1.4476e-03\n",
      "Epoch: 289680 | training loss: 1.8632e-03 | validation loss: 1.4496e-03\n",
      "Epoch: 289690 | training loss: 1.8632e-03 | validation loss: 1.4537e-03\n",
      "Epoch: 289700 | training loss: 1.8627e-03 | validation loss: 1.4515e-03\n",
      "Epoch: 289710 | training loss: 1.8627e-03 | validation loss: 1.4507e-03\n",
      "Epoch: 289720 | training loss: 1.8627e-03 | validation loss: 1.4518e-03\n",
      "Epoch: 289730 | training loss: 1.8627e-03 | validation loss: 1.4510e-03\n",
      "Epoch: 289740 | training loss: 1.8626e-03 | validation loss: 1.4514e-03\n",
      "Epoch: 289750 | training loss: 1.8626e-03 | validation loss: 1.4511e-03\n",
      "Epoch: 289760 | training loss: 1.8626e-03 | validation loss: 1.4512e-03\n",
      "Epoch: 289770 | training loss: 1.8626e-03 | validation loss: 1.4506e-03\n",
      "Epoch: 289780 | training loss: 1.8666e-03 | validation loss: 1.4468e-03\n",
      "Epoch: 289790 | training loss: 2.5404e-03 | validation loss: 1.8287e-03\n",
      "Epoch: 289800 | training loss: 1.9418e-03 | validation loss: 1.5217e-03\n",
      "Epoch: 289810 | training loss: 1.9181e-03 | validation loss: 1.5121e-03\n",
      "Epoch: 289820 | training loss: 1.8878e-03 | validation loss: 1.4661e-03\n",
      "Epoch: 289830 | training loss: 1.8744e-03 | validation loss: 1.4712e-03\n",
      "Epoch: 289840 | training loss: 1.8663e-03 | validation loss: 1.4556e-03\n",
      "Epoch: 289850 | training loss: 1.8650e-03 | validation loss: 1.4514e-03\n",
      "Epoch: 289860 | training loss: 1.8633e-03 | validation loss: 1.4507e-03\n",
      "Epoch: 289870 | training loss: 1.8632e-03 | validation loss: 1.4486e-03\n",
      "Epoch: 289880 | training loss: 1.8715e-03 | validation loss: 1.4436e-03\n",
      "Epoch: 289890 | training loss: 2.1647e-03 | validation loss: 1.5321e-03\n",
      "Epoch: 289900 | training loss: 1.8918e-03 | validation loss: 1.4483e-03\n",
      "Epoch: 289910 | training loss: 1.9178e-03 | validation loss: 1.4511e-03\n",
      "Epoch: 289920 | training loss: 1.9340e-03 | validation loss: 1.5156e-03\n",
      "Epoch: 289930 | training loss: 1.8768e-03 | validation loss: 1.4437e-03\n",
      "Epoch: 289940 | training loss: 1.8628e-03 | validation loss: 1.4528e-03\n",
      "Epoch: 289950 | training loss: 1.8625e-03 | validation loss: 1.4518e-03\n",
      "Epoch: 289960 | training loss: 1.8624e-03 | validation loss: 1.4495e-03\n",
      "Epoch: 289970 | training loss: 1.8623e-03 | validation loss: 1.4508e-03\n",
      "Epoch: 289980 | training loss: 1.8624e-03 | validation loss: 1.4516e-03\n",
      "Epoch: 289990 | training loss: 1.8624e-03 | validation loss: 1.4495e-03\n",
      "Epoch: 290000 | training loss: 1.8623e-03 | validation loss: 1.4506e-03\n",
      "Epoch: 290010 | training loss: 1.8623e-03 | validation loss: 1.4512e-03\n",
      "Epoch: 290020 | training loss: 1.8623e-03 | validation loss: 1.4516e-03\n",
      "Epoch: 290030 | training loss: 1.8630e-03 | validation loss: 1.4541e-03\n",
      "Epoch: 290040 | training loss: 1.8861e-03 | validation loss: 1.4795e-03\n",
      "Epoch: 290050 | training loss: 2.8428e-03 | validation loss: 2.0389e-03\n",
      "Epoch: 290060 | training loss: 2.3316e-03 | validation loss: 1.6048e-03\n",
      "Epoch: 290070 | training loss: 1.9209e-03 | validation loss: 1.4955e-03\n",
      "Epoch: 290080 | training loss: 1.8935e-03 | validation loss: 1.4861e-03\n",
      "Epoch: 290090 | training loss: 1.8808e-03 | validation loss: 1.4508e-03\n",
      "Epoch: 290100 | training loss: 1.8629e-03 | validation loss: 1.4486e-03\n",
      "Epoch: 290110 | training loss: 1.8635e-03 | validation loss: 1.4547e-03\n",
      "Epoch: 290120 | training loss: 1.8632e-03 | validation loss: 1.4491e-03\n",
      "Epoch: 290130 | training loss: 1.8626e-03 | validation loss: 1.4517e-03\n",
      "Epoch: 290140 | training loss: 1.8623e-03 | validation loss: 1.4500e-03\n",
      "Epoch: 290150 | training loss: 1.8621e-03 | validation loss: 1.4508e-03\n",
      "Epoch: 290160 | training loss: 1.8621e-03 | validation loss: 1.4503e-03\n",
      "Epoch: 290170 | training loss: 1.8620e-03 | validation loss: 1.4502e-03\n",
      "Epoch: 290180 | training loss: 1.8620e-03 | validation loss: 1.4505e-03\n",
      "Epoch: 290190 | training loss: 1.8620e-03 | validation loss: 1.4506e-03\n",
      "Epoch: 290200 | training loss: 1.8620e-03 | validation loss: 1.4507e-03\n",
      "Epoch: 290210 | training loss: 1.8623e-03 | validation loss: 1.4518e-03\n",
      "Epoch: 290220 | training loss: 1.8728e-03 | validation loss: 1.4641e-03\n",
      "Epoch: 290230 | training loss: 2.5939e-03 | validation loss: 1.8861e-03\n",
      "Epoch: 290240 | training loss: 2.3052e-03 | validation loss: 1.6581e-03\n",
      "Epoch: 290250 | training loss: 1.9258e-03 | validation loss: 1.5201e-03\n",
      "Epoch: 290260 | training loss: 1.9123e-03 | validation loss: 1.4725e-03\n",
      "Epoch: 290270 | training loss: 1.8896e-03 | validation loss: 1.4585e-03\n",
      "Epoch: 290280 | training loss: 1.8662e-03 | validation loss: 1.4573e-03\n",
      "Epoch: 290290 | training loss: 1.8630e-03 | validation loss: 1.4485e-03\n",
      "Epoch: 290300 | training loss: 1.8632e-03 | validation loss: 1.4534e-03\n",
      "Epoch: 290310 | training loss: 1.8619e-03 | validation loss: 1.4496e-03\n",
      "Epoch: 290320 | training loss: 1.8620e-03 | validation loss: 1.4507e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 290330 | training loss: 1.8618e-03 | validation loss: 1.4497e-03\n",
      "Epoch: 290340 | training loss: 1.8618e-03 | validation loss: 1.4505e-03\n",
      "Epoch: 290350 | training loss: 1.8618e-03 | validation loss: 1.4503e-03\n",
      "Epoch: 290360 | training loss: 1.8618e-03 | validation loss: 1.4502e-03\n",
      "Epoch: 290370 | training loss: 1.8618e-03 | validation loss: 1.4501e-03\n",
      "Epoch: 290380 | training loss: 1.8618e-03 | validation loss: 1.4501e-03\n",
      "Epoch: 290390 | training loss: 1.8617e-03 | validation loss: 1.4503e-03\n",
      "Epoch: 290400 | training loss: 1.8621e-03 | validation loss: 1.4528e-03\n",
      "Epoch: 290410 | training loss: 1.9194e-03 | validation loss: 1.5154e-03\n",
      "Epoch: 290420 | training loss: 3.0740e-03 | validation loss: 2.2120e-03\n",
      "Epoch: 290430 | training loss: 2.2543e-03 | validation loss: 1.7865e-03\n",
      "Epoch: 290440 | training loss: 2.0236e-03 | validation loss: 1.5931e-03\n",
      "Epoch: 290450 | training loss: 1.8953e-03 | validation loss: 1.4903e-03\n",
      "Epoch: 290460 | training loss: 1.8684e-03 | validation loss: 1.4643e-03\n",
      "Epoch: 290470 | training loss: 1.8620e-03 | validation loss: 1.4522e-03\n",
      "Epoch: 290480 | training loss: 1.8620e-03 | validation loss: 1.4484e-03\n",
      "Epoch: 290490 | training loss: 1.8617e-03 | validation loss: 1.4502e-03\n",
      "Epoch: 290500 | training loss: 1.8616e-03 | validation loss: 1.4499e-03\n",
      "Epoch: 290510 | training loss: 1.8616e-03 | validation loss: 1.4499e-03\n",
      "Epoch: 290520 | training loss: 1.8616e-03 | validation loss: 1.4502e-03\n",
      "Epoch: 290530 | training loss: 1.8616e-03 | validation loss: 1.4498e-03\n",
      "Epoch: 290540 | training loss: 1.8615e-03 | validation loss: 1.4496e-03\n",
      "Epoch: 290550 | training loss: 1.8615e-03 | validation loss: 1.4494e-03\n",
      "Epoch: 290560 | training loss: 1.8615e-03 | validation loss: 1.4493e-03\n",
      "Epoch: 290570 | training loss: 1.8615e-03 | validation loss: 1.4494e-03\n",
      "Epoch: 290580 | training loss: 1.8615e-03 | validation loss: 1.4494e-03\n",
      "Epoch: 290590 | training loss: 1.8615e-03 | validation loss: 1.4494e-03\n",
      "Epoch: 290600 | training loss: 1.8615e-03 | validation loss: 1.4493e-03\n",
      "Epoch: 290610 | training loss: 1.8615e-03 | validation loss: 1.4488e-03\n",
      "Epoch: 290620 | training loss: 1.8679e-03 | validation loss: 1.4459e-03\n",
      "Epoch: 290630 | training loss: 3.0062e-03 | validation loss: 1.9313e-03\n",
      "Epoch: 290640 | training loss: 2.8473e-03 | validation loss: 2.0389e-03\n",
      "Epoch: 290650 | training loss: 2.1374e-03 | validation loss: 1.6402e-03\n",
      "Epoch: 290660 | training loss: 1.9312e-03 | validation loss: 1.5130e-03\n",
      "Epoch: 290670 | training loss: 1.8752e-03 | validation loss: 1.4711e-03\n",
      "Epoch: 290680 | training loss: 1.8625e-03 | validation loss: 1.4552e-03\n",
      "Epoch: 290690 | training loss: 1.8622e-03 | validation loss: 1.4488e-03\n",
      "Epoch: 290700 | training loss: 1.8628e-03 | validation loss: 1.4470e-03\n",
      "Epoch: 290710 | training loss: 1.8618e-03 | validation loss: 1.4475e-03\n",
      "Epoch: 290720 | training loss: 1.8613e-03 | validation loss: 1.4491e-03\n",
      "Epoch: 290730 | training loss: 1.8614e-03 | validation loss: 1.4499e-03\n",
      "Epoch: 290740 | training loss: 1.8613e-03 | validation loss: 1.4494e-03\n",
      "Epoch: 290750 | training loss: 1.8613e-03 | validation loss: 1.4491e-03\n",
      "Epoch: 290760 | training loss: 1.8612e-03 | validation loss: 1.4494e-03\n",
      "Epoch: 290770 | training loss: 1.8612e-03 | validation loss: 1.4494e-03\n",
      "Epoch: 290780 | training loss: 1.8612e-03 | validation loss: 1.4492e-03\n",
      "Epoch: 290790 | training loss: 1.8612e-03 | validation loss: 1.4493e-03\n",
      "Epoch: 290800 | training loss: 1.8612e-03 | validation loss: 1.4492e-03\n",
      "Epoch: 290810 | training loss: 1.8612e-03 | validation loss: 1.4492e-03\n",
      "Epoch: 290820 | training loss: 1.8612e-03 | validation loss: 1.4492e-03\n",
      "Epoch: 290830 | training loss: 1.8611e-03 | validation loss: 1.4492e-03\n",
      "Epoch: 290840 | training loss: 1.8611e-03 | validation loss: 1.4492e-03\n",
      "Epoch: 290850 | training loss: 1.8611e-03 | validation loss: 1.4491e-03\n",
      "Epoch: 290860 | training loss: 1.8611e-03 | validation loss: 1.4491e-03\n",
      "Epoch: 290870 | training loss: 1.8611e-03 | validation loss: 1.4491e-03\n",
      "Epoch: 290880 | training loss: 1.8611e-03 | validation loss: 1.4490e-03\n",
      "Epoch: 290890 | training loss: 1.8611e-03 | validation loss: 1.4487e-03\n",
      "Epoch: 290900 | training loss: 1.8632e-03 | validation loss: 1.4458e-03\n",
      "Epoch: 290910 | training loss: 2.3192e-03 | validation loss: 1.6102e-03\n",
      "Epoch: 290920 | training loss: 2.5399e-03 | validation loss: 1.8577e-03\n",
      "Epoch: 290930 | training loss: 1.9491e-03 | validation loss: 1.5068e-03\n",
      "Epoch: 290940 | training loss: 1.9036e-03 | validation loss: 1.4770e-03\n",
      "Epoch: 290950 | training loss: 1.8824e-03 | validation loss: 1.4659e-03\n",
      "Epoch: 290960 | training loss: 1.8696e-03 | validation loss: 1.4596e-03\n",
      "Epoch: 290970 | training loss: 1.8635e-03 | validation loss: 1.4543e-03\n",
      "Epoch: 290980 | training loss: 1.8614e-03 | validation loss: 1.4504e-03\n",
      "Epoch: 290990 | training loss: 1.8610e-03 | validation loss: 1.4489e-03\n",
      "Epoch: 291000 | training loss: 1.8610e-03 | validation loss: 1.4489e-03\n",
      "Epoch: 291010 | training loss: 1.8610e-03 | validation loss: 1.4487e-03\n",
      "Epoch: 291020 | training loss: 1.8609e-03 | validation loss: 1.4487e-03\n",
      "Epoch: 291030 | training loss: 1.8609e-03 | validation loss: 1.4491e-03\n",
      "Epoch: 291040 | training loss: 1.8609e-03 | validation loss: 1.4491e-03\n",
      "Epoch: 291050 | training loss: 1.8609e-03 | validation loss: 1.4490e-03\n",
      "Epoch: 291060 | training loss: 1.8608e-03 | validation loss: 1.4489e-03\n",
      "Epoch: 291070 | training loss: 1.8608e-03 | validation loss: 1.4489e-03\n",
      "Epoch: 291080 | training loss: 1.8608e-03 | validation loss: 1.4489e-03\n",
      "Epoch: 291090 | training loss: 1.8608e-03 | validation loss: 1.4489e-03\n",
      "Epoch: 291100 | training loss: 1.8608e-03 | validation loss: 1.4489e-03\n",
      "Epoch: 291110 | training loss: 1.8608e-03 | validation loss: 1.4488e-03\n",
      "Epoch: 291120 | training loss: 1.8608e-03 | validation loss: 1.4488e-03\n",
      "Epoch: 291130 | training loss: 1.8607e-03 | validation loss: 1.4486e-03\n",
      "Epoch: 291140 | training loss: 1.8611e-03 | validation loss: 1.4463e-03\n",
      "Epoch: 291150 | training loss: 1.9564e-03 | validation loss: 1.4712e-03\n",
      "Epoch: 291160 | training loss: 2.1408e-03 | validation loss: 1.5453e-03\n",
      "Epoch: 291170 | training loss: 1.8632e-03 | validation loss: 1.4545e-03\n",
      "Epoch: 291180 | training loss: 1.8804e-03 | validation loss: 1.4796e-03\n",
      "Epoch: 291190 | training loss: 1.8817e-03 | validation loss: 1.4529e-03\n",
      "Epoch: 291200 | training loss: 1.8700e-03 | validation loss: 1.4664e-03\n",
      "Epoch: 291210 | training loss: 1.8641e-03 | validation loss: 1.4502e-03\n",
      "Epoch: 291220 | training loss: 1.8619e-03 | validation loss: 1.4534e-03\n",
      "Epoch: 291230 | training loss: 1.8611e-03 | validation loss: 1.4479e-03\n",
      "Epoch: 291240 | training loss: 1.8608e-03 | validation loss: 1.4501e-03\n",
      "Epoch: 291250 | training loss: 1.8606e-03 | validation loss: 1.4484e-03\n",
      "Epoch: 291260 | training loss: 1.8606e-03 | validation loss: 1.4475e-03\n",
      "Epoch: 291270 | training loss: 1.8606e-03 | validation loss: 1.4476e-03\n",
      "Epoch: 291280 | training loss: 1.8606e-03 | validation loss: 1.4474e-03\n",
      "Epoch: 291290 | training loss: 1.8611e-03 | validation loss: 1.4456e-03\n",
      "Epoch: 291300 | training loss: 1.8850e-03 | validation loss: 1.4418e-03\n",
      "Epoch: 291310 | training loss: 3.0372e-03 | validation loss: 1.8961e-03\n",
      "Epoch: 291320 | training loss: 2.3942e-03 | validation loss: 1.7859e-03\n",
      "Epoch: 291330 | training loss: 1.8609e-03 | validation loss: 1.4505e-03\n",
      "Epoch: 291340 | training loss: 1.9264e-03 | validation loss: 1.4540e-03\n",
      "Epoch: 291350 | training loss: 1.8607e-03 | validation loss: 1.4489e-03\n",
      "Epoch: 291360 | training loss: 1.8690e-03 | validation loss: 1.4630e-03\n",
      "Epoch: 291370 | training loss: 1.8615e-03 | validation loss: 1.4454e-03\n",
      "Epoch: 291380 | training loss: 1.8605e-03 | validation loss: 1.4469e-03\n",
      "Epoch: 291390 | training loss: 1.8607e-03 | validation loss: 1.4499e-03\n",
      "Epoch: 291400 | training loss: 1.8605e-03 | validation loss: 1.4469e-03\n",
      "Epoch: 291410 | training loss: 1.8604e-03 | validation loss: 1.4490e-03\n",
      "Epoch: 291420 | training loss: 1.8604e-03 | validation loss: 1.4475e-03\n",
      "Epoch: 291430 | training loss: 1.8603e-03 | validation loss: 1.4482e-03\n",
      "Epoch: 291440 | training loss: 1.8603e-03 | validation loss: 1.4481e-03\n",
      "Epoch: 291450 | training loss: 1.8603e-03 | validation loss: 1.4478e-03\n",
      "Epoch: 291460 | training loss: 1.8603e-03 | validation loss: 1.4479e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 291470 | training loss: 1.8603e-03 | validation loss: 1.4478e-03\n",
      "Epoch: 291480 | training loss: 1.8603e-03 | validation loss: 1.4475e-03\n",
      "Epoch: 291490 | training loss: 1.8608e-03 | validation loss: 1.4458e-03\n",
      "Epoch: 291500 | training loss: 1.8984e-03 | validation loss: 1.4460e-03\n",
      "Epoch: 291510 | training loss: 3.7354e-03 | validation loss: 2.2224e-03\n",
      "Epoch: 291520 | training loss: 1.9544e-03 | validation loss: 1.5194e-03\n",
      "Epoch: 291530 | training loss: 2.0552e-03 | validation loss: 1.5905e-03\n",
      "Epoch: 291540 | training loss: 1.9056e-03 | validation loss: 1.4986e-03\n",
      "Epoch: 291550 | training loss: 1.8606e-03 | validation loss: 1.4487e-03\n",
      "Epoch: 291560 | training loss: 1.8682e-03 | validation loss: 1.4401e-03\n",
      "Epoch: 291570 | training loss: 1.8615e-03 | validation loss: 1.4441e-03\n",
      "Epoch: 291580 | training loss: 1.8608e-03 | validation loss: 1.4515e-03\n",
      "Epoch: 291590 | training loss: 1.8603e-03 | validation loss: 1.4494e-03\n",
      "Epoch: 291600 | training loss: 1.8603e-03 | validation loss: 1.4467e-03\n",
      "Epoch: 291610 | training loss: 1.8601e-03 | validation loss: 1.4479e-03\n",
      "Epoch: 291620 | training loss: 1.8601e-03 | validation loss: 1.4481e-03\n",
      "Epoch: 291630 | training loss: 1.8601e-03 | validation loss: 1.4475e-03\n",
      "Epoch: 291640 | training loss: 1.8600e-03 | validation loss: 1.4479e-03\n",
      "Epoch: 291650 | training loss: 1.8600e-03 | validation loss: 1.4476e-03\n",
      "Epoch: 291660 | training loss: 1.8600e-03 | validation loss: 1.4478e-03\n",
      "Epoch: 291670 | training loss: 1.8600e-03 | validation loss: 1.4477e-03\n",
      "Epoch: 291680 | training loss: 1.8600e-03 | validation loss: 1.4478e-03\n",
      "Epoch: 291690 | training loss: 1.8602e-03 | validation loss: 1.4492e-03\n",
      "Epoch: 291700 | training loss: 1.8896e-03 | validation loss: 1.4822e-03\n",
      "Epoch: 291710 | training loss: 2.2111e-03 | validation loss: 1.7320e-03\n",
      "Epoch: 291720 | training loss: 1.9522e-03 | validation loss: 1.5338e-03\n",
      "Epoch: 291730 | training loss: 1.8602e-03 | validation loss: 1.4498e-03\n",
      "Epoch: 291740 | training loss: 1.8693e-03 | validation loss: 1.4493e-03\n",
      "Epoch: 291750 | training loss: 1.8722e-03 | validation loss: 1.4573e-03\n",
      "Epoch: 291760 | training loss: 2.0131e-03 | validation loss: 1.5677e-03\n",
      "Epoch: 291770 | training loss: 2.3117e-03 | validation loss: 1.7467e-03\n",
      "Epoch: 291780 | training loss: 1.8756e-03 | validation loss: 1.4438e-03\n",
      "Epoch: 291790 | training loss: 1.8917e-03 | validation loss: 1.4448e-03\n",
      "Epoch: 291800 | training loss: 1.8911e-03 | validation loss: 1.4838e-03\n",
      "Epoch: 291810 | training loss: 1.8685e-03 | validation loss: 1.4416e-03\n",
      "Epoch: 291820 | training loss: 1.8614e-03 | validation loss: 1.4525e-03\n",
      "Epoch: 291830 | training loss: 1.8603e-03 | validation loss: 1.4447e-03\n",
      "Epoch: 291840 | training loss: 1.8601e-03 | validation loss: 1.4493e-03\n",
      "Epoch: 291850 | training loss: 1.8600e-03 | validation loss: 1.4457e-03\n",
      "Epoch: 291860 | training loss: 1.8598e-03 | validation loss: 1.4481e-03\n",
      "Epoch: 291870 | training loss: 1.8598e-03 | validation loss: 1.4477e-03\n",
      "Epoch: 291880 | training loss: 1.8597e-03 | validation loss: 1.4470e-03\n",
      "Epoch: 291890 | training loss: 1.8597e-03 | validation loss: 1.4465e-03\n",
      "Epoch: 291900 | training loss: 1.8600e-03 | validation loss: 1.4453e-03\n",
      "Epoch: 291910 | training loss: 1.8692e-03 | validation loss: 1.4408e-03\n",
      "Epoch: 291920 | training loss: 2.3947e-03 | validation loss: 1.6266e-03\n",
      "Epoch: 291930 | training loss: 2.0125e-03 | validation loss: 1.5724e-03\n",
      "Epoch: 291940 | training loss: 2.0747e-03 | validation loss: 1.5076e-03\n",
      "Epoch: 291950 | training loss: 1.8723e-03 | validation loss: 1.4375e-03\n",
      "Epoch: 291960 | training loss: 1.8859e-03 | validation loss: 1.4735e-03\n",
      "Epoch: 291970 | training loss: 1.8623e-03 | validation loss: 1.4532e-03\n",
      "Epoch: 291980 | training loss: 1.8616e-03 | validation loss: 1.4430e-03\n",
      "Epoch: 291990 | training loss: 1.8608e-03 | validation loss: 1.4490e-03\n",
      "Epoch: 292000 | training loss: 1.8600e-03 | validation loss: 1.4478e-03\n",
      "Epoch: 292010 | training loss: 1.8597e-03 | validation loss: 1.4467e-03\n",
      "Epoch: 292020 | training loss: 1.8596e-03 | validation loss: 1.4473e-03\n",
      "Epoch: 292030 | training loss: 1.8595e-03 | validation loss: 1.4469e-03\n",
      "Epoch: 292040 | training loss: 1.8595e-03 | validation loss: 1.4470e-03\n",
      "Epoch: 292050 | training loss: 1.8595e-03 | validation loss: 1.4469e-03\n",
      "Epoch: 292060 | training loss: 1.8595e-03 | validation loss: 1.4470e-03\n",
      "Epoch: 292070 | training loss: 1.8595e-03 | validation loss: 1.4471e-03\n",
      "Epoch: 292080 | training loss: 1.8594e-03 | validation loss: 1.4471e-03\n",
      "Epoch: 292090 | training loss: 1.8595e-03 | validation loss: 1.4474e-03\n",
      "Epoch: 292100 | training loss: 1.8615e-03 | validation loss: 1.4506e-03\n",
      "Epoch: 292110 | training loss: 2.0406e-03 | validation loss: 1.5663e-03\n",
      "Epoch: 292120 | training loss: 2.4416e-03 | validation loss: 1.8782e-03\n",
      "Epoch: 292130 | training loss: 1.9525e-03 | validation loss: 1.4986e-03\n",
      "Epoch: 292140 | training loss: 1.9547e-03 | validation loss: 1.4635e-03\n",
      "Epoch: 292150 | training loss: 1.8723e-03 | validation loss: 1.4472e-03\n",
      "Epoch: 292160 | training loss: 1.8704e-03 | validation loss: 1.4587e-03\n",
      "Epoch: 292170 | training loss: 1.8623e-03 | validation loss: 1.4417e-03\n",
      "Epoch: 292180 | training loss: 1.8604e-03 | validation loss: 1.4446e-03\n",
      "Epoch: 292190 | training loss: 1.8600e-03 | validation loss: 1.4475e-03\n",
      "Epoch: 292200 | training loss: 1.8595e-03 | validation loss: 1.4448e-03\n",
      "Epoch: 292210 | training loss: 1.8593e-03 | validation loss: 1.4474e-03\n",
      "Epoch: 292220 | training loss: 1.8593e-03 | validation loss: 1.4463e-03\n",
      "Epoch: 292230 | training loss: 1.8592e-03 | validation loss: 1.4469e-03\n",
      "Epoch: 292240 | training loss: 1.8592e-03 | validation loss: 1.4468e-03\n",
      "Epoch: 292250 | training loss: 1.8592e-03 | validation loss: 1.4464e-03\n",
      "Epoch: 292260 | training loss: 1.8592e-03 | validation loss: 1.4464e-03\n",
      "Epoch: 292270 | training loss: 1.8592e-03 | validation loss: 1.4463e-03\n",
      "Epoch: 292280 | training loss: 1.8592e-03 | validation loss: 1.4458e-03\n",
      "Epoch: 292290 | training loss: 1.8604e-03 | validation loss: 1.4427e-03\n",
      "Epoch: 292300 | training loss: 1.9549e-03 | validation loss: 1.4554e-03\n",
      "Epoch: 292310 | training loss: 2.8192e-03 | validation loss: 1.8042e-03\n",
      "Epoch: 292320 | training loss: 1.9741e-03 | validation loss: 1.4659e-03\n",
      "Epoch: 292330 | training loss: 1.9440e-03 | validation loss: 1.5027e-03\n",
      "Epoch: 292340 | training loss: 1.8719e-03 | validation loss: 1.4671e-03\n",
      "Epoch: 292350 | training loss: 1.8737e-03 | validation loss: 1.4606e-03\n",
      "Epoch: 292360 | training loss: 1.8616e-03 | validation loss: 1.4508e-03\n",
      "Epoch: 292370 | training loss: 1.8605e-03 | validation loss: 1.4485e-03\n",
      "Epoch: 292380 | training loss: 1.8595e-03 | validation loss: 1.4435e-03\n",
      "Epoch: 292390 | training loss: 1.8590e-03 | validation loss: 1.4462e-03\n",
      "Epoch: 292400 | training loss: 1.8591e-03 | validation loss: 1.4477e-03\n",
      "Epoch: 292410 | training loss: 1.8590e-03 | validation loss: 1.4456e-03\n",
      "Epoch: 292420 | training loss: 1.8590e-03 | validation loss: 1.4464e-03\n",
      "Epoch: 292430 | training loss: 1.8590e-03 | validation loss: 1.4463e-03\n",
      "Epoch: 292440 | training loss: 1.8590e-03 | validation loss: 1.4461e-03\n",
      "Epoch: 292450 | training loss: 1.8589e-03 | validation loss: 1.4463e-03\n",
      "Epoch: 292460 | training loss: 1.8589e-03 | validation loss: 1.4462e-03\n",
      "Epoch: 292470 | training loss: 1.8589e-03 | validation loss: 1.4462e-03\n",
      "Epoch: 292480 | training loss: 1.8589e-03 | validation loss: 1.4461e-03\n",
      "Epoch: 292490 | training loss: 1.8589e-03 | validation loss: 1.4459e-03\n",
      "Epoch: 292500 | training loss: 1.8592e-03 | validation loss: 1.4448e-03\n",
      "Epoch: 292510 | training loss: 1.8974e-03 | validation loss: 1.4502e-03\n",
      "Epoch: 292520 | training loss: 4.0086e-03 | validation loss: 2.3688e-03\n",
      "Epoch: 292530 | training loss: 2.0114e-03 | validation loss: 1.5909e-03\n",
      "Epoch: 292540 | training loss: 1.9713e-03 | validation loss: 1.5643e-03\n",
      "Epoch: 292550 | training loss: 1.8873e-03 | validation loss: 1.4864e-03\n",
      "Epoch: 292560 | training loss: 1.8659e-03 | validation loss: 1.4508e-03\n",
      "Epoch: 292570 | training loss: 1.8648e-03 | validation loss: 1.4435e-03\n",
      "Epoch: 292580 | training loss: 1.8596e-03 | validation loss: 1.4466e-03\n",
      "Epoch: 292590 | training loss: 1.8593e-03 | validation loss: 1.4497e-03\n",
      "Epoch: 292600 | training loss: 1.8587e-03 | validation loss: 1.4460e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 292610 | training loss: 1.8588e-03 | validation loss: 1.4450e-03\n",
      "Epoch: 292620 | training loss: 1.8588e-03 | validation loss: 1.4464e-03\n",
      "Epoch: 292630 | training loss: 1.8587e-03 | validation loss: 1.4456e-03\n",
      "Epoch: 292640 | training loss: 1.8587e-03 | validation loss: 1.4461e-03\n",
      "Epoch: 292650 | training loss: 1.8587e-03 | validation loss: 1.4459e-03\n",
      "Epoch: 292660 | training loss: 1.8587e-03 | validation loss: 1.4460e-03\n",
      "Epoch: 292670 | training loss: 1.8586e-03 | validation loss: 1.4458e-03\n",
      "Epoch: 292680 | training loss: 1.8586e-03 | validation loss: 1.4459e-03\n",
      "Epoch: 292690 | training loss: 1.8586e-03 | validation loss: 1.4459e-03\n",
      "Epoch: 292700 | training loss: 1.8586e-03 | validation loss: 1.4459e-03\n",
      "Epoch: 292710 | training loss: 1.8586e-03 | validation loss: 1.4458e-03\n",
      "Epoch: 292720 | training loss: 1.8586e-03 | validation loss: 1.4459e-03\n",
      "Epoch: 292730 | training loss: 1.8586e-03 | validation loss: 1.4461e-03\n",
      "Epoch: 292740 | training loss: 1.8590e-03 | validation loss: 1.4489e-03\n",
      "Epoch: 292750 | training loss: 1.9416e-03 | validation loss: 1.5279e-03\n",
      "Epoch: 292760 | training loss: 2.7045e-03 | validation loss: 1.9694e-03\n",
      "Epoch: 292770 | training loss: 2.2137e-03 | validation loss: 1.7392e-03\n",
      "Epoch: 292780 | training loss: 1.9648e-03 | validation loss: 1.5522e-03\n",
      "Epoch: 292790 | training loss: 1.8756e-03 | validation loss: 1.4657e-03\n",
      "Epoch: 292800 | training loss: 1.8586e-03 | validation loss: 1.4470e-03\n",
      "Epoch: 292810 | training loss: 1.8595e-03 | validation loss: 1.4454e-03\n",
      "Epoch: 292820 | training loss: 1.8588e-03 | validation loss: 1.4430e-03\n",
      "Epoch: 292830 | training loss: 1.8587e-03 | validation loss: 1.4432e-03\n",
      "Epoch: 292840 | training loss: 1.8585e-03 | validation loss: 1.4450e-03\n",
      "Epoch: 292850 | training loss: 1.8584e-03 | validation loss: 1.4453e-03\n",
      "Epoch: 292860 | training loss: 1.8584e-03 | validation loss: 1.4456e-03\n",
      "Epoch: 292870 | training loss: 1.8584e-03 | validation loss: 1.4458e-03\n",
      "Epoch: 292880 | training loss: 1.8584e-03 | validation loss: 1.4455e-03\n",
      "Epoch: 292890 | training loss: 1.8583e-03 | validation loss: 1.4453e-03\n",
      "Epoch: 292900 | training loss: 1.8583e-03 | validation loss: 1.4451e-03\n",
      "Epoch: 292910 | training loss: 1.8583e-03 | validation loss: 1.4451e-03\n",
      "Epoch: 292920 | training loss: 1.8583e-03 | validation loss: 1.4452e-03\n",
      "Epoch: 292930 | training loss: 1.8583e-03 | validation loss: 1.4452e-03\n",
      "Epoch: 292940 | training loss: 1.8583e-03 | validation loss: 1.4452e-03\n",
      "Epoch: 292950 | training loss: 1.8583e-03 | validation loss: 1.4452e-03\n",
      "Epoch: 292960 | training loss: 1.8583e-03 | validation loss: 1.4456e-03\n",
      "Epoch: 292970 | training loss: 1.8625e-03 | validation loss: 1.4518e-03\n",
      "Epoch: 292980 | training loss: 2.5536e-03 | validation loss: 1.8621e-03\n",
      "Epoch: 292990 | training loss: 2.5329e-03 | validation loss: 1.7115e-03\n",
      "Epoch: 293000 | training loss: 1.8793e-03 | validation loss: 1.4695e-03\n",
      "Epoch: 293010 | training loss: 1.8985e-03 | validation loss: 1.4913e-03\n",
      "Epoch: 293020 | training loss: 1.8926e-03 | validation loss: 1.4843e-03\n",
      "Epoch: 293030 | training loss: 1.8672e-03 | validation loss: 1.4618e-03\n",
      "Epoch: 293040 | training loss: 1.8587e-03 | validation loss: 1.4475e-03\n",
      "Epoch: 293050 | training loss: 1.8599e-03 | validation loss: 1.4442e-03\n",
      "Epoch: 293060 | training loss: 1.8584e-03 | validation loss: 1.4455e-03\n",
      "Epoch: 293070 | training loss: 1.8583e-03 | validation loss: 1.4471e-03\n",
      "Epoch: 293080 | training loss: 1.8581e-03 | validation loss: 1.4454e-03\n",
      "Epoch: 293090 | training loss: 1.8581e-03 | validation loss: 1.4443e-03\n",
      "Epoch: 293100 | training loss: 1.8581e-03 | validation loss: 1.4452e-03\n",
      "Epoch: 293110 | training loss: 1.8581e-03 | validation loss: 1.4450e-03\n",
      "Epoch: 293120 | training loss: 1.8580e-03 | validation loss: 1.4450e-03\n",
      "Epoch: 293130 | training loss: 1.8580e-03 | validation loss: 1.4450e-03\n",
      "Epoch: 293140 | training loss: 1.8580e-03 | validation loss: 1.4450e-03\n",
      "Epoch: 293150 | training loss: 1.8580e-03 | validation loss: 1.4449e-03\n",
      "Epoch: 293160 | training loss: 1.8580e-03 | validation loss: 1.4449e-03\n",
      "Epoch: 293170 | training loss: 1.8580e-03 | validation loss: 1.4449e-03\n",
      "Epoch: 293180 | training loss: 1.8580e-03 | validation loss: 1.4449e-03\n",
      "Epoch: 293190 | training loss: 1.8579e-03 | validation loss: 1.4449e-03\n",
      "Epoch: 293200 | training loss: 1.8579e-03 | validation loss: 1.4448e-03\n",
      "Epoch: 293210 | training loss: 1.8579e-03 | validation loss: 1.4445e-03\n",
      "Epoch: 293220 | training loss: 1.8589e-03 | validation loss: 1.4423e-03\n",
      "Epoch: 293230 | training loss: 2.0160e-03 | validation loss: 1.4840e-03\n",
      "Epoch: 293240 | training loss: 2.0929e-03 | validation loss: 1.5066e-03\n",
      "Epoch: 293250 | training loss: 2.3078e-03 | validation loss: 1.5983e-03\n",
      "Epoch: 293260 | training loss: 2.0394e-03 | validation loss: 1.4955e-03\n",
      "Epoch: 293270 | training loss: 1.9248e-03 | validation loss: 1.4588e-03\n",
      "Epoch: 293280 | training loss: 1.8802e-03 | validation loss: 1.4468e-03\n",
      "Epoch: 293290 | training loss: 1.8630e-03 | validation loss: 1.4423e-03\n",
      "Epoch: 293300 | training loss: 1.8582e-03 | validation loss: 1.4431e-03\n",
      "Epoch: 293310 | training loss: 1.8580e-03 | validation loss: 1.4460e-03\n",
      "Epoch: 293320 | training loss: 1.8582e-03 | validation loss: 1.4468e-03\n",
      "Epoch: 293330 | training loss: 1.8578e-03 | validation loss: 1.4454e-03\n",
      "Epoch: 293340 | training loss: 1.8578e-03 | validation loss: 1.4446e-03\n",
      "Epoch: 293350 | training loss: 1.8577e-03 | validation loss: 1.4445e-03\n",
      "Epoch: 293360 | training loss: 1.8577e-03 | validation loss: 1.4449e-03\n",
      "Epoch: 293370 | training loss: 1.8577e-03 | validation loss: 1.4448e-03\n",
      "Epoch: 293380 | training loss: 1.8577e-03 | validation loss: 1.4446e-03\n",
      "Epoch: 293390 | training loss: 1.8577e-03 | validation loss: 1.4447e-03\n",
      "Epoch: 293400 | training loss: 1.8577e-03 | validation loss: 1.4447e-03\n",
      "Epoch: 293410 | training loss: 1.8576e-03 | validation loss: 1.4447e-03\n",
      "Epoch: 293420 | training loss: 1.8576e-03 | validation loss: 1.4446e-03\n",
      "Epoch: 293430 | training loss: 1.8576e-03 | validation loss: 1.4446e-03\n",
      "Epoch: 293440 | training loss: 1.8576e-03 | validation loss: 1.4447e-03\n",
      "Epoch: 293450 | training loss: 1.8576e-03 | validation loss: 1.4455e-03\n",
      "Epoch: 293460 | training loss: 1.8648e-03 | validation loss: 1.4602e-03\n",
      "Epoch: 293470 | training loss: 2.7510e-03 | validation loss: 2.1669e-03\n",
      "Epoch: 293480 | training loss: 1.8669e-03 | validation loss: 1.4379e-03\n",
      "Epoch: 293490 | training loss: 1.8660e-03 | validation loss: 1.4491e-03\n",
      "Epoch: 293500 | training loss: 1.8598e-03 | validation loss: 1.4400e-03\n",
      "Epoch: 293510 | training loss: 1.8587e-03 | validation loss: 1.4398e-03\n",
      "Epoch: 293520 | training loss: 1.8597e-03 | validation loss: 1.4481e-03\n",
      "Epoch: 293530 | training loss: 1.8589e-03 | validation loss: 1.4400e-03\n",
      "Epoch: 293540 | training loss: 1.8581e-03 | validation loss: 1.4414e-03\n",
      "Epoch: 293550 | training loss: 1.8577e-03 | validation loss: 1.4437e-03\n",
      "Epoch: 293560 | training loss: 1.8577e-03 | validation loss: 1.4455e-03\n",
      "Epoch: 293570 | training loss: 1.8604e-03 | validation loss: 1.4520e-03\n",
      "Epoch: 293580 | training loss: 1.9655e-03 | validation loss: 1.5357e-03\n",
      "Epoch: 293590 | training loss: 2.6810e-03 | validation loss: 1.9490e-03\n",
      "Epoch: 293600 | training loss: 1.8938e-03 | validation loss: 1.4436e-03\n",
      "Epoch: 293610 | training loss: 1.9184e-03 | validation loss: 1.4480e-03\n",
      "Epoch: 293620 | training loss: 1.8904e-03 | validation loss: 1.4803e-03\n",
      "Epoch: 293630 | training loss: 1.8577e-03 | validation loss: 1.4425e-03\n",
      "Epoch: 293640 | training loss: 1.8592e-03 | validation loss: 1.4401e-03\n",
      "Epoch: 293650 | training loss: 1.8590e-03 | validation loss: 1.4490e-03\n",
      "Epoch: 293660 | training loss: 1.8581e-03 | validation loss: 1.4413e-03\n",
      "Epoch: 293670 | training loss: 1.8576e-03 | validation loss: 1.4460e-03\n",
      "Epoch: 293680 | training loss: 1.8574e-03 | validation loss: 1.4431e-03\n",
      "Epoch: 293690 | training loss: 1.8573e-03 | validation loss: 1.4440e-03\n",
      "Epoch: 293700 | training loss: 1.8573e-03 | validation loss: 1.4444e-03\n",
      "Epoch: 293710 | training loss: 1.8572e-03 | validation loss: 1.4438e-03\n",
      "Epoch: 293720 | training loss: 1.8572e-03 | validation loss: 1.4435e-03\n",
      "Epoch: 293730 | training loss: 1.8573e-03 | validation loss: 1.4431e-03\n",
      "Epoch: 293740 | training loss: 1.8581e-03 | validation loss: 1.4410e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 293750 | training loss: 1.8999e-03 | validation loss: 1.4425e-03\n",
      "Epoch: 293760 | training loss: 3.3500e-03 | validation loss: 2.0439e-03\n",
      "Epoch: 293770 | training loss: 2.1443e-03 | validation loss: 1.6351e-03\n",
      "Epoch: 293780 | training loss: 1.9454e-03 | validation loss: 1.5226e-03\n",
      "Epoch: 293790 | training loss: 1.8798e-03 | validation loss: 1.4458e-03\n",
      "Epoch: 293800 | training loss: 1.8719e-03 | validation loss: 1.4356e-03\n",
      "Epoch: 293810 | training loss: 1.8606e-03 | validation loss: 1.4488e-03\n",
      "Epoch: 293820 | training loss: 1.8584e-03 | validation loss: 1.4499e-03\n",
      "Epoch: 293830 | training loss: 1.8579e-03 | validation loss: 1.4406e-03\n",
      "Epoch: 293840 | training loss: 1.8572e-03 | validation loss: 1.4447e-03\n",
      "Epoch: 293850 | training loss: 1.8571e-03 | validation loss: 1.4441e-03\n",
      "Epoch: 293860 | training loss: 1.8571e-03 | validation loss: 1.4433e-03\n",
      "Epoch: 293870 | training loss: 1.8570e-03 | validation loss: 1.4440e-03\n",
      "Epoch: 293880 | training loss: 1.8570e-03 | validation loss: 1.4437e-03\n",
      "Epoch: 293890 | training loss: 1.8570e-03 | validation loss: 1.4435e-03\n",
      "Epoch: 293900 | training loss: 1.8570e-03 | validation loss: 1.4438e-03\n",
      "Epoch: 293910 | training loss: 1.8570e-03 | validation loss: 1.4437e-03\n",
      "Epoch: 293920 | training loss: 1.8569e-03 | validation loss: 1.4437e-03\n",
      "Epoch: 293930 | training loss: 1.8570e-03 | validation loss: 1.4438e-03\n",
      "Epoch: 293940 | training loss: 1.8585e-03 | validation loss: 1.4459e-03\n",
      "Epoch: 293950 | training loss: 2.0041e-03 | validation loss: 1.5509e-03\n",
      "Epoch: 293960 | training loss: 2.3411e-03 | validation loss: 1.6136e-03\n",
      "Epoch: 293970 | training loss: 1.9101e-03 | validation loss: 1.5081e-03\n",
      "Epoch: 293980 | training loss: 1.9075e-03 | validation loss: 1.4567e-03\n",
      "Epoch: 293990 | training loss: 1.8866e-03 | validation loss: 1.4826e-03\n",
      "Epoch: 294000 | training loss: 1.8702e-03 | validation loss: 1.4414e-03\n",
      "Epoch: 294010 | training loss: 1.8621e-03 | validation loss: 1.4564e-03\n",
      "Epoch: 294020 | training loss: 1.8584e-03 | validation loss: 1.4433e-03\n",
      "Epoch: 294030 | training loss: 1.8572e-03 | validation loss: 1.4445e-03\n",
      "Epoch: 294040 | training loss: 1.8572e-03 | validation loss: 1.4463e-03\n",
      "Epoch: 294050 | training loss: 1.8568e-03 | validation loss: 1.4435e-03\n",
      "Epoch: 294060 | training loss: 1.8568e-03 | validation loss: 1.4421e-03\n",
      "Epoch: 294070 | training loss: 1.8571e-03 | validation loss: 1.4410e-03\n",
      "Epoch: 294080 | training loss: 1.8642e-03 | validation loss: 1.4367e-03\n",
      "Epoch: 294090 | training loss: 2.1691e-03 | validation loss: 1.5303e-03\n",
      "Epoch: 294100 | training loss: 1.8762e-03 | validation loss: 1.4443e-03\n",
      "Epoch: 294110 | training loss: 1.9848e-03 | validation loss: 1.4667e-03\n",
      "Epoch: 294120 | training loss: 1.9223e-03 | validation loss: 1.5003e-03\n",
      "Epoch: 294130 | training loss: 1.8570e-03 | validation loss: 1.4416e-03\n",
      "Epoch: 294140 | training loss: 1.8639e-03 | validation loss: 1.4371e-03\n",
      "Epoch: 294150 | training loss: 1.8608e-03 | validation loss: 1.4523e-03\n",
      "Epoch: 294160 | training loss: 1.8578e-03 | validation loss: 1.4403e-03\n",
      "Epoch: 294170 | training loss: 1.8570e-03 | validation loss: 1.4452e-03\n",
      "Epoch: 294180 | training loss: 1.8568e-03 | validation loss: 1.4417e-03\n",
      "Epoch: 294190 | training loss: 1.8567e-03 | validation loss: 1.4442e-03\n",
      "Epoch: 294200 | training loss: 1.8566e-03 | validation loss: 1.4426e-03\n",
      "Epoch: 294210 | training loss: 1.8566e-03 | validation loss: 1.4427e-03\n",
      "Epoch: 294220 | training loss: 1.8565e-03 | validation loss: 1.4432e-03\n",
      "Epoch: 294230 | training loss: 1.8565e-03 | validation loss: 1.4433e-03\n",
      "Epoch: 294240 | training loss: 1.8566e-03 | validation loss: 1.4440e-03\n",
      "Epoch: 294250 | training loss: 1.8588e-03 | validation loss: 1.4492e-03\n",
      "Epoch: 294260 | training loss: 1.9921e-03 | validation loss: 1.5482e-03\n",
      "Epoch: 294270 | training loss: 2.5694e-03 | validation loss: 1.8766e-03\n",
      "Epoch: 294280 | training loss: 1.9364e-03 | validation loss: 1.5192e-03\n",
      "Epoch: 294290 | training loss: 1.9380e-03 | validation loss: 1.4540e-03\n",
      "Epoch: 294300 | training loss: 1.8848e-03 | validation loss: 1.4339e-03\n",
      "Epoch: 294310 | training loss: 1.8619e-03 | validation loss: 1.4541e-03\n",
      "Epoch: 294320 | training loss: 1.8602e-03 | validation loss: 1.4536e-03\n",
      "Epoch: 294330 | training loss: 1.8581e-03 | validation loss: 1.4380e-03\n",
      "Epoch: 294340 | training loss: 1.8564e-03 | validation loss: 1.4434e-03\n",
      "Epoch: 294350 | training loss: 1.8565e-03 | validation loss: 1.4443e-03\n",
      "Epoch: 294360 | training loss: 1.8565e-03 | validation loss: 1.4418e-03\n",
      "Epoch: 294370 | training loss: 1.8564e-03 | validation loss: 1.4436e-03\n",
      "Epoch: 294380 | training loss: 1.8563e-03 | validation loss: 1.4423e-03\n",
      "Epoch: 294390 | training loss: 1.8563e-03 | validation loss: 1.4431e-03\n",
      "Epoch: 294400 | training loss: 1.8563e-03 | validation loss: 1.4428e-03\n",
      "Epoch: 294410 | training loss: 1.8563e-03 | validation loss: 1.4428e-03\n",
      "Epoch: 294420 | training loss: 1.8563e-03 | validation loss: 1.4433e-03\n",
      "Epoch: 294430 | training loss: 1.8585e-03 | validation loss: 1.4478e-03\n",
      "Epoch: 294440 | training loss: 2.0366e-03 | validation loss: 1.5981e-03\n",
      "Epoch: 294450 | training loss: 1.9536e-03 | validation loss: 1.4831e-03\n",
      "Epoch: 294460 | training loss: 1.9137e-03 | validation loss: 1.4787e-03\n",
      "Epoch: 294470 | training loss: 1.9500e-03 | validation loss: 1.4707e-03\n",
      "Epoch: 294480 | training loss: 2.1685e-03 | validation loss: 1.5285e-03\n",
      "Epoch: 294490 | training loss: 1.8633e-03 | validation loss: 1.4487e-03\n",
      "Epoch: 294500 | training loss: 1.8825e-03 | validation loss: 1.4706e-03\n",
      "Epoch: 294510 | training loss: 1.8760e-03 | validation loss: 1.4356e-03\n",
      "Epoch: 294520 | training loss: 1.8579e-03 | validation loss: 1.4400e-03\n",
      "Epoch: 294530 | training loss: 1.8586e-03 | validation loss: 1.4495e-03\n",
      "Epoch: 294540 | training loss: 1.8672e-03 | validation loss: 1.4597e-03\n",
      "Epoch: 294550 | training loss: 1.9503e-03 | validation loss: 1.5246e-03\n",
      "Epoch: 294560 | training loss: 2.3314e-03 | validation loss: 1.7537e-03\n",
      "Epoch: 294570 | training loss: 1.9752e-03 | validation loss: 1.4623e-03\n",
      "Epoch: 294580 | training loss: 1.8810e-03 | validation loss: 1.4706e-03\n",
      "Epoch: 294590 | training loss: 1.8568e-03 | validation loss: 1.4397e-03\n",
      "Epoch: 294600 | training loss: 1.8599e-03 | validation loss: 1.4376e-03\n",
      "Epoch: 294610 | training loss: 1.8601e-03 | validation loss: 1.4510e-03\n",
      "Epoch: 294620 | training loss: 1.8569e-03 | validation loss: 1.4460e-03\n",
      "Epoch: 294630 | training loss: 1.8560e-03 | validation loss: 1.4420e-03\n",
      "Epoch: 294640 | training loss: 1.8563e-03 | validation loss: 1.4405e-03\n",
      "Epoch: 294650 | training loss: 1.8623e-03 | validation loss: 1.4367e-03\n",
      "Epoch: 294660 | training loss: 2.1158e-03 | validation loss: 1.5140e-03\n",
      "Epoch: 294670 | training loss: 1.9707e-03 | validation loss: 1.4710e-03\n",
      "Epoch: 294680 | training loss: 1.9481e-03 | validation loss: 1.4491e-03\n",
      "Epoch: 294690 | training loss: 1.9335e-03 | validation loss: 1.5073e-03\n",
      "Epoch: 294700 | training loss: 1.8605e-03 | validation loss: 1.4505e-03\n",
      "Epoch: 294710 | training loss: 1.8645e-03 | validation loss: 1.4339e-03\n",
      "Epoch: 294720 | training loss: 1.8590e-03 | validation loss: 1.4511e-03\n",
      "Epoch: 294730 | training loss: 1.8563e-03 | validation loss: 1.4400e-03\n",
      "Epoch: 294740 | training loss: 1.8559e-03 | validation loss: 1.4427e-03\n",
      "Epoch: 294750 | training loss: 1.8558e-03 | validation loss: 1.4418e-03\n",
      "Epoch: 294760 | training loss: 1.8558e-03 | validation loss: 1.4429e-03\n",
      "Epoch: 294770 | training loss: 1.8558e-03 | validation loss: 1.4413e-03\n",
      "Epoch: 294780 | training loss: 1.8558e-03 | validation loss: 1.4424e-03\n",
      "Epoch: 294790 | training loss: 1.8558e-03 | validation loss: 1.4422e-03\n",
      "Epoch: 294800 | training loss: 1.8558e-03 | validation loss: 1.4418e-03\n",
      "Epoch: 294810 | training loss: 1.8562e-03 | validation loss: 1.4412e-03\n",
      "Epoch: 294820 | training loss: 1.8795e-03 | validation loss: 1.4501e-03\n",
      "Epoch: 294830 | training loss: 2.3320e-03 | validation loss: 1.7284e-03\n",
      "Epoch: 294840 | training loss: 2.2549e-03 | validation loss: 1.7188e-03\n",
      "Epoch: 294850 | training loss: 1.9518e-03 | validation loss: 1.4853e-03\n",
      "Epoch: 294860 | training loss: 1.8814e-03 | validation loss: 1.4750e-03\n",
      "Epoch: 294870 | training loss: 1.8620e-03 | validation loss: 1.4563e-03\n",
      "Epoch: 294880 | training loss: 1.8640e-03 | validation loss: 1.4340e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 294890 | training loss: 1.8576e-03 | validation loss: 1.4439e-03\n",
      "Epoch: 294900 | training loss: 1.8582e-03 | validation loss: 1.4480e-03\n",
      "Epoch: 294910 | training loss: 1.8578e-03 | validation loss: 1.4489e-03\n",
      "Epoch: 294920 | training loss: 1.8652e-03 | validation loss: 1.4584e-03\n",
      "Epoch: 294930 | training loss: 2.0145e-03 | validation loss: 1.5665e-03\n",
      "Epoch: 294940 | training loss: 2.2492e-03 | validation loss: 1.7042e-03\n",
      "Epoch: 294950 | training loss: 1.9703e-03 | validation loss: 1.4627e-03\n",
      "Epoch: 294960 | training loss: 1.8806e-03 | validation loss: 1.4709e-03\n",
      "Epoch: 294970 | training loss: 1.8624e-03 | validation loss: 1.4349e-03\n",
      "Epoch: 294980 | training loss: 1.8594e-03 | validation loss: 1.4503e-03\n",
      "Epoch: 294990 | training loss: 1.8587e-03 | validation loss: 1.4373e-03\n",
      "Epoch: 295000 | training loss: 1.8568e-03 | validation loss: 1.4460e-03\n",
      "Epoch: 295010 | training loss: 1.8555e-03 | validation loss: 1.4421e-03\n",
      "Epoch: 295020 | training loss: 1.8558e-03 | validation loss: 1.4397e-03\n",
      "Epoch: 295030 | training loss: 1.8560e-03 | validation loss: 1.4392e-03\n",
      "Epoch: 295040 | training loss: 1.8587e-03 | validation loss: 1.4369e-03\n",
      "Epoch: 295050 | training loss: 1.9184e-03 | validation loss: 1.4451e-03\n",
      "Epoch: 295060 | training loss: 2.7794e-03 | validation loss: 1.7915e-03\n",
      "Epoch: 295070 | training loss: 2.1655e-03 | validation loss: 1.6502e-03\n",
      "Epoch: 295080 | training loss: 1.9191e-03 | validation loss: 1.4518e-03\n",
      "Epoch: 295090 | training loss: 1.8598e-03 | validation loss: 1.4421e-03\n",
      "Epoch: 295100 | training loss: 1.8580e-03 | validation loss: 1.4506e-03\n",
      "Epoch: 295110 | training loss: 1.8572e-03 | validation loss: 1.4363e-03\n",
      "Epoch: 295120 | training loss: 1.8561e-03 | validation loss: 1.4458e-03\n",
      "Epoch: 295130 | training loss: 1.8556e-03 | validation loss: 1.4398e-03\n",
      "Epoch: 295140 | training loss: 1.8555e-03 | validation loss: 1.4416e-03\n",
      "Epoch: 295150 | training loss: 1.8553e-03 | validation loss: 1.4425e-03\n",
      "Epoch: 295160 | training loss: 1.8553e-03 | validation loss: 1.4411e-03\n",
      "Epoch: 295170 | training loss: 1.8554e-03 | validation loss: 1.4407e-03\n",
      "Epoch: 295180 | training loss: 1.8560e-03 | validation loss: 1.4405e-03\n",
      "Epoch: 295190 | training loss: 1.8727e-03 | validation loss: 1.4483e-03\n",
      "Epoch: 295200 | training loss: 2.2417e-03 | validation loss: 1.6739e-03\n",
      "Epoch: 295210 | training loss: 2.6351e-03 | validation loss: 2.0079e-03\n",
      "Epoch: 295220 | training loss: 2.0295e-03 | validation loss: 1.5017e-03\n",
      "Epoch: 295230 | training loss: 1.9043e-03 | validation loss: 1.4817e-03\n",
      "Epoch: 295240 | training loss: 1.8817e-03 | validation loss: 1.4344e-03\n",
      "Epoch: 295250 | training loss: 1.8632e-03 | validation loss: 1.4485e-03\n",
      "Epoch: 295260 | training loss: 1.8580e-03 | validation loss: 1.4379e-03\n",
      "Epoch: 295270 | training loss: 1.8567e-03 | validation loss: 1.4475e-03\n",
      "Epoch: 295280 | training loss: 1.8551e-03 | validation loss: 1.4413e-03\n",
      "Epoch: 295290 | training loss: 1.8554e-03 | validation loss: 1.4386e-03\n",
      "Epoch: 295300 | training loss: 1.8553e-03 | validation loss: 1.4398e-03\n",
      "Epoch: 295310 | training loss: 1.8557e-03 | validation loss: 1.4385e-03\n",
      "Epoch: 295320 | training loss: 1.8649e-03 | validation loss: 1.4346e-03\n",
      "Epoch: 295330 | training loss: 2.1881e-03 | validation loss: 1.5395e-03\n",
      "Epoch: 295340 | training loss: 1.8756e-03 | validation loss: 1.4429e-03\n",
      "Epoch: 295350 | training loss: 1.9323e-03 | validation loss: 1.4465e-03\n",
      "Epoch: 295360 | training loss: 1.9303e-03 | validation loss: 1.5020e-03\n",
      "Epoch: 295370 | training loss: 1.8648e-03 | validation loss: 1.4380e-03\n",
      "Epoch: 295380 | training loss: 1.8550e-03 | validation loss: 1.4410e-03\n",
      "Epoch: 295390 | training loss: 1.8559e-03 | validation loss: 1.4434e-03\n",
      "Epoch: 295400 | training loss: 1.8555e-03 | validation loss: 1.4393e-03\n",
      "Epoch: 295410 | training loss: 1.8551e-03 | validation loss: 1.4421e-03\n",
      "Epoch: 295420 | training loss: 1.8549e-03 | validation loss: 1.4409e-03\n",
      "Epoch: 295430 | training loss: 1.8550e-03 | validation loss: 1.4402e-03\n",
      "Epoch: 295440 | training loss: 1.8549e-03 | validation loss: 1.4414e-03\n",
      "Epoch: 295450 | training loss: 1.8549e-03 | validation loss: 1.4413e-03\n",
      "Epoch: 295460 | training loss: 1.8549e-03 | validation loss: 1.4413e-03\n",
      "Epoch: 295470 | training loss: 1.8550e-03 | validation loss: 1.4420e-03\n",
      "Epoch: 295480 | training loss: 1.8582e-03 | validation loss: 1.4482e-03\n",
      "Epoch: 295490 | training loss: 2.0480e-03 | validation loss: 1.5787e-03\n",
      "Epoch: 295500 | training loss: 2.1678e-03 | validation loss: 1.6504e-03\n",
      "Epoch: 295510 | training loss: 2.0130e-03 | validation loss: 1.5565e-03\n",
      "Epoch: 295520 | training loss: 1.9056e-03 | validation loss: 1.4423e-03\n",
      "Epoch: 295530 | training loss: 1.8827e-03 | validation loss: 1.4378e-03\n",
      "Epoch: 295540 | training loss: 1.8611e-03 | validation loss: 1.4517e-03\n",
      "Epoch: 295550 | training loss: 1.8567e-03 | validation loss: 1.4461e-03\n",
      "Epoch: 295560 | training loss: 1.8568e-03 | validation loss: 1.4373e-03\n",
      "Epoch: 295570 | training loss: 1.8549e-03 | validation loss: 1.4423e-03\n",
      "Epoch: 295580 | training loss: 1.8547e-03 | validation loss: 1.4409e-03\n",
      "Epoch: 295590 | training loss: 1.8547e-03 | validation loss: 1.4403e-03\n",
      "Epoch: 295600 | training loss: 1.8547e-03 | validation loss: 1.4409e-03\n",
      "Epoch: 295610 | training loss: 1.8546e-03 | validation loss: 1.4406e-03\n",
      "Epoch: 295620 | training loss: 1.8546e-03 | validation loss: 1.4405e-03\n",
      "Epoch: 295630 | training loss: 1.8546e-03 | validation loss: 1.4407e-03\n",
      "Epoch: 295640 | training loss: 1.8547e-03 | validation loss: 1.4398e-03\n",
      "Epoch: 295650 | training loss: 1.8635e-03 | validation loss: 1.4364e-03\n",
      "Epoch: 295660 | training loss: 2.7095e-03 | validation loss: 1.9257e-03\n",
      "Epoch: 295670 | training loss: 1.8705e-03 | validation loss: 1.4346e-03\n",
      "Epoch: 295680 | training loss: 1.8725e-03 | validation loss: 1.4668e-03\n",
      "Epoch: 295690 | training loss: 1.8623e-03 | validation loss: 1.4360e-03\n",
      "Epoch: 295700 | training loss: 1.8561e-03 | validation loss: 1.4407e-03\n",
      "Epoch: 295710 | training loss: 1.8573e-03 | validation loss: 1.4489e-03\n",
      "Epoch: 295720 | training loss: 1.8589e-03 | validation loss: 1.4516e-03\n",
      "Epoch: 295730 | training loss: 1.8738e-03 | validation loss: 1.4677e-03\n",
      "Epoch: 295740 | training loss: 2.1208e-03 | validation loss: 1.6325e-03\n",
      "Epoch: 295750 | training loss: 1.9342e-03 | validation loss: 1.5105e-03\n",
      "Epoch: 295760 | training loss: 1.8875e-03 | validation loss: 1.4345e-03\n",
      "Epoch: 295770 | training loss: 1.8678e-03 | validation loss: 1.4591e-03\n",
      "Epoch: 295780 | training loss: 1.8635e-03 | validation loss: 1.4334e-03\n",
      "Epoch: 295790 | training loss: 1.8613e-03 | validation loss: 1.4529e-03\n",
      "Epoch: 295800 | training loss: 1.8564e-03 | validation loss: 1.4360e-03\n",
      "Epoch: 295810 | training loss: 1.8547e-03 | validation loss: 1.4383e-03\n",
      "Epoch: 295820 | training loss: 1.8548e-03 | validation loss: 1.4425e-03\n",
      "Epoch: 295830 | training loss: 1.8556e-03 | validation loss: 1.4446e-03\n",
      "Epoch: 295840 | training loss: 1.8629e-03 | validation loss: 1.4547e-03\n",
      "Epoch: 295850 | training loss: 2.0238e-03 | validation loss: 1.5694e-03\n",
      "Epoch: 295860 | training loss: 2.2579e-03 | validation loss: 1.7050e-03\n",
      "Epoch: 295870 | training loss: 1.9338e-03 | validation loss: 1.4515e-03\n",
      "Epoch: 295880 | training loss: 1.8578e-03 | validation loss: 1.4476e-03\n",
      "Epoch: 295890 | training loss: 1.8547e-03 | validation loss: 1.4411e-03\n",
      "Epoch: 295900 | training loss: 1.8548e-03 | validation loss: 1.4378e-03\n",
      "Epoch: 295910 | training loss: 1.8543e-03 | validation loss: 1.4406e-03\n",
      "Epoch: 295920 | training loss: 1.8546e-03 | validation loss: 1.4419e-03\n",
      "Epoch: 295930 | training loss: 1.8549e-03 | validation loss: 1.4375e-03\n",
      "Epoch: 295940 | training loss: 1.8542e-03 | validation loss: 1.4405e-03\n",
      "Epoch: 295950 | training loss: 1.8544e-03 | validation loss: 1.4415e-03\n",
      "Epoch: 295960 | training loss: 1.8546e-03 | validation loss: 1.4421e-03\n",
      "Epoch: 295970 | training loss: 1.8570e-03 | validation loss: 1.4468e-03\n",
      "Epoch: 295980 | training loss: 1.9249e-03 | validation loss: 1.5020e-03\n",
      "Epoch: 295990 | training loss: 2.8875e-03 | validation loss: 2.0498e-03\n",
      "Epoch: 296000 | training loss: 2.1380e-03 | validation loss: 1.5263e-03\n",
      "Epoch: 296010 | training loss: 1.8628e-03 | validation loss: 1.4480e-03\n",
      "Epoch: 296020 | training loss: 1.8708e-03 | validation loss: 1.4637e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 296030 | training loss: 1.8689e-03 | validation loss: 1.4334e-03\n",
      "Epoch: 296040 | training loss: 1.8589e-03 | validation loss: 1.4496e-03\n",
      "Epoch: 296050 | training loss: 1.8554e-03 | validation loss: 1.4366e-03\n",
      "Epoch: 296060 | training loss: 1.8546e-03 | validation loss: 1.4427e-03\n",
      "Epoch: 296070 | training loss: 1.8543e-03 | validation loss: 1.4376e-03\n",
      "Epoch: 296080 | training loss: 1.8541e-03 | validation loss: 1.4413e-03\n",
      "Epoch: 296090 | training loss: 1.8540e-03 | validation loss: 1.4399e-03\n",
      "Epoch: 296100 | training loss: 1.8540e-03 | validation loss: 1.4390e-03\n",
      "Epoch: 296110 | training loss: 1.8540e-03 | validation loss: 1.4388e-03\n",
      "Epoch: 296120 | training loss: 1.8541e-03 | validation loss: 1.4378e-03\n",
      "Epoch: 296130 | training loss: 1.8589e-03 | validation loss: 1.4332e-03\n",
      "Epoch: 296140 | training loss: 2.1547e-03 | validation loss: 1.5746e-03\n",
      "Epoch: 296150 | training loss: 2.2285e-03 | validation loss: 1.6607e-03\n",
      "Epoch: 296160 | training loss: 1.8748e-03 | validation loss: 1.4318e-03\n",
      "Epoch: 296170 | training loss: 1.9122e-03 | validation loss: 1.4779e-03\n",
      "Epoch: 296180 | training loss: 1.8679e-03 | validation loss: 1.4392e-03\n",
      "Epoch: 296190 | training loss: 1.8618e-03 | validation loss: 1.4314e-03\n",
      "Epoch: 296200 | training loss: 1.8680e-03 | validation loss: 1.4354e-03\n",
      "Epoch: 296210 | training loss: 1.9226e-03 | validation loss: 1.4461e-03\n",
      "Epoch: 296220 | training loss: 2.2462e-03 | validation loss: 1.5620e-03\n",
      "Epoch: 296230 | training loss: 1.8681e-03 | validation loss: 1.4580e-03\n",
      "Epoch: 296240 | training loss: 1.8618e-03 | validation loss: 1.4519e-03\n",
      "Epoch: 296250 | training loss: 1.8766e-03 | validation loss: 1.4341e-03\n",
      "Epoch: 296260 | training loss: 1.8588e-03 | validation loss: 1.4494e-03\n",
      "Epoch: 296270 | training loss: 1.8582e-03 | validation loss: 1.4485e-03\n",
      "Epoch: 296280 | training loss: 1.8540e-03 | validation loss: 1.4411e-03\n",
      "Epoch: 296290 | training loss: 1.8537e-03 | validation loss: 1.4398e-03\n",
      "Epoch: 296300 | training loss: 1.8538e-03 | validation loss: 1.4405e-03\n",
      "Epoch: 296310 | training loss: 1.8591e-03 | validation loss: 1.4496e-03\n",
      "Epoch: 296320 | training loss: 2.2699e-03 | validation loss: 1.7107e-03\n",
      "Epoch: 296330 | training loss: 1.9268e-03 | validation loss: 1.4426e-03\n",
      "Epoch: 296340 | training loss: 2.1145e-03 | validation loss: 1.6145e-03\n",
      "Epoch: 296350 | training loss: 1.9064e-03 | validation loss: 1.4889e-03\n",
      "Epoch: 296360 | training loss: 1.8619e-03 | validation loss: 1.4378e-03\n",
      "Epoch: 296370 | training loss: 1.8666e-03 | validation loss: 1.4355e-03\n",
      "Epoch: 296380 | training loss: 1.8540e-03 | validation loss: 1.4399e-03\n",
      "Epoch: 296390 | training loss: 1.8552e-03 | validation loss: 1.4431e-03\n",
      "Epoch: 296400 | training loss: 1.8540e-03 | validation loss: 1.4380e-03\n",
      "Epoch: 296410 | training loss: 1.8536e-03 | validation loss: 1.4391e-03\n",
      "Epoch: 296420 | training loss: 1.8536e-03 | validation loss: 1.4396e-03\n",
      "Epoch: 296430 | training loss: 1.8536e-03 | validation loss: 1.4387e-03\n",
      "Epoch: 296440 | training loss: 1.8535e-03 | validation loss: 1.4393e-03\n",
      "Epoch: 296450 | training loss: 1.8535e-03 | validation loss: 1.4389e-03\n",
      "Epoch: 296460 | training loss: 1.8535e-03 | validation loss: 1.4391e-03\n",
      "Epoch: 296470 | training loss: 1.8535e-03 | validation loss: 1.4390e-03\n",
      "Epoch: 296480 | training loss: 1.8535e-03 | validation loss: 1.4389e-03\n",
      "Epoch: 296490 | training loss: 1.8535e-03 | validation loss: 1.4389e-03\n",
      "Epoch: 296500 | training loss: 1.8534e-03 | validation loss: 1.4389e-03\n",
      "Epoch: 296510 | training loss: 1.8534e-03 | validation loss: 1.4388e-03\n",
      "Epoch: 296520 | training loss: 1.8534e-03 | validation loss: 1.4384e-03\n",
      "Epoch: 296530 | training loss: 1.8561e-03 | validation loss: 1.4356e-03\n",
      "Epoch: 296540 | training loss: 2.2114e-03 | validation loss: 1.5621e-03\n",
      "Epoch: 296550 | training loss: 1.9944e-03 | validation loss: 1.5299e-03\n",
      "Epoch: 296560 | training loss: 1.9626e-03 | validation loss: 1.4558e-03\n",
      "Epoch: 296570 | training loss: 1.9381e-03 | validation loss: 1.4692e-03\n",
      "Epoch: 296580 | training loss: 1.8984e-03 | validation loss: 1.4628e-03\n",
      "Epoch: 296590 | training loss: 1.8712e-03 | validation loss: 1.4462e-03\n",
      "Epoch: 296600 | training loss: 1.8580e-03 | validation loss: 1.4384e-03\n",
      "Epoch: 296610 | training loss: 1.8535e-03 | validation loss: 1.4393e-03\n",
      "Epoch: 296620 | training loss: 1.8535e-03 | validation loss: 1.4395e-03\n",
      "Epoch: 296630 | training loss: 1.8536e-03 | validation loss: 1.4394e-03\n",
      "Epoch: 296640 | training loss: 1.8533e-03 | validation loss: 1.4391e-03\n",
      "Epoch: 296650 | training loss: 1.8533e-03 | validation loss: 1.4387e-03\n",
      "Epoch: 296660 | training loss: 1.8532e-03 | validation loss: 1.4388e-03\n",
      "Epoch: 296670 | training loss: 1.8532e-03 | validation loss: 1.4389e-03\n",
      "Epoch: 296680 | training loss: 1.8532e-03 | validation loss: 1.4387e-03\n",
      "Epoch: 296690 | training loss: 1.8532e-03 | validation loss: 1.4387e-03\n",
      "Epoch: 296700 | training loss: 1.8532e-03 | validation loss: 1.4387e-03\n",
      "Epoch: 296710 | training loss: 1.8532e-03 | validation loss: 1.4387e-03\n",
      "Epoch: 296720 | training loss: 1.8531e-03 | validation loss: 1.4387e-03\n",
      "Epoch: 296730 | training loss: 1.8531e-03 | validation loss: 1.4386e-03\n",
      "Epoch: 296740 | training loss: 1.8531e-03 | validation loss: 1.4383e-03\n",
      "Epoch: 296750 | training loss: 1.8546e-03 | validation loss: 1.4340e-03\n",
      "Epoch: 296760 | training loss: 2.3856e-03 | validation loss: 1.6697e-03\n",
      "Epoch: 296770 | training loss: 2.4206e-03 | validation loss: 1.9010e-03\n",
      "Epoch: 296780 | training loss: 2.0012e-03 | validation loss: 1.5232e-03\n",
      "Epoch: 296790 | training loss: 1.8707e-03 | validation loss: 1.4303e-03\n",
      "Epoch: 296800 | training loss: 1.8872e-03 | validation loss: 1.4548e-03\n",
      "Epoch: 296810 | training loss: 1.8577e-03 | validation loss: 1.4380e-03\n",
      "Epoch: 296820 | training loss: 1.8545e-03 | validation loss: 1.4354e-03\n",
      "Epoch: 296830 | training loss: 1.8540e-03 | validation loss: 1.4422e-03\n",
      "Epoch: 296840 | training loss: 1.8534e-03 | validation loss: 1.4399e-03\n",
      "Epoch: 296850 | training loss: 1.8531e-03 | validation loss: 1.4386e-03\n",
      "Epoch: 296860 | training loss: 1.8530e-03 | validation loss: 1.4388e-03\n",
      "Epoch: 296870 | training loss: 1.8530e-03 | validation loss: 1.4380e-03\n",
      "Epoch: 296880 | training loss: 1.8529e-03 | validation loss: 1.4380e-03\n",
      "Epoch: 296890 | training loss: 1.8529e-03 | validation loss: 1.4382e-03\n",
      "Epoch: 296900 | training loss: 1.8529e-03 | validation loss: 1.4380e-03\n",
      "Epoch: 296910 | training loss: 1.8529e-03 | validation loss: 1.4379e-03\n",
      "Epoch: 296920 | training loss: 1.8529e-03 | validation loss: 1.4379e-03\n",
      "Epoch: 296930 | training loss: 1.8529e-03 | validation loss: 1.4376e-03\n",
      "Epoch: 296940 | training loss: 1.8530e-03 | validation loss: 1.4365e-03\n",
      "Epoch: 296950 | training loss: 1.8648e-03 | validation loss: 1.4317e-03\n",
      "Epoch: 296960 | training loss: 2.9804e-03 | validation loss: 1.8712e-03\n",
      "Epoch: 296970 | training loss: 2.6414e-03 | validation loss: 1.9153e-03\n",
      "Epoch: 296980 | training loss: 1.9216e-03 | validation loss: 1.4970e-03\n",
      "Epoch: 296990 | training loss: 1.8620e-03 | validation loss: 1.4305e-03\n",
      "Epoch: 297000 | training loss: 1.8828e-03 | validation loss: 1.4337e-03\n",
      "Epoch: 297010 | training loss: 1.8601e-03 | validation loss: 1.4323e-03\n",
      "Epoch: 297020 | training loss: 1.8532e-03 | validation loss: 1.4401e-03\n",
      "Epoch: 297030 | training loss: 1.8543e-03 | validation loss: 1.4426e-03\n",
      "Epoch: 297040 | training loss: 1.8528e-03 | validation loss: 1.4375e-03\n",
      "Epoch: 297050 | training loss: 1.8529e-03 | validation loss: 1.4367e-03\n",
      "Epoch: 297060 | training loss: 1.8528e-03 | validation loss: 1.4387e-03\n",
      "Epoch: 297070 | training loss: 1.8527e-03 | validation loss: 1.4378e-03\n",
      "Epoch: 297080 | training loss: 1.8527e-03 | validation loss: 1.4376e-03\n",
      "Epoch: 297090 | training loss: 1.8527e-03 | validation loss: 1.4379e-03\n",
      "Epoch: 297100 | training loss: 1.8526e-03 | validation loss: 1.4377e-03\n",
      "Epoch: 297110 | training loss: 1.8526e-03 | validation loss: 1.4378e-03\n",
      "Epoch: 297120 | training loss: 1.8526e-03 | validation loss: 1.4378e-03\n",
      "Epoch: 297130 | training loss: 1.8526e-03 | validation loss: 1.4377e-03\n",
      "Epoch: 297140 | training loss: 1.8526e-03 | validation loss: 1.4377e-03\n",
      "Epoch: 297150 | training loss: 1.8526e-03 | validation loss: 1.4377e-03\n",
      "Epoch: 297160 | training loss: 1.8526e-03 | validation loss: 1.4377e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 297170 | training loss: 1.8526e-03 | validation loss: 1.4378e-03\n",
      "Epoch: 297180 | training loss: 1.8526e-03 | validation loss: 1.4387e-03\n",
      "Epoch: 297190 | training loss: 1.8603e-03 | validation loss: 1.4504e-03\n",
      "Epoch: 297200 | training loss: 2.9171e-03 | validation loss: 2.0634e-03\n",
      "Epoch: 297210 | training loss: 2.8293e-03 | validation loss: 1.8142e-03\n",
      "Epoch: 297220 | training loss: 2.0430e-03 | validation loss: 1.4806e-03\n",
      "Epoch: 297230 | training loss: 1.8734e-03 | validation loss: 1.4282e-03\n",
      "Epoch: 297240 | training loss: 1.8528e-03 | validation loss: 1.4368e-03\n",
      "Epoch: 297250 | training loss: 1.8574e-03 | validation loss: 1.4484e-03\n",
      "Epoch: 297260 | training loss: 1.8576e-03 | validation loss: 1.4487e-03\n",
      "Epoch: 297270 | training loss: 1.8536e-03 | validation loss: 1.4418e-03\n",
      "Epoch: 297280 | training loss: 1.8524e-03 | validation loss: 1.4370e-03\n",
      "Epoch: 297290 | training loss: 1.8527e-03 | validation loss: 1.4362e-03\n",
      "Epoch: 297300 | training loss: 1.8524e-03 | validation loss: 1.4374e-03\n",
      "Epoch: 297310 | training loss: 1.8524e-03 | validation loss: 1.4382e-03\n",
      "Epoch: 297320 | training loss: 1.8524e-03 | validation loss: 1.4374e-03\n",
      "Epoch: 297330 | training loss: 1.8523e-03 | validation loss: 1.4374e-03\n",
      "Epoch: 297340 | training loss: 1.8523e-03 | validation loss: 1.4376e-03\n",
      "Epoch: 297350 | training loss: 1.8523e-03 | validation loss: 1.4374e-03\n",
      "Epoch: 297360 | training loss: 1.8523e-03 | validation loss: 1.4375e-03\n",
      "Epoch: 297370 | training loss: 1.8523e-03 | validation loss: 1.4374e-03\n",
      "Epoch: 297380 | training loss: 1.8523e-03 | validation loss: 1.4374e-03\n",
      "Epoch: 297390 | training loss: 1.8523e-03 | validation loss: 1.4374e-03\n",
      "Epoch: 297400 | training loss: 1.8522e-03 | validation loss: 1.4373e-03\n",
      "Epoch: 297410 | training loss: 1.8523e-03 | validation loss: 1.4366e-03\n",
      "Epoch: 297420 | training loss: 1.8623e-03 | validation loss: 1.4351e-03\n",
      "Epoch: 297430 | training loss: 2.6400e-03 | validation loss: 1.9006e-03\n",
      "Epoch: 297440 | training loss: 1.8898e-03 | validation loss: 1.4348e-03\n",
      "Epoch: 297450 | training loss: 1.8537e-03 | validation loss: 1.4432e-03\n",
      "Epoch: 297460 | training loss: 1.8614e-03 | validation loss: 1.4552e-03\n",
      "Epoch: 297470 | training loss: 1.8651e-03 | validation loss: 1.4594e-03\n",
      "Epoch: 297480 | training loss: 1.9020e-03 | validation loss: 1.4919e-03\n",
      "Epoch: 297490 | training loss: 2.2324e-03 | validation loss: 1.7005e-03\n",
      "Epoch: 297500 | training loss: 1.8556e-03 | validation loss: 1.4336e-03\n",
      "Epoch: 297510 | training loss: 1.8605e-03 | validation loss: 1.4309e-03\n",
      "Epoch: 297520 | training loss: 1.8711e-03 | validation loss: 1.4621e-03\n",
      "Epoch: 297530 | training loss: 1.8634e-03 | validation loss: 1.4299e-03\n",
      "Epoch: 297540 | training loss: 1.8521e-03 | validation loss: 1.4363e-03\n",
      "Epoch: 297550 | training loss: 1.8550e-03 | validation loss: 1.4445e-03\n",
      "Epoch: 297560 | training loss: 1.8563e-03 | validation loss: 1.4465e-03\n",
      "Epoch: 297570 | training loss: 1.8723e-03 | validation loss: 1.4633e-03\n",
      "Epoch: 297580 | training loss: 2.1148e-03 | validation loss: 1.6243e-03\n",
      "Epoch: 297590 | training loss: 1.9475e-03 | validation loss: 1.5160e-03\n",
      "Epoch: 297600 | training loss: 1.9005e-03 | validation loss: 1.4361e-03\n",
      "Epoch: 297610 | training loss: 1.8756e-03 | validation loss: 1.4653e-03\n",
      "Epoch: 297620 | training loss: 1.8667e-03 | validation loss: 1.4307e-03\n",
      "Epoch: 297630 | training loss: 1.8592e-03 | validation loss: 1.4494e-03\n",
      "Epoch: 297640 | training loss: 1.8523e-03 | validation loss: 1.4348e-03\n",
      "Epoch: 297650 | training loss: 1.8533e-03 | validation loss: 1.4334e-03\n",
      "Epoch: 297660 | training loss: 1.8520e-03 | validation loss: 1.4358e-03\n",
      "Epoch: 297670 | training loss: 1.8519e-03 | validation loss: 1.4371e-03\n",
      "Epoch: 297680 | training loss: 1.8521e-03 | validation loss: 1.4384e-03\n",
      "Epoch: 297690 | training loss: 1.8586e-03 | validation loss: 1.4489e-03\n",
      "Epoch: 297700 | training loss: 2.2784e-03 | validation loss: 1.7158e-03\n",
      "Epoch: 297710 | training loss: 1.9063e-03 | validation loss: 1.4336e-03\n",
      "Epoch: 297720 | training loss: 2.1131e-03 | validation loss: 1.6145e-03\n",
      "Epoch: 297730 | training loss: 1.8662e-03 | validation loss: 1.4617e-03\n",
      "Epoch: 297740 | training loss: 1.8789e-03 | validation loss: 1.4355e-03\n",
      "Epoch: 297750 | training loss: 1.8553e-03 | validation loss: 1.4309e-03\n",
      "Epoch: 297760 | training loss: 1.8551e-03 | validation loss: 1.4443e-03\n",
      "Epoch: 297770 | training loss: 1.8525e-03 | validation loss: 1.4379e-03\n",
      "Epoch: 297780 | training loss: 1.8521e-03 | validation loss: 1.4345e-03\n",
      "Epoch: 297790 | training loss: 1.8519e-03 | validation loss: 1.4384e-03\n",
      "Epoch: 297800 | training loss: 1.8517e-03 | validation loss: 1.4358e-03\n",
      "Epoch: 297810 | training loss: 1.8517e-03 | validation loss: 1.4371e-03\n",
      "Epoch: 297820 | training loss: 1.8517e-03 | validation loss: 1.4362e-03\n",
      "Epoch: 297830 | training loss: 1.8517e-03 | validation loss: 1.4368e-03\n",
      "Epoch: 297840 | training loss: 1.8517e-03 | validation loss: 1.4364e-03\n",
      "Epoch: 297850 | training loss: 1.8516e-03 | validation loss: 1.4364e-03\n",
      "Epoch: 297860 | training loss: 1.8516e-03 | validation loss: 1.4365e-03\n",
      "Epoch: 297870 | training loss: 1.8516e-03 | validation loss: 1.4365e-03\n",
      "Epoch: 297880 | training loss: 1.8520e-03 | validation loss: 1.4367e-03\n",
      "Epoch: 297890 | training loss: 1.8759e-03 | validation loss: 1.4507e-03\n",
      "Epoch: 297900 | training loss: 2.6725e-03 | validation loss: 1.9203e-03\n",
      "Epoch: 297910 | training loss: 1.8960e-03 | validation loss: 1.4536e-03\n",
      "Epoch: 297920 | training loss: 1.8986e-03 | validation loss: 1.4786e-03\n",
      "Epoch: 297930 | training loss: 1.8886e-03 | validation loss: 1.4299e-03\n",
      "Epoch: 297940 | training loss: 1.8609e-03 | validation loss: 1.4490e-03\n",
      "Epoch: 297950 | training loss: 1.8528e-03 | validation loss: 1.4316e-03\n",
      "Epoch: 297960 | training loss: 1.8517e-03 | validation loss: 1.4379e-03\n",
      "Epoch: 297970 | training loss: 1.8517e-03 | validation loss: 1.4356e-03\n",
      "Epoch: 297980 | training loss: 1.8518e-03 | validation loss: 1.4388e-03\n",
      "Epoch: 297990 | training loss: 1.8517e-03 | validation loss: 1.4357e-03\n",
      "Epoch: 298000 | training loss: 1.8515e-03 | validation loss: 1.4364e-03\n",
      "Epoch: 298010 | training loss: 1.8515e-03 | validation loss: 1.4367e-03\n",
      "Epoch: 298020 | training loss: 1.8515e-03 | validation loss: 1.4368e-03\n",
      "Epoch: 298030 | training loss: 1.8517e-03 | validation loss: 1.4384e-03\n",
      "Epoch: 298040 | training loss: 1.8594e-03 | validation loss: 1.4503e-03\n",
      "Epoch: 298050 | training loss: 2.2323e-03 | validation loss: 1.6931e-03\n",
      "Epoch: 298060 | training loss: 1.8597e-03 | validation loss: 1.4279e-03\n",
      "Epoch: 298070 | training loss: 2.0588e-03 | validation loss: 1.5827e-03\n",
      "Epoch: 298080 | training loss: 1.8761e-03 | validation loss: 1.4328e-03\n",
      "Epoch: 298090 | training loss: 1.8643e-03 | validation loss: 1.4324e-03\n",
      "Epoch: 298100 | training loss: 1.8620e-03 | validation loss: 1.4527e-03\n",
      "Epoch: 298110 | training loss: 1.8522e-03 | validation loss: 1.4330e-03\n",
      "Epoch: 298120 | training loss: 1.8513e-03 | validation loss: 1.4349e-03\n",
      "Epoch: 298130 | training loss: 1.8514e-03 | validation loss: 1.4374e-03\n",
      "Epoch: 298140 | training loss: 1.8513e-03 | validation loss: 1.4353e-03\n",
      "Epoch: 298150 | training loss: 1.8513e-03 | validation loss: 1.4362e-03\n",
      "Epoch: 298160 | training loss: 1.8512e-03 | validation loss: 1.4361e-03\n",
      "Epoch: 298170 | training loss: 1.8512e-03 | validation loss: 1.4355e-03\n",
      "Epoch: 298180 | training loss: 1.8512e-03 | validation loss: 1.4359e-03\n",
      "Epoch: 298190 | training loss: 1.8512e-03 | validation loss: 1.4361e-03\n",
      "Epoch: 298200 | training loss: 1.8512e-03 | validation loss: 1.4363e-03\n",
      "Epoch: 298210 | training loss: 1.8513e-03 | validation loss: 1.4373e-03\n",
      "Epoch: 298220 | training loss: 1.8576e-03 | validation loss: 1.4472e-03\n",
      "Epoch: 298230 | training loss: 2.2839e-03 | validation loss: 1.7152e-03\n",
      "Epoch: 298240 | training loss: 1.9178e-03 | validation loss: 1.4379e-03\n",
      "Epoch: 298250 | training loss: 2.1156e-03 | validation loss: 1.6170e-03\n",
      "Epoch: 298260 | training loss: 1.8793e-03 | validation loss: 1.4713e-03\n",
      "Epoch: 298270 | training loss: 1.8703e-03 | validation loss: 1.4294e-03\n",
      "Epoch: 298280 | training loss: 1.8583e-03 | validation loss: 1.4286e-03\n",
      "Epoch: 298290 | training loss: 1.8540e-03 | validation loss: 1.4442e-03\n",
      "Epoch: 298300 | training loss: 1.8513e-03 | validation loss: 1.4372e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 298310 | training loss: 1.8517e-03 | validation loss: 1.4335e-03\n",
      "Epoch: 298320 | training loss: 1.8512e-03 | validation loss: 1.4372e-03\n",
      "Epoch: 298330 | training loss: 1.8510e-03 | validation loss: 1.4353e-03\n",
      "Epoch: 298340 | training loss: 1.8510e-03 | validation loss: 1.4359e-03\n",
      "Epoch: 298350 | training loss: 1.8510e-03 | validation loss: 1.4355e-03\n",
      "Epoch: 298360 | training loss: 1.8510e-03 | validation loss: 1.4358e-03\n",
      "Epoch: 298370 | training loss: 1.8509e-03 | validation loss: 1.4354e-03\n",
      "Epoch: 298380 | training loss: 1.8509e-03 | validation loss: 1.4354e-03\n",
      "Epoch: 298390 | training loss: 1.8512e-03 | validation loss: 1.4344e-03\n",
      "Epoch: 298400 | training loss: 1.8753e-03 | validation loss: 1.4394e-03\n",
      "Epoch: 298410 | training loss: 2.2962e-03 | validation loss: 1.6858e-03\n",
      "Epoch: 298420 | training loss: 1.8804e-03 | validation loss: 1.4349e-03\n",
      "Epoch: 298430 | training loss: 1.8764e-03 | validation loss: 1.4519e-03\n",
      "Epoch: 298440 | training loss: 1.8795e-03 | validation loss: 1.4500e-03\n",
      "Epoch: 298450 | training loss: 1.9228e-03 | validation loss: 1.4408e-03\n",
      "Epoch: 298460 | training loss: 2.3406e-03 | validation loss: 1.5920e-03\n",
      "Epoch: 298470 | training loss: 1.9440e-03 | validation loss: 1.5121e-03\n",
      "Epoch: 298480 | training loss: 1.8671e-03 | validation loss: 1.4273e-03\n",
      "Epoch: 298490 | training loss: 1.8510e-03 | validation loss: 1.4372e-03\n",
      "Epoch: 298500 | training loss: 1.8548e-03 | validation loss: 1.4452e-03\n",
      "Epoch: 298510 | training loss: 1.8553e-03 | validation loss: 1.4298e-03\n",
      "Epoch: 298520 | training loss: 1.8510e-03 | validation loss: 1.4333e-03\n",
      "Epoch: 298530 | training loss: 1.8513e-03 | validation loss: 1.4381e-03\n",
      "Epoch: 298540 | training loss: 1.8537e-03 | validation loss: 1.4429e-03\n",
      "Epoch: 298550 | training loss: 1.8883e-03 | validation loss: 1.4756e-03\n",
      "Epoch: 298560 | training loss: 2.5033e-03 | validation loss: 1.8456e-03\n",
      "Epoch: 298570 | training loss: 1.9923e-03 | validation loss: 1.4615e-03\n",
      "Epoch: 298580 | training loss: 1.9413e-03 | validation loss: 1.5089e-03\n",
      "Epoch: 298590 | training loss: 1.8928e-03 | validation loss: 1.4361e-03\n",
      "Epoch: 298600 | training loss: 1.8667e-03 | validation loss: 1.4557e-03\n",
      "Epoch: 298610 | training loss: 1.8568e-03 | validation loss: 1.4296e-03\n",
      "Epoch: 298620 | training loss: 1.8525e-03 | validation loss: 1.4409e-03\n",
      "Epoch: 298630 | training loss: 1.8507e-03 | validation loss: 1.4341e-03\n",
      "Epoch: 298640 | training loss: 1.8510e-03 | validation loss: 1.4334e-03\n",
      "Epoch: 298650 | training loss: 1.8506e-03 | validation loss: 1.4353e-03\n",
      "Epoch: 298660 | training loss: 1.8507e-03 | validation loss: 1.4365e-03\n",
      "Epoch: 298670 | training loss: 1.8517e-03 | validation loss: 1.4390e-03\n",
      "Epoch: 298680 | training loss: 1.8731e-03 | validation loss: 1.4613e-03\n",
      "Epoch: 298690 | training loss: 2.5731e-03 | validation loss: 1.8753e-03\n",
      "Epoch: 298700 | training loss: 2.0873e-03 | validation loss: 1.4986e-03\n",
      "Epoch: 298710 | training loss: 2.0142e-03 | validation loss: 1.5559e-03\n",
      "Epoch: 298720 | training loss: 1.8595e-03 | validation loss: 1.4315e-03\n",
      "Epoch: 298730 | training loss: 1.8566e-03 | validation loss: 1.4277e-03\n",
      "Epoch: 298740 | training loss: 1.8579e-03 | validation loss: 1.4496e-03\n",
      "Epoch: 298750 | training loss: 1.8535e-03 | validation loss: 1.4296e-03\n",
      "Epoch: 298760 | training loss: 1.8515e-03 | validation loss: 1.4395e-03\n",
      "Epoch: 298770 | training loss: 1.8509e-03 | validation loss: 1.4329e-03\n",
      "Epoch: 298780 | training loss: 1.8506e-03 | validation loss: 1.4363e-03\n",
      "Epoch: 298790 | training loss: 1.8504e-03 | validation loss: 1.4343e-03\n",
      "Epoch: 298800 | training loss: 1.8504e-03 | validation loss: 1.4339e-03\n",
      "Epoch: 298810 | training loss: 1.8506e-03 | validation loss: 1.4337e-03\n",
      "Epoch: 298820 | training loss: 1.8611e-03 | validation loss: 1.4336e-03\n",
      "Epoch: 298830 | training loss: 2.2682e-03 | validation loss: 1.6695e-03\n",
      "Epoch: 298840 | training loss: 2.0164e-03 | validation loss: 1.5479e-03\n",
      "Epoch: 298850 | training loss: 1.9199e-03 | validation loss: 1.4421e-03\n",
      "Epoch: 298860 | training loss: 1.9956e-03 | validation loss: 1.4601e-03\n",
      "Epoch: 298870 | training loss: 1.8811e-03 | validation loss: 1.4267e-03\n",
      "Epoch: 298880 | training loss: 1.8571e-03 | validation loss: 1.4498e-03\n",
      "Epoch: 298890 | training loss: 1.8795e-03 | validation loss: 1.4700e-03\n",
      "Epoch: 298900 | training loss: 1.9749e-03 | validation loss: 1.5348e-03\n",
      "Epoch: 298910 | training loss: 2.0490e-03 | validation loss: 1.5827e-03\n",
      "Epoch: 298920 | training loss: 1.8691e-03 | validation loss: 1.4289e-03\n",
      "Epoch: 298930 | training loss: 1.8734e-03 | validation loss: 1.4280e-03\n",
      "Epoch: 298940 | training loss: 1.8531e-03 | validation loss: 1.4419e-03\n",
      "Epoch: 298950 | training loss: 1.8657e-03 | validation loss: 1.4562e-03\n",
      "Epoch: 298960 | training loss: 1.9043e-03 | validation loss: 1.4867e-03\n",
      "Epoch: 298970 | training loss: 2.1196e-03 | validation loss: 1.6234e-03\n",
      "Epoch: 298980 | training loss: 1.8699e-03 | validation loss: 1.4574e-03\n",
      "Epoch: 298990 | training loss: 1.9111e-03 | validation loss: 1.4388e-03\n",
      "Epoch: 299000 | training loss: 1.8564e-03 | validation loss: 1.4451e-03\n",
      "Epoch: 299010 | training loss: 1.8627e-03 | validation loss: 1.4525e-03\n",
      "Epoch: 299020 | training loss: 1.8514e-03 | validation loss: 1.4387e-03\n",
      "Epoch: 299030 | training loss: 1.8503e-03 | validation loss: 1.4362e-03\n",
      "Epoch: 299040 | training loss: 1.8516e-03 | validation loss: 1.4392e-03\n",
      "Epoch: 299050 | training loss: 1.9010e-03 | validation loss: 1.4826e-03\n",
      "Epoch: 299060 | training loss: 3.0968e-03 | validation loss: 2.1588e-03\n",
      "Epoch: 299070 | training loss: 2.1935e-03 | validation loss: 1.5443e-03\n",
      "Epoch: 299080 | training loss: 1.8564e-03 | validation loss: 1.4255e-03\n",
      "Epoch: 299090 | training loss: 1.9011e-03 | validation loss: 1.4867e-03\n",
      "Epoch: 299100 | training loss: 1.8540e-03 | validation loss: 1.4290e-03\n",
      "Epoch: 299110 | training loss: 1.8516e-03 | validation loss: 1.4301e-03\n",
      "Epoch: 299120 | training loss: 1.8523e-03 | validation loss: 1.4411e-03\n",
      "Epoch: 299130 | training loss: 1.8509e-03 | validation loss: 1.4313e-03\n",
      "Epoch: 299140 | training loss: 1.8502e-03 | validation loss: 1.4363e-03\n",
      "Epoch: 299150 | training loss: 1.8500e-03 | validation loss: 1.4329e-03\n",
      "Epoch: 299160 | training loss: 1.8499e-03 | validation loss: 1.4352e-03\n",
      "Epoch: 299170 | training loss: 1.8499e-03 | validation loss: 1.4341e-03\n",
      "Epoch: 299180 | training loss: 1.8499e-03 | validation loss: 1.4346e-03\n",
      "Epoch: 299190 | training loss: 1.8519e-03 | validation loss: 1.4395e-03\n",
      "Epoch: 299200 | training loss: 1.9732e-03 | validation loss: 1.5490e-03\n",
      "Epoch: 299210 | training loss: 1.8565e-03 | validation loss: 1.4392e-03\n",
      "Epoch: 299220 | training loss: 1.9343e-03 | validation loss: 1.5171e-03\n",
      "Epoch: 299230 | training loss: 1.8537e-03 | validation loss: 1.4415e-03\n",
      "Epoch: 299240 | training loss: 1.8890e-03 | validation loss: 1.4670e-03\n",
      "Epoch: 299250 | training loss: 2.2683e-03 | validation loss: 1.7192e-03\n",
      "Epoch: 299260 | training loss: 1.8532e-03 | validation loss: 1.4307e-03\n",
      "Epoch: 299270 | training loss: 1.8510e-03 | validation loss: 1.4377e-03\n",
      "Epoch: 299280 | training loss: 1.8502e-03 | validation loss: 1.4306e-03\n",
      "Epoch: 299290 | training loss: 1.8500e-03 | validation loss: 1.4320e-03\n",
      "Epoch: 299300 | training loss: 1.8523e-03 | validation loss: 1.4411e-03\n",
      "Epoch: 299310 | training loss: 1.8523e-03 | validation loss: 1.4291e-03\n",
      "Epoch: 299320 | training loss: 1.8497e-03 | validation loss: 1.4341e-03\n",
      "Epoch: 299330 | training loss: 1.8504e-03 | validation loss: 1.4372e-03\n",
      "Epoch: 299340 | training loss: 1.8511e-03 | validation loss: 1.4387e-03\n",
      "Epoch: 299350 | training loss: 1.8606e-03 | validation loss: 1.4508e-03\n",
      "Epoch: 299360 | training loss: 2.0760e-03 | validation loss: 1.5969e-03\n",
      "Epoch: 299370 | training loss: 2.0688e-03 | validation loss: 1.5861e-03\n",
      "Epoch: 299380 | training loss: 1.8782e-03 | validation loss: 1.4398e-03\n",
      "Epoch: 299390 | training loss: 1.8532e-03 | validation loss: 1.4279e-03\n",
      "Epoch: 299400 | training loss: 1.8535e-03 | validation loss: 1.4440e-03\n",
      "Epoch: 299410 | training loss: 1.8512e-03 | validation loss: 1.4294e-03\n",
      "Epoch: 299420 | training loss: 1.8497e-03 | validation loss: 1.4357e-03\n",
      "Epoch: 299430 | training loss: 1.8499e-03 | validation loss: 1.4343e-03\n",
      "Epoch: 299440 | training loss: 1.8502e-03 | validation loss: 1.4327e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 299450 | training loss: 1.8495e-03 | validation loss: 1.4335e-03\n",
      "Epoch: 299460 | training loss: 1.8497e-03 | validation loss: 1.4346e-03\n",
      "Epoch: 299470 | training loss: 1.8500e-03 | validation loss: 1.4358e-03\n",
      "Epoch: 299480 | training loss: 1.8545e-03 | validation loss: 1.4422e-03\n",
      "Epoch: 299490 | training loss: 1.9912e-03 | validation loss: 1.5360e-03\n",
      "Epoch: 299500 | training loss: 2.4814e-03 | validation loss: 1.8246e-03\n",
      "Epoch: 299510 | training loss: 1.9047e-03 | validation loss: 1.4300e-03\n",
      "Epoch: 299520 | training loss: 1.8955e-03 | validation loss: 1.4692e-03\n",
      "Epoch: 299530 | training loss: 1.8796e-03 | validation loss: 1.4496e-03\n",
      "Epoch: 299540 | training loss: 1.8512e-03 | validation loss: 1.4375e-03\n",
      "Epoch: 299550 | training loss: 1.8503e-03 | validation loss: 1.4313e-03\n",
      "Epoch: 299560 | training loss: 1.8505e-03 | validation loss: 1.4359e-03\n",
      "Epoch: 299570 | training loss: 1.8498e-03 | validation loss: 1.4340e-03\n",
      "Epoch: 299580 | training loss: 1.8495e-03 | validation loss: 1.4330e-03\n",
      "Epoch: 299590 | training loss: 1.8494e-03 | validation loss: 1.4324e-03\n",
      "Epoch: 299600 | training loss: 1.8496e-03 | validation loss: 1.4311e-03\n",
      "Epoch: 299610 | training loss: 1.8559e-03 | validation loss: 1.4259e-03\n",
      "Epoch: 299620 | training loss: 2.1231e-03 | validation loss: 1.5217e-03\n",
      "Epoch: 299630 | training loss: 1.9488e-03 | validation loss: 1.4938e-03\n",
      "Epoch: 299640 | training loss: 1.9694e-03 | validation loss: 1.4907e-03\n",
      "Epoch: 299650 | training loss: 1.8544e-03 | validation loss: 1.4267e-03\n",
      "Epoch: 299660 | training loss: 1.8665e-03 | validation loss: 1.4611e-03\n",
      "Epoch: 299670 | training loss: 1.8514e-03 | validation loss: 1.4300e-03\n",
      "Epoch: 299680 | training loss: 1.8516e-03 | validation loss: 1.4325e-03\n",
      "Epoch: 299690 | training loss: 1.8496e-03 | validation loss: 1.4362e-03\n",
      "Epoch: 299700 | training loss: 1.8492e-03 | validation loss: 1.4323e-03\n",
      "Epoch: 299710 | training loss: 1.8495e-03 | validation loss: 1.4307e-03\n",
      "Epoch: 299720 | training loss: 1.8520e-03 | validation loss: 1.4297e-03\n",
      "Epoch: 299730 | training loss: 1.9194e-03 | validation loss: 1.4413e-03\n",
      "Epoch: 299740 | training loss: 2.8766e-03 | validation loss: 1.8373e-03\n",
      "Epoch: 299750 | training loss: 2.1365e-03 | validation loss: 1.6272e-03\n",
      "Epoch: 299760 | training loss: 1.8576e-03 | validation loss: 1.4258e-03\n",
      "Epoch: 299770 | training loss: 1.8639e-03 | validation loss: 1.4279e-03\n",
      "Epoch: 299780 | training loss: 1.8642e-03 | validation loss: 1.4531e-03\n",
      "Epoch: 299790 | training loss: 1.8545e-03 | validation loss: 1.4280e-03\n",
      "Epoch: 299800 | training loss: 1.8507e-03 | validation loss: 1.4380e-03\n",
      "Epoch: 299810 | training loss: 1.8497e-03 | validation loss: 1.4306e-03\n",
      "Epoch: 299820 | training loss: 1.8493e-03 | validation loss: 1.4350e-03\n",
      "Epoch: 299830 | training loss: 1.8491e-03 | validation loss: 1.4321e-03\n",
      "Epoch: 299840 | training loss: 1.8490e-03 | validation loss: 1.4326e-03\n",
      "Epoch: 299850 | training loss: 1.8490e-03 | validation loss: 1.4333e-03\n",
      "Epoch: 299860 | training loss: 1.8490e-03 | validation loss: 1.4335e-03\n",
      "Epoch: 299870 | training loss: 1.8491e-03 | validation loss: 1.4344e-03\n",
      "Epoch: 299880 | training loss: 1.8533e-03 | validation loss: 1.4414e-03\n",
      "Epoch: 299890 | training loss: 2.0458e-03 | validation loss: 1.5727e-03\n",
      "Epoch: 299900 | training loss: 2.1747e-03 | validation loss: 1.6485e-03\n",
      "Epoch: 299910 | training loss: 1.9129e-03 | validation loss: 1.4888e-03\n",
      "Epoch: 299920 | training loss: 1.9454e-03 | validation loss: 1.4481e-03\n",
      "Epoch: 299930 | training loss: 1.8494e-03 | validation loss: 1.4303e-03\n",
      "Epoch: 299940 | training loss: 1.8623e-03 | validation loss: 1.4509e-03\n",
      "Epoch: 299950 | training loss: 1.8515e-03 | validation loss: 1.4291e-03\n",
      "Epoch: 299960 | training loss: 1.8488e-03 | validation loss: 1.4328e-03\n",
      "Epoch: 299970 | training loss: 1.8489e-03 | validation loss: 1.4341e-03\n",
      "Epoch: 299980 | training loss: 1.8489e-03 | validation loss: 1.4318e-03\n",
      "Epoch: 299990 | training loss: 1.8488e-03 | validation loss: 1.4334e-03\n",
      "Epoch: 300000 | training loss: 1.8487e-03 | validation loss: 1.4326e-03\n",
      "Epoch: 300010 | training loss: 1.8487e-03 | validation loss: 1.4326e-03\n",
      "Epoch: 300020 | training loss: 1.8489e-03 | validation loss: 1.4343e-03\n",
      "Epoch: 300030 | training loss: 1.8681e-03 | validation loss: 1.4612e-03\n",
      "Epoch: 300040 | training loss: 2.4964e-03 | validation loss: 1.9629e-03\n",
      "Epoch: 300050 | training loss: 1.9366e-03 | validation loss: 1.5092e-03\n",
      "Epoch: 300060 | training loss: 1.8584e-03 | validation loss: 1.4319e-03\n",
      "Epoch: 300070 | training loss: 1.8551e-03 | validation loss: 1.4416e-03\n",
      "Epoch: 300080 | training loss: 1.8523e-03 | validation loss: 1.4322e-03\n",
      "Epoch: 300090 | training loss: 1.8524e-03 | validation loss: 1.4278e-03\n",
      "Epoch: 300100 | training loss: 1.8529e-03 | validation loss: 1.4258e-03\n",
      "Epoch: 300110 | training loss: 1.8834e-03 | validation loss: 1.4271e-03\n",
      "Epoch: 300120 | training loss: 2.3792e-03 | validation loss: 1.6069e-03\n",
      "Epoch: 300130 | training loss: 1.9101e-03 | validation loss: 1.4917e-03\n",
      "Epoch: 300140 | training loss: 1.8875e-03 | validation loss: 1.4293e-03\n",
      "Epoch: 300150 | training loss: 1.8686e-03 | validation loss: 1.4580e-03\n",
      "Epoch: 300160 | training loss: 1.8552e-03 | validation loss: 1.4261e-03\n",
      "Epoch: 300170 | training loss: 1.8493e-03 | validation loss: 1.4354e-03\n",
      "Epoch: 300180 | training loss: 1.8488e-03 | validation loss: 1.4342e-03\n",
      "Epoch: 300190 | training loss: 1.8496e-03 | validation loss: 1.4290e-03\n",
      "Epoch: 300200 | training loss: 1.8485e-03 | validation loss: 1.4319e-03\n",
      "Epoch: 300210 | training loss: 1.8487e-03 | validation loss: 1.4338e-03\n",
      "Epoch: 300220 | training loss: 1.8496e-03 | validation loss: 1.4364e-03\n",
      "Epoch: 300230 | training loss: 1.8651e-03 | validation loss: 1.4546e-03\n",
      "Epoch: 300240 | training loss: 2.3291e-03 | validation loss: 1.7440e-03\n",
      "Epoch: 300250 | training loss: 1.8773e-03 | validation loss: 1.4243e-03\n",
      "Epoch: 300260 | training loss: 1.9561e-03 | validation loss: 1.5184e-03\n",
      "Epoch: 300270 | training loss: 1.9089e-03 | validation loss: 1.4402e-03\n",
      "Epoch: 300280 | training loss: 1.8594e-03 | validation loss: 1.4462e-03\n",
      "Epoch: 300290 | training loss: 1.8493e-03 | validation loss: 1.4291e-03\n",
      "Epoch: 300300 | training loss: 1.8485e-03 | validation loss: 1.4337e-03\n",
      "Epoch: 300310 | training loss: 1.8485e-03 | validation loss: 1.4306e-03\n",
      "Epoch: 300320 | training loss: 1.8486e-03 | validation loss: 1.4339e-03\n",
      "Epoch: 300330 | training loss: 1.8485e-03 | validation loss: 1.4306e-03\n",
      "Epoch: 300340 | training loss: 1.8483e-03 | validation loss: 1.4322e-03\n",
      "Epoch: 300350 | training loss: 1.8483e-03 | validation loss: 1.4327e-03\n",
      "Epoch: 300360 | training loss: 1.8484e-03 | validation loss: 1.4330e-03\n",
      "Epoch: 300370 | training loss: 1.8489e-03 | validation loss: 1.4346e-03\n",
      "Epoch: 300380 | training loss: 1.8623e-03 | validation loss: 1.4504e-03\n",
      "Epoch: 300390 | training loss: 2.4458e-03 | validation loss: 1.8011e-03\n",
      "Epoch: 300400 | training loss: 2.0079e-03 | validation loss: 1.4695e-03\n",
      "Epoch: 300410 | training loss: 2.0535e-03 | validation loss: 1.5766e-03\n",
      "Epoch: 300420 | training loss: 1.8497e-03 | validation loss: 1.4283e-03\n",
      "Epoch: 300430 | training loss: 1.8690e-03 | validation loss: 1.4271e-03\n",
      "Epoch: 300440 | training loss: 1.8558e-03 | validation loss: 1.4447e-03\n",
      "Epoch: 300450 | training loss: 1.8484e-03 | validation loss: 1.4303e-03\n",
      "Epoch: 300460 | training loss: 1.8482e-03 | validation loss: 1.4309e-03\n",
      "Epoch: 300470 | training loss: 1.8483e-03 | validation loss: 1.4332e-03\n",
      "Epoch: 300480 | training loss: 1.8481e-03 | validation loss: 1.4312e-03\n",
      "Epoch: 300490 | training loss: 1.8481e-03 | validation loss: 1.4319e-03\n",
      "Epoch: 300500 | training loss: 1.8481e-03 | validation loss: 1.4321e-03\n",
      "Epoch: 300510 | training loss: 1.8481e-03 | validation loss: 1.4311e-03\n",
      "Epoch: 300520 | training loss: 1.8483e-03 | validation loss: 1.4301e-03\n",
      "Epoch: 300530 | training loss: 1.8714e-03 | validation loss: 1.4325e-03\n",
      "Epoch: 300540 | training loss: 2.3816e-03 | validation loss: 1.7137e-03\n",
      "Epoch: 300550 | training loss: 1.8984e-03 | validation loss: 1.4376e-03\n",
      "Epoch: 300560 | training loss: 1.8681e-03 | validation loss: 1.4626e-03\n",
      "Epoch: 300570 | training loss: 1.8697e-03 | validation loss: 1.4622e-03\n",
      "Epoch: 300580 | training loss: 1.8544e-03 | validation loss: 1.4411e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300590 | training loss: 1.8488e-03 | validation loss: 1.4290e-03\n",
      "Epoch: 300600 | training loss: 1.8604e-03 | validation loss: 1.4231e-03\n",
      "Epoch: 300610 | training loss: 2.1992e-03 | validation loss: 1.5334e-03\n",
      "Epoch: 300620 | training loss: 1.8542e-03 | validation loss: 1.4312e-03\n",
      "Epoch: 300630 | training loss: 1.9034e-03 | validation loss: 1.4334e-03\n",
      "Epoch: 300640 | training loss: 1.9111e-03 | validation loss: 1.4893e-03\n",
      "Epoch: 300650 | training loss: 1.8668e-03 | validation loss: 1.4247e-03\n",
      "Epoch: 300660 | training loss: 1.8510e-03 | validation loss: 1.4384e-03\n",
      "Epoch: 300670 | training loss: 1.8484e-03 | validation loss: 1.4289e-03\n",
      "Epoch: 300680 | training loss: 1.8482e-03 | validation loss: 1.4336e-03\n",
      "Epoch: 300690 | training loss: 1.8482e-03 | validation loss: 1.4293e-03\n",
      "Epoch: 300700 | training loss: 1.8480e-03 | validation loss: 1.4329e-03\n",
      "Epoch: 300710 | training loss: 1.8478e-03 | validation loss: 1.4313e-03\n",
      "Epoch: 300720 | training loss: 1.8478e-03 | validation loss: 1.4304e-03\n",
      "Epoch: 300730 | training loss: 1.8479e-03 | validation loss: 1.4302e-03\n",
      "Epoch: 300740 | training loss: 1.8482e-03 | validation loss: 1.4290e-03\n",
      "Epoch: 300750 | training loss: 1.8578e-03 | validation loss: 1.4252e-03\n",
      "Epoch: 300760 | training loss: 2.2833e-03 | validation loss: 1.5729e-03\n",
      "Epoch: 300770 | training loss: 1.8729e-03 | validation loss: 1.4664e-03\n",
      "Epoch: 300780 | training loss: 2.0458e-03 | validation loss: 1.4795e-03\n",
      "Epoch: 300790 | training loss: 1.8768e-03 | validation loss: 1.4546e-03\n",
      "Epoch: 300800 | training loss: 1.8575e-03 | validation loss: 1.4499e-03\n",
      "Epoch: 300810 | training loss: 1.8564e-03 | validation loss: 1.4265e-03\n",
      "Epoch: 300820 | training loss: 1.8499e-03 | validation loss: 1.4337e-03\n",
      "Epoch: 300830 | training loss: 1.8483e-03 | validation loss: 1.4329e-03\n",
      "Epoch: 300840 | training loss: 1.8479e-03 | validation loss: 1.4299e-03\n",
      "Epoch: 300850 | training loss: 1.8477e-03 | validation loss: 1.4322e-03\n",
      "Epoch: 300860 | training loss: 1.8477e-03 | validation loss: 1.4307e-03\n",
      "Epoch: 300870 | training loss: 1.8476e-03 | validation loss: 1.4310e-03\n",
      "Epoch: 300880 | training loss: 1.8476e-03 | validation loss: 1.4313e-03\n",
      "Epoch: 300890 | training loss: 1.8476e-03 | validation loss: 1.4309e-03\n",
      "Epoch: 300900 | training loss: 1.8475e-03 | validation loss: 1.4308e-03\n",
      "Epoch: 300910 | training loss: 1.8476e-03 | validation loss: 1.4308e-03\n",
      "Epoch: 300920 | training loss: 1.8495e-03 | validation loss: 1.4318e-03\n",
      "Epoch: 300930 | training loss: 1.9625e-03 | validation loss: 1.5005e-03\n",
      "Epoch: 300940 | training loss: 2.6247e-03 | validation loss: 1.9322e-03\n",
      "Epoch: 300950 | training loss: 2.2184e-03 | validation loss: 1.5617e-03\n",
      "Epoch: 300960 | training loss: 1.9657e-03 | validation loss: 1.5009e-03\n",
      "Epoch: 300970 | training loss: 1.8752e-03 | validation loss: 1.4355e-03\n",
      "Epoch: 300980 | training loss: 1.8588e-03 | validation loss: 1.4244e-03\n",
      "Epoch: 300990 | training loss: 1.8515e-03 | validation loss: 1.4349e-03\n",
      "Epoch: 301000 | training loss: 1.8485e-03 | validation loss: 1.4270e-03\n",
      "Epoch: 301010 | training loss: 1.8479e-03 | validation loss: 1.4340e-03\n",
      "Epoch: 301020 | training loss: 1.8476e-03 | validation loss: 1.4310e-03\n",
      "Epoch: 301030 | training loss: 1.8474e-03 | validation loss: 1.4306e-03\n",
      "Epoch: 301040 | training loss: 1.8474e-03 | validation loss: 1.4310e-03\n",
      "Epoch: 301050 | training loss: 1.8474e-03 | validation loss: 1.4308e-03\n",
      "Epoch: 301060 | training loss: 1.8473e-03 | validation loss: 1.4309e-03\n",
      "Epoch: 301070 | training loss: 1.8473e-03 | validation loss: 1.4311e-03\n",
      "Epoch: 301080 | training loss: 1.8478e-03 | validation loss: 1.4332e-03\n",
      "Epoch: 301090 | training loss: 1.8782e-03 | validation loss: 1.4651e-03\n",
      "Epoch: 301100 | training loss: 3.5011e-03 | validation loss: 2.3798e-03\n",
      "Epoch: 301110 | training loss: 2.1602e-03 | validation loss: 1.5361e-03\n",
      "Epoch: 301120 | training loss: 2.0050e-03 | validation loss: 1.4649e-03\n",
      "Epoch: 301130 | training loss: 1.8513e-03 | validation loss: 1.4255e-03\n",
      "Epoch: 301140 | training loss: 1.8698e-03 | validation loss: 1.4515e-03\n",
      "Epoch: 301150 | training loss: 1.8499e-03 | validation loss: 1.4365e-03\n",
      "Epoch: 301160 | training loss: 1.8496e-03 | validation loss: 1.4290e-03\n",
      "Epoch: 301170 | training loss: 1.8474e-03 | validation loss: 1.4299e-03\n",
      "Epoch: 301180 | training loss: 1.8476e-03 | validation loss: 1.4322e-03\n",
      "Epoch: 301190 | training loss: 1.8472e-03 | validation loss: 1.4297e-03\n",
      "Epoch: 301200 | training loss: 1.8472e-03 | validation loss: 1.4306e-03\n",
      "Epoch: 301210 | training loss: 1.8471e-03 | validation loss: 1.4305e-03\n",
      "Epoch: 301220 | training loss: 1.8471e-03 | validation loss: 1.4302e-03\n",
      "Epoch: 301230 | training loss: 1.8471e-03 | validation loss: 1.4305e-03\n",
      "Epoch: 301240 | training loss: 1.8471e-03 | validation loss: 1.4304e-03\n",
      "Epoch: 301250 | training loss: 1.8471e-03 | validation loss: 1.4303e-03\n",
      "Epoch: 301260 | training loss: 1.8471e-03 | validation loss: 1.4303e-03\n",
      "Epoch: 301270 | training loss: 1.8470e-03 | validation loss: 1.4303e-03\n",
      "Epoch: 301280 | training loss: 1.8470e-03 | validation loss: 1.4303e-03\n",
      "Epoch: 301290 | training loss: 1.8470e-03 | validation loss: 1.4304e-03\n",
      "Epoch: 301300 | training loss: 1.8471e-03 | validation loss: 1.4312e-03\n",
      "Epoch: 301310 | training loss: 1.8537e-03 | validation loss: 1.4409e-03\n",
      "Epoch: 301320 | training loss: 2.6274e-03 | validation loss: 1.8952e-03\n",
      "Epoch: 301330 | training loss: 2.6112e-03 | validation loss: 1.7398e-03\n",
      "Epoch: 301340 | training loss: 1.8798e-03 | validation loss: 1.4563e-03\n",
      "Epoch: 301350 | training loss: 1.8590e-03 | validation loss: 1.4460e-03\n",
      "Epoch: 301360 | training loss: 1.8702e-03 | validation loss: 1.4464e-03\n",
      "Epoch: 301370 | training loss: 1.8613e-03 | validation loss: 1.4417e-03\n",
      "Epoch: 301380 | training loss: 1.8498e-03 | validation loss: 1.4349e-03\n",
      "Epoch: 301390 | training loss: 1.8470e-03 | validation loss: 1.4295e-03\n",
      "Epoch: 301400 | training loss: 1.8475e-03 | validation loss: 1.4299e-03\n",
      "Epoch: 301410 | training loss: 1.8469e-03 | validation loss: 1.4302e-03\n",
      "Epoch: 301420 | training loss: 1.8469e-03 | validation loss: 1.4305e-03\n",
      "Epoch: 301430 | training loss: 1.8468e-03 | validation loss: 1.4302e-03\n",
      "Epoch: 301440 | training loss: 1.8468e-03 | validation loss: 1.4301e-03\n",
      "Epoch: 301450 | training loss: 1.8468e-03 | validation loss: 1.4301e-03\n",
      "Epoch: 301460 | training loss: 1.8468e-03 | validation loss: 1.4301e-03\n",
      "Epoch: 301470 | training loss: 1.8468e-03 | validation loss: 1.4301e-03\n",
      "Epoch: 301480 | training loss: 1.8468e-03 | validation loss: 1.4301e-03\n",
      "Epoch: 301490 | training loss: 1.8468e-03 | validation loss: 1.4302e-03\n",
      "Epoch: 301500 | training loss: 1.8468e-03 | validation loss: 1.4309e-03\n",
      "Epoch: 301510 | training loss: 1.8497e-03 | validation loss: 1.4396e-03\n",
      "Epoch: 301520 | training loss: 2.2750e-03 | validation loss: 1.7956e-03\n",
      "Epoch: 301530 | training loss: 2.1928e-03 | validation loss: 1.6069e-03\n",
      "Epoch: 301540 | training loss: 1.9835e-03 | validation loss: 1.5116e-03\n",
      "Epoch: 301550 | training loss: 1.8986e-03 | validation loss: 1.4299e-03\n",
      "Epoch: 301560 | training loss: 1.8592e-03 | validation loss: 1.4373e-03\n",
      "Epoch: 301570 | training loss: 1.8486e-03 | validation loss: 1.4248e-03\n",
      "Epoch: 301580 | training loss: 1.8468e-03 | validation loss: 1.4302e-03\n",
      "Epoch: 301590 | training loss: 1.8468e-03 | validation loss: 1.4292e-03\n",
      "Epoch: 301600 | training loss: 1.8470e-03 | validation loss: 1.4322e-03\n",
      "Epoch: 301610 | training loss: 1.8468e-03 | validation loss: 1.4294e-03\n",
      "Epoch: 301620 | training loss: 1.8466e-03 | validation loss: 1.4294e-03\n",
      "Epoch: 301630 | training loss: 1.8466e-03 | validation loss: 1.4297e-03\n",
      "Epoch: 301640 | training loss: 1.8466e-03 | validation loss: 1.4302e-03\n",
      "Epoch: 301650 | training loss: 1.8469e-03 | validation loss: 1.4320e-03\n",
      "Epoch: 301660 | training loss: 1.8572e-03 | validation loss: 1.4461e-03\n",
      "Epoch: 301670 | training loss: 2.3819e-03 | validation loss: 1.7713e-03\n",
      "Epoch: 301680 | training loss: 1.9771e-03 | validation loss: 1.4511e-03\n",
      "Epoch: 301690 | training loss: 2.0675e-03 | validation loss: 1.5816e-03\n",
      "Epoch: 301700 | training loss: 1.8465e-03 | validation loss: 1.4296e-03\n",
      "Epoch: 301710 | training loss: 1.8748e-03 | validation loss: 1.4276e-03\n",
      "Epoch: 301720 | training loss: 1.8498e-03 | validation loss: 1.4378e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 301730 | training loss: 1.8470e-03 | validation loss: 1.4319e-03\n",
      "Epoch: 301740 | training loss: 1.8476e-03 | validation loss: 1.4261e-03\n",
      "Epoch: 301750 | training loss: 1.8470e-03 | validation loss: 1.4321e-03\n",
      "Epoch: 301760 | training loss: 1.8466e-03 | validation loss: 1.4282e-03\n",
      "Epoch: 301770 | training loss: 1.8465e-03 | validation loss: 1.4302e-03\n",
      "Epoch: 301780 | training loss: 1.8464e-03 | validation loss: 1.4288e-03\n",
      "Epoch: 301790 | training loss: 1.8464e-03 | validation loss: 1.4294e-03\n",
      "Epoch: 301800 | training loss: 1.8464e-03 | validation loss: 1.4295e-03\n",
      "Epoch: 301810 | training loss: 1.8463e-03 | validation loss: 1.4293e-03\n",
      "Epoch: 301820 | training loss: 1.8463e-03 | validation loss: 1.4291e-03\n",
      "Epoch: 301830 | training loss: 1.8463e-03 | validation loss: 1.4289e-03\n",
      "Epoch: 301840 | training loss: 1.8466e-03 | validation loss: 1.4276e-03\n",
      "Epoch: 301850 | training loss: 1.8648e-03 | validation loss: 1.4247e-03\n",
      "Epoch: 301860 | training loss: 3.2030e-03 | validation loss: 1.9776e-03\n",
      "Epoch: 301870 | training loss: 2.5048e-03 | validation loss: 1.8336e-03\n",
      "Epoch: 301880 | training loss: 1.9385e-03 | validation loss: 1.5079e-03\n",
      "Epoch: 301890 | training loss: 1.8551e-03 | validation loss: 1.4238e-03\n",
      "Epoch: 301900 | training loss: 1.8757e-03 | validation loss: 1.4241e-03\n",
      "Epoch: 301910 | training loss: 1.8491e-03 | validation loss: 1.4251e-03\n",
      "Epoch: 301920 | training loss: 1.8485e-03 | validation loss: 1.4357e-03\n",
      "Epoch: 301930 | training loss: 1.8468e-03 | validation loss: 1.4320e-03\n",
      "Epoch: 301940 | training loss: 1.8466e-03 | validation loss: 1.4273e-03\n",
      "Epoch: 301950 | training loss: 1.8462e-03 | validation loss: 1.4292e-03\n",
      "Epoch: 301960 | training loss: 1.8462e-03 | validation loss: 1.4299e-03\n",
      "Epoch: 301970 | training loss: 1.8462e-03 | validation loss: 1.4286e-03\n",
      "Epoch: 301980 | training loss: 1.8461e-03 | validation loss: 1.4294e-03\n",
      "Epoch: 301990 | training loss: 1.8461e-03 | validation loss: 1.4288e-03\n",
      "Epoch: 302000 | training loss: 1.8461e-03 | validation loss: 1.4292e-03\n",
      "Epoch: 302010 | training loss: 1.8461e-03 | validation loss: 1.4289e-03\n",
      "Epoch: 302020 | training loss: 1.8461e-03 | validation loss: 1.4284e-03\n",
      "Epoch: 302030 | training loss: 1.8496e-03 | validation loss: 1.4259e-03\n",
      "Epoch: 302040 | training loss: 2.2274e-03 | validation loss: 1.6381e-03\n",
      "Epoch: 302050 | training loss: 2.1180e-03 | validation loss: 1.6272e-03\n",
      "Epoch: 302060 | training loss: 1.8844e-03 | validation loss: 1.4747e-03\n",
      "Epoch: 302070 | training loss: 1.8490e-03 | validation loss: 1.4345e-03\n",
      "Epoch: 302080 | training loss: 1.8535e-03 | validation loss: 1.4281e-03\n",
      "Epoch: 302090 | training loss: 1.8497e-03 | validation loss: 1.4254e-03\n",
      "Epoch: 302100 | training loss: 1.8493e-03 | validation loss: 1.4233e-03\n",
      "Epoch: 302110 | training loss: 1.9490e-03 | validation loss: 1.4424e-03\n",
      "Epoch: 302120 | training loss: 2.6346e-03 | validation loss: 1.7136e-03\n",
      "Epoch: 302130 | training loss: 1.9408e-03 | validation loss: 1.5045e-03\n",
      "Epoch: 302140 | training loss: 1.8589e-03 | validation loss: 1.4449e-03\n",
      "Epoch: 302150 | training loss: 1.8795e-03 | validation loss: 1.4242e-03\n",
      "Epoch: 302160 | training loss: 1.8568e-03 | validation loss: 1.4457e-03\n",
      "Epoch: 302170 | training loss: 1.8475e-03 | validation loss: 1.4255e-03\n",
      "Epoch: 302180 | training loss: 1.8461e-03 | validation loss: 1.4303e-03\n",
      "Epoch: 302190 | training loss: 1.8460e-03 | validation loss: 1.4272e-03\n",
      "Epoch: 302200 | training loss: 1.8460e-03 | validation loss: 1.4300e-03\n",
      "Epoch: 302210 | training loss: 1.8459e-03 | validation loss: 1.4274e-03\n",
      "Epoch: 302220 | training loss: 1.8458e-03 | validation loss: 1.4287e-03\n",
      "Epoch: 302230 | training loss: 1.8458e-03 | validation loss: 1.4291e-03\n",
      "Epoch: 302240 | training loss: 1.8458e-03 | validation loss: 1.4290e-03\n",
      "Epoch: 302250 | training loss: 1.8458e-03 | validation loss: 1.4295e-03\n",
      "Epoch: 302260 | training loss: 1.8473e-03 | validation loss: 1.4335e-03\n",
      "Epoch: 302270 | training loss: 1.9191e-03 | validation loss: 1.4935e-03\n",
      "Epoch: 302280 | training loss: 3.1533e-03 | validation loss: 2.1877e-03\n",
      "Epoch: 302290 | training loss: 1.9179e-03 | validation loss: 1.4504e-03\n",
      "Epoch: 302300 | training loss: 1.9612e-03 | validation loss: 1.4438e-03\n",
      "Epoch: 302310 | training loss: 1.8574e-03 | validation loss: 1.4354e-03\n",
      "Epoch: 302320 | training loss: 1.8601e-03 | validation loss: 1.4531e-03\n",
      "Epoch: 302330 | training loss: 1.8475e-03 | validation loss: 1.4258e-03\n",
      "Epoch: 302340 | training loss: 1.8469e-03 | validation loss: 1.4245e-03\n",
      "Epoch: 302350 | training loss: 1.8466e-03 | validation loss: 1.4333e-03\n",
      "Epoch: 302360 | training loss: 1.8459e-03 | validation loss: 1.4264e-03\n",
      "Epoch: 302370 | training loss: 1.8457e-03 | validation loss: 1.4294e-03\n",
      "Epoch: 302380 | training loss: 1.8456e-03 | validation loss: 1.4277e-03\n",
      "Epoch: 302390 | training loss: 1.8456e-03 | validation loss: 1.4287e-03\n",
      "Epoch: 302400 | training loss: 1.8456e-03 | validation loss: 1.4280e-03\n",
      "Epoch: 302410 | training loss: 1.8455e-03 | validation loss: 1.4284e-03\n",
      "Epoch: 302420 | training loss: 1.8455e-03 | validation loss: 1.4283e-03\n",
      "Epoch: 302430 | training loss: 1.8455e-03 | validation loss: 1.4281e-03\n",
      "Epoch: 302440 | training loss: 1.8456e-03 | validation loss: 1.4278e-03\n",
      "Epoch: 302450 | training loss: 1.8517e-03 | validation loss: 1.4290e-03\n",
      "Epoch: 302460 | training loss: 2.3446e-03 | validation loss: 1.7283e-03\n",
      "Epoch: 302470 | training loss: 2.4961e-03 | validation loss: 1.9224e-03\n",
      "Epoch: 302480 | training loss: 2.0264e-03 | validation loss: 1.5205e-03\n",
      "Epoch: 302490 | training loss: 1.8965e-03 | validation loss: 1.4886e-03\n",
      "Epoch: 302500 | training loss: 1.8563e-03 | validation loss: 1.4357e-03\n",
      "Epoch: 302510 | training loss: 1.8475e-03 | validation loss: 1.4287e-03\n",
      "Epoch: 302520 | training loss: 1.8485e-03 | validation loss: 1.4363e-03\n",
      "Epoch: 302530 | training loss: 1.8459e-03 | validation loss: 1.4252e-03\n",
      "Epoch: 302540 | training loss: 1.8468e-03 | validation loss: 1.4237e-03\n",
      "Epoch: 302550 | training loss: 1.8472e-03 | validation loss: 1.4234e-03\n",
      "Epoch: 302560 | training loss: 1.8592e-03 | validation loss: 1.4211e-03\n",
      "Epoch: 302570 | training loss: 2.1181e-03 | validation loss: 1.5009e-03\n",
      "Epoch: 302580 | training loss: 1.9281e-03 | validation loss: 1.4381e-03\n",
      "Epoch: 302590 | training loss: 1.8499e-03 | validation loss: 1.4365e-03\n",
      "Epoch: 302600 | training loss: 1.8472e-03 | validation loss: 1.4322e-03\n",
      "Epoch: 302610 | training loss: 1.8482e-03 | validation loss: 1.4227e-03\n",
      "Epoch: 302620 | training loss: 1.8461e-03 | validation loss: 1.4314e-03\n",
      "Epoch: 302630 | training loss: 1.8453e-03 | validation loss: 1.4283e-03\n",
      "Epoch: 302640 | training loss: 1.8460e-03 | validation loss: 1.4251e-03\n",
      "Epoch: 302650 | training loss: 1.8457e-03 | validation loss: 1.4304e-03\n",
      "Epoch: 302660 | training loss: 1.8453e-03 | validation loss: 1.4290e-03\n",
      "Epoch: 302670 | training loss: 1.8452e-03 | validation loss: 1.4275e-03\n",
      "Epoch: 302680 | training loss: 1.8453e-03 | validation loss: 1.4268e-03\n",
      "Epoch: 302690 | training loss: 1.8465e-03 | validation loss: 1.4243e-03\n",
      "Epoch: 302700 | training loss: 1.9027e-03 | validation loss: 1.4298e-03\n",
      "Epoch: 302710 | training loss: 3.2189e-03 | validation loss: 1.9766e-03\n",
      "Epoch: 302720 | training loss: 2.0384e-03 | validation loss: 1.5604e-03\n",
      "Epoch: 302730 | training loss: 1.9187e-03 | validation loss: 1.4974e-03\n",
      "Epoch: 302740 | training loss: 1.8753e-03 | validation loss: 1.4319e-03\n",
      "Epoch: 302750 | training loss: 1.8522e-03 | validation loss: 1.4198e-03\n",
      "Epoch: 302760 | training loss: 1.8502e-03 | validation loss: 1.4362e-03\n",
      "Epoch: 302770 | training loss: 1.8463e-03 | validation loss: 1.4299e-03\n",
      "Epoch: 302780 | training loss: 1.8455e-03 | validation loss: 1.4252e-03\n",
      "Epoch: 302790 | training loss: 1.8452e-03 | validation loss: 1.4295e-03\n",
      "Epoch: 302800 | training loss: 1.8451e-03 | validation loss: 1.4267e-03\n",
      "Epoch: 302810 | training loss: 1.8450e-03 | validation loss: 1.4281e-03\n",
      "Epoch: 302820 | training loss: 1.8450e-03 | validation loss: 1.4272e-03\n",
      "Epoch: 302830 | training loss: 1.8450e-03 | validation loss: 1.4278e-03\n",
      "Epoch: 302840 | training loss: 1.8450e-03 | validation loss: 1.4274e-03\n",
      "Epoch: 302850 | training loss: 1.8450e-03 | validation loss: 1.4274e-03\n",
      "Epoch: 302860 | training loss: 1.8449e-03 | validation loss: 1.4274e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 302870 | training loss: 1.8450e-03 | validation loss: 1.4275e-03\n",
      "Epoch: 302880 | training loss: 1.8463e-03 | validation loss: 1.4285e-03\n",
      "Epoch: 302890 | training loss: 1.9489e-03 | validation loss: 1.4917e-03\n",
      "Epoch: 302900 | training loss: 2.7042e-03 | validation loss: 1.9705e-03\n",
      "Epoch: 302910 | training loss: 2.2053e-03 | validation loss: 1.5533e-03\n",
      "Epoch: 302920 | training loss: 1.9228e-03 | validation loss: 1.4686e-03\n",
      "Epoch: 302930 | training loss: 1.8749e-03 | validation loss: 1.4436e-03\n",
      "Epoch: 302940 | training loss: 1.8620e-03 | validation loss: 1.4199e-03\n",
      "Epoch: 302950 | training loss: 1.8510e-03 | validation loss: 1.4317e-03\n",
      "Epoch: 302960 | training loss: 1.8468e-03 | validation loss: 1.4227e-03\n",
      "Epoch: 302970 | training loss: 1.8454e-03 | validation loss: 1.4275e-03\n",
      "Epoch: 302980 | training loss: 1.8450e-03 | validation loss: 1.4254e-03\n",
      "Epoch: 302990 | training loss: 1.8449e-03 | validation loss: 1.4283e-03\n",
      "Epoch: 303000 | training loss: 1.8448e-03 | validation loss: 1.4272e-03\n",
      "Epoch: 303010 | training loss: 1.8448e-03 | validation loss: 1.4268e-03\n",
      "Epoch: 303020 | training loss: 1.8447e-03 | validation loss: 1.4269e-03\n",
      "Epoch: 303030 | training loss: 1.8447e-03 | validation loss: 1.4270e-03\n",
      "Epoch: 303040 | training loss: 1.8447e-03 | validation loss: 1.4271e-03\n",
      "Epoch: 303050 | training loss: 1.8447e-03 | validation loss: 1.4271e-03\n",
      "Epoch: 303060 | training loss: 1.8447e-03 | validation loss: 1.4275e-03\n",
      "Epoch: 303070 | training loss: 1.8483e-03 | validation loss: 1.4356e-03\n",
      "Epoch: 303080 | training loss: 2.7978e-03 | validation loss: 2.0029e-03\n",
      "Epoch: 303090 | training loss: 3.0946e-03 | validation loss: 1.9172e-03\n",
      "Epoch: 303100 | training loss: 2.2130e-03 | validation loss: 1.5372e-03\n",
      "Epoch: 303110 | training loss: 1.9465e-03 | validation loss: 1.4414e-03\n",
      "Epoch: 303120 | training loss: 1.8698e-03 | validation loss: 1.4261e-03\n",
      "Epoch: 303130 | training loss: 1.8511e-03 | validation loss: 1.4269e-03\n",
      "Epoch: 303140 | training loss: 1.8462e-03 | validation loss: 1.4273e-03\n",
      "Epoch: 303150 | training loss: 1.8448e-03 | validation loss: 1.4270e-03\n",
      "Epoch: 303160 | training loss: 1.8446e-03 | validation loss: 1.4269e-03\n",
      "Epoch: 303170 | training loss: 1.8446e-03 | validation loss: 1.4267e-03\n",
      "Epoch: 303180 | training loss: 1.8446e-03 | validation loss: 1.4264e-03\n",
      "Epoch: 303190 | training loss: 1.8446e-03 | validation loss: 1.4262e-03\n",
      "Epoch: 303200 | training loss: 1.8445e-03 | validation loss: 1.4264e-03\n",
      "Epoch: 303210 | training loss: 1.8445e-03 | validation loss: 1.4268e-03\n",
      "Epoch: 303220 | training loss: 1.8445e-03 | validation loss: 1.4270e-03\n",
      "Epoch: 303230 | training loss: 1.8445e-03 | validation loss: 1.4268e-03\n",
      "Epoch: 303240 | training loss: 1.8445e-03 | validation loss: 1.4267e-03\n",
      "Epoch: 303250 | training loss: 1.8444e-03 | validation loss: 1.4267e-03\n",
      "Epoch: 303260 | training loss: 1.8444e-03 | validation loss: 1.4268e-03\n",
      "Epoch: 303270 | training loss: 1.8444e-03 | validation loss: 1.4267e-03\n",
      "Epoch: 303280 | training loss: 1.8444e-03 | validation loss: 1.4267e-03\n",
      "Epoch: 303290 | training loss: 1.8444e-03 | validation loss: 1.4267e-03\n",
      "Epoch: 303300 | training loss: 1.8444e-03 | validation loss: 1.4267e-03\n",
      "Epoch: 303310 | training loss: 1.8444e-03 | validation loss: 1.4267e-03\n",
      "Epoch: 303320 | training loss: 1.8443e-03 | validation loss: 1.4267e-03\n",
      "Epoch: 303330 | training loss: 1.8443e-03 | validation loss: 1.4266e-03\n",
      "Epoch: 303340 | training loss: 1.8443e-03 | validation loss: 1.4266e-03\n",
      "Epoch: 303350 | training loss: 1.8443e-03 | validation loss: 1.4266e-03\n",
      "Epoch: 303360 | training loss: 1.8443e-03 | validation loss: 1.4266e-03\n",
      "Epoch: 303370 | training loss: 1.8444e-03 | validation loss: 1.4270e-03\n",
      "Epoch: 303380 | training loss: 1.8799e-03 | validation loss: 1.4519e-03\n",
      "Epoch: 303390 | training loss: 3.7149e-03 | validation loss: 2.4648e-03\n",
      "Epoch: 303400 | training loss: 2.0491e-03 | validation loss: 1.4944e-03\n",
      "Epoch: 303410 | training loss: 1.9870e-03 | validation loss: 1.4726e-03\n",
      "Epoch: 303420 | training loss: 1.9125e-03 | validation loss: 1.4659e-03\n",
      "Epoch: 303430 | training loss: 1.8446e-03 | validation loss: 1.4268e-03\n",
      "Epoch: 303440 | training loss: 1.8547e-03 | validation loss: 1.4337e-03\n",
      "Epoch: 303450 | training loss: 1.8454e-03 | validation loss: 1.4281e-03\n",
      "Epoch: 303460 | training loss: 1.8448e-03 | validation loss: 1.4256e-03\n",
      "Epoch: 303470 | training loss: 1.8445e-03 | validation loss: 1.4267e-03\n",
      "Epoch: 303480 | training loss: 1.8443e-03 | validation loss: 1.4279e-03\n",
      "Epoch: 303490 | training loss: 1.8441e-03 | validation loss: 1.4261e-03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2446463/912438713.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Predict the trajectory using the NICE network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNICE_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdstrain_tv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musvars0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mpred_svars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_stress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_diss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/granular_Hydrodynamics/NModel/nice_module.py\u001b[0m in \u001b[0;36mintegrate\u001b[0;34m(self, u, y0, t, idx)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0modeint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"step_size\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Extract normalized variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchdiffeq/_impl/odeint.py\u001b[0m in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevent_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mevent_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate_until_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchdiffeq/_impl/solvers.py\u001b[0m in \u001b[0;36mintegrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchdiffeq/_impl/fixed_grid.py\u001b[0m in \u001b[0;36m_step_func\u001b[0;34m(self, func, t0, dt, t1, y0)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPerturb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEXT\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperturb\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mPerturb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0my_mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhalf_dt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhalf_dt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchdiffeq/_impl/misc.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# Do nothing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/granular_Hydrodynamics/NModel/nice_module.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t, y)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mueps_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps_dot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mueps_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps_dot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprm_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Interpolate external data for inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the state variable at the first time step for training\n",
    "par_svars0 = svars_tv[0]\n",
    "par_svars0.requires_grad = True\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, Nepochs):\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # De-normalize elastic strain at the first time step\n",
    "    ueps_e_0 = NICE_network.DeNormalize(NICE_network.e0, NICE_network.prm_ee)\n",
    "    # Concatenate de-normalized elastic strain and the first state variable\n",
    "    usvars0 = torch.cat((ueps_e_0, par_svars0), -1)\n",
    "    # Calculate stress at the first time step\n",
    "    stress0 = NICE_network.stress([ueps_e_0, par_svars0[:,:1], par_svars0[:,1:]])\n",
    "\n",
    "    # Predict the trajectory using the NICE network\n",
    "    pred = NICE_network.integrate(dstrain_tv, usvars0, t, np.hstack((ntrain, nval)))\n",
    "    pred_svars, pred_stress, pred_diss = pred\n",
    "        \n",
    "    # Calculate training loss for each sequence\n",
    "    training_loss_stress = 0.\n",
    "    training_loss_r0 = 0.\n",
    "    training_loss_svars = 0.0\n",
    "    for i in range(len(ntrain)):\n",
    "        training_loss_stress += MSE(NICE_network.Normalize(pred_stress[:,ntrain[i]]-stress_tv[0,ntrain[i]], prm_deltas[i]),\n",
    "                               NICE_network.Normalize(stress_tv[:,ntrain[i]]-stress_tv[0,ntrain[i]], prm_deltas[i]))\n",
    "        training_loss_svars += MSE(NICE_network.Normalize(pred_svars[stop_loading_train[i]:,ntrain[i],-1]-svars_tv[0,ntrain[i],-1], prm_deltaz),\n",
    "                               NICE_network.Normalize(svars_tv[stop_loading_train[i]:,ntrain[i],-1]-svars_tv[0,ntrain[i],-1], prm_deltaz))\n",
    "        \n",
    "        \n",
    "    for i in range(len(ntrainval)):\n",
    "        training_loss_r0 += MSE(NICE_network.Normalize(stress0[i]-stress_tv[0,i], prm_deltas[i]),\n",
    "                          NICE_network.Normalize(stress_tv[0,i]-stress_tv[0,i], prm_deltas[i]))\n",
    "        \n",
    "    # Calculate normalized dissipation and its training loss\n",
    "    norm_d = torch.max(torch.abs(pred_diss)).detach()\n",
    "    training_loss_dissipation = MSE(NICE_network.relu(-pred_diss[:, ntrain])/norm_d, pred_diss[:, ntrain].detach()*0)\n",
    "\n",
    "    # Regularization term for model weights\n",
    "    l_reg = torch.tensor(0., requires_grad=True)\n",
    "    for name, param in NICE_network.named_parameters():\n",
    "        if 'weight' in name: l_reg = l_reg + pow(param, 2).sum()\n",
    "\n",
    "    # Total training loss with regularization\n",
    "    training_loss = (torch.mean(training_loss_stress)\n",
    "                     + torch.mean(training_loss_r0)\n",
    "                     + torch.mean(training_loss_svars)\n",
    "                     + torch.mean(training_loss_dissipation)\n",
    "                     + w_reg * l_reg\n",
    "                    )\n",
    "\n",
    "    # Backward pass and optimization step\n",
    "    training_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Learning rate decay if necessary\n",
    "    if scheduler.get_last_lr()[0] > 1.e-4:\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate validation loss for each sequence\n",
    "    validation_loss_svars = 0.0\n",
    "    validation_loss_stress = 0.0\n",
    "    for i in range(len(nval)):\n",
    "        validation_loss_stress += MSE(NICE_network.Normalize(pred_stress[:,nval[i]]-stress_tv[0,nval[i]], prm_deltas[nval[i]]),\n",
    "                               NICE_network.Normalize(stress_tv[:,nval[i]]-stress_tv[0,nval[i]], prm_deltas[nval[i]]))\n",
    "        validation_loss_svars += MSE(NICE_network.Normalize(pred_svars[stop_loading_val[i]:,nval[i],-1]-svars_tv[0,nval[i],-1], prm_deltaz),\n",
    "                               NICE_network.Normalize(svars_tv[stop_loading_val[i]:,nval[i],-1]-svars_tv[0,nval[i],-1], prm_deltaz))\n",
    "\n",
    "    # Calculate normalized dissipation and its validation loss\n",
    "    validation_loss_dissipation = MSE(NICE_network.relu(-pred_diss[:, nval])/norm_d, pred_diss[:, nval].detach()*0)\n",
    "    \n",
    "    # Total validation loss\n",
    "    validation_loss = (torch.mean(validation_loss_stress)\n",
    "                       + torch.mean(validation_loss_svars)\n",
    "                       + torch.mean(validation_loss_dissipation))    \n",
    "\n",
    "    # Extract training and validation loss values\n",
    "    training_loss_value = training_loss.item()\n",
    "    validation_loss_value = validation_loss.item()\n",
    "    \n",
    "    # Append loss values to history lists\n",
    "    training_loss_hist.append(training_loss_value)\n",
    "    validation_loss_value_hist.append(validation_loss_value)\n",
    "    \n",
    "    # Print loss at specified frequency\n",
    "    if not epoch % verbose_frequency:\n",
    "        print(f\"Epoch: {epoch}\"\n",
    "              + f\" | training loss: {training_loss_value:.4e}\"\n",
    "              + f\" | validation loss: {validation_loss_value:.4e}\")\n",
    "\n",
    "    # Check early stopping criterion\n",
    "    early_stopping(validation_loss_value, NICE_network)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        # Load the best model state from the checkpoint\n",
    "        NICE_network.load_state_dict(torch.load(checkpoint_path))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3273d120",
   "metadata": {},
   "source": [
    "\n",
    "#### 4.3 Training and validation set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf71b20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE stress :  tensor(0.0012, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "MAE z :  tensor(0.0589, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Creating a time vector for evaluation\n",
    "t = prm_dt * torch.linspace(0., data_size-1, data_size).to(device)\n",
    "\n",
    "# Enabling gradients for the state variables\n",
    "svars_tv.requires_grad = True\n",
    "\n",
    "# Solving for elastic strain using the root-finding method\n",
    "sol = root(NICE_network.find_elastic_strain,\n",
    "           args=([svars_tv[0, :, :1].reshape(-1, 1), svars_tv[0, :, -1:].reshape(-1, 1), stress_tv[0].reshape(-1, 2)]),\n",
    "           x0=np.zeros((len(ntrainval), 2)),\n",
    "           tol=1e-12)\n",
    "eps_e_0 = torch.from_numpy(sol.x.reshape(-1, 2))\n",
    "\n",
    "# De-normalizing elastic strain\n",
    "ueps_e_0 = NICE_network.DeNormalize(eps_e_0, NICE_network.prm_ee)\n",
    "\n",
    "# Creating input for the NICE network\n",
    "usvars = torch.cat((ueps_e_0, svars_tv[0]), -1)\n",
    "\n",
    "# Predicting with the NICE network\n",
    "pred_svars, pred_stress, pred_diss = NICE_network.integrate(dstrain_tv[:], usvars, t, np.hstack((ntrain, nval)))\n",
    "\n",
    "# Evaluating error using L1 loss\n",
    "loss = torch.nn.L1Loss()\n",
    "MAE_stress = loss(NICE_network.Normalize(pred_stress, prm_s), NICE_network.Normalize(stress_tv, prm_s))\n",
    "MAE_z = loss(NICE_network.Normalize(pred_svars[:, :, -1:], prm_z), NICE_network.Normalize(svars_tv[:, :, -1:], prm_z))\n",
    "\n",
    "# Printing Mean Absolute Error (MAE) for stress and z\n",
    "print(\"MAE stress : \", MAE_stress, \"\\nMAE z : \", MAE_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e702240f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOoAAAC9CAYAAAC9H+EBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAxOAAAMTgF/d4wjAABwQklEQVR4nO29d3xcx3Uv/j0zt23FohNgbyKpSkmUrFi2JRfJTW5JXFLsOFVOYifvJS/PL92JneJferPjNCeO7cSx4zS5V9mSZfVOUewkQPSy2Hb3lpnz+2PuAgsIIAESEkhqv5/PBXZvOTt37j1zzpw5hZgZLbTQwvkNsdYNaKGFFs6MFqO20MIFgBajttDCBYAWo7bQwgWAFqO20MIFgBajttDCBYAWo7bQwgWAFqO20MIFgDVnVCIqENG/EVGZiE4R0c+c5ty3E9HTRFQhom8R0a4Fx9+d0CgT0aeIKP/s30ELLTz7WHNGBfCXACwA/QBuA/B+InrpwpOI6EUA/hTAWwEUAHwdwH8TkZUcvwXAbwJ4LYD1AFwAf/HsN7+FFp590Fq6EBJRBsAUgKuZeX+y748A9DDz2xec+wcA2pj5p5LvFoAagFcy8zeI6BMATjHz/02OXwHgfgAdzFx7zm6qhRaeBay1RL0EZrDY37TvEQCXL3KuAEBN3xufr0z+Xw7g0abjTwCQAHauSktbaGENYa3x72cBlBbsKwLILXLu5wF8hog+AuAxAL8B0/50E62ZxsnMzERUWowWERGMql0+x/a30MJykAMwxOegvq41o1YALDT4tGERBmLmrxHRrwD4BIBOAP8EYD+AwdPQyi9GC4ZJBxfZ30ILzxY2ADh1thevNaMeBMBEtIeZn0r27YVRW58BZv4wgA8DABG1A7gdwH3J4ScAXAXgk8nxywEoAIcWIdVg3g04N6n6uwB+5RyuX006LRrnZ1tyMELhnLS3NWVUZq4S0WdgLL0/CmAbgHcCeMvCc4nIBbALhiG7AXwIwGeZ+enklH8E8InEqHQMwPsBfOoMhqQyMy9UvZcNIorO5frVpNOicX62xcyyzh1rbUwCgJ8FwACGAXwBwG8w89cBIFkvfXFyngPgYzBz2icADMFIVAAAM38Fhjm/mNCKALznWW77uvOITovGs0NntdpyTlhr1RfMXATw5iWOZZs+l2HU4tPR+gs8t2unHzuP6LRoPDt0Vqst54Q1XUddKyQeSzMw67LnrGK10MJSWK137XxQfVtooYUzoMWo5wAi2n6+0GnROL/bcq5Y8znqBY7gPKJzTjT23X6H7Nj50tS+2+/YBrOW3Xg3GmbLKswSQwlA+YGP3KaejXasIo3VorNabTkntOaoz5M56r7b75Awy197km0rgI0ANsGsJxdWSHIawAiMhX2kaRuGWTc8BeDUAx+57XntZ71a71qLUS9CRt13+x2dAK5NtqsAXArjV+2uQXOKaGLcpq1538QDH7ntonwRLxpGJaICgL8B8GoYtep3mPlDS5z7FgDvg5EEwwDez8z/nBy7GSb0rXkE/11m/t1F6KxO5xGlVyMy51zo7Lv9jhSA6yJ/5iY71XYVgH0ANp/mkgqApwAcBnASwECyjVfHDvmZnp0jMM8hglnfBoz6m4bxssknWzeAPph1xnXJ5z4V1ful7fVhZdOqEGZdfBDAqbheHrW83AnMZ+ihBz5yW7gCmqvyfM6Vxmq9a+fDHLU5HnUHgK8S0VPM/I3mk4hoI4CPA/g+AHcAeBGALxHRg03RN2PM/FwuUF+OORfG54TOvtvv6AVwY9N2DQA7LI/CTrU1nxoCeBwmGmkARlX1YdwqM01bH4AuAGG9eHJdpmfnfgCTACaS/yMwEm8GTUEPS4GIrr/2p/7nAQA9MHHBjW3Dgu/rMeeb7QDYkmwISsOwvGfGZey7/Y5xAKMAxhbZFu6vYnWez2o943PChRSPeiOA/2DmnqZ9jwP4LWb+TCJR/3U5jHqhqL77br+DAGwHcDPMwHQjzGC2EBrA0zCuk2UYa34XjMtl/yo0pQ4jfU8k/08u+D6wUmkHAPtuvyOHZzLvQoZeh/nhjStpMwOwYfonhhmkfJhBq7ENwPTbcQBHABwFMLVaqvhFofoS0dUA7mVmp2nf2wH8AjNfveBcC8A3AfwRgP8CcBOATwG4ipmHE0b9CowUCGBcCX+ZmacW+d08gJn27S/pmTp85/izcGtnjX2337ERwEsBvCzZNi5yWg3mhSrDqKRbYCy1Z0JDSlaTrQbzEtvJ5jBzHiY6qauRPWMZYBjV9QTMC7/w/8kHPnKbv0xa87Dv9jtsGGZdDzPodAPohZHYm2EGsMLZ0F4Gfv2Bj9z2gXMhcLEw6othpGRX077XAfgTZn6G5CCiH4dJx5KCecF+gpk/lhxbB/OCPQXzQD8CQDPz6xahkwcws/uNf/Typ/7jF76+6je2Auy7/Y4ezGfMxSTmFMyc3IKROJklyNVhIpKebmys1ZF6cXB86IFPTBWP3wMYNdNu2iwYCSyTzYAksr27stm+y7tSHZvXOZmuddLNrZNOep2w3D5hOetI2L3LZWZmHoWRvseI6BkM/cBHbqssdW2iWayHMY5dA+BqAJfBWK5Xx+t9ccwA6HjgI7fpsyVwsTDqYhL1hwH84iIS9ZUA/hXAawDcC7PEcAeAdzPz5xahvRXGYJJbaAxodJ6d7vxCVJs8kOz+CoyF8j5mVsl52wEEzDyYfE8DuJyZ70u+70munWTmsWRfAcAWZn6k6ff2AjjOzMV9t98hiyfuu1VI+/X5DVdfD+Aaf3oAOvKR6tgEYXkAUAnKo1OsYtcrrO8BQKwVquOHkOneCRISAMZrk8f2g/VjqY4t3z5+55/XZ07c+5AKq1fADFZusl0C86JMwzBkHmYgO4K5l3wHzFy0A0YNbIMJxD/e1G2XJdfUk+89JGy3ffuL69ne3X1Otmu9Pz1wlZPtbcuvvzIrLKcvqEz0S8t1nKwZh3Vchz91EpmeS2aJ+tMDsLwcLC9fZq1HYn96bPrYPVb3nleNgESaSHTXJo5sddv62izXuH5HfhFxvYxU+6yyMVo69chgumvHo5abPQ5gYPrYd/yJp76U2fma3/oUgNqDf/M6AeD6hc/XznTFV/7QR6sAOoPSyI7KyJOv7Lzk5T0wy1a/8ODfvK4TwP3Lfb7J97fAvKdTMAPjz+ICZ9TGHHVvIx6ViP4QQO8ic9T/A+BmZr6tad9fwSRzePcitDfDvHQ5Zq4uOLYqEpWIehoP8HTYd/sd7QBeCZN47VUw88dZRLWib6cLozBqbM8zKSAE8BCzvieulx+YOXn/Yyfv+nCRVejBaBdecl0F86WlbNqWI/nyaGTcIEK6a0cu13d5l1fY2Gmn29ssr61NupmCsLyckJZHwkqRkCmQTJEQaYDcuD5DdqqwKHFm1jCaEGAGCUGLxIFFfhGL0UjeVQ1wANZVZp5hrcfBahJADUREJCyQ8IhENqxNtTuZLpHcu01ECmag8WHU/nEYTWUYRnU/AjO4jzTmqMt9xkvhorD6riQeFcby9itEdB0z35+kCn0NTGAvksyFR2HUq14Afw7gywuZdJXbv+gDTFS1y2EY87UAXohnumuOwRg3eu10IYXE4pnQnWStvqOC8oO1yWOPnrr3o4f8qeMWjJRzYNTAbZjPkMBZztVSnduyhS03bE53bd/s5tdttlOFDdJJ91Ii3leCpZgUAIhIYBluq0vRSHhaAJQCiRQBXZBY0sXPzXaf6aeWxOVv+5vffvJTt//+uTDpauJ8WJ75WQB/CzOqlbAgHhXAq5n528z8rUYqFiLqg1HlPg7g7xM6V8OEJHXAqLBfBPD/nqub2Hf7HRmYOeZrYQaQhUagEGZkbYfp91nJycw1HQf3RdWJe4sn7n906IF/HmQVOTBM6cIw/ao8K8vL2527btmW679il9e2/hIrVdgkba9jqfOZmXUcFHVcn9aRX1RhbUaFtZKOA1/H9bqK/DpALKTtkOW6lpPJCifdLp10h5BOnqSdEUbyeiAhiQQwK0Rp1QKrVxteW/9vXPtT//Mb22/55R88+tXf/1SiDawZ1pxRlxuPmnz/awB/vcS5fwzgj1e7fafD9lt+eW/7thtfDMOcN+OZnj9VmD52YZiuGwCYWemo9kRQGnmkdOrRR4cf+tSojmr15BwJMz8iCMtKd27ttDNdbURSkBCCSFBcL1WC8shkUBqpgHVj7pLGfGcPAICVKtg9l922O9t32R4337/LThW2kxDPeO7MOg7KY4Mch8eC0vCQsNyNJGR3HFRKOg58Zg4sL5eyvHzBSndskE4qK61UTthumkjMSsk4qKAxlzxbrAaN1aKT7tr+ywC+ATOHXzOsOaOuJcrDT2wios3MfGI55++7/Q4HwIthJOZr3XzfvEz9yagbE1HDODZrnVVh7XhQGn60PPzk/vEn/+doUBpRACwn25P3CutvzfTsYjvd0W15+S7Ly3VKJ9su3WyBSEgjgRZIHmawjpWK6yXlF4cqI/vLZDn7g+LgCeGko/YtL9yV7tp+pZ3uuIyEcOZfyqzj+sHYn3kkKI/tr40/fXDsic8di2qT2wAMCTu1Y9sr/t8vOJnePV77JkjbA0nnjNKPtYY/PYBUYT20iqBVFLMK6joKair0S3FQnmQVVSwv51mpQsHJdHZLN9cupGU30wnLY4syGGvFWkXEKgLrGFpHYKVgpr2mjxisOA5mVORPV8cOBk6ud4zjoKLjoKqiepVVUNFx6DNzTKBIOinHShXyVipfkE6m3XIyvcJOdTvZrs0A6kMP/cu/Yy7T5ZphzV0I1wKNCX7ftT/wruEH/+VOZj6w1Ln7br+jD4YxX8PMtxDRPJcZZlZEJBdepyJ/sl4cfLo2eWywNnGkSEBKem3t0sl0SCfdLmyvXVjpgrBdT0obwvYgLc/8tz0Ia/luuawVtIrAWgFggASIBEhICGmDmRWr8DEV1e+OalP3TB78+r1jj//nTKZnl+i79ge3eoUNlwnbu1kI60oGtkrLzZGQi3KliurQcR06CqDiOnQcQMchWEVIjKlJH0uQtEHCghA2hJs29yUdCGmbY0swvlYRVFBFXC8jDspQQQUqrGHOoxFgrSKtw4BVVFdxUNdRvcY6jkBCCOm4wnI8IR2HhLRBJI3UJwHWisEazJq1irUKAx2Hda3CQAflcuSXZnTkn6pOHD6pw+oozBLSY8w8sOwH0oSLYnlmrbCQUWEyFVoALK99k7P5Je+53s31vlJY3i3Csi9nrRMmAEjIRV8yFdWjsDIxE/lTYVyvEKDTICvLKpbMEYR0Ie0ULC8PO12A5eUhpL2waUtCq0ixVpqEJCIhQEKcUcIxQ6sQrGKANbTmaRI0QYBmrQsQ1CWkI4XlwNh55kOFPlRYgwor5nNUA7M2opNZA8QgMEgSSYcs2zGGYNsTwnJA0jZMKU7DlHGAOKgg9kuzTKnjc4ssI2HD/L5lBgwhkyWt5jYwWMdgrYx0jkPoOERjMBC2h+GHP/2n/sSh78CsST/FzNGK23IxWH3XGvXi4F4A6VTn1lfkN12312tbf4Xl5S+N/UpWxRFsNwPpZiFsDwSAWYMjNSvBVGhG/ag2DRXWbNZhF0PATrXByXTATnfASXdAutklX1RmDVYRtAqhVQxWcfICJS+RYQwAkMwsVFiZ0XGoAc6D2SISELYLYadZWF7RctKh9LIZaXlZIoK0XGBOOrdrFbXrOEByQ0YSkgCkDR35flAenfCnTpzM9u7abGe6NkgvC+lmADAwO6gLAUGiYRgikssyChn+VjACjc29gSFsD66dgotebYYXSliKyHycI05EK9I2zgasFYrH74Xt5Wq+MUzaMGvLE8/qD58Gz2tGhZV6Y9dlr8t7+b60cNKQlmdURscBAYjDCuKgDM0a0NowUxyCtZFQcb2ETM8lyPTsZMvNwnKzdLqXSIU1Iz3qZaiwmkirKvtTJ0Pppn0d+jXmOCRhs7BcSCcNJ9OVtlJtBWG5LhGR5eYKzSYr1krH9dJE8fg3h6WdmmRWeelktkgnk7XTBTjZHjiZTlhezqihiZSbvZ4ZKigjmB5AefjxlI6DjSDZ2771exwSFsQKrbK1iaNId21b9BgJATr96ow4E43VasvpQEKiXhy8qzT40AyMJ9gz3FCfazyvVd8N33M7hJNCqq0fEA2pYDYiNgKEAGgFhoCwHEjLhbBTkE4aOg4WN3qwho58qNBHHNVYR/WGOkcEAQgy0okZDI04qGqCiFRUDaLadMmfPnmqNnFk2k61bXAy3VuE7WWl5UJ6ediZTm056ap0MkJYbpOxyod0UgBrM3eMAmgVMeuIoDVABOnmWFj2jLC8wEm3O3ams0AkZjmxYSXVKjbz0LgO1npWignLnTUqMSeqozEaJfPUEFFtGmQ5QCI9ORnk5n1OpLOQFiwvD+nlYbk5NG5npdZaFdVNf8/On+tQUR1xUIa0U2Dzo4nm25DVAmQm8zBaSQrSzcByMigNPTFx8lt/9mUdBw/AhNgdwBqrvmvOqKsVj5ocfzeAX4bxsPk8gJ9crHManbf5pp8HkYST7Z59kYSwQNIxTGmnIOwULDcDeuaKBgCjJjVekOaXttnwsYK+gHGssUBCQsV1RLUigvII/MkTA/XpgSdVWJ52c70brEz3bifV1m2l22Gn2iG9LIR0GmryPOg4UGFlvFgde/pUfXqgKiw367b1d6XaN7d77es9yyvA8nKQdqrhnrjgHjXi+gzC6mSiEZQQ1yuYVYcbrJ6os8ZQLQApIYQD6aQg7bRhBjsFaXuQXs6o5acBawUV1hAFZR37xXpYmagFpdGSP32iWJ88WgyrUxUdVReL2lloJqcF/xc6Xiw8HsKouYdgCo99k5nPauJ8MTHqx2HM3z+CJB4VwPcvEY96BAviUQHsY+b9ZOqjfhLALTAeSh8DMMPMP7LIb+YBzGy5+Rch3SzSnVsSa+viL2oDZg4ZQMVhYu0MwGqRQZYZKvJjHddDFQW+juo1HdeqOg5qrOIya1Uhy1bSyQrLzUon09lvpdq3WG4mL+wULWbcYdYIq5OqNnFEVkb2Q4e+kc+WW5NuXtmZzqzt5chKtcFy0mBQnUhaYGXpOMC89XpWUHEIjgOoODASKKxBxwGE7Zn5daYbbrYLdrp90b5greYkdxyAdZSo1maQM8Ycd1lzV61C6ChgFdX8uF6aCSsT4/70yVPV0acGaxNHJmK/2AhZA5YeAZ+NF7kM4E5mfvSMZy6Bi4JRVzkeddn1URudd8nrPwjby8ErzHciYtazSw5mXhrMzU0b52ilg/JonbUqxf70TFgZnw4r4+WgNFwMZoanWEc+jF9pGUZ9agRuRwCUW9iQ7rnstle5betfr+Ngm3RS5vcivx7Wph52sr3kta3fbafbC9JJzWNcZo2oVozrM6fYnx60Oa4jCkoQ0q6EpdHD9dLoeKpjY3+6a8f6TNf2NstrIxISIAGwmhtkEks2WCHyi/CnBhCHFYQzwwhqU4COIWwP6Y6tSHduRapzK9KdW06jXcSol8cgLS8ZxAIjcEkAJAFK3jWtocIawto0ouq4sQNEQePeYh3XY2l5MUPHYIpAHDNznOjyMbSOmVXEWkWAjlmrxhayVhFzHLFWsY4DS9hpJaTlkLBtAKBZlYPYrCirWMdRXauwruMgUGG1FtfLFY7rQfLsvsrzy4KuCBeL1Xep+qi/sMi59wI4SERvwlw8ai+Au5Pjl8Oouw0010dddETUcYQoqEJUp+bNtxoMqeMAOqohDn3osKZVVNNxWI1VUK2y1lOqXqxq1k/7k8eORJWxMZg42DpMkHIMw5SNz5MABra+/P9uzvTs+jFpe28FiSxYw58+CSfT/UBQGvnPOKg40sm8PvaLe0vl0UTKpZHq2BykCustK9UmiQScTIflZDqQW3cpIn9aTR+9uxJWp7Xbtv7ydNd2CyBAa/hTJ0FCQOsIOgpAELAz7bAznbC8AswAEUK6ObDWcBL/WB3VdVAeDcLKWBCUR+vjT3+troKSlnbac/J9dqZzq+11bHK9tj7byfZYIlkzhVZwMh1NfRwgrE0hqk4jqk0hqpn/rNXssgkJCcvNIhlMrLg+Y9mpNswKkUSlZq0BKPN/1oDAgBDzDcTJZZE/szDrxbIhpI1U147jx776e4+cFYFVxloz6rLrozJzTEQfhVFpm+NRh5toLas+agNxbRrMMXTkzxk8tIJWMcARtNIAx9AqBoGFsFLCdXIW5aVH0uq00+1ws917SdrQUfB0vTT8tbHHPntHeeixCcxFiUDYabH1pb/wkmzfZW+23OwLmppQYuZ/dLLdD8b18iukm3lv2t2RAxqL/uWx2uSRR+LaNEInc6Wql9YxNNzsOji5LlheHiQknEyX7L3iDW1axYjrRQSlMYSVcYSVcdRnTkFHdYAYcVCtRbWpqWBmZETVp6cBBFaqXbVtvKYj139FR6pza4eb6+0E4MShL6SXT3mFDSmtYxAIOg4iFZT8OPQD1pEfFE/V68VBISzXlk7GtVJttuVmZX1mREjbm51GWE4WlpOdC0sjSgZGswyl1dxy1DM1WFr0o7DcxEHEzHlPN2U5G1THD8MrrN+S67/SIaJuZl7TBANrzajLro+axKP+IYBb0RSPSkSTSTzqSuqjAgBGn/jvUEibLS8n0z27peVkKNW5JZln2Yhq0xCWCzfXAxWFU/7k4Uemj313OtO7q2zJtk4dh33+zKndma7tWa/Qv8sr9O/K9Fzyk+VTj9xVGdn/J/7k0dLGF/7Um1Qc/mCqfWN3w5IZlEeOVUae/k8331sB6C0k5M/5Uyfg5vtA0grD8vh9xZP37c90be/rveJNN0rb64iDKsrDT0AIL66M7LcwaiSHW+hHtmeXWevkGFF1Crm+S6HjAFGtiKlj94TB9LGp0uATx6PqWCNNiwujhnNcnxHlocdIRX4xrIz7dqZrStrpdrKcXLZnlyed9TYJierYQWS6t9nC8mwACCsTYB3BzfcBMHPW6vghpDu2zDJNUBoGCRvLiUdtRMzEQQVheWzeskpt4iicXA8WxqM26AJAdexgczzvku1riuc9bfu8tv7w8Bd/+7cqw090w9hOxpP3sICVx6OeM86XOeo5x6Mmc9RBZn5vcuxyAA/gNHPU3IZr/rY8+NDjAE6SdHSqY4uX7tzqpru2d3iFDeudXM8ldqqwV1julsa1rNVMbfLo3x350u98OqpNxiRt2vzid6/vvORlNzHz24moC5iLvWxkQGBmHfvT354ZePgeIrrUbet/OQlrdnklDipHa+OHvlUeevyxvqu//yWZ3ktfKaSVMtdqBKVRqKACJ98LaXnGCuvPIPaL5lhYRbprGzI9l8D25o9XKvQR+UVUJ4+VlD9dIWnFdrrD9tr6M06mOyMsZ9niiJkThwU155ChG58VnEzXmYk8hzDtVYmvxjOlNRElLpfzjV6Th7555/Fv/NEXYKZiR5j58Nn8/kVhTAKAhMFcAI141K8BeEsj1K3pvJcA+E8Ar2yKR/0iTErQv02svp8A8AqYgPGPwaiWS1p9uy59zU9P7P/8t2BiWPUimwBg7Xj1+/am2jffbqfb30xCZgBARfXDx77+Rx+bOfHdxrx44Nqf+p9+AH/JzDc2HjwzI6pNPT11+FtfdHM9L7CzPdc1fINZx4E/eezB8ae++CQJ+fCG7/nx2zI9u24Rwjipx0EV/tRxMDOcdHsyTzMqYlQvcVSZoLA6Dh0FqE8fr0Z+ecifGZjMdGxxu3bf2p9dv7dLWo4kEiDLhTiNYz1rhfLwEyrdtQ2stWy41s16SrFu9pJa5PpYk7CECirIb9h7ukd+RlRG9iPVuc0MCMlgMNueZEDgZHBA0iatQqWieqQjP4zDWqiDalgZOxAIIadVHJaJRGQYdTYDKoOEIhIMEtpyMq70cp7lZNK1iSOHpo9++2mYeOEigH88Wya7mBi1ABOP2lhH/UBjHbU5HjX5/i4YQ1NzPOqvNWIFieg9mL+O+hOnW0ddaYaHvmveuqvzklf8mptf9zYislToq7A68dHi8XseyPTs+uFM944XSccISda6GPlFjvzpdizoYxXVipXhJ787vv8LD1hu1lr/gh95fX793t1aRULHAeJ6GfXSEIRwtJ0uzJp7WYU6rE6JsDKROCSEsV86NeiPHToRVcdrJB27beO1vfmN13Tl+q9s89r6nxH1wcyJ1bcOHUdgHc1atHUcQlhzmpoKa2HsF2txUPZVUA7iejmI/BmO66Ugrs/U46Dsq7Dm68ivgbXjtq23C1tv3Oq19WcsL58RtucI6dhCOhYTCzAB0FBBJWFCTlwKGVrHULEPHdUTCzDHOqz5UVCusI7rBIosNyusdLtnp9pSlldISzfjNTtsLETjfljHsY5qlTioVuKgUlH1UjWuz1TC6lQlqk1WgtJIJfaLi62TTgG4C3Oug5MARpi5vsi5i+KiYdS1wLmkYiGitvXX/8irui+77bel7V0CACoKEJRHAB3DznQfqk+f/HRYm8q6uZ43k7D6GtfqOEBUnSxOHPz6f4WViWL/tW97U7pr+6YGs5h55QxbXiaWTsYGEYR0oFWsVFiROg4SgWDivZRSETiG7bXZTq5burl1tpDzzQ5aRYjrJcUqIq1jYWg0PXNmsI4Qh9Umx/gqorCitF+shf7MTFybng4r45OsgjrMmrcl7JRMdWzJSSdrSzfrSMtxhJuxpZ22pZ2yheXYJCxJJJ/hMxhWp8A6gnSzs0tgCwezZT4NCMsGM1fAXCUhI+mkKYmbzQjLOyunYNYqlLZXlG52lEiMqsgfCcujBw5/6QNfYhWGAPYzc3xGQmgx6jnhXBh13+13pOozQ++K66X/7WR7NtrpduNSp5WeGXjwq/7kcZXtv+ImIe00YMLgosr4w2FtQhM51zLH0k63w0l3ghKm0nGAyC8FTqZTCtu1AMNgHNehghp07CdLEwCDwcwQ0oblZGClkigcSryBSBimV8b9j1UME+ElzX8dIw6qiSeVcYqfVQcTR/05K6w2/rk06xfPcVCJWcVRdt2lnuXlROM3WUVouOOZOPLmuV/jegJrperFQc3Mun3L9bOM1FBlGwEJWisz900c9+fmxnPJ+6nJJXAutE/MfmYdz1heW0ZIa1WMpmFl/D8e/+SP/Q6AY7xIGtrFcLGso14w2Hf7HR0AfoaZf85r6++O7BQq44e1ZaeeSnfv2CJtL1PYfP2tbq4X/vQgrHSh5k8eu3Ni/xe+o8KKm1l36c35/r1xqmOrlE5mNgKEVcgqrJGb63JVWEPsT0FFwdxcUEgQSQ1BikiS5WYsy0kZB4IEKg5Qnz4FEtKEtTUcGWBebh3XWUUhtArAKiIzt2OQEBBWGlba+Nqy0rBSBVjSOO9DWIBSUNo4f5CQRNKxOY5tStwVyfIACBiVnxBWxmGn2+bmlsrMbbWKkvmkknG9JEk40Cqa9RsGYBhdWIj8mXlrscvHfKETVitt4Ax0vCzh1wQxOwUIKxOzVmEV1kYX/aHnAM9rRl1Ohod9t9+xlbX6RZD4USJKmxdL17WK7igPPqS8tvUviGrFTLbvUjiZDqQ6NoOkPX3ky7/7GQB226brfiTTd9m2dPsmQbYHYgUdh1CRDx0HUJFPQXEI0k0bh/qobiJcnAxLN8sEFqy1EJYrGowY1yvQcYjIL8YqrIBZW3G9BMs1S8YqrFWD0vCoP31yLCwNlxZYgAjCcYRluyRsl4T0SFgOSUtaTjbjdW7z3Fx3xnIz5t1gI7m0ihGHNYSVMcT1EjD4AOaspgRiAkkLWoWwUwUIYRlJC5hBoakBcVACa42Zk/dBSMvEq0ppfJylBRX6MJqKnG+VpURSzyLRBhhgNBz+kyPMrMNqaKfbI2mnQ2ZWzLFi1sw6VqyiSEdBoON6qGM/UFEQcBxEOo5CHddD1lHAjKmgPFJRQekhYXnQkX8AxqFlBs8xnteq7+kyPFzxg//wIsvL/ZKwvNuS7HlgrYr1maH/9qcH2E613cbMnUFpCCqoIqhODhY2XqPbNl6zCTDqbFCdgu3ljB9t5CdSRSEOfZO1IKgaCcgawnLhZDphZ7ogpIRWCkLIJFoFs766rGOlwrpmHUnWsdAqAqsYcVAO6tMDY9XxgyP+xOFpHQcx5ko5NOaJje+NTcG8+Y1shhkAHkikLa+Qc/K9HXa6M2+72XlGKQabWNywAlYKQtqQVgrCSUFYHqTjQdgmbFBYHqRtIm+Ek4K0UokaLDAXz9rwKOJ51mWtFbiRTSLyzeAW+YjrJYR+EXyGAHNpObCzPUh1bDmzzzHzvAgf86IAbr4Pk09/9Y8mDnzpYRgvuMHlzk+Bluq76iAiz/La8ttu+eXXpTu3/LSb67m2cSwOaoPVsQPfCCpj6y0v/zYS0mEdQdoZnem+pBjVi5Rdd2kfiGR1/AgsL5u4Hwbwa5MgYSPyp+PIn4GOfEvFIYikcjLtSLX1SWG5gBCgZN7FWiXvrQJUYNTfxmanpJ3ukLOSaM4jx4WJKtoImIwTKqwGKqzWVVCtxUG5qoJqHAcVxGFFqqBq6ci3dRy4rLVDUrokbIdIOEQg3ViaMcYo0z5hQVouSFhYd+Ub56nfK0eDIZfWI4WQgJOBdDJYNBcGGy8yZgWOk8glHQHK8JGVKkA6aaTaN0I6K0t75E+dhFYh3FwvcuuvKk4c+NITAE7wGkm25zWjxn6pC8DOVOfW/m23/Mrr0p1b3yQsd7MKq4j8GYS16SNxdXxYem3bnWz321MdW2C52dn5i45DwTruELYHf+o4dOSbuRkAL9dnMi9YHuql4VjVK5b0cnCz3Y0IE4nE0Z5Zz+Y0aKwNJuFyrKKaYlaCSAjMOrhTDBKKpISUnhSWS8JyhXTSNJf+xLLtdIftZDrPGNjZMNSY8DyAOW4y7jSvW8aJlVglDv5NubQTQxKDE7WXkkACmpWcc4kaqOlv41NiMAKbhGXGdpX87lxmiHkgCWElg8UinGwcQWpw8+uw0iFFOmlMH7j7rurEoc+VTt7/ZRgD0pqpn2vOqMuNRyWiH4KpJzO7C2ap4PuY+bNEdDOWWR+1gag2c3nftT/4mvzGfddIO+U1XiSSbsV2crab79tOpqwFACTW1NDEYwYVXRp8hJ1ct9SJdTaul5SVapOWk0EUlLRr9xBJi1JtfRYvdG6cpVnn6thBdnLrhEkQFiMoTygdVKSVLpCdKljSySQxnV5DillY8OyqYweR6txivjDDSCw28zxpYlxn8xgJB2Q5Zn44O/+T8KdPINu7e6nuWhYqoweQ6t16zjQWa0fDy2jWKq1i6OZgCjU/NDWqT8NyckhmrCahmRHjynSOsEgIuzndKQDoOKwN3vsPfz595NsPwbhaPtJYq18rrPkcdbnxqItc92qYWjR9zFxLGHVFZRd3vOYDsNNtSHdsmT1mLJWxcQRI4k8jvxQHpZGpenFwUkU1D8LtdlL5rElU1g473aEsNysBQKsAKqw1WV5nCUOrECqsol48FQflsYqdLniW1+aRkBBCoD4zUhROSnr5/qzl5ZZM68I6ZhXXAaVo1mFAhfNVyEQ6mSyAJhjeMOx82UJkwt8ayb+EsEFSgMgyqZGSjIGN6JgzpwxV5+wgvxo04qCK0Sf+pzjy4Cf+FsaDbXIFl/sAhpi5QkSSm9MrrhAXxRw18fV9M0w8ahnAw0T0jwB+DCbp8enwYwA+tdCPdyVQ9RlAR/BJgnUMFfpJSg/fBH6H1eEoqIw6qUIq1bF5fbpr+27Ly5OZ73CS8sOHCmsyrM7lvWrQUlHNpL0MymAwyHjisHAynF9/VUG6aUgnCyKKpZ2W+Q3XFmaTjiXZE3QUcByUadaQElSg48howlolwkFDq5h17MdCOMpK5bWdahd2uuCQsBZNUqTjehDXy8ZTJyiXY3+mHNWmS1FtalrF9SqBIoBjJorBFBNRRCQj4XiOtDM2oHtJeik3193vZHtyQkrAZKYQICGldGzhpD1pp9LCcl0hHYekJRvZK2azQCyC1YiEsdwMOnfcVB558BOfB3DXSgxAzTgXJl1NrLXqu5J41FkQUQeA18Nkp29GJxGN4Az1URuIo1qdAZtqRSks28w/c93KcjJaOmkbTcYZE0weQEc+gpkidOxDq4S544BZRxokJOsYcW0aUX0GKqiBwZBOltMdm6Js7x7HSrWRsFx7gWQ63XN4hgjTKk4GgTJH/kyg4yAUwhbSzXokpDcXMmagwko5KI2O1GeGxurFgYnaxJHJuDZVwVy8rMKcJbhZxdNNx5rPiWFcOBnAw1aqPWNnOnJEllBhtU5SCiIpSViChLSIhAMhHWG5aTtV6LG8ti7p5bqklSoY5pWJw0bilEEUaRVNQ0XTzKrMKq4AHAhpC7IcScKxpGVbJCyLpPlvefm8k+3uF9Ke9YGcOvzNL8FUJ9hGRD5MipUweT/Cs2XetcBaM+qy41EX4IcBHGXme5r2HQCwF/Pro/4TgGfUR22gc/tLvEWsgZJ1LFVYm00JGvnTiP1SENWmhlXoxyTtLaxjS4dVONluJSxPkhDS5FhqZzffB2mnycl2Q9ouYJjtGeFODeNJvXgKdrojUZnj2bzBjbnjbGKxhLmFtCBkHraXJ69tvQfAC0rDsyFdhraGinwd10sh6zi2M52OkJYrnQycdLuuFwej2uSxko78ZonRgfkZ9xq5hZZKQEwAdP++H3ppumv7DcB8B4GzRViZsFNt/T1g3TPnkK9Zq9DXUb0U+zMTtcrYKX/y6InaxJGxuD6zlO+tBRO73I/FQycZZrBq3sIF3zcy88FzuqFVwFoz6rLjURfgRwH8Q/MOZh7BXH2QwSTR2WEiSi+lHh/75p9A2mmto3rotm8IhLAdJ9uTamR5iGpTKvaLx2qTx+5jFTq5/qteTpbbnl23G06qYAwYYOlkuuHlewEAcVChsDyGVPuG2d+pThyGne6ajaesz5xCMDMEEhbisIKoNo04qMLN9YJ1ZNKUlMfAWsHJdIJZBSqshySk6Nx5s5Xq2OQIaVMjntLOdIJBiIMqgplTyPRcAiIBy8mIqDrlufl1XjZVaAewOQ4q+5rjPXUcROXh/YF00jNhZbwMYNifOlatFwdDAENhZdwPSsNVFVTWARiFeXmRPDcLwJQ/fWLQ69jCwfQAyVQeDWt2WJkACQt2ugBmrXQUclAeolT7FgLMWBSURiHddJOzho+wmjB74goYlEZhpzvITrWlkWpLSze9zvLyl7dvu9E8ex2r6vihunQykyooD/tTA8dnBh6ciGsTNsx8cwBmwLkMwJOY0xo2wjBmo2KbB2B7ck4D1xLRJhgNAjDCpR/GljLFplLCXrTiUZ9xzV6YXEgbE+ZcivYZ66PmN91wRFiOm+7e3ivtlC3tNKSTgpBOhcETRKJuudmMlSr0WG7WPVPiZ04SWmsVwRhyjLeNiuqoTw8g9KeBOErcYCWMT30IZoW4XopVvWoRAWQ5YBUV/emTTwczQyd6LnvNxvbtN+1xMh2F2d/SKvanTxz0pwdCjqPtwnZzwsnAcrJgYAo6qkBIIZ10VlqplLAcR0ibaJlGoWfcm1bMOtZaxTHrWLOOlVaRZh1raKUZZAMshLCEV1ifWhFt1rPODgv/M6t58a6s1ay1d6mQuwaC8nipXjx5JNOz+wkhxJGgPDpUGdl/avro3eM6qq2GFbcC4NDpLMIXjVP+cuNRm87/c5gI+9cv2P9SzK+P+hEALjO/ahEaeQAzV73jE7C8JdZNToO5Wi8xGp5BRGSso5Zj5rNRHSqswp8eQFSZgOVlIawUIASielXVxg5Mh7VplW7f3G1n2gWIoOOQg5lTx6ujBx4Vllvpv/Zte3Pr9+4S0p5VPcPq1GRQGp4Kq5OhtD3PyXSl7UxXxvJy+ZUy3zPua/ZdMPmIzpXecwGTfM4szTQ+m2gc3ZiywMl2z3N4YK1jHdeKcVCdjuulaVUvFSN/phTVJmfCyng5mBkuqbDSWOuZhpme+TCSd6F2duh0DHhRWH0TLKs+avLdAfCDAH5iETpXY4X1URdm02PWrOMgSELCHFbKOLCj4b6nwdo4lwvL5Okhy0McVgEVsbAc0kn60DiooD51ApaXh9e+Aaw1VFCFXxyMwso4SyfVkWrrF0qFoHqJvXwvwU6Tm+vu6d7zyn3prh0bZoPPtTKlM4yVutPJdHSaFKfpWWbScT1xkG++n0bEiYbJ0BdXWccVsI6YOQLrmJkVWMWslY7DmiWkIwF2ALITLwpPWI7HQqTIFBFO8lg3EpU3uQGCwKzNeu85QMd1LLeGsik65aCpcN4sJg9/C1HVrMossDBbMFXfu6TlQWY9ONkemDx4CV3bS0qRSJC06k66c5x1NKHC2ogKq/cf+sL7/odVyHiOeGjNJepaoDHKdV76mqPB9MCwcNIq3bVzU6qtv9dK5VPCckBizn4ipAVhmQprWoUqrE5BRXVZGz9s1lFTeTQKurGOEVYnwVrBLawHsTJhZVEd9cpYwuQOoE2meRMtMo38hmuQ67t0XrXtxPEecb0MEgLSzcZCumM68scif3pKBeVSXC9VIr9YLp96LEvSPhz7xXrkF0MhU6nC1hu+J9W17UbLSc2GomilwmBm6GBl+PEniyfuO7KgbmEfTGpTwIjVhhe8ABBLN2s7me42O9OZt9OFdmlnOqWT6iDLaxfSzamwmg4rY3Cy3SApAbKaqreZPFRCOrPO92apRmgiqUhIJmFpEhb86ROU7dkNYXtOUhTrrJZspo7cBR35S6Y3XQ5UVEOmZxfsdDucTOfs/qA89u9P/MuPfwDAE6ezHq+JRCWiXgCvgrGutsOoBY8A+NLp5ovnK7I9uzZ2br9pq3RSyctgzVZra8R4qrCGuD6DeumEVkFFCNuTrGKEtUlINw0hJFRQNQwXlOBPDSg338OCyKqNHQSz8iN/ZtTN9XSm2jbk4rCKqDIeqdAfsTMdcaZ7e1emZ3fWTuVn9UwV1Y2BqT4DFdUbeYbjqPrkpArKE5FfHI8q46P14qnRoDRcnBd1TcLx2tYXvPb1edbhdFga+San2jdJN7NZWG6nkMJJdWy6PNW+8fKuPa/SzDoCcwQQmwJx8710mspANNU8ZOamRVzzouop5u6JVMdmATCBWTJYApBglknuKEuryCIVSo7QcMoQzCzAjSJUDK+wAVaS5tNkngiMB9LsXDVxLtJNqWFMiQoz/5c2oBUsLwdtexpaK2alWMexiZ5RilUcG28lpVkrZoYmaIJ00tJy841+sFPtSTvmL6eyjg8DOPhcLfEsS6Im+Yk+AOBlAB6EWQIpwyyj7AFwLYz73q8vFolyvqExyu1956dm5y7MOqnMNmWKOCXRLlJaYK2hlYkRjevGIC3dHKSThrA8xEGlNnHgywetVC6Xat+6BawkgziqTRwTZGeslDEJMyMmYNjOdmivfet6L99tWV7brPqqwlpUGn5iMKxOpm0312u5OZDlclyfIR3OnxqRsCDdLKSTjoXl1qWTUsJO20JYnlah0EntUlbh2SVPWEO4+T40ldU5I+Ybn/Ssum9SwM6p/7PTgHlGq+Zj/AwDlVYRhLThtfWjMvrU/zv8hfd9VbrZg3G9fKaVCQDPvUT9JIA/APD2xfLFEJEL4E0wOYz2nW1jnmuURw8irE5AEEHr2NSakU7iTxoCDKjEghvVS4jDKtzsOnbzPSQsF6zCaOLprz1VHnzYL2x94aWWl88BCsy6ElRGipaT3SbtDKx0O4SQdUC4RLRRelkIAWMAietgrSrj+79w99ADHz8E1gyg1L7jpZu697zyTUJaWSfTCe3lTxJQtNMd/VYqX5B2ymSCiENLhdWsinxElXE0GyBZx6zjMGYVhSYTfL2mIr+so3qVVayFk+oQltchbK8dSSgcUVL8GFyD1hWw9hkIpOVIko4UxrvIbOazIDHr3CBBgiiZtBIRkbBszOY1WpiVYXFj1VIeS0vBzJMBWrHr/dLQcQh/egBefh1I2pBOGtl1l/0ogI8ul0lXE8/vOeruV0PaLtKd2xaekPjlmopgkV+EV1gf53r3yEYl7urYwVPjT35u0smty2Z6L9lERm/mqDY1Spbb4eV6HSfXCyEkwtoUoDXIcmCn2llYDgGAjkN/6sidj07s/8I0wMMw/qglAJztu6Kja88rr8z3X/UCK5XP0WykDSeV4iqIg4qO/ZmSCv2wXjwlrXS7BHEODGkc8pEEXycvPkmz/GS5obDcurBcJSwXwnaltFJeUBlz0h2bz6lv/emBuUTba0hjNehE9dJUVJmspzq3ZACUiegND3zktodWQuNisvquGby2fkgnBSe/zvjVxgFivwwQYLl5WKkCsqld7OZ6IaRrJfl4mEgg071jffu2F66fZ/xRMbGK1pllmxBhZQJheRQqqFUtNxu5ud4CAGKtdXno0cOjj/3HfhWUKzDSLMz07tncvu3Grbn1e7emChu6mg0oKvIRVScRVqcQ14sjpYGH7yievP8J6aRgewVBwupwsp1KutmMk+nu9wobttqZzj7Ly7VLO50StmsL6TREmINFFuIXln8w88HmXEXzkqIxwIrRWOyEqadIQqiwqk3FiYZlGAQIr/liQywpSWGKIQshTcya5Z3JMW15qBdP3VkaePDL7dtufNzJdteB2SpvzZsNM4XLJ1tb8v2vH/2nH3yCiHqYeWzxX3jucE4SlYgeZ+YrVrE9zwkWm6M+W2ik5zTO+1X4UwPTIw/9y73+1LEygNjJ9XL79pdsza27dJeVKsxaZ036E1UJSkOj9dJwRYcBZ7p3bHHzPQWQNO96IyxPyNnSDiJR30+3BjrPKNOcTuUsravnK/ypE9/d/5l3/xaAB5h5TaqFny8S9dz0JKxePGpyzrLqozZjzrBgYh1Vkk5TSMt4GrGGDn2O/BnSKkxKCrqwnAzsdGHW9M9Jdr84KGsiASvVJmSyziltz6wtptvhZnvavfy6V9TLI9MAtJPp7JlXTLg+U/enB+KoNu2BdVZIN0uWDSfTARXVEFanYKc7IN0sLCeTGLQW91LTKopYx1UTriXKJKyikNY0CVkjIQPMOalHME73SPo0m2wpmAXKNIAsM2cA5BKPsqT/uPF8lujfJMWJjmfTtgCA1sZRpNH/c1JbJ9P0s6uuqONwCqynVVgbOvr1P/hHAJ1N93bB4lwlapmZz0lPWcV41BXXR011XzIY1cu6rf+qPrIs282tQ6Zrm7Gm2h4gLMT+zLz4SGG5cLJdEJaHYGYIcb2MoDyiiifuv7t49NsPAKw7dr60b+MLf+q1lpstAEDkz/hgTVaq4DW/0Ga+WUdQHkFYm4I/ccw45jdy3YJB0g5BNAWtp1Mdm0WmZ1fBK6zvFNKeN8hG/owW0npMRf4XdFS/S0X17z712f+1rJSWTf1SaPirng77br/DBtAD4zTQAeMJdhmA3XG9tFO6uXUA2hMHlRWjueK4bq6wl2gCs2lidPTMuN8mVMcPfWvgrg+9D8DjZytRl9snp7n+vJCot5/Lxascj/pOAB/lpHgPEf06gPuJ6KeXcspPd27vt72cSHVtRap9C5xsp7H6kkDkFxHVzHveqC1qSgPa8KcHwqA04gQzQ5BO5uTkwa99wp88Mi0sL7X91l+5Mbd+7y1ERFrF0cRTX/z8qfv++cnuy15zbfelr3qhm+vNAUiyLxCkk4KK6vByvbDsFKJasVKbOHKwOv70k/XiqXLbpmv7Oy95+e5s7+7twnJnX3xmzWFlfEgFFQg7tT4sT4hs3569tuWepIz1rw985LYVMWmCLTDr4qfFAx+5LYJxjDi18BgR7W08g32339GlQv9SEO2OqhM3OrmeW4V0zhjYH5bHZhlVNNa0lwEVJRUAkjjhsDw6BKOlVc907WmwBcvok2cby11H/WUAf3a6IO2E6X6OmX9v2T9OdDWAe5nZadr3dgC/wMxXn+a6DhiXw5sboW5E9CiA/4+ZP5F8Jxi1bh8vqBhNTYWMvbZ+NK9lxkEVYXUCcb1UjaqT4wBcO9vdJywHUHEU1qYtVgGRtMPa2OHPjT722TsBzmTXXZbb9or3/oCdbt8MAEF5bODEt/7ys5nuHdvbt934Mq99U1pIG0kRYgQzw9BAbFmWz6A0NU8OTeSLtrx8LJ3UHHNqpWuTx46XBh9+aOrQ179TLw6OAUDHjpt2dux82Q9ZXn5Hw4tKq/DLYXn8/Ye/+L57mXmRsujPLZJgCnOPJNG2aV8h139Fn9fW32el2vssL7dROtntwnI3CWl1npbYMuFPnfjQ/s+8+7eZefTMZz87eK4lqgPgCBF9Bcax4QDMSJUHsBvAS2FUzr9e4e+vZjzqiuujpto3zY7cKvSj0uBDR4vH73m0Ovb0AEkn1Xvlm97kFTb0mQiYWqQC35ZOCnE9Ojzy8Kc/5U8enQaQ7bvmbRvWXf3mdwjppJm1nj76na/70ye47+q33O629Xl2ugMAI6oVEflTilV8d1Sb/PzwI//23frUCQZRvueKN7ysbdML3uAVNmy1U/mGh5ADAHFYQ1SbmqpNHnuqMvT4gekj3xpQYTUNExRdnjp856Gpw996X8/lr7u2bdN1b5JudguAW61U2607XvWbD/dd87aPjTzymS+BVQUmL20AoM7M4SLd8mxhrhoaK8ycuBczJ+5d8mTp5tB39Ztz2XWX9VpeW6+w3R4Sdg8J2U0k2kHUQ0JeRiR2LkXDa99kryWTriaWPUdNpNg7ALwWwJWYcyF8DMDnAHzsdNkUlqC5mET9YQC/eAaJ+jCATzLzHzTtexTAB5n5k037IpxGonbsuBnC9uLaxOHB2uTxr0PHkwCKnXtedU3Htpe8nlVgCyfNwk6B45CiejGceOrL95ZO3v8fAEsSlrXhhh97Zfv2F99spwpQYa06eeTbpwjYlum5xHJzvRB2CqVTjwE6eoJ19Jnp4/f+19TBrxVgjBzHAaDvmrdtdQsb/ldh8/XfI+2UAICgNMqRP01OtgeWlwORmK3vCSAOSiMnisfvGaqMPnWiPPTEfh3VpgFEAK3vueIN7W0b971JupnNjXhPgjhRHT/4hbEn/vvbsV+0YSqyaxim3QYTEjhhaMwuVTzOyQtCRNfD+LXWku8bYKKTjiTfJYDrAdzXSF+SJIYLmHkw+Z4GcDkz39f0LPYAmGwsgdDy6o/2AOhshEY22md5bU9c9Y6PA0Dv5MGvXXryrr9+WkX+4TVo38J41J/FhRzmtprxqHQW9VHXv+DHjgw/9C+f05F/L4A2J7/e3fySd78j07X1amF5YB2roDQsw+oUgpmhk+MHvvSZ+tTxQwDavPbNon/fD/1E+9bv2QYAYXUyqheHbGl7kE4Kbn4d4tCfqY0f/PeJA1/5ZPHY3U/CvPyzEr5rz6vX9V3z1p+OasXXZLq3N5wgTlXHD33y5F0f+p+uXbdsbd/+ordaqfZbwMoGaNFYUtZKzQw8OCGkfdSfOnGoNPjw/tKpRwa7dt96edum615rpzuumDs39oOZobuLJ+77evHYd/YD7MOEbtVhwkcWZjNopGs509ZI23I5Mz+8+BNfHprnuWtN51xptOJRnxmPuuL6qJl1l3+iOvLEwwAe3/yS91zbsePm/ycsJw8AYXWSq+OHSQWVuDp24GsTT3/ly9CqTNLp7L/u7bu797zqbaxj13Kz8IunoMIKiCTsVBuEk50oDTz4B8e+9sGvwsylFYD1SNz08huuzm+88V0/5ub73kpEdhxUQMI6Uhs/+Pcnvv1X/xTMDI0wc6MIcldh6wt39l39lu/zChu/X1jOeq1igBUAioXlWMB8SykAqKhejWpTJ6La1PG4Xq4Iy9sq3cwVRHOOBzqqDfvFwe8Wj91zT/nUI0NJ+6ZgmLahIq/0BcnC+IE3GFct+MyYqz/b/Ln5e2Ma01zFqnnDEvtnt2Tqc04WW+D8sfqeD4xawPLrozoAhmDqnv7nIrRWVB+1Y9cr/rp08sF7d77mt34w3bn1FgDQKuTKyNMUVScQBeVT04e/+e9eYUMl13/Vbulk9niF/h1e23rb1IGJUC8OJpntCXa60w8rY3956Avv+4wKysWkretgvF1A0qGdr/nt12d7d7+HhCwkvzdUGXnqQ0e/8vsfU2FleLFojMQwlpdurnfzi3/21bn+K95sefkbG8eVCkMVVI9DK7a83AaxhEc7s9Y6qpe1iizWKtPIRqFVCFUvHfenT95bPvXIIzMnHxho4s8ISTKwBduFkBisCqNlVdaqARcNo64FGp3Xc+X3PZbp2bWtY9v3ZAEgrE6iMn4YBK04jgatVCHKdF+yXkXVVFSbhnQz8JIEYiqsKX9mWBPYFtICSe8bQw98/IPTR+4cg8nREwLYisRg13/d23f0XPbaX5ZO5ioAYK1mapNH/+74N/7kH+rFgcPLNewkARBdvVd9796u3a98m5vrfT0JOZumQkX+gXpx8O6wPO5bqbbtdrp9i50qrJdO+rSpLGazVhjGrcf18mBYGTtQGXnqserYgeHq2NNFVtHClyXGXBKw5s9qwbYoMj27M71XvnGvinw/qk7M1IunZioj+2fCythqWqk1gCefY8PZLM4LRiWirrVyzToXNDpvy8vfC8t2kd9wTVKlrIxGMmon0wnpZhGWRxHXq7BSed/N9aQAIPJnKmF1MhvXZ2Cn2maC0sgHj37l974Mo/IdhzEU9QOAne60dr72/T/hFTb8aGLQQFgZ/8+Be/7uj4vHvvM4MxfPxp+UjKd9Id29Y9OGG378TXa6/Qe8tvWzFlBmjmK/+M2ZgQc/d/KuD+9PdWzqza+/eleqY8tWN79uk5Vq67XcXLd00u2NayK/OC9wvRnMzKzCqo6DaRXVp3Xkl1Xkl1RYLcX1cjn2i6WoNlmuTR5jHfljQXnMj+szURJjt5BxFQB9xQ9+9NecbNf3LPytsDrpW25umnU0pVU0qeNwUsf1SRXWJuN6aTIoDY+Uhx4fnjn5QBGnT7vbMHgeZ+aVJOCexbn6+q4poxLRdQD+HGZOk4Zx1fvu2TbiuUYzowKMVGGj8XwBYKfykE52JvJnjlVGnjoZVcfHOy952a1OtnsjAITVKT+qTaUAoDZ+6KHiyfvfWzp5/xTM4r8Ps1zVDcDu3HXL1g0veOd7LC+/FQDioHxy9NH/+MjII5++C8Ag5nLMbgHwOEyu2RUn3SKiNIhu2HDDj/cUNr/gjU62+9XNUlaraCwsj31++th3vjp0/8cHAN1wC/SsVMHOr7+qJ921o4tZbytsuaHd8vJbhXR6Scj0Sn1/myNWmLViHfus4hrr2NcqqrEKfR2Hvlahn+ne+Yoz0TgdzNhRG1FhZSyqFYfD8uiQP3XsVGnw4aHaxJEyjO3jCQAHzpZJiGhPs3X5LK5/bhmViN4MY9o+QUS/ClPXhYmoE8DvA/hTZn7y9FTODzQ6b8ONPwMdltnN94+E5bGH47DydGXo8cPVsQODAFLt21+yZ9OLfubnLDfTzswcVsY5rpcEswrqUyf/6sS3/+pfwMqBeRBZGN9nG0TYfsuvvrpt03VvJSEsZq3LQ4//55Ev/86/68gfxOk9ZWI0JYlesEWnc15I5rKFVOe2deuv/5HXZ7p3vkG62RuoyUys43AgrIx9tXj83q+cuu9jhwDtwTBtCiZdZmNtvW5nuqK+a966I9d3xTV2un2vsLwdJIQ76587/8e52Wf5fMDkwa995Pg3//Q/AXz5bAbA1cBaMKoF4DqYl/HFAO6EWWdrLKu8l5k/eLYNeS7R6LzOPa8eLR67505VL34FRhrOwKSAzG644cdf3H3ZbT8npOWxVjoojQoV1aDC6onJg19//9Shrw8n5MZhRu4NAKTXvjG945W//i4333cNAMRBZWjogY//9fiTn3sUJu/wuTqIM+Yc6ZsNOws3BlDoueINl3Ve8orvd/PrbpW2t6OZUMK0XykNPnzn4L3/uD9J1iWT+3FhGNeFCQVTJJ2g5/LXrWvbeO0OJ9e7zXKzu4Tt7SQSS+ZRZaMzl6F1hVnXYIpAa9axAIiJSAgnfQkR2UhSvhCRCYEjonNh/rAy/s3HP/ljvwSTKXDmjBc8C1hr1fenmfnDyZrmHhgV+IUAPg1T+ap4tg16LjC7PNN3xWeqw48/CpOx8ASADEj27njVr9+e33DNO4mIdBzqemlYsIpQnz75pYF7/v6fVFACTOrIIoyFuR8Auva8esuGF7zz56WT7gaA6vihrx7+4m9/IvaLg5hL4PxcgmGYlgDkuna/clfHzptfmurYeqPlZrY0n6hVNBPVpu/3J4/dO77/8/eVBh+awtxyh4Z5xhYM01owDK2EndZdu16xPtd/5RY3v67f8nKbhJ3aLCx3M5FYUX7fFd8cc6xVOMxxNKzj+qiK/DEd12eE5eVYx8HAd/7mM5XhJyowJRPPxvf5nLHWjPpaAHlm/pemfb8Es3Z5dXLs3862Uc82Gp2X33T9n5dO3vc1AHcA2GKnO7bsfM1v/XqqY8vNgMnaXi8NQ8dBeerwNz4+sf8L98I4B5yCUU2/BwkDbrn5f724Y8dLf5yEsFjH9YkDX/67k3d9+C7MzV2XbA6Ay2HyUDWy/gHPXBtsrCkuhUYW+DPBApDt2PmyPZ07b74p3bX9esvLbwJM6cZMzyVgZo7rM8fq0wOPl4effGry4FcPh+XRpcpGNK9tWjDz7YMgEef6r2rL9u7q8ArrO+10Z7fl5Xqlk+kVdqpbWF63kNaiy0iNdpwrTt79kb8bf/KOv8IZMgWeDkR0fbOn0llcv7ZWXyL6XzB5kr4LE+b0pWbGXQGdApYRj5qc6wH4IIAfgFHLDsE45peJ6GYssz5qo/MKW1/0O8Vjd90H4HC27/Irtr7sl37byXRcAgCRX0JYGUNYmTh46v6P/UtQHDictG8Yhmm6APSTtKNLXvuBt2bXXXobAKiwOlU8cf/dJKw407vblpab1ipM6zhMsY5cHQdaxxFrFWpWEbNWsQprIREqWoW+joOqjgLf5Deq+Um18FpUK1aj6kQ1rpfqSQauxtYo4GQl995c1Ek3fV8MEkAmv+HqDR07X/ZCr7Dh2lTH5suEtOdF0zNrHdfLJ4LSyNO1iUMHpw7d+XR17MBSqqQDo44vhnl1XqST7vfaN3d6hQ1pN7cuZ2c6spaXz5Ow2u1UmyssN03SyQhpp0jaGSHtlLDcwhK0n4Ha5PHvPvXv7/kpGEY9qxedTlMSZZnXnxfLM1th/CefZOYnzpLGsuNRyYTAZQC8G2ZueAWMRS9IGHVF9VHzG67949Lgg4Ndu19JG77nx39F2qlOZkZYnUBUm1blwYe+OvTgv3werCdh5pfFhESv17F1a8f2F1/VteuW77XThU7AeAcFM0OAkLDcDHQUoJGQe+nGNCfyagjTRYw1CdjkefFZx1Wt4hrrqMoqqmkV1bQKqjoOazqq13TkV1VYq6mwUo38Ui2uz1TCykRZBSUfiy+XKACWsFNez+WvuzK/fu91XmHDZVaqbSstYvpVoT8e+dMnwvLo8drkseMzJ+8/URl5cvpsUx72XPHGfR07XvKzzLoIraa1jqdZqUlWwaSK6hMqrE0G5dGR6tjTw2F5rJJq3+g52S7bSrV7lpv1pJNJSSeVstOd65xs1x7WKjh138c+On3025M4Qzb7ZxPnBaOeK5p8fa/mpPQiEf0RgJ6Fvr5EdAmM7+6mxebAZ8Ooqe6d/9q2fm+6f98PvYqEdFgrBKVRhNXxqdFHP/up0uBDj8Iw52B23aVW75VvuoaEdbOwnGuEk9vgtfXNZoWvl0bhTxwGCaso3ewQ62hCx9G4dFJSOum0tNN5YacKwnIKRDJFQqbIpHhfErPZJ5rrrmg9+3m2JouOoZM0u2cCm5qMAWsVgFWdtQ61VnXouM46DphVyCoOVBzWdeSHrGLY2c52N9fb5WS7uu10Z5eQ1qJRVzoOqpE/cyqqTQ2FlfHh+vTAqfLwk4OVkScnweq0L1r/de94RX7D1e884w0AYFYBx8GkiuqTOqpPqKg2HtWmT9WLA4OlU4+dUqa6W7Nb4gmYxHENDaPZNVEv+L/o53OQyBcFoy47HjXZ/39hJO7bYSTqnzDz3yTHbwbwFZgHctr6qI3O23brrw20b3nBRsCkh6yXhlEdfeqBU/f/8x0AjXftekV3+/abNnr53uuEnbosqk7IyJ8BCQteoR+xX4Kd6eCZkw/dM3HgS98MKqMPFTZdj8Lm61/g5Hqus9zctSTkGTNgrFKpQlPVzVQ01kmNVMGsLW5OXt2onbrIasXpHB6AhvdSCCJpSj7YHqSTSQLq5ZL3YurI1qAin3Vc1zqqaxXVNce+iqMgJlaKLDcjTNQ+4qCUxAgDmJeHDAA1/188/WhSdANDD33qQ5WhRx+CcUJZqP7zGf430AXzrjX2L6whC5gqd0OLLQGtFqOudRbClcSjboQxunwWxsH9SgBfIaJDiZq84vqo+f4rNgJGZfWnT/mVkSeeZBWqLTf/73emO7dtcjIds3ppWJ1EWJuKwHw01bF5k5B2SkV+MPHQp/545sR9Q33X/tCVuf7LPyhtb/vC32GtijquH1dh7VhcL51QYbUYB5Vi7BdnwupU2Z863u3m+4bBikGCLDfnSTfrSTvlSSedkU4mL510QVheQVhugaRdENJuJ2F1kLS6iITHOgIRgaRlQZ75sTKzSmrR1FhFgVZxGNcrELYXsFk+gVaxYK0kmC0SZAPkGGYSFuvIiYPYjoOKXS8OAiQgLCfx7iKTI9n2QCRMSZBUHkk1AAnMT8Cr49Bkw1chdBwimDkFy2ub/c6NQlC6seq0PKQ6Nr+iMvTogzD+1vPQfdlt2+rFwZnq2NPFBTViF6IH5j1tRgVzwRaAsdF4aI65XWWsNaOupD5qDcYI8f7Egnc/EX0aJu7vG3wW9VFPfPtDRsIww82vS9mptn35LS9Aqm09hOWiXhyKtQoPx/Xiw8UT9z0qLI/bNl7720Laro7DcmX4yc9KJ/t9W176iztT7RsIaDD9wLib7f5OvTh4/+Shbzw4dejrXTCxngGSpZJkOwYzOp+C8Wg6DBO1ApiHbyfHCOZZXQbgacwtl2wEoJxcr5/q3NZpp9u7deRfkt9wjS+dTLt00u1hebzPznZm3VxPnoTdrmPfDiuTSHVslkQm7tSfOgEn24NM93aACJE/Ax2HSHdsAgAw63pleH/ZbesfJyEmWcUlvzgYq6ASuPl1IzoOayqsheWhR7u99i1H/MkjQVyvBNWp4z1evq8j27unzU4X+klYPVF1ojfXf1Va2l6ehLT86QFYXs5EHbGHOKhCenmkOrfMPfjJY3CyPWber+I4rEyEsT+t3HxvYBKMh1Fl9KBw8z0xIDKAzoXliRoJeaT3qu+7VoXVQLrZrrAyuUlY7pG4NjW16cbb/09QGgbIYsvLllRYK4aViZnq6P7YznQORLXpUlSbrtQmj7EKq6mwPDoU+8UIxp6yqfGeJf8vATBERMeZOV4kHvWcsdaq77LjUYnoZTDqbLphaieivwVQZOZfWoT2ZpyhPuqGF74LUXUCqc4tsFKFwEm3D0oncziulw7XJo48OvbE/zzsTx6tAujpvux1L9hww4/+ppB2WkX1sj95fEZY1gY33w/ppKFVNB5Wxr4wdfhbnx9+8JOnYB6oDcNUjQiUAM9uRrzG+mbz1tgnAJJOfl2b19bfYac7OiyvrV262YJM5s5COm0k7TYSVp6klQfQVBOmUfLBTN24kTkQAMBNxi+eK9/IMKq4qRYXgnXY+MwqCpkZQkoiYUsSthDSkiRtS1iOLaTrCMuxSLouibNzehCWAyfT9aymhFVh9ZGjX/uDXywNPFgE8CjzfEPBRTFHBZYfj5p4Ru2HKZvxuzAW368DeAMzf4vOoj5qdv1Vj4Sl0SNWqvCl2sSRJ6GjRvzlDMxc1wVwQ+euWy7d9KKf/lUh7YxWoaoXhyQ4hp3pARG+Uxp85JPHv/UXj3Jcb4NhjGqyLTdiQ+CZ856VYiENa5Gt2VmhsdkwErvB1ARAgIRlp9vzdqo9L718m+Vm26STyQvLy5Flp4SwPbJsj8jyhLQ9EtIjUyvRWmlJimaQkLDSHSAYlVipOlQUgMAgmBKPIILJVCNAwlQlJ5JJQIU5RwjLlKMTFlKdW5edIG0hmjNQLoXK6NMffPq//s+HmPnkM+7nIpmjAsusj5qoFK9Pzn0vTKznLzDztxI6V2OF9VE5jo+E5ZG7wvLInTCqdQVAmU25960AXtCx4+ZNm170078mpJ3WKka9OCS1ijSR+Mbxb/7xnaWBB78Fo75nYJj7bEK0LoNxyl8pGnmVXJg5+1HMZcFfrhRqSIAYRpU28yzWiKqTU436oqf5/ebSjETC2m1nOsakk00LO5UW0nGEZXskHIcsx5XS9khYLgnbIWk55rOwSUiXyLLzG69ep+OgkO3dPe+HWMdQUaB1HLCO61BxQDoOhY5N5kEV1WZvWVoOZLoD9cnjSHVugQqrIDcLZkZUmzYGsSRLP4NA4EQheKbQqk4cQbZnF4TtwXLSsxX/Zjsv8h+vjj71T4sx6WpizSXqWmCBw8PdzPyFpmMugEsBXNq5+9atm258168LaTtaxajPnEIwM/RA8cS9H5o6+LVDMINCBU1J1c4S86ShdHMy03NJ1mvrz9qZrqx0MikhbZdZZ2FU6oyJmJGuKdpk28xaCmnBiJuGKZQoccinpFBTUrxJ2iSEQyRtkLCJyAYJi7WWpqwECRCs5L8gkARIJmIrUaMbkpeI0BB1EForEkImNRAb55Ew2+neNXPMyXQD4HPK2K/jECqqISiPg4iS0iVnp/6yVpg+ds8XheWoTPdOK/KLnznypQ/8c1AeDZZz/cUkUc8LJC90N4DtAG3deOPtt3btvvXtQtrEOkZ1/PDAxFNf/KepQ1+7G4apJBbJa3s6ZPsuz7ZvfeFmt62/z/Laui030yWsVI+wnG4SVicJkQWJ7LPtI3uxw1QzcEAkocIKhH323UlCojTwwO9PHvzaPTwbfP6eVWrp8tFiVMy6Jm4G0Jnp3XNt/7U/9K5s36WXCmmDdYyJA1/9+sm7P/R1mJQe3TAGsBim/57pnkcSvVe+cVN+/d49XmHDTsvL7SZpbycS3WfTPrM+xyGMEa2RdyiZpJEEqDH3XIrCErvMHwYrMOJZ10SzHsgwxSXmak080xmAF1Kc/z1Z11xUCW9eAAUBTMwsF57QGEHnzl1AjUgKabdjEYSV8Q+cuv+fP57UyABrlfyPGUTmPwBW8dx+AFpFSQQPwFpN1WeGnvMyiwvRYlSTBeBS6ebTfVe/5T3ZvsvfmOrY5AhpQ6tYnfzORz42+dQXj8EYhqZhXtRCsvXAeL0UO3a+1O3adcsLvML66y2v7ToS8nQeDMMw0niIdTw8c/JB2On2obheqquwWonrpUrkF6uWm0+lOrdsc3O9l1pe/jJhezuIxMI1PQBAUBqGm+8Da1VhHY81ZUaY0JE/EQeVKRVWZ3RUn4qDcjGqTk7XJo5OVUb3V1hFjdC4xhy14b3T2NRy4zmJaDsnKUTPFqtBYzXbgsWXC59TPK8ZVce1HgDldde8bWuu74qftbxcn1dYD8OkUXT8m3/2d9NH7hyHWdscWnh9qnNbpu+at70x27v7Bjvdvlgi6AkYt8cHYTINPA3g4IN/8zofZoDohjFC9QAYy2+4Ot9zxRtfkO277NWW13adkHbvYu1mrcoqqh/WkX8yDsqDUW16sHji3mowM/Ro+dQj40gSbGN+3Gp0pggSInJXwSd2WXO354DGatFZrbacE57XxqT85hu+lO3dvbmwad9urTVShfUQlgOtoujUA5/4XHXkQMlyM3U721MmAqugXI3Daq2w+YZt+Q1Xv9DJdl/enD2BtSrG9dJ9UW3y20Fp9PPTR7/9yNSRu3TT77owzNmJZJDs2HFzV88Vr7/Va+t/ubDTV9CCtQ3WqqzC6mNRbfrx+sypAzMnHzg0dejrI6xVDSZ8rvE/OF32hxbWBi1j0iqgsOWGWziuC60VvLZ+FpZDrBVKAw/ZXr7vjdmeXbAzXcY1T1iwvDwsLw/R5KLHWgc6rj9dnzn1lVP3/uMd5aHHGmoSAbiCiCZhHlQnjJWYUh1bvI03vuuVqY7Nr5JOZt98Zte1OCjfF8wM3VMafOihkUc/e5xVGMJYlxubf7ZO4i1cmFhzibpa8ajJ8WXVR22Mcv373gHhpNC586Ww3AxYKxRPPgAdVkBWCnamI7bcrLZTBSGdjNXgJ2Y2fqesYNb456Dj4HgcVJ4IKxNPlgYe3D/y2GdnOA6yAKqdl7wss+7qt77azfW+gYScnWtG9cqkDitfqYzs/+ap+z72SFSbjGDmRSWY0pFLBW0339M5xU1ebDTOl7ZcTBL1L2Ha0Y8kHpWInlosHhWmCFUGxiupEY8aAgCZTPm/ifn1Uf8CJs51UcSxX+3ZebNtuRmHteLRJ/77rpnj3z0gU4Vo3ZVv6kl3bbtJSGfWUqtVNBLVpv+Dof+L47CnNPDQre3bXpSTbmaXsLwdRGQJy91iS3sLgNsKW65HftO1ddbqlJtfl7ZT7X2zzK5VPapNfbU8/OTnj9/5ZzXo+HEYxpyCYc6VuhpeDuCsMxFchDTOt7acE84XX9/ViEf9BIBTzPx/k+9XwNSoWbL2zKXf/1d+qmNTirVSY/s//zlWoUh3X7Int+7S7Y0Fd2bWcb10V3X0wKcHv/v3/1qfGZpYQCsNoMsrbOjvufx1u+1057XMfLXlZndIL9frZDrnlZrQKkJUnZipjh95rF4cOFCbPPrQzIn7vgvWQzAMel4YL1pYHVwUvr6rHI/6KFZYH3XvOz8FYbnsFwfKrOK8sBx4betBQkLH4UB95tR/jz3+35+bPPjVAwAGTmc1TQxFVwHYlO7a3rX5pp9/W6pjy0sa808V+kFYm5I68i0iQDpZWKk2SDsF1mpGhdXHw+rko/XpgcdmTt736NThO0dgjESNOjBRa1564eFiUX1XMx51xfVRWWvUS8NkmNSFk+k4HpRH7yoPPf7Nk3d9+BGwqsEw6Glrl5ApSbmJpKN3vua3r8z27vp5ElYeAOKgcmrq0Dc+PXDP3z4grJTVecnLOtq3vXiznfYuJ2FfBaCLhGyzvPyLLC//onTnVnTseAk23/TzYzryD8b10sGgPHqwMvzkQcvNHlVhtZHvd2GVNbXgs24x9sWDtWbUVYtHXYJWfglaAIAjX/1gxCoo6sgfBsn/9iePHIHJ5FeHiW3Nwzg2VIBZNXe2fiYRXQagDwDWXf2WtnVXfe+vMfNef+okUh2bi5WRJ//y8Bd/+24dB5cBiHRUOzz+5B3x+JN3tAP4Nkge79p96zon0/kKO91xWX7Tvk12qrCdiKzYn+lhHfV4hQ0v8gobkF+/F/lN10ap9i0nWUcnVVA9MTPwYCmsTpwMioOPzJx8oADwCIDtyT0g8a7ZDOOoMQHjvJBJ2rwfc95Eu2Hq5XTDODx0JPd9CHMeSFfBrAP7yfc+GOf/E5jzlLo66e+nE7qbYdYhGzmQveS3Hm767Z1N7Ws8sxfBGAMFzPuwByb2s/EsO2DWoZudGa6EKRnpN53TzcyfT/ribOujvhbA/Xxu9VHPGWut+q5aPCqdRX1U6bb9qwpmnoIpxMwwL8IkgOkzSaPEAn0dSNYuue133phdt+f/NEoahpXx/zzx7b/6i9LAgzMJzYGEfnuyLXQ+bdRIgZ3utHouf+3WTM+uXXame6fl5XZJO7WThGw7XXvC6lRNOulhVuGIiuojKqyOxPXSSFgeG6kXBycqI/unquOHqsus1XIuOF9oNOgcaDDi2YAu5Nozq4lVjEddcX1UO9Pzwag69iBMrqXKmTx3mq5vA7A1139VYevLfvFX7XT7KwBAx+Hg1OFvvu/Et/7iERjpP8DMz3jhiMjGXFXvLM406pJE586X9uQ37N3q5NZtslNtm6WT2SwsdxNJu795HfZ0YNYhazXFKp5iHU3pOJzWcTCpIr+o46Cso1o5DirluF6qhJWJcjBzqlIdP1SJ/eKalljc/spfvznbu+f7VOSfUkFlKKpNDdVnhobKpx4Zmhl46EyFogDDrKcrI/Ks4WJi1AKWXx91d3LuNTAufb/LzB9torWi+qhtm2/4/pkT332SmQ+soL3rAKxff/07dvZc8cY/FtLuA4CwOvn5o1/5/Q9Wxw5UYbSEk8tdYiFT9zWTbB6MxF2WyuTm1zkdO1+2IdW+qc/OdPZZbnadsNPrpO2uI+msI2F1J2rfWYNZ11nrKljXmXUdrP3kf521qjMr8z/ZwCpirSOTzUFH5nscs1YR6zjSKo5YRxGrKNYqjlmFEetYmU1ps8Vaq1iBFfde9b0/6WS6bluqfToOB1TkH1VB+WhYmThSmzx6bOrQN074U8cb688neI2qDl40jLoWOFtGJaKNAHq23/qrN7Vtvv4DKqylpJ2ulYcf/71Dn/u1L8AYcU4sJkXPQPcZVa0TV0IP8+u/NILCF4uWySKZSzdD2GmR33B1Id25tcPJ9XbYqbYO6WbbpZ3qFJbXLiynk4RVICGzcejn7FQ+e7paMmfCwsrna0UDAGYGHvzw4S+87+8B7Gfm01UrWBKLPZsVXn9RWH0vCCSq5RaQ7Njzpj/+kVTn1ncTEflTJ0dLgw/97MjDnzoOk3rlKJ9dwdwtAB5p3pFEq9QwP/P/wjY10qpYMIac45iXIwlCRzVRPHb3ZPHY3ccwl5GhEfi9cNsN4KCT7XHSXdtzblt/zsl05iyvLSdszxOWmxLS9kjaHgk7JaTVSMGSAkmPhPBq44c7Mr27Q6PekwUim0jYANkgsgCyk2mMDXOOvVB1D8tjq8KoKqpfCqPZnBWTJtiCBc9mLdBi1DMgkWzbhJ1uv/T7/+L/ubme7wWAOKg8MPLIv703MRhNwrwQZ5X3qNmCuIJrGHPlIQDg22fz2wuwcL15QVJds3uFn097zE63I9d/lWWnC5bltUlheUI6KRmUhiVrZUW1qYywHEHCEiQsSdKSRJJIWBIkBAkpSUhJRASSkkhIk1rVBYH//VwDFc7m2TwbaDHqaZC8qNutVKFjz/f+2fucTMerAGPVPfCfv/T7UW0yhjEYnbVV8HxGMhg8/+ZG5yFajLoEEibd5ubXde16wx/+rp1quxkA6sXBf3jy0+/+UFKi4ei5zF9aaGG5OPu8jhcxGnNSN7+uZ/cb//iPG0xamzj6F0/+209/CKwUjDPAllX6vb0tGqtLY7XorFZbzhUtibo4NlmpQs+uN/zh71le7gYAqIwe+P2n/+uXPgPjoneImWtEdHyVfm816LRoPDt0VoPGOWPNJSoRFYjo34ioTESniOhnTnMuE1GViCrJ1pzm82Yi0k3HKkT0K2fRnnXCTvdc+n1/9j471XYTMI9JFRImBYDVUntXg06LxrND53yZ2pwPEnUl8agAcO1p1j3HeBllF5cCEXWA5PrL3vyXv2ynjeGoNnH0LxIm1Whi0hZaeC6xphI18fV9M4BfY+YyMz8M4B8B/NgatCULYMul3/dnP+lku98EAP70wN8/9dmf/ycYJj280A2NiHpW6bfPmU6LxvndlnPFWqu+l8B4R+1v2vcITDjbUvg6EY0S0eeS6JVmdBLRCBGdIKKPJOFnZ0TiYL9952s/8NpUx+bbASAoj35m/2fe82EYaV8E0EFEu4nociK6ioiuBPACItpDRJcQ0WYi6iOiDiLKJv68y0XnCs5t0Xhu6axWW84Ja636riQeFQBuBnAPjEvdewF8mYj2JK5ZK66PmkAC2LH5pv/1glz/lb8BAHG9dNdT//5zHwarTUn7lnJKH4IpMYHF2pyEmYUwoV7N/8OEZsTMqhE5dC5o0Ti/23KuWGtGXUk8Kpj5zuRjCODXyGR9eCGAL/JZ1EctDz36MwBmrHS7JZ3My6tjT1te2/qDT/3HL/y1Cmt9MANCc2IxD03xngm2wPhyNvx7szADxUEYLxwXJtJnCHO+uO3JfR5PmDmCibk8AOMyqGBiQ22Y/E8Nj6erYUL3GgmytyTtG4RxTEjB1M15oNFlAHbBeE41qma3wcSJPoo5Z4YrYaybjcD7RkrTZlvAdTC5iRvueOuTe2vEhAok8Z5N7d0GMzg1Sn+kYLSl+5vo7k7a13AaaUvuq9lLau9ZtK8fpprfEeCc4lH3AJhsxaMuMx51ieuPAfgZbiry1HTsjPVRs+sue7cKa7Xdb/zD9wvLWa/jYOLQ53/zfZWRJ4swg8WKasu0cN6hBhOjvGaZ7i8Kp3xmrhLRZwC8n4ga8ajvBPCWhecm81EXwGMwo9T/hRmh70mOL6yP+ucAvny6OEQSVtumF/30zwvLWa9V7J+866//KGHSZ2bGJ4lU+0Yv07un4LX1FYSd9qpjT+9s27RvWEjHJWmbUoemTgwzszY1T1gzszYhXWGg4zDScT3QUT1SYS2Ig3JYGXlqq+VmH49qU2FYnYyWEV+5GC7DfEl/0dHIrrs023PFG/ZGtaliafCRwZmTD5wpFvU6AB4RPXmWwRIgouubJexaYa1VX2CZ9VFhyj58GCZ3kg+j3r2yaZ1rxfVRu/a88p2Wl93JrPXoo//+55MHvzpgpzvEuqvf2pnpueRGy8tvlHZqg7CcfpJ2TyODQwOdO296Rl7fs4GO6/PoMOsQzCHAITOHYI6YdWgKRemQmaPkfwjWEbMOVVDVwnJew6zNMa1C1ipiTv7rOGQVh6zjiLVSSeVvNbdFcVyfkUK6u1lHSqtYsQqVVpHScRjrOFA68mMd15UKfaWimlJBNY6DsmIVNqtl51wzZikaW1/+3vc6mY5XA0DPZbeBtapqFZ5UYe1YXC8dDctjxyujTx2dOPDlUyooq4SOgLEfnLbQ62nwxFlet6p4XsejXvL6D8L2cohqM9+OapPKznRtS3fv3CAtZznWcEZTlkA2aT41wBIMCfBsqBnAVhLm5QDkLDcjw4WExIF/tgocGvNUhk6+Nxz8E41j9tz513HjvFkanJDX0vZ2rKRNxePf/cUjX/6dOwEcXCv196JQfc8HhNUipJN6sVfYCC+pPcPMWsfBEYCfEJb3VDIXPglj8JhItuCBj9y27FEuYU5bulk31bE17eZ705bXlhGWazbppElanpCOR8LySMgUSdsV0nZIWDYJyyEhHRLSJiEdzBUjdkDCMZ+FDWr8J4dIOGaAEA4ROUi+E0HAFCS2YMo2WkmM6Fkjub8kk0TTOLTEkPRcjFSZ3t23AfhvLBJQf6Hhec2ocVADqwAkaMxOFR73pwce9yePPjF1+FvfKQ89evhMsYxEtGG5ibMSidNYmpk3ui9Fp+nlbw72bg76bv68HnPZ/haL/zzTf5C0+1Ptm8atVLtleXlpuVlLOmlbWJ4lLFcKy7FI2hYJWwppWSQsScKyQSSIBIGIqmOHurO9u6ZBQiS3QIAQAKSpQk5ERAJESZVyErNxr2af8CePtae7tpUAIhCEjuopHQce0LhutnfI0GjcB0G66R7LzW1jraYG7/unPwVw5EyJ6k6HlTzjZxPPa0YtHvn2f5WGHn0ork1+A8ZCWIbJtr/cRFhnnbJkOXSSF2y5Cdc6mPls52ENGunq+OGT50jjPKqP+o7t58KkCVbrGZ8TntdzVK9j6x/Wp459F2Zdb3KtMtW1cPFiteaoa+1CuKZws933AXiSmU+2mLSF8xnPa0YFibPKcTR7+Tmm4VxNOi0a53dbzhVrzqirFY+aHH93QqNMRJ9K1I5nE9efR3RaNJ4dOqvVlnPCmjMq5sej3gbjpfTS05x/LTNnk+3VjZ00Vx/1tZjzQ/2LZ6/ZAEx9lvOFTovGs0NnNWicMy6meNR3AvgoMz+STNp/HcBbE0frZwu3nEd0WjSeHTqr1ZZzwlpL1NWMR70c8yMunoBZg9y5Wo1toYW1wlqvo65mPOqK66OqyE8ByJzDXLZrlebBq0GnReP8bMuS79+KwMxrtsE40ocL9v0wgIeXef0JAK9KPj8K4AcXHI8AXLXIdesx53va2lrbc7GtPxdeWWuJehAAJ1KxEUm/F8uPWNCYc4F7AqbY7icBNOqjNvLvLsQQgA04TZHjFlpYReSwMGxyhVhzzyRafn3UxeJR3wVgNzMXaQX1UVto4ULDWhuTABOPyjAO5V/AgnhUInpxcl4PjLScgYlkuQFN8ajM/BUA74eJQx2GUXvf89zdRgstPHtYc4naQgstnBnng0RtoYUWzoCLllFX6Jp4ExE9QUQ1IvpuY312uTSI6AYi+hIRTSbb54hoZ9PxZbel6Zp3Ji6T7zqL+/GI6M+IaIyISkT0IBHlVkjjLUS0Pzn3IJmMjw03zQeIKCCifz3DPSzVr8uicbp+XUk7mujN69OzuJ+l+nUlNBbt1zNiLZdnnuWln48D+CyMxe1qmJw5L13kvE6YtdsfgjFW/TKAwzBrzMul8WoAb4VJdekA+CCAp1balgVtOgDgcQDvWikNGO+uT8MkeRMw1nB3BfezESbA/XUwVvUXw8TrXgrgewG8Ecb181/PcA9L9etyaSzZr8ulcbo+XSmd0/Trcu9nyX49Y/vXmqGeJSbNwOSTvbRp3x8B+OdFzv1JmFyvje8SwCiMz/CyaCxCswfGQNa5krYseCF+AsA3YSzbK7mfS2CcSArn0Cc3wtTxad73OIDvb/r+vjO8lEv168uXS+N0/bpSGgv7dJHjZ7qfRft1hTTO2K9LbRer6rsS18R5rodskjM/AeCmFdBYiJsAjLDJuLAiN0kiujm55u/P8n5eAOMI8ptENEFETxHRT62Qxr0ADhLRm4hIkAmS6AVw96J3uziW6tfl9N9SaO7XZWOJPl0plurXleCs+3WtHR6eLazENTGLuSz3zee2r4DGLIhoC0zUzs+ttC1E5AL4KwA/zMxMc8kKV3I/G2GY4bMwHlhXAvgKgN9YLg1mjonoozBr0SkYx5KfYObhheeeBkv161m51C3Sr8u9bqk+XSkW7VciOsRLVx6ch3Pp14tVoq6kVMZS506vgAYAkwgLxmHjg8z8b2fRlvcC+CqbKKLltHExGjWYNeT3M3PAzPfDzKuuXS4NInolgD8EcCvM3PBqAL9FRK9d5PeWworKlZwOS/TrcrFUn64US/Xra5ZL4Fz69WJl1FnXxKZ9e7G4a2LD9RAAQCbL3RUA7lwBDRDRegDfAPC3zPwnZ9mWlwH4kUS1moCZ0/whgF9aAY3HFmsfzMCzXBpXALibme9hZs3MTwL4PIxxZ7lYql9XlND6NP26XCzap0T0sRXSWapfV4Kz79flTuQvtA3GnfAzMKrWVTC5eF+2yHkN6+QPYC4qp2GdXC6Nfhif4t88x7Z0w/ggN7Z7YLL9d66AhgUzOPxG8vlqGCZ9yQpovASmJtB1yfddMG6ZP5nQ9AB8AMC/JZ/tFfbrcmks2a8roLFkn66Qzun6dbk0luzXM77Pa81QzyKjFmBUkwqMQ/TPNB2rAHhx0/ebYWqd+DAT/stWQgMmswQn+5q3TStty4J7+CbmlmdWcj+7AXwbQDV50X/0LGi8K3kxyzAum78Lo4G9D8+MDPnHFfbrsmicrl9X0o6l+nQlbTlDv66ExqL9eqb3ueVC2EILFwAu1jlqCy1cVGgxagstXABoMWoLLVwAaDFqCy1cAGgxagstXABoMWoLLVwAaDFqCy1cAGgxagstXABoMWoLLVwAaDFqC8sCEb2PiH71LK4jInpoQTBACytEi1FbOCOIqBPGR/XPm/b9KRFNE9E9SYRLY/+LiOizje9sfFT/ECaVawtniRajtrAcvAMmprMMAER0PUxo1kYA34LJhwQismGY8ucXXP8fAF5GRH3PWYsvMrQYtYXl4LUAmisXbAXwIDNXkv3bkv2/CODfmHmg+WJm9gE8COBVz0FbL0q0GPV5BCJ6IRH9HRH9KxF9kYg2L/PSK2Ey+DXwJIB9ZKqcvRTAk0mqlNejST1egKdgAtVbOAtcrDmTWlgcN8MEKTMRpWBSVy4H8/JHMfMTRPRhmNjMAwB+GsBHAfwCgO9L8gUXAfwsMw8ml5VhVOUWzgItifo8ARFJmNSUbyOiVzGzzyYz4HLwjPxRzPwnzHwVM78VRqqeggmI/kOYvLWfTj43kMMzk521sEy0JOrzB7cB+HIyX1wpHoPJbvCdhQeIKAtjTHoFTGqRAWYuEdH9AH6l6dQ9SEpitrBytCTq8wf9MFn0AABEtC/5/3oiejERveM0134ORm1eDO8H8EdsquqdBLCLiHphpOzR5Dc8mCyIXzrHe3jeoiVRnz/4EoBPJHltT8GUhwCAN8BIzL8+zbUfA/BeIso1lmgAgIj2wtSn/d8AwMzDRPT7MMamMZhyFIAp9/ANZj6nYr7PZ7RyJj2PQUTfC5PkbC+AKjP/82nOfR+AiJl/Z4W/QTBLMz/M87P0t7ACtBj1eQwiehGMkagLwBeZeWyNm9TCEmgxagstXABoGZNaaOECQItRW2jhAkCLUVto4QJAi1FbaOECQItRW2jhAkCLUVto4QJAi1FbaOECQItRW2jhAkCLUVto4QLA/w8CvUaWEp++5wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 200x160 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 200x160 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 200x160 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting strain vs. state variable (phi)\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots(1)\n",
    "for i in np.arange(len(ntrainval)):\n",
    "    # Plotting actual vs predicted state variable (phi)\n",
    "    ax.plot(strain_t_tv[:, i, 1], svars_tv[:, i, 1].detach().numpy(), linewidth=5, color='black', alpha=0.2,\n",
    "            markersize=0, marker='.')\n",
    "    ax.plot(strain_t_tv[:, i, 1], pred_svars[:, i, 3].cpu().detach(), linewidth=2, color=colorb, markersize=0,\n",
    "            marker='.')\n",
    "ax.set_ylabel('$\\phi$ (-)')\n",
    "ax.set_xlabel('$\\\\varepsilon_s$ (%)')\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plotting strain vs. stress (q)\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots(1)\n",
    "for i in np.arange(len(ntrainval)):\n",
    "    # Plotting actual vs predicted stress (q)\n",
    "    ax.plot(strain_t_tv[:, i, 1], stress_tv[:, i, 1].cpu().detach(), linewidth=5, color='black', alpha=0.2,\n",
    "            markersize=0, marker='.')\n",
    "    ax.plot(strain_t_tv[:, i, 1], pred_stress[:, i, 1].cpu().detach(), linewidth=2, color=colorb, markersize=0,\n",
    "            marker='.')\n",
    "ax.set_ylabel('$q$ (MPa)')\n",
    "ax.set_xlabel('$\\\\varepsilon_s$ (-)')\n",
    "ax.set_ylim(0, 30)\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plotting strain vs. stress (p)\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots(1)\n",
    "for i in np.arange(len(ntrainval)):\n",
    "    # Plotting actual vs predicted stress (p)\n",
    "    ax.plot(strain_t_tv[:, i, 0], stress_tv[:, i, 0].cpu().detach(), linewidth=5, color='black', alpha=0.2,\n",
    "            markersize=0, marker='.')\n",
    "    ax.plot(strain_t_tv[:, i, 0], pred_stress[:, i, 0].cpu().detach(), linewidth=2, color=colorb, markersize=0,\n",
    "            marker='.')\n",
    "ax.set_ylabel('$p$ (MPa)')\n",
    "ax.set_xlabel('$\\\\varepsilon_v$ (-)')\n",
    "ax.set_ylim(0, 20)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04950300",
   "metadata": {},
   "source": [
    "#### 4.4 Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99f44fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE stress :  tensor(0.0018, dtype=torch.float64, grad_fn=<MeanBackward0>) \n",
      "MAE z :  tensor(0.0941, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Enable gradients for state variables in the test set\n",
    "svars_test.requires_grad = True\n",
    "\n",
    "# Creating a time vector for evaluation\n",
    "t = prm_dt * torch.linspace(0., data_size-1, data_size).to(device)\n",
    "\n",
    "# Initializing interpolation for the NICE network using test data\n",
    "NICE_network.init_interp(dstrain_test, t)\n",
    "\n",
    "# Solving for elastic strain using the root-finding method on the test set\n",
    "sol = root(NICE_network.find_elastic_strain,\n",
    "           args=([svars_test[0, :, :1].reshape(-1, 1), svars_test[0, :, -1:].reshape(-1, 1), stress_test[0].reshape(-1, 2)]),\n",
    "           x0=np.zeros((stress_test.shape[1], 2)),\n",
    "           tol=1e-12)\n",
    "eps_e_0 = torch.from_numpy(sol.x.reshape(-1, 2))\n",
    "\n",
    "# De-normalizing elastic strain\n",
    "ueps_e_0 = NICE_network.DeNormalize(eps_e_0, NICE_network.prm_ee)\n",
    "\n",
    "# Creating input for the NICE network using the test set\n",
    "usvars = torch.cat((ueps_e_0, svars_test[0]), -1)\n",
    "\n",
    "# Predicting with the NICE network using the test set\n",
    "Ntest = np.arange(0, dstrain_test.shape[1])\n",
    "pred_svars, pred_stress, pred_diss = NICE_network.integrate(dstrain_test, usvars, t, Ntest)\n",
    "\n",
    "# Evaluating error using L1 loss\n",
    "loss = torch.nn.L1Loss()\n",
    "MAE_stress = loss(NICE_network.Normalize(pred_stress, prm_s), NICE_network.Normalize(stress_test, prm_s))\n",
    "MAE_z = loss(NICE_network.Normalize(pred_svars[:, :, -1:], prm_z), NICE_network.Normalize(svars_test[:, :, -1:], prm_z))\n",
    "\n",
    "# Printing Mean Absolute Error (MAE) for stress and z on the test set\n",
    "print(\"MAE stress : \", MAE_stress,\n",
    "      \"\\nMAE z : \", MAE_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4fe1294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 200x160 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 200x160 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 200x160 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 200x160 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting strain vs. stress (q) on the test set\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots(1)\n",
    "for i in np.arange(len(ntest)):\n",
    "    # Plotting actual vs predicted stress (q)\n",
    "    ax.plot(strain_t_test[:, i, 1], stress_test[:, i, 1].cpu().detach(), linewidth=5, color='black', alpha=0.2,\n",
    "            markersize=0, marker='.')\n",
    "    ax.plot(strain_t_test[:, i, 1], pred_stress[:, i, 1].cpu().detach(), linewidth=2, color=colorb, markersize=0,\n",
    "            marker='.')\n",
    "ax.set_ylabel('$q$ (MPa)')\n",
    "ax.set_xlabel('$\\\\varepsilon_s$ (-)')\n",
    "ax.grid()\n",
    "ax.set_ylim(0, 30)\n",
    "plt.show()\n",
    "\n",
    "# Plotting strain vs. stress (p) on the test set\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots(1)\n",
    "for i in np.arange(len(ntest)):\n",
    "    # Plotting actual vs predicted stress (p)\n",
    "    ax.plot(strain_t_test[:, i, 0], stress_test[:, i, 0].cpu().detach(), linewidth=5, color='black', alpha=0.2,\n",
    "            markersize=0, marker='.')\n",
    "    ax.plot(strain_t_test[:, i, 0], pred_stress[:, i, 0].cpu().detach(), linewidth=2, color=colorb, markersize=0,\n",
    "            marker='.')\n",
    "ax.set_ylabel('$p$ (MPa)')\n",
    "ax.set_xlabel('$\\\\varepsilon_v$ (-)')\n",
    "ax.set_ylim(0, 20)\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plotting strain vs. state variable (phi) on the test set\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots(1)\n",
    "for i in np.arange(len(ntest)):\n",
    "    # Plotting actual vs predicted state variable (phi)\n",
    "    ax.plot(strain_t_test[:, i, 1], svars_test[:, i, 1].detach().numpy(), linewidth=5, color='black', alpha=0.2,\n",
    "            markersize=0, marker='.')\n",
    "    ax.plot(strain_t_test[:, i, 1], pred_svars[:, i, 3].detach().numpy(), linewidth=2, color=colorb, markersize=0,\n",
    "            marker='.')\n",
    "ax.set_ylabel('$\\phi$ (-)')\n",
    "ax.set_xlabel('$\\\\varepsilon_s$ (-)')\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plotting dissipation (d) on the test set\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots(1)\n",
    "for i in np.arange(len(ntest)):\n",
    "    # Plotting predicted dissipation (d)\n",
    "    ax.plot(pred_diss[:, i].cpu().detach(), linewidth=3, alpha=0.3, color=colorb, markersize=0, marker='.')\n",
    "ax.set_ylabel('$d$')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac6c346",
   "metadata": {},
   "source": [
    "### 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5042e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data for benchmark2 inference\n",
    "file = './dataset/benchmark2_inference_UndrainedTriaxial_phi08'\n",
    "with open(file, 'rb') as f_obj:\n",
    "    data = pickle.load(f_obj)\n",
    "\n",
    "# Extracting data from the loaded file\n",
    "[stress_t, strain_t, svars_e_t, epl, r_t, rs_t, stress_tdt, strain_tdt, svars_e_tdt, epl_tdt, r_tdt, rs_tdt, dt, n_reset] = data\n",
    "\n",
    "# Setting batch_time and data_size based on the loaded data\n",
    "batch_time = n_reset\n",
    "data_size = n_reset\n",
    "\n",
    "# Calculating the strain protocol\n",
    "dstrain = (strain_tdt - strain_t) / prm_dt\n",
    "\n",
    "# Reshaping the data for further processing and analysis\n",
    "strain_t = np.reshape(strain_t, (batch_time, -1, dim), order='F')\n",
    "dstrain = np.reshape(dstrain, (batch_time, -1, dim), order='F')\n",
    "r_t = np.reshape(r_t, (batch_time, -1, 1), order='F')\n",
    "rs_t = np.reshape(rs_t, (batch_time, -1, 1), order='F')\n",
    "el_strain_t = np.reshape(svars_e_t, (batch_time, -1, dim), order='F')\n",
    "stress_t = np.reshape(stress_t, (batch_time, -1, dim), order='F')\n",
    "\n",
    "# Setting data_size and number of initial conditions based on reshaped data\n",
    "data_size = strain_t.shape[0]\n",
    "number_IC = strain_t.shape[1]\n",
    "\n",
    "# Converting reshaped data to PyTorch tensors and moving to the specified device\n",
    "svars = torch.cat((torch.from_numpy(np.float64(r_t)), torch.from_numpy(np.float64(rs_t))), -1).to(device)\n",
    "stress = torch.from_numpy(np.float64(stress_t)).to(device)\n",
    "dstrain = torch.from_numpy(np.float64(dstrain)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd993c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the NICE network for inference mode\n",
    "NICE_network.inference = True\n",
    "\n",
    "# Creating a time vector for the inference\n",
    "t = torch.arange(0, prm_dt * data_size, prm_dt)\n",
    "\n",
    "# Extracting initial conditions from the first time step of the loaded data\n",
    "initial_conditions = torch.cat((svars[0, :, :1].reshape(-1, 1), svars[0, :, -1:].reshape(-1, 1),\n",
    "                                stress[0].reshape(-1, 2)), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03688d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an array of indices for initial conditions\n",
    "idx = np.arange(0, number_IC)\n",
    "\n",
    "# Enabling gradients for initial conditions\n",
    "initial_conditions.requires_grad = True\n",
    "\n",
    "# Initializing interpolation for the NICE network using the strain data and time vector\n",
    "NICE_network.init_interp(dstrain, t)\n",
    "\n",
    "# Solving for elastic strain using the root-finding method on the initial conditions\n",
    "sol = root(NICE_network.find_elastic_strain,\n",
    "           args=([initial_conditions[:, :1], initial_conditions[:, 1:2], initial_conditions[:, 2:]]),\n",
    "           x0=np.zeros((number_IC, dim)),\n",
    "           tol=1e-12)\n",
    "eps_e_0 = torch.from_numpy(sol.x.reshape(-1, 2))\n",
    "\n",
    "# De-normalizing elastic strain\n",
    "ueps_e_0 = NICE_network.DeNormalize(eps_e_0, NICE_network.prm_ee)\n",
    "\n",
    "# Creating input for the NICE network using the solved elastic strain and initial state variables\n",
    "usvars = torch.cat((ueps_e_0, svars[0]), -1)\n",
    "\n",
    "# Predicting with the NICE network using the strain data and initial conditions\n",
    "pred = NICE_network.integrate(dstrain, usvars, t, idx)\n",
    "pred_svars, pred_stress, pred_diss = pred\n",
    "\n",
    "# Converting the predicted values to NumPy arrays\n",
    "pred_svars = pred_svars.cpu().detach().numpy()\n",
    "pred_stress = pred_stress.cpu().detach().numpy()\n",
    "pred_diss = pred_diss.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eff33555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating stress ratios from the loaded stress data\n",
    "stress_ratio = stress_t[:, :, 1] / stress_t[:, :, 0]\n",
    "\n",
    "# Calculating stress ratios from the predicted stress values\n",
    "pred_stress_ratio = pred_stress[:, :, 1] / pred_stress[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdd8ac21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 250x150 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 250x150 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 250x150 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 250x150 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 250x150 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 250x150 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nid = 1\n",
    "\n",
    "# Loop over different figures for visualization\n",
    "for idd in range(6):\n",
    "    # Create a new figure with specified properties\n",
    "    fig = plt.figure(dpi=100, figsize=(2.5, 1.5), tight_layout=True)\n",
    "    axes = fig.subplots(1)\n",
    "\n",
    "    # Plotting reference stress ratio with markers and lines\n",
    "    axes.plot(strain_t[:, idd:idd + nid, 1], stress_ratio[:, idd:idd + nid], marker='o', markerfacecolor='white',\n",
    "              linestyle='-', color='black', alpha=0.2, linewidth=5, markersize=0, label='ref')\n",
    "\n",
    "    # Plotting a subset of reference stress ratio with smaller markers\n",
    "    axes.plot(strain_t[::2, idd:idd + nid, 1], stress_ratio[::2, idd:idd + nid], marker='o', markerfacecolor='white',\n",
    "              markeredgewidth=0.0, linestyle='-', color='black', alpha=0.5, linewidth=0, markersize=2.5)\n",
    "\n",
    "    # Plotting predicted stress ratio with specified properties\n",
    "    axes.plot(strain_t[:, idd:idd + nid, 1], pred_stress_ratio[:, idd:idd + nid], alpha=1, linewidth=2, color=colorb,\n",
    "              markersize=0, markeredgewidth=0.0, marker='.')\n",
    "\n",
    "    # Set labels and customize x-axis tick visibility\n",
    "    axes.set_ylabel('$q/p$ (-)')\n",
    "    axes.set_xlabel('$\\\\varepsilon_s$ (%)')\n",
    "    every_nth = 2\n",
    "    for n, label in enumerate(axes.xaxis.get_ticklabels()):\n",
    "        if n % every_nth == 0:\n",
    "            label.set_visible(False)\n",
    "\n",
    "    # Customize the plot appearance and limit the y-axis\n",
    "    axes.grid()\n",
    "    axes.set_ylim(0, 2.5)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d54e7ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 250x150 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 250x150 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 250x150 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 250x150 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAACICAYAAAALdGxmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdl0lEQVR4nO2deXxc1ZXnv6dUkkpLabfkfcdgxywxZutmHTB0mjCJYUgykISkm0A6EwdCgGQIhHRoBkKSJgybgwkGPtA9NCR0gjsJNhBMbDYvMcY2eMW7LdlarbWkqjN/3FfWU7lKKklV0iv5fj+f95F0695b5z3Vr+567hFVxWKxZA6+4TbAYrH0DytaiyXDsKK1WDIMK1qLJcOworVYMgwrWoslw7CitVgyDCtaiyXDsKK1WDIMz4pWRGaIyJ9EpEVEakTkIRHJS6JcgYjcLyLbRaRVRLaKyI9FJDcmn8a5DqbvjiyW1OAfbgPiISIlwBvALuAqoBL4V6Ac+HIfxR8HPg/8ENgAnAncA5QB34nJ+zDwb66/Q4Oz3GJJP54ULXAjUAqcpqqHAUSkC3heRO5V1Y/iFRIRP3A18ICqPuwk/1lEJgFf5FjR7lbVd9NyBxZLmvBq9/jvgdeignX4DdDhvJYIwXwRNcakNzivWSwZj1dFOxPo0Zqqagew3XktLqraCSwGFojIWSJSKCIXAd8AHolT5Aci0ikiDSLygohMTN0tWCzpwavd41JM6xhLPWZs2hv/BCwE3N3eh1X1JzH5ngWWANXAbOAuYIWInKqq9QMx2mIZCrwqWoB4jr6SIN3N/cBngRuAzcDpwD+LSL2q3n20ctXrXGXeEpEVwFpMq/xAvIpFpBy4DNgJtCd3GxZLUgSAycCrqlrbW0avirYe09rGUkJMt9mNiMwGbgU+p6q/d5LfEpEI8HMReVRVa+KVVdX1IhIVeSIuA55Pwn6LZaBcS88VjWPwqmg/Imbs6qyzTgOe6qXcLOfnupj0dZh7nQTEFW30bfqwa6fz81rg4z7yDiVPA18bZhvi8TTWrmQ5CdMg7Owro1dF+wfgLhEpd3UV5gO5zmuJ2OX8PB3Y7Uqf6/zcmaigiJwGzKD3L4Vol/hjVV3bS74hRUQOesmeKNau5BE52l70Oezyqmh/BSwAfici99C9ueJ59xqtiPwauE5Vo/exGngfWCgiVZgx7RnAj4AXVPWQU+5WYCqwHNPyzsZsxtgDPJn+20s5Xh1fW7vSgCdFq6oNIvLfMDuWfgu0Av8OfD8ma5ZzRcuFReQKzA6o7wOjMUJ8GLjXVW4zZqfVl4AgcAj4L+BOVW1Iwy1ZLCnDk6IFUNUtmImf3vJ8jZixiTPRdGMf5V4BXhmchelBRHxATi9XNj3X1yNAUESmAy2YL7gWVe0aSrstQ4dnRTsSETNwyXauRKI89n8iWQTHzA4WTZgztnjUpHFVlaPKqg/XNRGJhMePygu/9eJPP2pvaShGw8Wu92oFjkQvVY2k/QaPZdkwvGcyeNWupLCiHRh+ZzZbnMtHd1fdH/O7W6Txn7dkUTTulKKq6WdPmDB15vRDRyjyZeePDuQHJ4YivlHiyx4jvqyCaPYmIG+02bxVC0w4/2aC4075qp/OXe3NdaubD+1YW7NxybrmAxvygSpARaSFbhE369AceJ1weW6Y8apdSSH2sPLkEZE5wBqMp1FSSz6B0omBwqqZJYGS8SW5RaMr/Xkllbl5RaPzC4PjO7qkhKycSp/PX4X4cvuurR+oaiTcsS3c3rh6VO6RTZtXPL9i79bVR5xXI0Az3SJuHSIRWxLg+myd3tfMtmdFKyIzgP8LnIcZq/078ANVbeujXAFmS+LVwBhgH2b96z5n/3I0XzbwE8yYuBh4D7hJVdf3UvccYE3F5Lm3l08/t37cxCnjq+tDfl92XmmwqKiqpUPzxZddkp2TWx6OSBBfVqmILzDwpwA+IVKUn9USDPiaSgr9zUV5vvrCQFakuTXUEVbN6ghnlR1u7Aw2tIbLjrRFgglth0iks/XDfF/r+kO71/9l+4rF67vaGqLj3jA9u9K9PmNL6sl40Tr+tBsw667uJZ8/qWqv/rQi8izx/WkXqup3XPkeAb4KfA+zfns7cBpwsqrGdYaPPtiZVz5IfsX0Ad+fm+ws6SoI+JqDeVktRXm+hooif2t+dqS6siir6cQJRZECf8cBf5ZQWFhY3NTUVHf99de/MmfOnOCyZcsaAObNm1eydu3aI08++eQVbZG8cau2HqnYUdM5cVdNqKr2SFe5Jtgwohppzdb2D1saDrxd+8n77xxY+8IONBx9uZOeIu6IV4cldYwE0X4fs7Y6yeVPew2mxZzVhz/tEYw/7d2u9MeAq1S1yvl7HOYL4Tuq+piTFgQ+AZ5U1R8kqD8p0QpoIMcXys/1tef6pTWY7+8sDGS15+fQVB70txbn++rHluV0FGaH9hfl+zqLgsGEgoz9vbq6ugvTvY3iA6iqqvK7BVxUVFS2+2Bj25odHZN21kZO2HGwY1xTW6Qkoc0arutsb3yvrX7fe7Vb3ni/dstr7p1jnZjudItztQ7TxNaIZSSIdjnQoKqfc6XlYvxkf6iqv0hQLhuz5PF9Vf1XV/r/Aa5X1Urn769jdj6Vq2qdK99iYK6qnpyg/jnAmouu+8WOidNm15cX50SyCNcF83xtYyvyfdmEasqDWW3jK4sC7W0t9YAUFhYWNzc3N0R/702cLkGGerk63YJxlojmYf7hBUBhVVVVYazg165de+RH9y+8dkuNfGrTntYxe2u7JrSFIgmP74l0hXZFOhpWlee2bNq+Zsk7uz5Yesj1sgJtuEQMtMeOi0XkQlV9M9F7pBJnZj6X7om/2KUxBbowX0BnA8u89MUzEkRbAzwV2+KJyEbgHVW9vpeyT2A+xF8CNmJ2RP0HLvc8EXkA+Kqqjo4pextmE0Yg3j80+mDz8/OvCwaDm+O1hL39fumllxa//vrrNfv372/DfHiOEeVA1ldFZLqqbnP9nYPZNBIEijAf4B6t8ROLFl2xvyln2qptzaN3VIcm760NjekKJ15N0HBob450fNRct3917a61fz247qVPNBxyf3giGCG3YUTcCoxV1a39vZ8k7tcH5GO+pAowHjIBkj/oYDywF/PM252rLfpTtXucMFSMBNF2Anep6v0x6SuAGlW9speyWRh/WrewH44Zzy4CzlPVk2LKXg8sAopVtSlO3dEHezXmC0FdV9h1dbl+74xew7XhQUQCdIs4CPhju9O5eYUVq7cdKf1ob2jK1oMdY6sbOqtUexGBRo6EQy0fhtvr14/Ob93x0du/Wb1387uxz0wxp430EAVxWuVebBeMIAtcV9wegj+vxF88YW5Z2ZipZZOnzhi3t7qxU8OhjoljS3N2bNta3XH445rzT5/qf23Z0gY4pofjpjPWXtIs5pEi2jtV9acx6SuBg6p6VS9lf4ZZkvkRLn9a4MHoONcR7bmqGutJ9A3gCaBIVY8Qg+vBfkBPhwSAD4FFqrrTlX888ClVfTWmnnOBw6r6sSutwrF1uaq2u9LnAl2qus6VVgj8LfCee9ul45pY6D73yhnnXwx8EJ1gE5F84GRgOrCpqqoqJyrgyy+/fMGZZ54ZKSofW7NmR8fEPfVM+3jr7tH79u4cFRx/eo+TTpoPbsQfKCJQMgEADYd203pw98HNyw/nF1W+ftoE3feXpS/td0QxE/MltsUp3o7pvs7GHFhwCCNwn/McyjH/vwK618HPALZKVk7dqJmXjS2fPGdGji9ydmNj48TSaeeX+bJyJmHul6a968gtHkNusOqovaHmw4Qad3eNnT63obTAVzthVKC5OBDe88Qv7vxgQoX/wO5Ptq5zCbgE43mzlu4D/zoxnmatTnq7Y3NOP/8f9wBznLJRqjCTphkr2gF1j52H9CE9/WkRkZuAnwPjVLVmsN1jkniwmYLTkhXgtMJVVVUl8+bNK40dDz/y+KLP76rzz/hwV0vFntqu8fvrQlXtndrnclYwIHVNjfWrC/wdOw/t276heuvbW+p3rDhMMo2WZDFlzmfHTDvlgpOrm/1VZAen+HMLpvv8udNSva4tQiQYkLrGhob1+f7OfXU1ezZ11G7bfuZUX/Mby/5YF6c1dhPtUcS7QsmMnUdCSzvQiagvAC9gZp13u9IvAN4EzlTVVc5E1K+BioFMROEx0YrI6ETLVAOoywcU0t2VzgcktjtdGAyWrd9eLxv3hsZXNzFpX12oor45XB6OdDtwdLbWk50f5ywDjRyJdLZti3S2bi8OdB2o2bdji4Y7Q2MnTJ5wuMU3ypcTnJQTKJgWxj9exNfnWdcAPkFLCrLqSwv9RwLZ0lwYyOoMh7tawhHNysryFzQ0d0pzeyTY1klJbe2hQn9eWVbftUKWj3BhwFfbUF+3Lt/fua+2etemjkMfbz/7hNy215e9Wg+9drOjdOEIGNgXbwltSETrHL1yIXAWxpsmD7OrbjPwF1VdPaCKObrkcxdGfLVO2pcwGyx6W/I5C9PVulJVX3alfw/T0laq6iHXks+3VXWhk6cQs17b55IP3hPtZbFd8BTW7cP8bwswYi6oqqrKjzc7/djCRZ87cCR72l+3NxXsq+sa/fH6d6f4Rp0ejGhqDxAsys9qGlXkr68IZtVMqsxtqiyM7Jxamd1QXhoMJrN09tJv/3PmuNOu3DfppLkn17fnVE2dMuncpjYd3dASLknWVr+PrmCe73BJga9+bFmgrSAnvO8Pf1y6ZFp5qH7l0hd2o5FEYt4w5KIVkQuBmzBHmWZjxnaHMf37UmAi5h+8E9OaPRxvUqeP9yjBbIzYSc/NFa+6N1fE+tM6k1BvY87auZue/rRLVPVLrrKPAF/BbK7YhTmm5nRgdl+bK/CeaP1DOcnlLK0VYGamRwEUTzqrZNxp//2cklET57SFc6b7svNP1kgY8SXVoPUbv4/OrlDL2tFFunPv9g+Xd1Wv2fa1qy+pWLx48a5Nmza5RdGIWbvPonspKB+zD1zANav+wabW23784JcPt/inbNvfkn+oKVzR2KZVjS3h4kSbVOKRnUVHMM9XP6oopzk/J1L9zntrXt34uzuihzesjTcJlzbRishSTMv6MvAisFJj/E+dMdIs4HLMAeGTMOPH3k6ciPdeMzB+sOfi8qd1b7ETkacxohVXWiVG6JfS7U/7G+BeVW125csh/jbGD3qxyZOiHS5EZBQwcfKFN88tn3HxwuG2J4pGug5PCL1907uvPru9urq6KV7PzLWuG8D0JAJAnrPG3XNMv2F7+3fvuO8rta3+ydv2t+YfPhIe1dimlU2t4aJk7BHtOrB60fwrMCsIcbfJplO0PwQeUdXYw8B7K3M+ZjZ2SdJv5FGsaHsiIpOB8llXP3pDXunEG/pTNpDjC48rD7SUFOagGmnzZ4nmB7Jz6po6wgfqOgoONYbyB2NbedBf++OrihfNnz//5erq6lX9XGLKwSVkEoj5jRVrWvLHnzVu0ozTTqxr9VfMPHH6ua0hKmuPdAVbQ3pU0OGOI++ue+aab2O8qzYneN+kP1v9cs1T1Xv7znVMmbf6W8aSMbQD7F/13Iuf/uz3ZjaFcs8DmDAqr/2EcYXNsyYGm08cX9BcUZzbWVSQrdlZEgYzTlbVcH19/bZIJBLx+XyiqqiqOr8LEHHyAURCoVD7f7yyfO3fXnDpjdsOtJZu2NlUuGVvc2F9c2dOPMMKcn1tRUVFZfPnz897/PHHk26ZHHFHZ36PIiLy3HPPHRXzc889dxDIZedHjdtWsA1gf1XV8/PmzSt5Z9myhqy8suxPX/iFk3YcoritpSnaw0tJrChPzh57Fa+2tCJytg5DTCJniDET8M+aNSv3tddeuzsQCJR3dHTU7dmz58M1a9bMmjNnzkZAS0tLR9fX11e7f1fVcGFhYUlzc3MjoL39Hp1geuqppz5fXl4+pqGh4WAkEpGysrKq6O/+QNH419ZW+0OdEd9nzqioDbU1H16wYMFDq1evPhhjd0qfl3SfNhLAdLmjV/Rgg+jk1kFV3ZegjqFd8nGMfg24MR3b1ryCh0U7W1U3DNN7B4AJQNGsWbNyv/71r09avHjxrtra2vCECRNO3bNnzweQ3DbPvn6vrq7uik4a9VVm/vz5gVWrVm2OFaxj85A+L2fiLgezSSaux9RwiDYLs1tkbqo+zDIAf1pnjPVJgpdDqprryhvvxqtjN1zE1O9J0XoB54vbj9mDPJzdN8H0cod8//BgSNuYdqiQgcenPQCcE1sd8Efgz3Hy2/i0KcLZ9WOf3xDgSdEywPi0Ttejx1jFWVcuJn6oBRuf1pJxpGSnitMVuQizmSEVDDQ+bTyuwZyF5skjU1OB0zPxHNau9JCy7WWqulxVW1JU3YDi08biTABcBbzs9pxxMVLi05413AYkwNqVBvolWhH5trNxvz9lThWRS/pn1qDi07r5jJM/Xtf4WUws24uBO4DzMfFp40Xr8zorh9uABFi70oGzqJ3UhYmTcxAzKXQOkJ0g31jgHzHLQEcw5zP15306MVsWY9NXAr/pRz0vOPZmJZH3FIw3xu295JmDmRldB/w+5roXmByTfzxwWZx6zgVOikmrwERUCMSkz8WM7d1phU7ekpj02cDZMWl+J+/omPTpwIVxbLvE3kfa7+M+4NWYz897zmdrTl+f1YE4DFyJcRg4DzNbuIVuB+YSYApmtrcW05r9TFWr+/keAz5uxpW3EBPl/UlVvSnJ992I8cL4YoLX7ZKPJS2kdclHVX8L/FZEpmC+zU7HnC8cwCzRLMW0iG+qamd/63cYaHxaN/Mx3hy9BuiNIWlPDotluBjwko+qfoI5T2lR6sw5ykDj07q5Btiuqu8lk1mSi0/rSUTkNHUdR+MVrF3pIaXOySnkV5iJqN+JyGUi8hXMRohj4tM667c9cFzGLsHsojoGEblVRB4TkS+KyEUisgD4E5kbn9ar6+3WrjTgSeN1gPFpXXwBc2+JusYjKj6tDuKUkHRi7UoP1sunH9iJKEu66M9ny6vdY4vFkoCUiFZErhWRl0TkaRH5m1TUaUkexz3Oc1i70sOgRetM4pyKOcTtXeAxMecMW4aOC4bbgARYu9JAv0UrIjeJyLdE5AzHjzZbVW9X1T+qOY70DGCyiMQ9O7gf7zNDRP4kIi0iUiMiD4lIr2fgishkEdEEV+zxIdkicp+IHBCRVhH5s4icMhibh5E1w21AAqxdaWAgmyseco4ZOQP4FnCO09quAlapaqeYc4bvxpz232+GyJ/2QY6NT/u6iCSMT+tVtKc3lGewdqWHAS35qGoIs+tppdP6PYYjYsezJgQEnYO5BhJOMK3+tGIOK/8mJj7tIiftXcypFzcDcQ8rt1i8QComorKAqar6rqo+rCYu7K8we5H/l4jcLCLf7Ged6fanvdSx+/9FE9QE3HoFc16zxeJZUiHah4BHReQzrrQiTLSwh1X1lxgR94d0+9POxJwHVRdTZBNwonPeUcYgIif1nWvosXalh0HviFLVVmeb4TMi8iRmK2ABRizRPP3dwZFuf9re6s/GuFr1K5TJMFMx3AYkwNqVBlKyjdFxvfs7x/OnHFing48tE0/okiA9Eddi3PNe70f9iV5z85SIeCY+raqucNKTik/rpE8HxqvqmzG2XQJsS9F9rOjPfbjSvXYfqY4XnCg+bXL05XA7HBdQA9wfJ30jxj82mToKMUevPhTntQcwB0fHpt+GmUTzJagz6gTfp6OyvezVn6s/ny2vjt1686eNO3Mch978aT8CKkUktqs9C9isA5vxtliGBK+K9g/AxWJi4EZJpT/tUsyh2l+IJjhdnCsw3j4Wi2fxqmjT6k+rJp7KQuCnInK9iMzDLCkB/DKVNzIUiMhlw21DPKxd6eF49acFuAVoBv6F7vi0F2uG7YZy2DjcBiTA2pUGrD9tP7D+tJZ0Yf1pLZYRjBWtxZJhWNGOAMSE+PQc1q704FnRDsSf1lW2zDlt8YCItIvIFhG5MSZPPJ/bTJyEAvjGcBuQAGtXGvDk7PEg/Gmj663LgTZMJIQa4ATMnuJYRkp82kEdOJBGrF1pwJOiZYD+tA53AHnAmdodNf7NBHltfFpLxuHV7vFg/Gn/Afi1S7AWy4jCq6IdkD+t42VUBdSLyBIR6RCRWhF5NMF4eKTEp7UcR3i1ezxQf9rRzs+fAS9iWuVZmNCCOfScgHgWWIJx3ZsN3IWJT3uqqtYnqD969OZJIp6K1TXRWZz3Gtau5Ik65vd9vOtwuyQlcFMaUHxajE+jAu/HpN8ChImJbRqTJ5n4tNc49dvLXum6rulLH15taesxrW0sJfTumhc9PuaNmPQ3MEOBmZgg08egqutFZDPG8TkRr2Ic63cC7b3ks1j6SwCYjPmM9YpXRTvQ+LTbib9sE+3L9uUn22ufV03Yzf7Eu7VY+sPbyWTy6kTUgPxp1RztugxzLImbizFd302Jykp3fNpVAzPZYhkaPOnl42yu2IDpht5D9+aKV1X1y658vwauU1W/K+1MYAXmeNTnMBNR9wJPqOp3nTy3AlMxmzBqMBNRP8RsyPi0ZmC4S8vxgye7x4Pxp1XV90XkcsyM8StArVPPXa5sIyo+reX4wpMtrcViSYxXx7QWBu40ISJvJnCIyOhDuvtCRKaLyEIRWSciXSKyIclyGfW8PNk9tgzOacJhJXBrTNrO1FnoST6FCevyHqZB6k+jlDHPy4rWuwzGaQKg4Th0hnhFVX8HICJPA3P7UTZjnpftHnuXVAYhOy44Xs6rtqL1LoMNQnaBMxZuF5HlInJ+OowcQWTM87Ki9S6DCUK2HHMAwN8B12EiLbwmIrEBty2GjHpedkzrbeKtx/UZhExV7+5RQGQJ5qzfu7Bd62PItOdlW1rv0pvTRH1/KlLVFszmkd6cISwOXn9eVrTeJRVByHoUT4VRxxGefV5WtN4lFUHIABCRAsz6pXWGSAKvPy87pvUuvwIWYIKQuZ0mjglChstpQkTOw2wSeBmzMWMs8D3MqR5XD+kdDDEikk/3GHQSUCQi/8P5e7mqHhoJz8uK1qMMwmniAKY1vg+ze6oF46f5TVV9P912DzOVmGOG3ET/vghzKmfGPy/rMGCxZBh2TGuxZBhWtBZLhmFFa7FkGFa0FkuGYUVrsWQYVrQWS4ZhRWuxZBhWtBZLhmFFa7FkGFa0FkuGYUVrSQsiskpEvuP6e4aIrBSRJhH5LxGpjMl/gojUicj4OHXdKSLLhsLuTMCK1pJyRORKjJfNIlfyM5gjSa8GJmA8ltz8Evi5qu6NU+UjwFmOA8Vxj3UYsKQcEXkLWKuqNzt/FwDNQKXjHvdF4GFVrXRevxx4CPiUc3hdvDqfAUpU9XNDcQ9exra0lpQiIlOB84CXXMm5zs8252drNE1EcoAHgVsSCdbhReDvRWRUai3OPKxoLanmYqAT16kPqloH7AAWiEgZcIPr9VuAHar6+z7qXYnx/74w1QZnGla0lqOISK6ILHAmit51Jo4u7Gc1c4EtcVrNfwJ+gIliOAe4RUTGArcBN/dVqarWA7uBs/ppz4jDitbi5nZgqaperqpnA58BPuxnHWMwoUN7oKpLMUe4nARMVtX1wAPAYlX9WET+UUR2iUitE2gs3qkqh506jmvscTMWNxOBr4pINXBAVWOPbkmGACZ0yTGoahsmNjAi8jeYrvSJInIy8DgwD/gEeAtz4uTCmCragT6jBo50bEtrAY5OCCnmi1yB8ACrqsOczdzbe/kwZ1/9b1VtwpzftF5Vl6vqbswk1rw4RUsx3evjGtvSWqJ8A/gXRzTHICKzMGPRIuANVf04QT2bMSLsjeuBLszabZR81+8Fcd7fh+kJbO6j7hGPbWktUUpxfR5EJCAiV7hevxHYBuwHKnqpZyVQGW9nk1NvCXAPsEC7Nwm8CZwkIreJyNXA/wRejyk6CyPmvyR7QyMVu7nCAoCIlAKPAZMxY8cPgH92Zm0RkdOBnwGFwDmqGrf77HSz9wF3qOqiOK8/BARV9R9i0m8A7sQI89+A76pql+v124FvAVP0OP/QWtFa+kRErgIKVfUZEXkUuN2Jd5Mo/y+AT6tqyrYdisha4D9V9SepqjNTsaK19ImInA1MwWyakL5mlUVkNCaO7rmq+tcUvP8FmAgAU1W1YbD1ZTpWtJa04IxNG5312cHWdQWgqrpk8JZlPla0FkuGYWePLZYMw4rWYskwrGgtlgzDitZiyTCsaC2WDMOK1mLJMKxoLZYMw4rWYskwrGgtlgzDitZiyTD+P28LqqCy+1yuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 250x150 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 250x150 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop over different figures for visualization\n",
    "for idd in range(6):\n",
    "    # Create a new figure with specified properties\n",
    "    fig = plt.figure(dpi=100, figsize=(2.5, 1.5), tight_layout=True)\n",
    "    axes = fig.subplots(1)\n",
    "\n",
    "    # Plotting reference value of state variable with markers and lines\n",
    "    axes.plot(strain_t[:, idd:idd + nid, 1], rs_t[:, idd:idd + nid, -1], marker='o', markerfacecolor='white',\n",
    "              linestyle='-', color='black', alpha=0.2, linewidth=5, markersize=0, label='ref')\n",
    "\n",
    "    # Plotting a subset of reference values with smaller markers\n",
    "    axes.plot(strain_t[::2, idd:idd + nid, 1], rs_t[::2, idd:idd + nid, -1], marker='o', markerfacecolor='white',\n",
    "              markeredgewidth=0.0, linestyle='-', color='black', alpha=0.5, linewidth=0, markersize=2.5)\n",
    "\n",
    "    # Plotting predicted values of state variable with specified properties\n",
    "    axes.plot(strain_t[:, idd:idd + nid, 1], pred_svars[:, idd:idd + nid, -1], alpha=1, linewidth=2, color=colorb,\n",
    "              markersize=0, markeredgewidth=0.0, marker='.')\n",
    "\n",
    "    # Set labels and customize x-axis tick visibility\n",
    "    axes.set_ylabel('$\\phi$ (-)')\n",
    "    axes.set_xlabel('$\\\\varepsilon_s$ (%)')\n",
    "    axes.grid()\n",
    "    every_nth = 2\n",
    "    for n, label in enumerate(axes.xaxis.get_ticklabels()):\n",
    "        if n % every_nth == 0:\n",
    "            label.set_visible(False)\n",
    "\n",
    "    # Customize the plot appearance and limit the y-axis\n",
    "    axes.set_ylim(0.65,0.85)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8b43b5",
   "metadata": {},
   "source": [
    "### 6. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "160eac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for saving the model state dictionary\n",
    "PATH = './saved/[state]NICE_benchmark2_sparse'\n",
    "\n",
    "# Save the state dictionary of the NICE_network model to the specified file path\n",
    "torch.save(NICE_network.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bc4dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for saving the norm_params list\n",
    "params_PATH = './saved/[params]NICE_benchmark2_sparse'\n",
    "\n",
    "# Save the norm_params list to the specified file path using pickle\n",
    "with open(params_PATH, 'wb') as f_obj:\n",
    "    pickle.dump(norm_params, f_obj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
